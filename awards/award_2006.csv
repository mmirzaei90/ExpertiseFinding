"id"	"researcher_name"	"application_title"	"application_summary"
"338953"	"Aamodt, Tor"	"Statistical synthesis of application specific computer architectures"	"This research program investigates sophisticated techniques for the economical development of microprocessor architectures in the approaching multi-billion to trillion transistors on a chip era.   The typical approach when developing a new computer architecture is to begin with a hypothesis of which particular architecture design will be fastest or most efficient.  This hypothesis is then tested using simulations to determine the actual performance of the design.  As the number of transistors on a chip grows, this methodology becomes increasingly ineffective as the initial hypotheses grow less accurate in the face of the increasing complexity of these devices.  Such inaccuracy means a growing amount of time must be spent refining the initial design.  The lack of analytical techniques to optimize the architecture in the initial development stages thus presents an imminent economic challenge to the computing industry.  This challenge provides an opportunity for Canadian industry to take a leading role in the development of architectures for the high performance computing systems that will power tomorrow's information technology systems.  This research program investigates statistical analysis techniques to rapidly and accurately explore the immense design space designers are already being confronted with today.  These statistical analysis techniques will be used to develop statistical synthesis techniques that will provide the tools necessary to maintain the rapid pace of innovation in information technology well into the future. The emphasis of this research program is on architecture design techniques for processors customized to application domains such as computational biology, network processing, and real-time computer graphics.  However, the techniques will also be applicable to the development of architectures for general purpose computing tasks (such as database servers and traditional desktop and portable computing systems).""338927,""Aarabi, Parham"
"338944"	"AbdelRaheem, Esam"	"Design and VLSI implementations of DSP algorithms for communication systems"	"The proposed research will leverage the invistigation of innovative solutions in the design and VLSI implementations of digital signal processing (DSP) algorithms for communication systems. Solutions that would lead to high performance, low complexity and low-power consumptions will be adopted. Objectives of the project are: (1) To further develop blind equalization techniques that have shorter time-to-convergence (TTC) quality to suit high-speed communication systems and for other  modulation schemes implemented in wireless communication standards (i.e., DPSK, DQPSK, GMSK, etc.). (2) To further develop computationally-efficient blind adaptive equalization techniques that reduce both the amount of computations and hence increase the system's throughput, and/or decrease hardware complexity. (3)To develop programmable/reconfigurable blind adaptive equalizer so that their filter lengths, adaptation parameters, and error measure are variables according to the modulation utilized in the receiver for multi-standard future communication equipments (also as in software-defined radios). (4) To develop low-lardware-complexity and/or computationally-efficient adaptive algorithms for echo cancellation applications in communication systems. (5)To achieve low-power in blind adaptive equalizers and adaptive echo cancellers (AECs) to respond to the increased demand of fast-growing mobile communication services. (6)To develop VLSI implementations of blind adaptive equalizers and echo cancellers that incorporate both high-speed, low-power, and low-complexity requirements. (7) To achieve performance enhancement and robustness of 802.11a/RA for high vehicular velocity, so that higher data rates may be achieved in a 5.9GHz Dedicated Short Range Communication (DSRC) system.   (8)To confirm feasibility, consider the complexity of the receiver design, and maintain consistency with the 802.11a/RA standard.""337534,""Abdelrahman, Tarek"
"338855"	"Aboulnasr, Tyseer"	"Signal and system modelling: algorithms, implementations and applications"	"While we may not recognize it, as people our senses never receive input in a clean, organized form.  Typically, audible signals are a mixture of conversation of several people in the same room, desirable speech mixed with background traffic noise, etc.  While we do hear the full mixture, we routinely manage to separate these signals and focus on the one that we deem important.  We are also able to shift attention quickly when a new signal arrives depending on the importance we attach to such a signal (e.g. fire alarm).  Man-made systems attempt to simulate this powerful performance of human beings in various applications.  For example, measuring the heart beat of a baby in its mother's womb will necessitate separating the fetus's heart beat from the mother's own heart beat since any sensing device will only be able to measure a mixture of both.  Similarly, microphones in a multiple speaker room will invariably pick up more than the desired speaker.  Hearing aids for a hearing impaired person in a party environment or even a kindergarten class will pick up the speech of all individuals around and some mechanism will have to determine which signals are undesirable, remove them and focus on the desired one. This research will study the signal processing aspects associated with the above problem with the objective of developing faster, lower complexity algorithms that can effectively model signals and systems with the goal of applying these algorithms to improve performance of hearing aids as one possible application.""343423,""AbouRisk, Nicholas"
"338951"	"Achenbach, Sven"	"Deep X-ray lithography processes for high resolution applications"	"The 1959 quote of Richard P. Feynman, Nobel Prize laureate in physics, that ""there's plenty of room at the bottom"" today holds more than ever. Microsystems technologies have revolutionized the way we perceive and control the physical world, and the world market of $33 billion US for smallest units commercially available is anticipating an annual growth of 11% to 16% [NEXUS III market analysis]. Microsystems technologies focus on fabricating miniaturized components with feature sizes on the order of 1/100 of the diameter of a human hair. Systems such as read/write heads for hard discs, inkjet printhead nozzles or digital projection systems strongly support the economy. Other components drive technical and scientific progress by allowing for new systems and solutions in sensing and actuating, for instance by providing specialized filters for space missions or novel focusing optics for X-rays. Among the broad fabrication portfolio, X-ray lithography using synchrotron radiation provides particularly outstanding structure quality. This technology, however, hasn't been available in Canada so far. With the Canadian Light Source, the largest research endeavor in Canada in a generation, getting operational, this technology now gets accessible as SyLMAND, the Synchrotron Laboartory for Micro and Nano Devices, is being developed. A research program will ensure that SyLMAND not only brings X-ray lithography to Canada, but that cutting-edge research can be carried out, including process technology and related applications. This includes novel approaches in radio frequency microsystems which will help to improve wireless communications, as required in cell phones, satellite communications and wireless LANs.""385123,""Achenbach, Sven"
"337843"	"Aggarwal, AkshaiKumar"	"Design of G-CODE algorithms and components"	"The research aims at developing algorithms and protocols required for an optimized network- aware scheduler for the Grid Computing Development Environment (G-CODE) system. The scheduler may use sender-side algorithm and historical experience to estimate bandwidth. It would use heuristic techniques like genetic algorithm to obtain a near-optimal schedule in near-real time. In early nineties, the traffic models using self-similar processes and long range dependencies were used. However the studies conducted in 2000 have shown that traffic follows Poisson distribution. We, therefore, propose to investigate a self-induced sender-side algorithm based on Poisson distribution. The Meta scheduler is to permit advance reservations and work-flow based task scheduling. We propose to investigate the use of genetic algorithm up to the lowest level, where its use may depend upon the depth of parallelism of the application. If the resources are represented by a dynamic graph, if each node is represented by a tuple and if applications are represented by graphs with dual weights, optimization process will match the requirements of the processes of an application with the compute-powers of  the heterogeneous nodes, after considering the  communication delays, latencies, prior reservations and the trust that the client is able to build. NOVELTIES OF THE WORK: There is no reported scheduler with sender-end bandwidth algorithm and with the ability to handle workflow type of jobs, prior reservations or communication delays. The scheduler algorithms become too slow, when even a thousand processes are required to be mapped. None of them tries to continuously evaluate trust factor at every level. Moreover our research is trying to develop algorithms and protocols, which are in alignment with the moving target of open Grid Computing standards. EXPECTED SIGNIFICANCE: The new approaches mentioned here are worth exploring if we are to progress towards the vision of a global grid, with unlimited number of ubiquitous devices connected to it.""326934,""Aggarwal, Priyanka"
"338853"	"Ahmad, Omair"	"Signal processing algorithms for applications in communication systems"	"The field of digital signal processing (DSP) has experienced an explosive growth during the past couple of decades. The DSP techniques have become integral parts of the products and services that we need or encounter in our daily lives. The research efforts of the applicant during the past five years have led to some very concrete results that have been shared with international scientific community, both from academia and industry, and have given rise to new ideas and directions that need to be further investigated. The overall objective of the proposed research is to develop efficient algorithms and to establish the underlying mathematical foundations for reliable processing of speech, image and video signals, and for their efficient software and hardware implementation for multimedia communication applications. The distortion of images by additive or multiplicative noise is common during their acquisition, compression, storage, transmission and reproduction. Presence of the noise makes the automatic interpretation of image data very difficult. Voice activity detection, pitch estimation of noise-corrupted speech, and identification and modeling of noisy speech systems are vital for reliable design of speech communication systems. Emerging communication and information networks deal with high volumes of digital speech, image and video signals and with their transmission through over-congested communication channels. Fundamental to these activities are the signal compression, and pre- and post-processing of such signals. Efforts will be put to reach solutions to these problems through design and implementation of DSP algorithms that are cost effective and are superior in performance. Discrete transforms are essential to signal processing tasks. To this end, new transforms exploiting more efficiently the intrinsic properties of signals will be investigated. Fast and efficient techniques for computing these and the existing transforms will be undertaken. Research efforts will also be directed towards efficient software and VLSI implementations of the algorithms developed in this proposal.""326921,""Ahmad, Shafique"
"338932"	"Aimez, Vincent"	"Innovative photonic devices realization using heterogeneous integration and quantum well dot intermixing"	"Contrary to the microelectronics industry, the photonics industry is not based upon a single core technology as important as CMOS on silicon. The realization of photonic components requires multiple fabrication processes specific to the variety of materials involved. A direct consequence of this situation is the absence of large scale fabrication technology for the production of Photonic Integrated Circuits (PICs). One of the reasons for this situation is related to the need for materials that exhibit both low propagation losses, like silica for example, as well as high optical gain such as III-V semiconductors - to date, no one single material can provide both properties efficiently. Furthermore, epitaxial growth of active materials does not allow the efficient realization of monolithic structures incorporating multiple distinct functions. As a result, there are currently no established fabrication processes that allow large scale integration of diverse optical functions on monolithic components. Therefore, complex photonic circuits are currently fabricated from numerous discrete components and the packaging costs associated with such devices often exceed that of the discrete components themselves. This research project aims to further develop two fabrication processes for integrated photonic components: quantum dot/well intermixing and heterogeneous integration of low loss silica waveguides with III-V materials gain sections. The tools required for this work, are involved in standard commercial microelectronic processes currently used for the fabrication of microprocessors, memories, and silicon on insulator (SOI) substrates. The processes developed within this research are expected to have a significant impact on increasing the functionality and reducing the fabrication cost of integrated photonic components for both the telecommunications and ""lab on a chip"" type bio-sensors.""329241,""AinDack, Lisa"
"338922"	"Aitchison, Stewart"	"Periodic structures for nonlinear optics"	"Nonlinear optics is the term used to refer to the interaction of light and matter where the strength of the interaction depends on the optical intensity. The nonlinear effects of interest in this research program are those based on the change in refractive index induced by a high intensity laser beam. Such effects have been used to demonstrate a range of all-optical switching and signal processing devices. However, the relatively small nonlinear coefficients present in most materials result in prohibitively high optical power levels and limit the potential applications of such devices. Periodic structures have fascinated scientists and engineers for decades because of their novel optical properties. For example the colors observed in opal crystals and butterfly wings are due to the underlying periodic structures and not a consequence of pigmentation. One of the associated benefits of such periodic structures is that they lead to a high degree of confinement for light and as a result make nonlinear optical interactions more efficient. The challenge to be overcome is to engineer periodic structures which will enhance nonlinear optical interactions and which in the long term will result in practical all-optical switching and signal processing devices. The aim of this application is to combine current and developing nonlinear materials with engineered, periodic photonic structures to demonstrate novel all-optical functionalities. In addition, the use of optical gain is such periodic structures will  be investigated with a view to further reducing the optical power requirements.""335286,""AitKadi, Daoud"
"338964"	"Ajib, Wessam"	"Link layer transmission techniques for multiple input multiple output MIMO-based future wireless networks"	"It is widely accepted that Multiple Input Multiple Output (MIMO) is a technology of choice for next generation wireless networks as it provides high spectral efficiency by using multiple transmit and receive antennas. To date, MIMO research projects mainly focus on physical transmission and coding techniques while the new spatial paradigm of MIMO systems can be exploited to improve the performances at higher levels (layers). The lack of higher layers algorithms adapted to be efficiently used with MIMO coding techniques is considered a limitation on the implementation of MIMO in wireless networks. The new spatial dimension of resources introduced by MIMO creates new challenges for re-designing the link level transmission techniques for single-user and multi-user wireless systems. In this research program, theoretical performance bounds will be investigated and new link control protocols and multiple access mechanisms applicable to next generation MIMO wireless networks will be developed. More specifically, the functionalities of (i) error control, (ii) medium access control, (iii) resource allocations, and (iv) scheduling with call admission control and Quality of Service (QoS) guarantees providing will be explored. New cross-layer channel-aware QoS-based transmission algorithms that aim to optimize the frame (link level data unit) level performances will be proposed and evaluated, analytically and by simulations, for different MIMO physical coding techniques. The theoretical study of diversity-multiplexing trade-off will be also performed to consider the transmission delay in the context of ARQ retransmission system for single-user as well as for multi-user systems with the impact of multi-user diversity. This program will give better understanding of the impact of MIMO technology at the higher layers and will propose theoretical and practical link control solutions to improve the global system performances. Thus, it contributes to resolve the industrial problem of implementing MIMO technology in wireless networks. The resulting intellectual property will be transferred to Canadian industries and helping them to be at the front position for the development of commercial MIMO wireless networks products.""331230,""Ajji, Abdellah"
"337853"	"AmarBensaber, Boucif"	"Développement de nouveaux services pour les réseaux ad hoc avec une évolution réelle vers les réseaux spontanés auto configurables (P2P, réseaux de capteurs)"	"Les développeurs de systèmes et réseaux comme Cisco systems projettent de développer d'ici les 15 prochaines années un réseau d'information intelligent. Ce réseau sera basé sur plusieurs technologies dont la diffusion (Multicast) et les réseaux mobiles. Le Multicast permet l'envoie de données, en une seule fois, d'un émetteur vers plusieurs récepteurs assurant une économie des ressources et offrant des services de groupe (gestion du groupe, routage,...) aux utilisateurs partageant souvent des intérêts communs (enseignement à distance, jeux en ligne, téléconférence, diffusion de publicités). De son côté, le développement des technologies sans fil offre aux usagers une grande flexibilité de communication en cassant les barrières de la mobilité et de l'éloignement. D'ailleurs, il est prévu que d'ici les trois prochaines années, les périphériques sans fil d'accès à Internet seront plus nombreux que les périphériques câblés. Dans certains cas, ces réseaux sans fil sont complètement dépourvus d'infrastructure fixe (station centrale, borne, relais) de réseaux : ce sont les réseaux ad hoc. Ils sont utilisables lors des communications en milieu restreint, dans des opérations militaires ou lors des sauvetages d'urgence (incendies, naufrage, tremblement de terre,...). Ces réseaux exigent la facilité de déploiement, les noeuds pouvant joindre et quitter le réseau de façon totalement dynamique. En plus, les noeuds sont alimentés par des sources d'énergie autonomes. Ces réseaux sont caractérisés par une bande passante limitée et par une grande exposition aux perturbations et aux attaques à cause de la libre exposition au média de transmission. Le programme de recherche proposé vise le développement d'une architecture générale capable de supporter la création de services et d'applications sécurisés sur les réseaux sans fil mobiles. La recherche abordera les problématiques liées à l'ingénierie des services suivants : sécurité des réseaux mobiles Ad hoc et des réseaux spontanés auto-configurables (P2P, Sensor Network), distribution des clés de cryptage Multicast et détection d'intrusions dans un environnement ouvert (réseaux radio mobile), routage Multicast et qualité de service. Le Canada ne saurait donc rester indifférent à de tels enjeux de recherche à incidence hautement stratégique.""339146,""Amari, Smain"
"337830"	"Andrews, James"	"Efficient, effective methods of software verification and validation"	"As software becomes more important to our everyday lives, it becomes more important to make sure that that software is doing what we want it to do, without bugs.  The goal of software verification and validation (V&V) is to find as many bugs as we can, as fast as we can, and to collect as much information as we can that helps us to eliminate those bugs. We plan to investigate efficient, effective methods for software V&V.  The primary method we plan to investigate is randomized unit testing.  This is the practice of exercising small pieces of closely-associated program text a huge number of times in a random order, allowing V&V practitioners to perform ""torture tests"" on individual pieces of program text.  We have developed tools and techniques for randomized unit testing that have already found bugs in widely-available public-domain software.  We plan to extend this work and make it available to all V&V practitioners. We also plan to investigate ways in which researchers can experimentally measure the effectiveness of software test techniques.  Key among these is the practice of automatically generating faulty ""mutants"" of a program and seeing whether the test techniques can catch the faults.  The percentage of faults caught is a measure of the effectiveness of the test technique.  Our work concentrates on measuring exactly how closely these automatically-generated mutants mimic the behaviour of real bugs.""325954,""Andrews, Mark"
"338434"	"Androutsos, Dimitros"	"Identification, segmentation and tracking of objects in colour images and sequences"	"Identifying and segmenting of specific complex objects in general images is still an open problem and an important research area in image processing and analysis, due mostly to the importance of such a task in many applications. Moreover, tracking such objects in a video sequence is of increasingly greater significance, a fact intensified by the fast prevalence and penetration of digital video technology. Research from this proposal will focus on identifying specific image objects in images and video and track them as they move.  By improving the computational methods of such a cognitive task can advance application that are based on visual identification and extraction. Areas such as object-based compression, video surveillance systems, interactive television and rotoscoping for film editing are a few imporant areas that stand to benefit. Research will follow three stages: i) identify specific image objects in general colour images using a modified spatial colour statistical measure;   ii) incorporate this measure and vector colour information into the energy functional, the underlying mathematical mechanism of active contours (snakes), to perform object segmentation; iii) extend concepts in i) and ii) into the video domain and track objects as they move from frame to frame.  This investigation will involve the development and testing of two new experimental concepts, which will: a) consider an image objects path from frame to frame as a 3-D volumetric solid and use this model to improve active contour tracking accuracy; b) apply active contours across frames to localize and lock onto an objects trajectory path. The goal of this proposal is to develop and advance methodologies and tools for identifying specific objects in images and video to then accurately tracking them in a video scene using novel methodologies based on active contours.""343674,""Andrusiak, Tara"
"338935"	"Anpalagan, Alagan"	"Radio resource management for emerging wireless communication systems"	"Emerging wireless communication systems are expected to support applications such as multimedia, streaming video and other high data rate services. Current wireless systems are unable to support the demanding throughput requirements of these services with stringent quality of service (QoS) requirements. OFDM, as a multicarrier communication technology, has been proposed in 4th generation research to provide significantly higher rates with robust performance especially in rich scattering radio environment. It is capable of supporting high rates by employing several low rate subcarriers that are resistant to inter symbol interference. Radio resource management (RRM) refers to techniques to increase efficiency of scarce radio frequency spectrum by controlling wireless interface effectively. OFDM offers flexibility in radio resource management in terms of subcarrier allocation, power allocation and bit loading. CDMA has been combined with OFDM (one variant is OFCDM), and has been proven effective in providing multiple access while increasing the performance through frequency, multipath and macro diversity.  OFCDM offers challenging opportunities to improve the system performance using several RRM techniques. Our long term research program is to develop core expertise in the areas of RRM for mobile communication systems. We will enable the knowledge transfer to the industry through HQP  development and disseminate our innovative ideas and findings for further advanced research by others. In the current NSERC-supported program in QoS-aware RRM in CDMA networks, we developed performance enhancing techniques and associated algorithms for rate adaptation, distributed power control, asynchronous  scheduling and interference mitigation for wireless systems. In the next phase of the program, we propose to develop techniques and algorithms for multicarrier systems that employ  CDMA. In particular, the following will be investigated: adaptive subcarrier allocation, bit loading and power allocation with QoS constraints, dynamic opportunitistic scheduling and cross layer optimization, in multicarrier systems.""337428,""Ansari, Athar"
"337904"	"Antle, Alissa"	"Mixed reality for cognitive acceleration"	"Sensor-based tangibles and smart displays that augment the real world are part of emerging paradigms in human computer interaction called tangible computing and augmented or mixed reality. Sensors, whether for measurement or monitoring, represent a multi-billion dollar industry. My research explores the development of tangibles and mixed reality environments that support the underlying processes of children's cognitive development. My research program builds on my recent work* adapting methods from human computer interaction to children and investigating how cognitive and embodied processes can be used to design highly interactive environments. By embedding sensors in everyday objects, tracking the movement of these objects, and using them as controls for the smart display of images and sounds that help to explain real world phenomena, we are able to create situations where children can successfully make the transition through the stages of cognitive development. This type of early stage research, which grounds the development of new technologies in understandings of perceptual, cognitive and embodied processes, is a critical precursor to the intelligent and human centred development of sensor-based technologies - technologies which will soon be commonplace in millions of people's everyday lives. I am involved in a range of mixed reality projects through collaboration with researchers at the School of Interactive Arts and Technology (SFU), the University of British Columbia, Eindhoven Technical University, BCIT, Banff Centre for the Arts, Telus, Nokia, Nortel Networks and Science World. * Funded by a NSERC PGS B and Heritage Canada's New Media Research Network Funds.""340312,""Antle, Michael"
"338440"	"Anton, CalinDoru"	"Structure and randomness of NP-complete problems"	"NP-complete problems, such as Satisfiability(SAT), Constraint Satisfaction(CSP), Planning, Hamiltonian Cycle etc, have many practical applications, ranging from formal verification of hardware and software to finding the best route for mail delivery. The last decade witnessed impressive progress of the solvers of NP-complete problems in general, and Satisfiability in particular. This progress was the result of a permanent competition between finding new, challenging instances and designing efficient solvers able to tackle the challenges. We believe that producing new challenging instances/models is not enough for improving our ability to solve hard problems - characterizing and classifying the new instances is also necessary. This ensures that any progress made in solving some instances can be used (or adapted) to solve an entire class of instances. The first moves in this direction are to investigate the structural properties of these instances and to assess the influence of these properties on the instances difficulty. The main goal is to better understand, characterize, classify and compare instances. We intend to pursue this goal by investigating instances of two NP-complete problems: Subgraph Isomorphism and Satisfiability, their conversions, their structural properties and the relationship between these properties and instance hardness. This research is expected to make both empirical and theoretical contributions. Although this research work focuses on Subgraph Isomorphism and Satisfiability, its results may be applied or adapted to other NP-complete problems. We believe that the investigation of the structure and its influence on instances hardness has the potential of improving our ability to solve hard combinatorial problems by identifying some properties of the instances which can be used for designing more efficient solvers.""330585,""Antonelli, David"
"337888"	"Antoniol, Giuliano"	"Supporting micro-architecture and feature evolution"	"Evolution of software features characterizes the entire life span of any software system, from inception to retirement. Evolution and maintenance involve tedious and costly activities, such as program understanding tasks to identify high-level abstractions required by maintainers to perform their tasks.  Evolution is also a risky activity in that changes in any software system of realistic size may produce unwanted and unexpected side effects. The first goal of this research is to develop static and dynamic approaches to identify and map user observable features into microarchitectures implementing the given functionality in absence of reliable documentation. A second goal is to study micro-architecture evolution with the aim being to identify co-changing microarchitecture components.  Co-changing components are components that underwent changes almost in the same points in time during the past software system history.  This information will be used to build a prediction model to assess the likelihood that given a change in one microarchitecture component, other components of microarchitectures have to be modified. Finally, our third goal is to alleviate the risk of unexpected or unwanted modification side effect.  In aiming for this, genetic algorithms will be used to generate test data with the goal of exposing unexpected behavior (with reference to the unmodified microarchitecture) and common security threats (e.g., buffer overflow).  ""387994,""Antoniol, Giuliano"
"337890"	"April, Alain"	"Software maintenance maturity model (SMCMM)"	"Process improvement has been a growing activity in the field of software development. However, the smaller setting IS/IT services, such as small software maintenance activities, lack specific process improvement models, assessment methods and training tools. The software maintenance capability maturity model (SMCMM), developed by the applicant during the course of his doctoral studies, was designed to address some of these issues. The objective of the proposed research program is twofold:(1) to develop an assessment method for the SMCMM that will make it possible to be used in industry; and (2) to develop assesssment and training support tools such as an assessor support tool and a decision support system so as to train specialized software maintenance assessors, end users, maintainers and students in charge of improving software maintenance processes in Canadian organisations. Both analytical and experimental research will be carried out so as to meet our objectives. Analytical research will be required to complement the SMCMM maturity model with: - An assessment method to make it possible to use the SMCMM maturity model to assess maintenance organisations; - Training tools, such as an assessor support tool and a decision support system. Experimental research, using case studies, will also be carried out to assess: 1) the effectiveness of the developed training tools; 2) the strenghts/weaknesses of the assessment method developed in the framework of this research program. Feedback from experimental research will be analyzed to improve the SMCMM maturity model practices, its new assessment method, as well as the new training and support tools.""328702,""ApSimon, Megan"
"338347"	"Ascher, Uri"	"Numerical solution of differential problems with constraints and applications"	"The general objective of my research is to develop efficient and reliable computational methods and software for models involving differential equations (DEs) that arise in applications. The focus is on constrained DEs, such as those arising in graphics, image processing and virtual reality simulations, and on problems involving DEs and optimization, such as those arising in distributed parameter estimation. I am also interested in conservative discretizations for time-dependent DEs. While my research focuses on the transfer of knowledge and expertise among areas of application, emphasis will be placed within this framework on particular applications: - Algorithms and software for large scale distributed parameter estimation problems involving discontinuities -- reconstruction of piecewise smooth surfaces, level sets and interfaces, and large sparse problems. - Methods of scientific computing for graphics applications, e.g. cloth simulation and image processing. - Compact, conservative methods for hyperbolic systems of partial differential equations, and time reversal applications.""331510,""Asfour, AbdulFattah"
"337803"	"Atwood, John"	"Secure and accountable multicast data distribution"	"Although multicast data communication has potential for substantially reducing network resource requirements when used to distribute identical data to many participants, and although it has been used in specific, controlled environments, it has not been offered as a general service by Internet Service Providers, since the necessary security and accounting functionality is not available. Solving this problem requires an integrated approach, with attention paid to Security and AAA (Authentication, Authorization, and Accounting) in the entire protocol stack.  During the previous grant period, the overall architecture has been formulated, and solutions proposed for the following areas: key management, router-to-router security, router-to-host security, router-to-AAA server communication, interaction with financial representatives (e-commerce interaction), host and user identification. During the coming grant period, there will be three main areas of activity: 1) Integration of the key management solution into an existing extensible router (XORP); 2) Design and validation of the necessary extensions to the AAA and host-to-router protocols to facilitate the accurate control and monitoring of multicast services, and their integration into the XORP platform. 3) Design and validation of a framework for support of the tree management for a range of reliable and unreliable multicast data distribution protocols, and the integration of the resulting protocols into the XORP platform. The long-term goal is the demonstration of the feasibility of a secure, accountable multicast data distribution system.  Achievement of this goal will permit the introduction of multicast data distribution in commercial systems, with direct benefits to the operators of such systems (lowered costs and increased revenue generation), and indirect benefits to the user communities, through lower costs and by enabling applications that would otherwise be infeasible.""343419,""Au, Adley"
"338442"	"AuclairFortier, MarieFlavie"	"Modélisation 3D de décors à partir d'images panoramiques"	"Une des problématiques du milieu  cinématographique est la modélisation 3D de décors dans divers contextes. Pour des fins de pré-production, un réalisateur peut vouloir simuler de façon informatique (par des animations) différents scénarios, mouvements de caméra ou éclairages d'une scène. Dans les cas où un personnage virtuel interagit dans un environnement réel, la modélisation 3D est très utile pour l'animation réaliste du personnage et le rendu final de la scène. Présentement, cette modélisation se fait par des artistes infographistes de façon manuelle. Le programme de recherche proposé a pour objectif de construire automatiquement une modélisation 3D d'un décors réel existant, en utilisant les possibilités offertes par la vision panoramique (VP). La VP comprend l'ensemble des capteurs et algorithmes permettant de construire des images panoramiques, c'est-à-dire ayant un champ visuel très large (allant même jusqu'à 360°). Certains capteurs utilisés pour la VP sont des capteurs à champ traditionnel (photo ou vidéo) et donc les images panoramiques sont créées de façon essentiellement algorithmique tandis que d'autres permettent une vision pluri-directionnelle. Par contre les images produites par les capteurs pluri-directionnels sont déformées géométriquement et photométriquement et donc nécessitent un traitement avant leur utilisation dans un contexte plus large. Les deux types d'approches ont leurs avantages et leurs inconvénients. Par exemple, les capteurs traditionnels sont beaucoup moins dispendieux et offrent une résolution beaucoup plus élevée que les capteurs pluri-directionnels mais ont besoin d'artifices algorithmiques de recoupement d'images (mosaïques) pour créer les images panoramiques.  Le programme de recherche proposé s'intéressera donc en premier lieu à la formation des images panoramiques, ensuite, enrichira les méthodes de reconstruction 3D en ajoutant divers indices de profondeurs qui ne sont pas présentement exploités en VP.  Finalement nous validerons le tout, entre autres, en insérant et retirant des objets d'un décor.""329009,""Audas, Timothy"
"337909"	"Ayanso, Anteneh"	"Efficient support for multi-attribute top-k relational queries:  a cost-based approach"	"          Querying for ""approximate matches"" is very common in document and multimedia retrieval systems.  In search engines, for example, users specify a set of keywords and expect in return a ranking of relevant pages or documents related to the keywords.  In contrast, the querying methods available in relational database management systems (RDBMSs) are designed to return only results found within specified selection conditions.  Due to this, users of online business applications such as product recommendation systems, price comparison and shopping agents routinely face the challenge of specifying value ranges of attributes in search of relevant results.  Often, they get either too few or too many results that are of limited relevance to their request.  This conventional querying process is very frustrating for the user and extremely inefficient for the system.          Alternatively, users of the above applications should be able to specify target values of attributes and expect to obtain a ranked set of a desired number of results that best match the specified values across all the attributes (e.g., the top 10 best matches).  In this type of querying, also known as top-k querying, results are not limited to exact matches but include close matches around the target values of interest.  This research studies cost-based strategies for efficient support of this class of queries in RDBMSs.  The objective is to provide methods that work within the technical constraints of the existing design of RDBMSs but avoid a full sequential scan of the database to obtain the top-k set.  In particular, the proposed research introduces techniques that systematically incorporate the relevant performance cost factors and their underlying trade-offs for efficient top-k retrieval.  The methodology encompasses analytical modelling and extensive computational and experimental analyses using real and synthetic data sets over a wide range of experimental settings.""327292,""Ayatollahi, Mina"
"338370"	"Bacchus, Fahiem"	"SAT and beyond, new algorithms for fundamental reasoning problems"	"The proposed research will mainly focus on developing new algorithms and techniques for solving the Satisfiability problem and its extensions. A wide range of fundamental and practical problems in Artificial Intelligence, and in Computer Science in general, can be cast as instances of satisfiability (SAT). This is not surprising since SAT is the canonical NP-Complete problem. What is surprising is that despite its worst case complexity, the most effective way to solve a variety of practical problem is to cast them as SAT and then solve the resulting SAT problem with a state of the art SAT solver. For example, many problems in hardware verification are now best solved using SAT technology. There are also a number of useful generalizations of SAT. Two particularly important generalizations are #SAT, the problem of counting the number of satisfying models, and QBF (Quantified Boolean Formulas), which is like SAT except that some of the variables can be universally quantified. These generalizations cover an even wider range of important practical problems. For example, the problem of inference in Bayesian Networks can be cast as a weighted #SAT problem (where each satisfying model has a different weight). The proposed research program will focus on developing new and better general algorithms for solving these core satisfiability problems: SAT, QBF, and #SAT (in both its weighted and unweighted versions). It will build on previous work by the applicant. This previous work has already been successful in achieving significant advances in the state of the art, and in our fundamental understanding of these problems. Nevertheless many important and subtle problems remain. Addressing these problems will be the main focus of the applicant's research activities. This research will contain many opportunities for the training of highly qualified personnel.""337034,""Bachewich, Catherine"
"337844"	"Badri, Mourad"	"Assurance qualité des systèmes orientés objet et aspect"	"La programmation orientée objet a indéniablement fait avancer l'ingénierie des systèmes logiciels. Elle éprouve, cependant, des limites sérieuses quant à la manière de prendre en compte « proprement » les préoccupations transversales dans un programme. La programmation orientée aspect cherche essentiellement à résoudre ce type de problème en modularisant les éléments transversaux des applications sous forme d'aspects. La combinaison aspect-objet ouvre une avenue intéressante et prometteuse dans le développement de logiciels. La compréhension des fondements du concept permet, en effet, de mieux saisir le sens des bénéfices importants que cette technologie promet. Elle permet, par ailleurs, de se rendre compte des nombreux challenges que les aspects apportent en termes de recherche. Le concept est relativement jeune et pose, en effet, une multitude de problèmes (à différents niveaux). Beaucoup reste à faire pour sa maturité. Bien que le domaine de la programmation orientée aspect commence à augmenter son audience, des solutions sérieuses pour le reste du processus de développement restent à définir. L'objectif principal de ce programme de recherche est de développer et d'expérimenter de nouvelles approches fédératives pour l'assurance qualité des systèmes orientés aspect. Il intègre plusieurs volets complémentaires (critères et techniques de test, attributs qualité, analyse statique et dynamique, détection de conflits, description formelle, etc.).""337477,""Baecker, Ronald"
"338915"	"Bajcsy, Jan"	"Coded multi-terminal communication systems:  theory, applications and experiments"	"A vast majority of practical communication scenarios involves multiple terminals and/or users, e.g., in wireless and optical data access networks, in transmission over multi-antenna (MIMO) channels, in distributed compression of correlated surveillance images or sensor measurements.  Results in network information theory (information theory of multi-terminal systems) indicate that appropriate multi-terminal coding can result in huge performance gains, in terms of transmission rates, compressibility, achievable spectral efficiencies, etc., when compared to communication architectures based on traditional single-terminal communication techniques. These gains are often of the order of magnitude, e.g., the aggregate throughput of a multi-access channel with Gaussian noise can be theoretically increased by as much as 20 times with multi-user decoding, when compared to traditionally used random access network protocols or single-user based CDMA transmission. Similarly, space-time codes and MIMO communication systems can offer more than ten-fold increases in capacities and spectral efficiencies of wireless communication links. Finally, efficient distributed compression in wireless sensor networks can be only achieved with appropriate network coding for the Slepian-Wolf problem.              Realizing the mentioned large performance gains leads often to open problems and novel unexplored scenarios in communication systems theory, coding, modulation/demodulation, compression, etc. The main objective of this proposal is to continue building a research program in coded multi-terminal systems, i.e., communication systems that involve more than one transmitter and/or receiver. The principal investigator aims to use the momentum and expertise in this area, obtained by him and his research team during the past 5 years. We have successfully demonstrated several world firsts, e.g., turbo compression in the Slepian-Wolf problem, overloaded OCDMA network transmission with multi-access channel coding,  near-capacity transmission for uplink MIMO systems with tens of antennas yet practical complexity.                The specific research objectives and approaches of this proposal can be outlined as follows: 1) THEORY: (a) Design near-Shannon limits achieving turbo codes and iterative decoding algorithms for selected multi-terminal problems of practical interest;  (b) Derive analytical, closed form BER evaluation techniques for turbo coded multi-terminal systems; (c) Perform robustness and sensitivity analysis of system parameters using applied tools from information theory.   2) APPLICATIONS: Apply the designed turbo codes, decoding algorithms and/or performance evaluation techniques to communication problems of practical interest, e.g., (a) MIMO-based wireless data access networks envisioned for the 3G and 4G cellular standards; (b) Overloaded optical CDMA network transmission; (c) Magnetic and optical recording systems with multiple tracks and read-heads; (d) Error robust and distributed turbo compression techniques for surveillance data and wireless sensor networks. 3) EXPERIMENTS: Using our recently set-up FPGA-based hardware-software platform for multi-terminal algorithm development and real-time testing (a) Develop real-time implementations of the proposed coded techniques; (b) Test these techniques in real-time, emulated communication scenarios using off-line channel measurements and real data sources; (c) Enable and participate in potential transfer of developed technologies to system demonstrators and prototyping platforms.             In terms of the significance of the proposed research program, global communication systems are progressing rapidly towards high-speed digital services available anytime and anywhere. Every year, these systems are generating multibillion dollar revenues and rely on system level solutions which are specific to the particular communication scenario (wireless, optical or recording; voice, data or video; uplink or downlink; macro, micro or pico cell, etc.)   Expected theoretical and applied contributions are in coding, modulation and performance evaluation of multi-terminal communication systems. Obtained research results are expected to have impact on the low complexity design of power and bandwidth efficient transceivers for the global communication infrastructure.   In addition, graduate (and undergraduate) students trained in close mentorship during the proposed program and supported via the requested funding will be in demand by prestigious research labs and telecom companies, as indicated by the principal investigator's track record over past 5 years.""338952,""Bajic, Ivan"
"338980"	"Ban, Dayan"	"Terahertz quantum cascade laser and its application"	"The terahertz gap, lying roughly between 300 GHz (0.3 THz) and 30 THz in the electromagnetic spectrum, exists because the frequencies generated by semiconductor-based transistors and lasers don't overlap. Generation of coherent terahertz radiation has traditionally involved either extending electronic techniques to higher frequencies, or extending photonic sources to longer wavelengths. In both cases, the efficiency drops rapidly as the frequency approaches the terahertz region. Hence, the terahertz electromagnetic region has remained largely underdeveloped, despite it's identified potential for wide-ranging applications in chemical detection, astronomy, medical imaging and optical wireless communication. Progress in this area has been hampered by the lack of compact, low-consumption, solid-state terahertz sources. The objectives of this NSERC discovery proposal are 1) to develop a solid-state terahertz source using electrically-pumped quantum cascade lasers (QCL), 2) to develop a comprehensive knowledge base about the underlying physics of intersubband transition and electron-electron, electron-phonon interactions occurring in the quantum cascade semiconductor heterostructures, and 3) to apply prototype laser devices to practical applications. First, fundamental studies of physical processes occurring inside quantum cascade structures will be investigated to gain insights which are crucial for the subsequent design and fabrication of THz devices; (2) effects of important design parameters of the active region on device performance will be explored; (3) novel device design and fabrication processes will be evaluated for building prototype quantum cascade lasers towards terahertz lasing frequency operation at higher temperatures; (4) resulting QCL devices will be applied in optical wireless communication and terahertz imaging. Terahertz imaging would be as effective as X-ray imaging, but would be much safer for humans and the environment. This research will provide a broad base of new knowledge about the THz frequency range with potential impact in the areas of telecommunication, biological and medical sciences and security. In due course, this will benefit the economy and welfare of Canada.""332769,""Ban, Yifang"
"338869"	"Bandler, John"	"Simulation, modeling and optimization of devices, circuits and systems"	"First-pass success and time-to-market are crucial for manufacturability-driven design of telecommunication circuits.  This necessitates powerful, accurate and fast computer-aided design (CAD) tools for component modeling, circuit simulation and automated design optimization.  Simulations based directly on physics and electromagnetics (EM) are traditionally computationally intensive, but offer high accuracy, handle arbitrary geometrical shapes and are valid to millimeterwave frequencies.  Their direct exploitation in optimization, statistical design and design with tolerances remains challenging. Pioneering work in design centering, tolerance and statistical design for manufacturability, tuning and yield-driven optimization (IEEE MTT-Society 2004 Microwave Application Award) will be broadened into a flexible theoretical and software framework for computer-aided engineering of wireless, radio-frequency (RF) and microwave components and circuits.  The framework will be based on our novel space-mapping optimization technology, which we pioneered in 1993 and which has already resulted in diverse and significant modeling and design successes in the engineering community in Canada and internationally.  New algorithms are expected to deliver in a handful of fine or high-fidelity model evaluations the accuracy expected from classical direct optimization using sequential linear programming. We will continue to work with Dr. Kaj Madsen (Technical University Denmark) on the convergence theory for space-mapping algorithms.  This should help engineering practitioners in developing or discriminating between coarse engineering models.  We will develop prototype software engines which exploit both full-wave EM simulators and fast, empirical, coarse or surrogate device models.  We will continue to explore links with artificial neural network technology for device modeling, e.g., neuro space mapping (Q.J. Zhang).  We will develop efficient, robust algorithms for EM-based optimization exploiting space mapping, available adjoint sensitivities, surrogate models (John Dennis) and the trust region methodology.""336002,""Bandrauk, André"
"338369"	"Barron, John"	"The measurement and interpretation of 2D and 3D image motion"	"The central theme of Dr. Barron's research deals with the measurement and interpretation of 2D and 3D image image and 3D range motion. 2D image motion, approximated by 2D optical flow, is the local 2D motion at each image point in an image sequence. 3D image motion, approximated by 3D optical flow, is the volumetric motions of voxels in volume sequences. 3D Range motion, approximated by range flow, is the local 3D motion at surface points (and as such can be considered 3D optical flow on a surface). Some of the applications of his research could be: (1) Recovery of camera translation and rotation parameters and scene depth maps from a sequence of images generated by the moving camera using time-varying 2D optical flow in a Kalman filtering framework. (2) Measurement of 2D/3D corn seedling growth from 2D optical flow (including in the dark using near-infrared imagery). (3) Measurement of leaf motion and growth using 3D range flow computed from time-varying 3D surface range datsets. (4) Measurement of voxel motion in sequences of gated MRI cardiac datasets leading to analysis, say, of the motion of the left ventricular heart wall. (5) Tracking of severe weather storms in 3D Doppler precipitation density and radial velocity datasets. This research contributes to the areas of autonomous vehicle navigation, monitoring plant growth in controlled environments (greenhouses), diagnosis of heart disease, and automation of Doppler radar weather nowcasting/forecasting.""345557,""Barron, Marcus"
"337889"	"Bartram, Linda"	"User interfaces for complex information environments"	"The research described will centre around how people derive and use information in complex environments and on the design of perceptually and cognitively effective interface technologies to aid comprehension and integration. I am particularly interested in the use of motion for visual communication, and in the intersection between single-user and shared technologies. This involves projects that span information visualization, human-computer interaction and computer-supported collaborative work. The long term goals of this work are twofold: 1. develop a more complete  understanding of how people visualize, commuicate and and manipulate information in technologically heterogeneous and information-rich environments; and 2) develop more cognitively and perceptually efficient visualization and interface techniques to support  integration and comprehension of that information. The proposed major areas of this work are: 1. the investigation of perceptually efficient representation techniques for distributed visualization environments, both multi-device and multi-user; 2. a principled exploration of the perceptual and interpretative properties of motion for visualization and communication; 3. investigation of subtle attentional cues and representations, especially for peripheral and ambient interfaces; and 4. investigation and development of interface techniques that span the intersection of private and shared information, accomodating different user roles and display emvironments.""326017,""Barzda, Virginijus"
"337812"	"Basu, Anup"	"Biologically motivated multimedia systems"	"The overall goal of my research over the next five years will be to develop new biologically motivated multimedia systems or advance existing ones proposed by me in my earlier research. My research will include the following novel advancements: (i) Automatic detection of Regions-of-Interest for foveated video and stereo visualization. (ii) Understanding the tradeoffs between structure (3D) and texture (2D) information in 3D visualization. (iii) Wireless 3D transmission with packet loss;  considering heterogeneous and Peer-to-Peer (P2P) networks. (iv) Active calibration of sensor networks, extending my prior work in active calibration. (v) Intelligent interaction among sensors. (vi) Extending my earlier work on statistical confidence guarantees  to address statistical Quality-of-Service (sQoS) over distributed P2P networks for 3D multimedia retrieval. (vii)  Strategies for optimizing stereo perception, considering foveation and panoramic 3D displays, to give viewers a feeling of true presence in a virtual environment with optimal use of network and memory resources. The significance of my work will lie in: (a) Reducing bandwidth, memory requirements and optimizing perceptual quality in 3D multimedia transmission and visualization; with applications in 3D panoramic TVs. (b) Robust 3D transmission strategies under packet loss; resulting in improved 3D games over wireless devices. (c) Robust calibration and intelligent coordination of a network of cameras, allowing better and more automated video surveillance and security.""327941,""Basu, Niladri"
"338890"	"Beauvais, Jacques"	"Nanolithography techniques for optoelectronic and electronic device fabrication"	"The pursuit of ever smaller electronic and photonic devices has led  current microfabrication and nanofabrication technologies to the edge of fundamental physical limits. It has become critical to understand how we can overcome the restrictions imposed by these limits upon current technology and cross to the promised land of nanotechnology where innovation will operate at the atomic and molecular scale. The proposed research program aims to study devices that include features smaller than 20 nm in size, and to explore the limits of current fabrication technologies and  the impact of these limits on some of the devices' performance. This work will rely on electron beam lithography, which is an important, leading-edge technique for exploring fabrication limits and developing new components and technologies.  This technique is extremely versatile, making it possible to easily and rapidly change the patterns used for making the devices, both electronic and photonic, and to explore new chemical and physical strategies to fabricate these devices. This work could help pave the way for the development of innovative devices that truly exploit the effects that appear when working at the scale of nanometers. The results obtained by exploring the methods used for writing patterns at the nanoscale could also have a significant impact on innovative methods currently being developed for fabricating ultra-small devices. One such example is nano-imprint lithography, which uses simple stamps to press patterns into a soft layer, and new electron beam systems, which make use of thousands of beams to write a large pattern on a wafer using a parallel approach. Both of these methods could help to reduce the cost of writing nanoscale patterns dramatically, and accelerate the transfer of innovative ideas into real applications.""329104,""Becalska, Agata"
"338436"	"BenDavid, Shai"	"Theoretical foundations of statistical clustering"	"Clustering is one of the most widely used techniques for exploratory data analysis. Across all disciplines, from social sciences to biology to computer science, people try to get a first understanding of their data by identifying meaningful groups among data points. There has been extensive work on algorithms for clustering over the last several decades. However, the vast majority of that work is heuristic in nature. Despite the large number of algorithms and applications, the goal of clustering and its proper interpretation remain fuzzy and vague. While there exists a significant amount of work on the computational complexity of clustering tasks, the theoretical foundations of clustering, especially in statistical settings, seem to be distressingly meager. Common analysis of clustering algorithms does not tackle the problem of clustering in a principled way. While the question 'What clustering is' is difficult to answer in such generality, we believe that there are important sub-questions which are well defined and can and should be investigated in a general statistical framework. The objective of this work is to formulate and analyze general principles, in the form of necessary as well as sufficient conditions, for successful/meaningful clustering. In this project we address clustering in a statistical setting. Namely, a setting in which the input data to a clustering algorithm is a sample from some unknown underlying domain distribution, and the goal of the algorithm is to output meaningful information about that underlying distribution. Apart from the apparent theoretical importance of developing a theory for statistical clustering, our work has the potential of yielding useful model-selection methods for clustering. The principles we propose to develop may provide measures for the fit between different clustering algorithmic approaches and a given clustering task. This will enable the choice of appropriate algorithms and parameters (such as the number of clusters in center-based clustering, or the stopping point for agglomerative algorithms) to be based on soundly defined methodology (as opposed to the currently used ad hoc approaches).""334879,""BendellYoung, Leah"
"337874"	"Benoit, Darcy"	"Autonomic database management systems"	"Autonomic computing is an approach to self-managed computing systems with a minimum of human interference. Autonomic systems are intended to perform with little human interaction by configuring themselves when installed (or when new hardware is added), manage their own resources for peak performance, heal themselves when software breaks (or detect broken hardware and ask for repairs) and protect themselves from both internal failures and outside attacks. Automated diagnosis and resource management will result in better performance and lower operating costs. One area where the impact of autonomic computing is significant is that of embedded systems and applications. Embedded systems and applications usually have a limited amount of resources available for application to run, and as such, performance is based on the efficient and intelligent use of resources. Limited resources are further exacerbated by the lack of regular system interfaces in which resources can be adjusted. For typical embedded applications such as an address book stored in a telephone, it is not possible to adjust resources in order to improve the performance of the system. In such cases, the embedded application must be responsible for managing its own resources and achieving the best possible performance given the resources available. The application of autonomic principles to embedded applications and systems is necessary as we expect more functionality from resource-constrained devices. Autonomic computing has been especially popular in database management systems (DBMSs) in the past several years. As databases increase in size and complexity, the ability to control performance by hand becomes impractical. This problem is further intensified as DBMSs are embedded into everyday devices and expected to handle significant amounts of information and perform well. DBMS performance on these embedded applications becomes more apparent and data volume increases. Advancements in this area will be applicable to the DBMS, embedded systems and autonomic systems community.""342353,""Benoit, Jérémie"
"337834"	"Benyahia, Ilham"	"Testing embedded complex applications- modeling methodology and experimental environment"	"Recent technological innovations in information acquisition, processing and transmission have encouraged the development of complex applications integrating hardware components. Such applications interact with environments characterized by perturbations (noises, faults, etc.) that can cause performance degradation during runtime. Despite the importance of these embedded complex applications, their development methodologies still fail to guarantee and optimize the required quality of service (QoS) even when adaptive and architecture reconfiguration techniques are considered at the design phase. The main reason is that the total package of components is not taken into account during design and development. This applies especially to hardware components: real hardware components are too costly to be used for testing purposes, and the rapid technological evolution of hardware components makes them difficult to simulate with existing software modeling methodologies. The purpose of our research is to study and develop a methodology for specification and modeling of software and hardware components that compose complex applications. We will investigate the impact of evolutionary processing to optimize the QoS of these components and make them adaptive to changes in their environment. Our study will contain inevitably experimental dimensions since we simulate behavior changes in the environments of components to define strategies that will maintain the QoS of these components. To define our methodology we combine and extend well-established methodologies in object software engineering such as HRT-HOOD, with evolutionary and simulation techniques.  We develop a generic testbed to test evolutionary techniques on various components. Then, we validate our experiments on real hardware components. To provide more powerful adaptive software capabilities, we test component results on entire complex applications defined from our development framework.""335393,""Benyoucef, Morad"
"338950"	"Bhattacharya, Prabir"	"Applications of intelligent image processing"	"The project involves investigations on the applications of pattern recognition and machine learning techniques to problems in image retrieval, image scene analysis and biometrics. The use of machine learning techniques in computer vision applications is relatively new except for applications to face recognition. We propose to investigate an efficient classification and retrieval from large image databases, and also its applications to dermatological images to provide a diagnostic tool to dermatologists and oncologists, problems in biometrics involving iris recognition. We also investigate problems in mobile computing involving biometrics-based identification and verification. We propose an approach to image retrieval by classifying images into different semantic categories and using probabilistic similarity measures. With the growing availability of information technology and digital imaging revolution in the medical domain, large collections of digital dermatological images are regularly being generated by clinics for image-guided diagnosis. We propose an intelligent content-based image retrieval approach of digital medical images in the domain of dermatology. It may be used as an efficient training tool for medical students, residents and researchers to search efficiently large collection of disease-related illustrations by their visual attributes. As an application of machine learning to biometrics, we propose an iris recognition system as a biometrically based technology for person identification using support vector machines. The project is partly interdisciplinary and would provide interactions with the industry and medical doctors. The project would train three  graduate students and a Postdoctoral Fellow.""388051,""Bhattacharya, Prabir"
"338435"	"Bilodeau, GuillaumeAlexandre"	"Detecting and tracking moving and stopping objects in videos"	"Security has become a priority for many countries, including Canada. Many ways are possible to secure a given site. One of these is the use of cameras. In this case, security agents monitor screens showing the scenes happening in the field of view of each camera. This way of using cameras has many limitations. First of all, a security agent cannot monitor more than a given number of screens. Secondly, typically the videos are watched only after a criminal event because of a lack of working force. Hence, the cameras do not contribute at preventing crimes. A possible answer to these difficulties is to use computer vision systems. These systems have the potential to monitor automatically a site and involve security agents only when an intervention is needed or when a suspect event is detected. However, many difficulties still need to be resolved. The video of a camera must be analyzed to detect moving objects, model them and interpret the ongoing action. This research program studies these difficulties for the case of a single camera. The market of automatic video surveillance is very promising and has the potential of bringing important economical benefits and creating jobs and new companies in Canada. The proposed research program aims at designing novel computer vision solutions to video surveillance, applicable in other domains as biomedical and domotic, while training highly qualified personnel in this field.""340216,""Bilodeau, JeanFrançois"
"338859"	"Bosisio, Renato"	"Multiport impulse radio (MIR)"	"New radio technology is hastening the development of impulse radios such as MIR, Smart radios, Software Defined Radios (SDR) and Cognitive radios. Research in radio has an impact on wireless communications, and new spectral friendly radios must be low cost, low power consumption, and conform to Ultra Wideband (UWB) radio regulations. Proposed MIR radio is flexible and adaptable to UWB for implementation in low cost chip format using integrated circuits. MIR research is separated in  phase I and phase II with common aim to acquire needed knowledge for industrial prototype. Commercial components are assembled , in phase I, alongside a digital signal processor to test new multi (six-port) circuits and MIR radio software. In phase II, MIR is  fabricated with in-house integrated circuits (MHMICs) to obtain needed knowledge to design and fabricate an industrial MIR radio prototype. MIR radio will provide short range /high data rates communications found in consumer/ commercial / military multi-user markets.  MIR hardware and software are based on 1995 paper by applicant published in IEEE as ""Computer and Measurement Simulation of a New Digital Receiver Operating Directly at Mm-Wave Frequencies"". This first paper on direct digital six- port receiver won for applicant the 2004 McNaughton Medal, IEEE Canada's highest award. A similar six-port receiver publication was made, five years later, by an international core research laboratory (Sony). Nevertheless,the six-port demodulation concept of applicant remains in public domain due to priority 1995 IEEE paper. Present MIR project adds new important elements to 1995 publication: 1) a new wideband multi (six) port modulator circuit, 2) a new wide band digital modulation scheme and 3) operating software. Knowledge from MIR research will lead to low cost radio chips and software operated in a laptop or personal computers. This new knowledge will add to Canadian experience in UWB radio. MIR is expected to surpass other UWB radios in cost, power consumption and size to perform basic wireless radio functions (e.g. between computers in office or home) over short distances at high data rates. Similar MIR technology applies to radar (cars, helicopters , aircraft landing and item tagging).""344330,""Bossé, Jessica"
"337842"	"Boucheneb, Hanifa"	"Vérification formelle de systèmes de sécurité au moyen de réseaux de Petri temporisés de haut niveau"	"Les organisations qui utilisent Internet sont quotidiennement victimes d'incidents liés à la sécurité (l'infection des ordinateurs par un virus, une panne causant l'arrêt des systèmes d'information, la perte de données et l'usurpation d'identité). Ces incidents pourraient avoir des répercussions néfastes pour ces organisations et leurs clients (pertes de clients, faillite,etc...). Il est alors vital que les systèmes utilisés soient dotés de mécanismes efficaces qui garantissent la sécurité et la protection des données confidentielles qu'ils gèrent. Les méthodes formelles de vérification, parce qu'elles s'appuient sur une base mathématique rigoureuse, sont  fortement recommandées lorsque les requis de sûreté et de sécurité d'un système sont primordiales (système de sécurité). C'est dans ce contexte que ce programme de recherche propose d'utiliser les méthodes formelles pour étudier et vérifier les systèmes de sécurité.  Ce programme comporte quatre objectifs spécifiques : 1. Étude et proposition d'un modèle de sécurité permettant de décrire efficacement les diverses politiques de sécurité qui peuvent cohabiter, à différents niveaux, au sein d'une même application répartie. 2. Élaboration d'un modèle à base de réseau de Petri de haut niveau temporisés pour le modèle de sécurité proposé. 3. Conception de méthodes de vérification par model-checking plus appropriées pour le modèle formel élaboré. Pour appliquer la technique de model-checking au modèle proposé, il faut d'abord établir une procédure convergente de transformation du modèle, qui est en général infini, en un système de transitions fini permettant de vérifier les requis. Cependant, le critère de convergence n'est pas suffisant car cette technique se heurte aussi au problème d'explosion combinatoire. Un intérêt particulier sera accordé à ce problème. 4. Implémentation, test et validation des approches de vérification proposées au cours de ce programme de recherche.""336603,""Boucher, Claude"
"338388"	"Boufama, Boubakeur"	"Scene manipulation in the context of uncalibrated images"	"Images contain a considerable amount of rich and concentrated information that is usually deciphered immediately by humans. In comparison and despite the remarkable progress made so far, image understanding by artificial vision systems remains limited. This research proposal aims at advancing current vision systems to make them capable of manipulating parts of a scene, where the latter is represented by a sequence of two-dimensional uncalibrated images. In particular, we will deal with the following three problems: adding new objects(augmented reality), removing existing objects and, moving/changing objects within the scene. In this proposal, we consider the difficult case where images are taken with unknown cameras and the only data to be used are the arrays of pixels. Furthermore, the solutions to be proposed here will be all vision-based, with no need for specialized equipment. In addition to the scientific advancement aspect, the results from this research could be applied in areas ranging from teleoperation to entertainment. More precisely, we will investigate the following related problems.(1) Registration of virtual objects in the images. Currently, some vision-based solutions, which use calibrated cameras exist for this problem. However, the registration becomes complex when the only data we have consist of uncalibrated images. In this case, we need to retrieve some metric (Euclidean) information about the observed scene in order to achieve correct registration. We will investigate the two possibilities to get such metric information: either by using recent techniques for camera self-calibration or by using Euclidean constraints from the scene/object geometry.(2) Segmentation and object tracking across images. Removing or displacing objects requires proper segmentation and identification of the target objects. The latter should be identified and matched across the images, in order to be able to move it to a different location.(3) Updating the whole image sequence. Once the first two images have been changed, either by the addition of new objects or by the removal/displacement of existing objects, the rest of the sequence must be updated accordingly.""336290,""Bouferguene, Ahmed"
"338895"	"Boukadoum, Mounir"	"Fluorescence measurement instrumentation with pattern recognition capability"	"Fluorescence is the property of some materials and organisms to absorb light and reemit it at different, warmer colors. The study of fluorescence has useful applications in several areas, of which agriculture, food processing, environmental studies, biochemical analysis, substance identification, and bacteria detection. Fluorescence measurements can be very accurate, with much better sensitivities than those obtained with using other spectroscopic techniques. Still, more innovative research is needed to improve further, and thus overcome some weaknesses of fluorescence methods. These include the bulk and cost of the required instruments, the need for precise calibration procedures, the need to compensate for environmental changes and the usually manual or semi-automatic measurement process. Also, the processing of the acquired data is a complex task and requires sophisticated techniques to extract useful information from the obtained spectra. This has led to the development of ad hoc acquisition techniques that are specific to given problems and cannot be generalized. This is in part because the sensor developers speak a technical language that is often obscure to end users who come from other areas of expertise.    Ongoing research in our laboratory aims at investigating basic questions such as the above, and ultimately propose integrated, system-on-a-chip, solutions that allow the study of the physical and chemical properties of various substances and/or the identification of these substances with their concentration in various contexts. In the present proposal, we address concerns of size and efficiency, and adding functionality such as the ability to perform automatic, online analysis of the acquired fluorescence signals in the absence of comprehensive models. We believe that our research will lead to the development of generic system-on-a-chip devices that can be easily adapted to various applications, including biomedical ones, while enhancing their ease of use by non technical experts.""335259,""Boukas, ElKébir"
"338941"	"Boyer, FrançoisRaymond"	"Variable clock period for low power and high performance"	"The cycle by cycle variable clock period is still a new concept, which could have a big impact on the way we see synchronous circuits (controlled by a clock) relatively to asynchronous circuits (using self-timed circuits without clock).  The idea of the variable clock period is to permit to change the length of each cycle to follow precisely a schedule.  By this concept we are aiming high performance but also low energy computation, trying to maintain relatively high performance and reducing power consumption, when performance is needed, and reducing power to a minimum when performance is not needed. High-performance and low energy consumption portable SoC applications have become very important. For example in hearing aids, where we want a very small package, and high quality audio processing, but not consuming too much, for extended battery life. We want to explore: The advantages of the variable period relative to fixed period or asynchronous designs;  How to generate multiple variable clocks from a single clock reference, at very low power;  Ways to interconnect parts with different clocking schemes (asynchronous, fixed period, or another variable period design), for system-on-chip (SoC) integration and to interact with the outside world;  Fast wake-up, reacting to asynchronous events;  A methodology to design circuits that use efficiently the possibilities of the variable period;  An approach to obtain schedules, synthesis of data paths and register placement;  Tools that permit the development, analysis and verification of these circuits, from a high level language, down to current low level tools;  OS and controller requirements for low-energy usage with real-time constraints.""327693,""Boyer, JohnChristopher"
"338453"	"Brodsky, Alexander"	"The role of sunchrony in the implementation of shared memory objects"	"Presently, most of the algorithms for solving problems in a distributed fashion do not make any assumptions regarding the degree to which the components of the underlying distributed system are synchronized.  Although such assumptions allow for a very general design methodologies, they ignore a useful attribute of most distributed systems;  namely, that the components of many distributed are comparatively synchronized.  The amount of synchrony in a system is the degree to which the system's components operate in concert. I propose to investigate how this synchrony among a system's components can be leveraged in order to design faster and more efficient distributed algorithms.  Specifically, I am interested in determining the types of synchrony present in distributed systems, which forms are most useful, and how to design distributed algorithms so that they can leverage these forms of synchrony. This is useful for several reasons.  First, it improves our understanding of the foundations of computer science.  We know that that synchrony plays an important role in determining whether and how efficiently a particular problem can be solved in a distributed manner; the challenge is to determine what this role is. Second, by determining what forms of synchrony are useful, we can design more efficient distributed systems.  These lessons are applicable to both the hardware designers and the software architects, and would facilitate better integration between a system's hardware and software.  This would improve the operation, efficiency and safety of distributed systems, which are being utilized more and more in research and industry.""339482,""Broer, Abraham"
"338433"	"Brooks, Stephen"	"Semi-automatic control for data-driven computer graphics"	"Many computer graphics applications remain in the domain of the specialist.  They are typically characterized by complex user-directed tasks, often requiring proficiency in design, colour spaces, computer interaction and file management.   Furthermore, the demands of this skill set are often exacerbated by an equally complex collection of image or object manipulation commands embedded in a variety of interface components. The complexity of these graphics applications often requires that the user possess a correspondingly high level of expertise; the user is often required to be both a skilled artist as well as possessing considerable technical competence.   The focus of this research is to further democratize computer graphics by reducing the complexity of graphics applications.  This will be achieved through semi-automation and by incorporating aesthetic constraints into graphics applications thereby increasing accessibility to the average user. To achieve this, I aim to develop novel data-driven techniques applied to principal graphics problems. Unlike analytic methods, data-driven graphics generates new graphics elements (such as materials, geometry, or motion) from existing data extracted from the real world.  The proposed research applies to many graphics applications including video production, architectural design and all forms of digital entertainment.""343426,""Brookshaw, Marcus"
"337908"	"BurtonJones, Andrew"	"A theoretical foundation for defining and representing internal controls"	"Internal controls broadly refer to practices that companies implement to safeguard assets, ensure the integrity of their financial data, comply with laws, and improve business operations.  In recent years, internal control failures have led to the ruin of major corporations such as Enron, Arthur Anderson, and WorldCom.  In response, new laws such as the Sarbanes-Oxley Act in the USA have been passed that require corporate managers and auditors to formally attest to the strength of companies' internal controls on an annual basis.      Although internal controls are critical in practice and widely researched, core issues remain unstudied.  Most fundamentally, little work has been conducted to define the meaning or nature of internal controls.  While such a problem is highly theoretical, it is also a highly practical problem because companies cannot attest to the strength of internal controls without a deep understanding of what those controls are.  Thus, the long-term objective of this research program is: to develop a theoretical foundation and an associated method for defining and representing internal controls in organizations.  The specific short-run objectives of the research program are as follows: 1. Develop a necessary and sufficient set of concepts for defining and representing internal controls. 2. Develop a method for accurately representing internal controls and implement it in a software tool.   3. Test whether the proposed software tool helps practicing auditors to understand and test controls.    The research promises several contributions.  For academics, the research will clarify the nature of internal controls and contribute to a general theoretical foundation for defining, classifying, and representing internal controls.  For practice, the research will provide a software application and method that will help practitioners to document, understand, and test internal controls in organizations.""329554,""Burzynski, Tom"
"338359"	"Buss, Jonathan"	"Research in complexity of algorithms"	"The basic theory of computation measures efficiency, or complexity, against the size of the problem instance to be solved.  Often, however, the amount of input data does not predict the true difficulty of a particular instance. By way of analogy, although a larger crossword puzzle (for example) will tend to require more time than a small one, solving an easy large puzzle may require less time than solving a hard small one. Many computational problems have the aspect that problems which appear large and daunting may actually be simple and solvable.  Examples of such problems occur frequently in communications, transportation, manufacturing, retailing, drug design, and many other areas.  In order to investigate this phenomenon, researchers have developed the concept of ""fixed-parameter"" complexity, in which a recognizable feature of a problem (the ""parameter"") is used as a measure of how hard the given instance actually is.  For some problems, instances having a small parameter admit of a quickly-obtained, optimal solution.  In other cases, however, even instances having a small parameter remain insoluble in practice. The proposed research will investigate the hard cases, in order to identify and characterize what makes a problem ""fixed-parameter intractable"".  Understanding these characteriztics may lead to discovery of new techniques so that apparently hard problems become actually easy.""331952,""Bussière, Bruno"
"338408"	"Butz, Cory"	"Modern Bayesian network inference and its practical applicaitons in probabilistic expert systems"	"One often has to make decisions under uncertainty. In computer systems, one approach to uncertainty management is to use Bayesian networks. In order to make sound decisions, Bayesian networks use probability theory as a solid mathematical foundation to answer questions (queries). They have been used by many companies, including Microsoft, NASA, Lockheed, Hewlett Packard and Nokia. To answer questions efficiently and quickly, the key is to exploit independencies holding in the problem domain. Current approaches, however, are not fully taking advantage of all available independencies. By utilizing independencies that remain unnoticed in all previous methods, we have recently suggested the first method that can precisely identify the probability information being passed in a Bayesian network. In addition, we can identify these messages significantly faster than they can be physically constructed. Our method can scout the Bayesian network in order to guide the physical computation needed to answer a question. The point to remember is that by doing so we can answer questions faster than before. It is also widely acknowledged in the Bayesian network community that beginners have great difficulty understanding query processing algorithms. The reason is that these previous algorithms are unable to identify the messages being passed in a Bayesian network when answering a question. We can. This research program will produce two major results. First, we will build a state-of-the-art computer system for using Bayesian networks in uncertainty management. Second, we will write a textbook on Bayesian networks describing our new approach. Since our approach can more precisely describe the inference process, it should make Bayesian networks accessible to a wider audience.""336867,""Byers, David"
"338868"	"Cada, Michael"	"Nonlinear optical devices"	"The aim of the proposed research is further advancing of analytical and simulation methods leading to the development of design tools for active nonlinear optical structures and devices for applications in communications, computing, signal processing and sensing. Novel theoretical approaches and models based on our long term experience with solving various light propagation and waveguiding problems including optical nonlinearities and optical gain will be further developed and applied. The objective is to design novel structures and devices with either newly implemented functionalities or significantly enhanced performances. The research will be focussed on a few concrete problems, namely the development of the analytical, simulation and design tool for a generic nonlinear switching/logic element with optical gain. Our original methods using a new solution to Maxwell's equations and the extrapolation property of linear prolate functions will be exploited. Higher-order nonlinearities of materials such as CdTe will be the focus of the research work. Nonlinear interactions involving external-field-assisted optical waves will be considered as means to establish efficient, well-controlled desired functionalities, for example all-optical switching, intelligent sensing or light amplification. The anticipated theoretical achievements will contribute new results to the already well established and well studied area of optical and electromagnetic issues of analysis, modeling and design of structures and devices. Expected applications of our approach are in the areas of modeling, simulations and design of photonic devices for optical communications, information processing, storage, computing, diagnostics and treatment in biomedicine, sensors, testing and measuring instrumentation. This could lead to significant innovation in the future.  ""334644,""Cade, William"
"338954"	"Cai, Lin"	"Exploiting network diversity for next generation wireless internet"	"Our long-term objective is to foster the evolution of the wireless Internet to support heterogeneous multimedia applications anywhere, anytime, with substantially higher data rates and energy efficiency, wider coverage, better quality of service, and at a lower price. Our short-term objectives in the following five years are exploiting network diversity, including multi-radio diversity and user-cooperation diversity, by protocol and algorithm development, performance analysis, and architecture design. As the price of RF transceivers (radios) has fallen dramatically in recent years, the low cost allows us to use multiple radios in the same device to access the Internet through various wireless links simultaneously. To exploit multi-radio diversity, we will systematically study the interdependency issues among different layers of the protocol stack, and propose how to optimally deliver traffic over multiple wireless links. On the other hand, due to the broadcast nature of wireless channels, cooperation among neighboring users can take advantage of the inherent spatial and multi-user diversity to increase system capacity and efficiency. In order to practically deploy physical-layer user-cooperation schemes in networks, we will study the unexplored networking issues analytically and via experiments. An emulation testbed will be built to validate the analysis and evaluate the protocol performance in multi-radio and user-cooperation networks. The proposed research will develop important new theories, algorithms, and protocols that will significantly improve radio resource and energy efficiency, service coverage and quality, and even enable services otherwise impossible to deliver in traditional single-radio and non-cooperation networks, and thus leads to the fulfillment of our long-term objective. The success of the proposed research will provide a vital competitive edge to Canadian Telecommunications, wireless, and software industry, and bring low-cost, high-speed, and rich-content wireless multimedia services to Canadian consumers.""336539,""Caillé, Alain"
"338392"	"Cercone, Nick"	"Integrated methodologies for data analysis, natural language processing and their applications (particularly in health)"	"My primary research objectives include: (1) the design and implementations of natural language (NL) systems; and (2) machine learning, knowledge discovery, data mining and analysis in large structured (databases), unstructured (internet) and semi-structured information repositories. Ancillary research goals for the natural language research comprise: (1) developing formal NL and speech representations; (2) NL interfaces for information access; (3) devising accurate, novel multilingual machine translation systems with speech prosody based on HPSG's (head driven phrase structure grammars) [note that multilingual is used generically and current work includes translation between NL's and sign languages to aid deaf learning and communications]; and (4) language analysis application of n-gram analysis, e.g., language identification, author attribution, and so on. Ancillary research goals for the knowledge discovery and data mining and analysis research include: (1) developing  soft computing techniques employing rough sets theory for attribution selection, classification, association and prediction (survival analysis); (2) evolving our design of hybrid architectures for data analyses; (3) developing rule quality measures and rule importance measures to provide fewer, more accurate ranked rules for classification and association; and (4) application of n-gram analysis for information extraction and classification, e.g., classifying levels of dementia in patients with mild cognitive impairment, malicious code detection, classification and annotation of biological terms for extraction, and so on.""336784,""Ceri, Howard"
"338397"	"Chali, Yllias"	"Information delivery vehicule: text summarization and question answering"	"In today's information society, we have access to an enormous amount of information. Without appropriate thories, technologies, and tools to obtain the information we need in a clear and useful way, there is an ever increasing risk of information overload (or information fatigue), leading to poor information use and inefficiencies in decision-making. Information retrieval engines, text summarizers, and question answering systems provide complementary functionalities which can be combined to serve a variety of users, ranging from the casual user asking questions of the web to a sophisticated knowledge user. A particular useful complementarity exists between text summarization and question answering systems. From the viewpoint of summarization, question answering is a way to provide the focus for the query-based summarization. From the viewpoint of question answering, summarization is a way of extracting and fusing just the relevant information from a heap of text in answer to a specific question. I am interested in developing techniques for delivering information that is appropriate and relevant to the user, avoiding information overload and assisting informed decision making. This will lead to build up systems that capture the user needs and preferences and generate customized and personalized documents. This research proposal deals with ways in which summarization and question answering technologies can be combined to form truly useful information delivery tools. Based on our previous research experience, the purpose of this project is to design and build up tools at several increasingly sophisticated stages. I am planning to develop theories, technologies and tools to deliver to users information that is relevant and appropriate for the users receiving it, information that they can easily assimilate to enable them to perform their tasks, usually enabling better and more efficient decision-making.""330964,""Chalifour, FrançoisP"
"338382"	"Chan, Christine"	"Fundamental studies and their applications for ontological engineering and decision support systems for energy production and pollution processing systems"	"Concerns about Canada's laggard productivity have been widely reported in the national media.  Given the importance of the oil and gas industry in Canada, the significant socio-economic impacts and costs of declining productivity in the industry requires urgent attention from the R&D sector of the economy.  It has been suggested that the reduced productivity in Canadian industries is due to our relatively low investments in information and communications technologies compared to the United States (Globe & Mail, Sept 9, 2005).  The proposed research program will address this concern by enhancing understanding of issues and problems related to development and application of advanced information technologies for improving productivity of industrial processes.  This program aims to build infrastructure for supporting construction of knowledge-based decision support systems, which assist operators and managers in performing complex tasks in industrial processes.  The research objectives include: (a) developing methods and tools for ontological engineering, which support building the knowledge foundations for intelligent decision support systems, and (b) developing artificial intelligence and knowledge-based system technologies for modeling and analysis of energy production and pollution processing systems. The advanced computing solutions developed will help enhance productivity and efficiency of some petroleum- and carbon dioxide-related processes in Canadian industries, which are in urgent need of cost-effective means of improving productivity.""386951,""Chan, Christine"
"337806"	"Chan, Edward"	"Moving object data management"	"With the technological advances in areas such as GPS, wireless communication and sensing technology, positions of moving objects can be easily tracked and recorded into a database. There are many existing or potential applications that require a database system with moving object management capability, but no such systems exist at the present time. Consider an application in which a fleet of trucks or taxis are monitored, and the traffic information on a road network is updated constantly, possibly in real-time. Customers may ask for a pick-up, and vehicles need to be dispatched. It would be desirable to locate the closest vehicle for the pick-up, and be able to find the optimal route to deliver the client, within certain time, to the destination. Consider an inventory control system in which each item in the inventory is tracked. One may want to know, for instance, the current location of certain items, or the movement of an item in certain period of time, or what items have been moved in the past number of hours, or what items have been removed from a warehouse in the past twenty days. Another potential application is an insurance application where the premium of an insured vehicle is based on the location (which areas) and distance it traveled. This problem requires a database with information on the movement of vehicles and road networks to determine the premium of a vehicle. Yet another application could be an air traffic control system in which planes are monitored so that they will not, at any time,  be too close to each other, or too close to certain air space.  To make all these applications a reality, database systems with moving object data management capability are needed.   Data on moving objects are inherently huge, and it is non-trivial to extract information efficiently from these data. We propose to investigate the design and implementation of indices and query evaluation algorithms for moving object databases. The objectives of this proposed research are to advance database technology and to widen its application to other non-traditional applications such as location-based services, inventory control systems, real-time traffic information systems, fleeting management systems, monitoring, graphic and simulation systems.""343364,""Chan, Esther"
"338888"	"Chan, WaiYip(Geoffrey)"	"Robust and quality-assured audio-visual communication"	"Auditory perceptual modeling of distortions plays a vital role in enabling today's widespread utilization of compressed speech and audio. Every voice conversation over a cellular telephone or download of MP3 music entails the use of perceptual-model based measurement of distortions.  When a speech or audio signal is compressed into a digital format, a distortion model is used to determine where in the signal to ""hide"" distortions to make them least perceptible. We propose to research using a new generation of perceptual models to build more powerful tools for speech communication and audio distribution over emerging and future network configurations.   One goal is to build machine intelligence to enable computers to rate the quality of a speech signal in the same manner as a panel of human listeners.  This would enable communication network elements and electronic devices to better optimize resource usage in accordance with human-perceived speech quality. Another goal is to devise new digital formatting algorithms that would enable efficient and reliable distribution of audio to a very large number of clients. The format will be very flexible so that the user-perceived audio quality at each client is the best possible given the particular quality of the client's network connection.  One anticipated benefit is that high-fidelity audio net-casting can reach a much greater number of users.""336185,""Chan, Warren"
"338885"	"Chapman, Glenn"	"Laser applications in large area optical sensors and biomedical imaging"	"This research focuses on methods of enhancing detecting arrays and their detection capabilities. These have the general connecting theme of being optically connected methods, especially those making use of lasers, in either sensor operation or fabrication. This program extends four projects started under my previous NSERC discovery grants. The first creates sensing arrays which are very immune to defects. This explores a set of methodologies such as the application of Fault Tolerant Active Pixel Sensors, software algorithms to detect defects and identify the defect type from images taken under normal operating conditions, and algorithms to recover array information from knowledge of the defect type. This enables the building of very large area sensor arrays (yield improvement) and extends imager lifetime, especially in hostile environments where failure rates are high. The second creates optical imagers, based on Active Pixel Sensor (APS) concepts, that allow detection in ways that existing imagers cannot. Previous APS research has pointed us to a number of new detectors: phase sensitive detection of encoded illumination, better resolution by removing the loss of APS sensitivity as pixels shrink below a few microns, expanding dynamic range for imagers, enhanced photogate APS' which, unlike current devices, are highly sensitive in blue light. The third explores a new approach, called Angular Domain Imaging, to optical tomography in scattering media such as tissue. This employs lasers and micromachined collimating arrays combined with optical imaging arrays to separate the highly scattered light from the unscattered or slightly scattered light which carries internal structural information. Unlike existing systems, this methodology lends itself both to highly scattering media of short distances (eg. medical imaging) and lower scattering over longer distances (fog, or turbid water). Finally in the previous NSERC discovery grant, we developed a new inorganic resist Bi/In, which oxidizes on laser exposure to a transparent conducting oxide. This laser direct write oxidation allows us to explore this material's application in electronic devices, such as new types of X-ray detectors and transparent transistors.""330376,""Chapman, Hanah"
"337808"	"Chau, SiuCheung"	"Dependable optical multistage interconnection networks"	"Multistage Interconnection Networks (MINs) are used in multiprocessor systems to connect processors with other processors or memory modules.  These networks provide a compromise between networks of low latency and high cost such as the crossbar, and networks of high latency and low cost such as the shared bus.  Moreover, MINs can be pipelined to provide a bandwidth that is comparable to that of the crossbar for specific traffic patterns.  A MIN with electro-optic switch elements (SEs) offers a higher channel bandwidth and lower communication latency than the electronic MIN.  A large body of work has been conducted on the structure, operation, and performance of Optical multistage interconnection networks OMINs.  However, two paths, sharing a switch in an OMIN can experience some undesirable coupling from one path to another within the switch. This is called optical crosstalk.  Due to crosstalk, traditional routing and scheduling algorithms that are used for MIN implemented by electronic switching are not applicable for OMIN.       In this research, we will consider how to schedule and route messages through an OMIN efficiently. We would also try to come up with new OMIN designs that are better than previously proposed designs. We will extend our investigation to other communication patterns in OMINs such as multicasting, broadcasting, all-to-all personalized exchange, and gossiping.  We will also look at the fault-tolerant aspect of OMIN.  That is to try to investigate different OMIN designs that have built-in redundancy to tolerate switching element faults and processor faults, and to come up with dependable communication algorithms and dependable OMIN that can tolerate faults.""335518,""Chau, Thomas"
"338925"	"Chen, Chunhong"	"Circuit-level analysis and design methodologies for nanoscale integrated systems"	"With the rapid progress in semiconductor technology toward nano-scale electronics with smaller dimension and less power, the circuit designers of single-electron tunneling (SET) devices and circuits are facing two unique challenges: (a) efficient performance evaluation, and (b) advanced design methodology. The fast performance analysis is needed for large circuits where the existing simulation is computationally prohibitive, while the investigation of advanced design methodologies is required to deal with such special problems as reliability and adaptability with the technology. The primary goal of this proposal is to meet the above challenges. First, analysis of SET circuits is much difficult due to the stochastic nature of the tunneling events within the device and their strong correlation. We are going to tackle this issue by developing new models for both delay and power dissipation with SET networks. Secondly, we will look at design methodologies toward highly reliable and adaptive SET circuits. Reliability is a key issue because of random background charge (RBC) problem. Adaptability is important as SET circuits are designed typically in a bottom-up fashion. We will focus on the combination of SET technology and neural networks. Our circuit-level solution to the reliability problem is to use hardware redundancy. To address the adaptability problem, we will borrow some ideas from biological systems and implement certain learning algorithms in the neural networks. Finally, we will apply the proposed techniques to real circuits for image processing. In the long run, we are going to develop computer-aided design (CAD) tool support for SET circuit/system design. This will eventually lead to various applications (such as quantum computing, automotive electronics, and medical diagnosis systems) of SET or CMOS/SET hybrid integrated circuits, contributing to rapid growth of next-generation electronics industry nationwide in the foreseeable future.""341564,""Chen, DaleZhuDong"
"338430"	"Chen, Liang"	"Theory of multi-level electoral college for multi-candidte elections and electoral college based face recognition surveillance and intelligent textual information retrival system"	"On the theoretical front, we will study the stability of the multi-level Electoral College voting system for multi-candidate elections. Its simplest form, the two-level Electoral College, has been used many years in US presidential elections, and applied in areas such as image processing, pattern recognition, information retrieval, and other political elections. Our model will be the first in AI research that illustrates the stability of multi-level Electoral Colleges with respect to region/prefecture size and number of levels. Our theory will guide users of the applications about choosing an adequate number of levels, as well as an adequate region size for each level, when using a multilevel Electoral College to improve the performance of pattern recognition and information retrieval approaches. Developments made on the theoretical front will be applied in two important fields. (1) A face recognition surveillance system will be developed. This system will combine proven active infrared illumination technology for pupil location with a mathematical approach developed in this research program to match unknown faces to known faces on watch lists. The system is expected to be a significant improvement compared to any appearance-based face recognition system in recognition rate vs. false acceptance rate, and being disguise-proof. (2) An intelligent textual information retrieval system will be developed. This system will employ a novel approach, where a document is treated as a physical object which can be described from different ""view angles"", for document representation so that each document can be divided into blocks and blocks be further divided into subblocks. A vector space model will be employed as the basic strategy for matching a query with each lowest level ""block"" in a document, and an Electoral College will be employed in higher level blocks. The system is expected to have much better performance in precision vs. recall compared to other approaches. These systems can be further developed for use in national security systems and internet intelligent document retrieval systems respectively.""336425,""Chen, Liang"
"337831"	"Chen, XiaoJun"	"Advancing the applicability of model checking and FSM-based testing techniques to complex software systems"	"With the widespread use of the Internet and web applications, software systems are becoming more and more pervasive in our society. To make sure we can produce highly reliable, secure and more sophisticated software systems to meet our increasing demands and with lower cost, we need effective software engineering methods and techniques applied during the software development for the software quality assurance. The advance of the current state-of-the-art methods and techniques for the correctness and reliability of our software products lies heavily on methodologically sound solutions. In this regard, formal verification and formal testing techniques have made significant contributions to the establishment of the theoretical foundation of software development over the past two decades. However, despite of the great effort we made, there is still a big gap between the theory and its practical application to most of our real life software systems. Many essential issues are still left to be resolved. The proposed work is to address some of these important issues, with focus on significantly improving the applicability of existing formal verification and formal testing techniques towards large-size applications. In order to apply formal verification and formal testing techniques, it is essential that we have the system under consideration properly modeled. The proposed work also includes the formal modeling of some widely used and complex systems such as web systems, middleware systems, and web services. The difficulties of the work come from the fact that these systems are both complex and dynamically evolving. On the other hand, they are gaining increased popularity, and their advances indeed represent the emerging new technologies. Applying formal verification and formal testing techniques on these systems are extremely important as they are strongly affecting the quality of our daily life.""326278,""Chen, Xiongbiao"
"337902"	"Chen, Yuanzhu"	"Scalable ad hoc and sensor networks"	"While technology cannot prevent disasters such as earthquake or tsunami, mobile ad hoc networks (MANETs) promise significant benefit to forecasting these and to the disaster relief afterwards.  A MANET is a wireless communication network that does not rely on existing infrastucture and supports mobile users.  Since nodes in such a network are not required to be able to communicate with each other directly, a MANET is usually multi-hop and intermediate nodes are often needed to relay messages between nodes in the network that are far apart.  In addition to applications such as disaster relief and rescue tasks, wireless sensor networks (WSNs) is one of the most implementations of the mobile ad hoc networking technology.  Despite many research activities in this area, the most significant factor hindering wider deployment of large MANETs is the scalability issue.  That is, the percentage of network bandwidth and device energy consumed by user data, and the per-node throughput decrease dramatically and approach zero as the number of nodes in the network, the node density, and movements of node increase.  To address the scalability issue, the approach employed in this project is to introduce a hierarchy among the network nodes to help overcome these problems, whether using device heterogeneity or superimposing logical structures.  This involves constructing a hierarchy with desired properties in a localized distributed fashion and maintaining it when the network conditions change.  The goal is to have a hierarchical structure that is easily maintained and will allow network protocols to scale well without consuming too much of the limited network resources.       To solve these problems, this project will resort to both analytical and experimental methods.  In particular, mathematical optimization and approximation will be used to formulate and solve these performance optimization problems.  On the other hand, to overcome the possible inaccuracy of analytical models, computer simulations will be used to verify the protocol performance.  More realistic and faithful performance evaluations will be carried out by the use of testbed networks that execute various protocols under study.""339838,""Chen, Yuming"
"338966"	"Cheng, Julian"	"Interference analysis and mitigation for digital wireless communication systems"	"Performances of mature wireless communication systems are limited by fading and interference. These limitations are degraded voice and data services, frequent call drops, and smaller number of users that service providers can accommodate. Fading is caused by multipath propagation of radio signals. Interference is caused by multiple wireless users occupying on the same channel or by imperfect transceiver design in communication systems. Mitigation of fading and interference will improve the quality of wireless services and increase the user capacity. Therefore interference study in fading environment is an important research topic. This research proposal addresses the analysis and mitigation of interference in fading environment for broadband wireless communication systems. The study will focus on the following three areas: (1) Asymptotic error rate performance analysis of practical modulation schemes in fading and interference. (2) Development of near-optimal practical receiver capable of interference mitigation for wireless applications. (3) Development of robust data detection scheme capable of interference mitigation for the next generation wireless networks. We expect to obtain better and deeper understanding of the role of interference in fading environment so that we can improve the design of wireless communication transceivers. Such improved design will allow service providers to provide more users with reliable and affordable high data rate wireless services.""344663,""Cheng, Leslie"
"338903"	"Cooperstock, Jeremy"	"Enhanced video for shared reality environments"	"In order to bring videoconferencing technology to a new dimension of quality, we are developing the Shared Reality Environment, which supports high-bandwidth, high-fidelity telepresence.  The environment consists of two rooms with video projection, cameras for video acquisition, multichannel sound system for spatilalized audio rendering, and a vibro-mechanical platform to reproduce vibrosensory stimulus at the floor level. Unlike conventional videoconference environments, Shared Reality provides users with a sense of video and audio immersion in a common, shared space.  Remote users appear as if part of the local environment, and can manipulate synthetic (computer-generated) objects in the view.   To improve the quality of Shared Reality experience, this research now seeks to utilize additional cues available from input video in order provide faster, higher-resolution displays that do not suffer from shadows cast by users who block the projector output, and can render views that appropriate to the current participant position(s).  Some of the techniques employed include image mosaicing, robust tracking and gesture acquisition, dynamic background segmentation, and depth estimation from stereo.   Applied together, this research has the potential to support the demands of distributed immersive visualization, such as required by groups of engineers working collaboratively on a complex problem that requires interaction both with each other and the synthetic model.""341180,""Coops, Nicholas"
"338352"	"Corneil, Derek"	"Algorithmic graph theory"	"A graph is a mathematical object consisting of a set of nodes together with a set of edges, each joining a pair of nodes.  Graphs can be used to model many different types of networks, for example Protein-Protein Interaction (PPI) networks (the nodes are proteins and an edge between two proteins indicates that the proteins interact).  The first job of a graph theorist is to determine the type of graph that occurs in practical instances of the application.  In the case of PPI networks, we have recently shown that the model that most closely captures the currently available biological information is geometric random graphs, not scale-free graphs as presently accepted.  Once a good model has been identified, the next step is to design fast, easily implementable algorithms to solve various problems on the particular model.  Such algorithms often have to work on huge data sets, as anticipated for PPI networks. My research includes the study of restricted families of graphs that arise in practice and to design fast practical algorithms.  One of the fundamental algorithmic notions in graph theory is searching, i.e. following a particular algorithm to visit all nodes and edges of the graph.  Many graph algorithms include some aspect of graph searching, and we've recently determined that many other problems can be solved most efficiently using multi-sweep search algorithms.  Our work on fundamental graph searches has shown that these searches all possess similar characterizations.  We hope that these characterizations will lead to new simpler algorithms.""329847,""Cornet, David"
"338956"	"Coulombe, Stéphane"	"Multimedia content and transport adaptation for real-time telecollaboration applications"	"Many emerging applications and new business realities are relying on real-time multimedia telecollaboration technologies. The latter allow employees in distributed organisations to not only hear and see each other, but also to share (view and edit jointly) documents, specialized applications, etc. Telecollaboration is also important for other applications such as tele-medicine, distant learning, and distant social relationship. However, there is no system currently ready to be used by the general public for everyday communications as developing robust telecollaboration applications especially for pervasive environments is very challenging. The main objective of this research program is to develop novel core technologies for multiparty telecollaboration applications capable of offering a high level of control, quality and interoperability between diverse, and possibly incompatible, user terminals (such as PCs, mobile phones and Personal Digital Assistants (PDAs)) and networks. The research program will first aim at developing innovative (e.g. distributed) architectures for multiparty telecollaboration applications capable of performing real-time transparent media content and transport adaptation. It will then investigate rich session and floor control, as well as transcoding request protocols for centralized and distributed multiparty telecollaboration applications. Subsequently, it will focus on the development of dynamic media and transport adaptation strategies and algorithms to provide the best performance for each end user under various terminal capabilities, network conditions and session events (e.g. departing/joining users).  Finally, we will explore inter-modal adaptation (e.g. text-to-speech) and augmented reality as part of the transcoding process to enhance the usability of multiparty telecollaboration applications. An adaptive telecollaboration prototype including all the technologies resulting from this research program will be developed.""325651,""Coulombe, Sylvain"
"338976"	"Cretu, Edmond"	"SYMBIOM (Synergic inertial microsystems for bio-medical applications)"	"The present research program is related to the development of an inertial sensor cluster for navigated surgery, complemented by other intelligent surgery tools. Medical navigation systems are mainly used for monitoring the position of surgical instruments relative to the body of the patient. Present optical navigation systems have a limited applicability (e.g. in neurosurgery), hindered by the high cost and the need of a direct line of sight between the surgical instrument and a video-camera. A micro-inertial measurement unit (IMU) attached to surgical tools or bones can track their position, and improve the global performance. Increased complexity surgical applications will become possible, e.g. bone repositioning in case of facial fractures and osteotomies of craniofacial segments. Several innovative techniques are envisaged, in order to increase the resolution: parametric amplification of mechanical motion, operation in a controlled regime on the stability border, and the use of modern sensing architectures (displacement sensing by optical interferometry or displacement to frequency conversion techniques). The expected performance improvements will widen the application field of computer-assisted surgery (CAS) systems, and complement existing research developments in Canada, focused on other CAS subsystems. In particular, the increased resolution will allow better synchronization with 3D maps of the human body, for automatic or assisted guidance of the tip of surgery instrument inside the patient. Future research will expand the application of these IMUs to medical robotics and intelligent surgery tools. An increased functionality will target active position correction, or cancellation of undesired motion components (e.g. hand-tremor cancellation). This approach will require a more adaptive sensing scheme, with real-time reconfigurable transfer functions.""336957,""Creuzenet, Carole"
"338354"	"Culberson, Joseph"	"Investigations into the nature of hard problems"	"Many optimization problems require a search over vast sets of possible solutions for the best solution. Examples range from the scheduling of crews to planning to solving combinatorial puzzles. These problems are frequently very difficult to solve because they involve what is called ``the combinatorial explosion''. Basically, this means that increasing the size of the problem a little bit, say from 100 units to 110, may cause a hugely disproportionate increase in the number of cases that have to be considered. My research attempts to understand the issues inherent in developing effective algorithms for problems of this type.  Even when the possibility of exponential blow-up lies within a problem, many large instances of the problem may be easy to solve in practice. Trying to understand what distinguishes the hard from the easy instances is one of the approaches to developing better algorithms.""333395,""Culham, Jody"
"338364"	"Czyzowicz, Jurek"	"Applied graph algorithms"	"Algorithmic graph theory has numerous applications either directly in practice or in other research fields. In this project I propose to deal with some graph theory algorithmic problems arising mainly in the fields of computational geometry, formal languages and distributed computing.         Graph drawing is an important domain, related, besides computational geometry, to the fields of knowledge representation, graph theory, human cognition and many others. The fundamental question here is how to visualize a graph representing structural knowledge, so that a human confronted with this visualization will easily perceive some of its important properties, for example in order to make critical decisions fast. I would like to design the drawing algorithms for some classes of graphs.         A motion of polygonal objects in the plane may be represented by geometric graphs. Their mobility within the plane environment, without interfering with obstacles, was studied in robotics, motion planning and grasping. In order to immobilize a polygonal object, motionless obstacles are placed around its boundary, preventing any rigid movement. I propose to study optimal immobilizations of polygonal objects with a set of points, according to different criteria of optimality.         One of the approaches to data packet classification uses finite state automata. An automaton reads an input string and stops in the final state corresponding to the result of this classification. Several problems concerning such automata transform to graph-theoretical questions related to some classes of graphs. In my research, I will study some algorithms for this class of graphs, important in practice, since this portion of my project comes directly from an industrial application.         Several problems in distributed computing relate to graph investigation by use of sets of collaborating mobile agents, walking along the graph. I intend to study models of graph investigation by mobile agents for some classes of graphs.""325573,""Daayf, Fouad"
"338340"	"Dahl, Veronica"	"Constraint based and hypothetical reasoning for human and molecular biology languages"	"Recent advances in the computational study of language grammars and rules, a branch of Artificial Intelligence, have the potential for practical, real-world applications. Most notably, the discovery of the human genome, partly through methodologies I helped develop (logic grammars), has highlighted the potential benefits of viewing molecular biology problems as specialized linguistic problems: DNA can be seen as a language with an alphabet of four letters (the nucleotides A, C, T and G), and  the genome can be viewed as the 'book of life'.      The proposed research aims at extending our understanding of not only human languages, but the language of life, by further interleaving these fields. I will explore the potential of promising methodologies for language processing, in particular HYPROLOG, our novel logic programming language which extends the possibilities of classical logic by allowing the user to explore ""what if"" scenarios, and to hypothesize possible explanations using abduction (the unsound but useful inference of p as a possible explanation for q given that p implies q).      I will also further explore a promising new linguistic paradigm, that of Property Grammars, which allows incomplete and even incorrect input to be parsed. Research will continue on my recently developed parser that invisibly endows descriptions made by non-computer specialists (e.g. linguists) with direct and automatic, while efficient, executability. Semantic aspects, and relation to current computational linguistic theories, will be examined in the medium term. In the long term, this research is expected to result in industrially useful expert systems for linguists, and perhaps even to have an impact on linguistic theory itself.      Finally, I will continue cross-fertilizing the natural language processing field with that of knowledge based systems, focussing on knowledge discovery. My recent research has uncovered the usefulness of my methodology for parsing property grammars as applied to knowledge discovery, in particular, for extracting concepts from arbitrary text found in the internet. This has many interesting short-term potential spin-offs, e.g. applications to phylogenetic classification.""327229,""Dahman, Yaser"
"337845"	"Dargham, Joumana"	"FSM behavioral modelingapproach for hypermedia web applications"	"In the period between 1995 and 1998, websites were almost entirely composed of static html files. They were 100% interfaces but in 1998 the percentage decreased to 90% and in year 2002 they constituted less than 50% of those available on the Internet. This percentage and the complexity of the web applications are inversely proportional. Interfaces websites are in continuous decrease and the complexity of web applications is in a continuous increase. Nowadays, web applications are powered by distributed software implemented using a myriad of languages like HTML, XML, JavaScript, etc. This heterogeneity encouraged the researchers who have had a certain experience with software engineering to explore the WWW and thus they proposed a new approach called Web Engineering. Web engineering is not a perfect clone of software engineering but it borrows many of its fundamental concepts and principles emphasizing the same management and technical activities. Its work products lead to analysis models, design models and test procedures. Within this context many efforts have been addressed towards defining design models for web applications to implement reusable and well structured web applications. The newly proposed approach is the fruit of such efforts. For instance, the FBM-HWA approach falls within the basket of design models.  It is not purely based on an object oriented concept and does not refer to UML for notation purposes , as it is the case in most existing approaches. It models the behavior of web applications using Finite State Machines (FSMs) and simplifies the task of developers and testers at once by providing them with a total overview of the application.  We believe that by modeling web applications with FSMs, it becomes possible to create models that present deterministic responses for unanticipated actions in almost any condition or situation.""324550,""Darling, Emily"
"337821"	"Debbabi, Mourad"	"A game theoretic paradigm for the design and analysis of security protocols"	"The tremendous success of Internet related technologies (web services, voice over IP, mobile telephony, etc.) coupled with advances both in hardware and software engineering will invigorate the existing proliferation of laptops, desktops, servers, Internet-enabled and mobile devices. This will allow for new services and applications that will recede increasingly into the background of our lives. This information systems era raises challenging and very interesting research problems. In this research program, we propose to address a well-recognized challenge in information systems that is security. Actually, security is emerging as the most important challenge that has ever faced computing research. Individuals, corporations and organizations are relying more and more on information systems to transmit various types of information (text, audio and/or video) with various levels of sensitivity. In this context, information must be protected against any malicious attempt that may affect its secrecy (by leaking sensitive data), integrity (by corrupting information), authentication (by impersonating authorized principals), availability (by denying service to legal users), etc. Security (or cryptographic) protocols are a cornerstone in securing communication networks. They use cryptographic primitives in order to enforce security objectives. Nowadays, cryptographic protocols are heavily deployed in information systems: Secure Socket Layer or SSL (for Web services), Kerberos (for authentication services), SET (for e-payment), Secure Shell or SSH (for secured remote access), etc. Their design is very subtle and many cryptoprotocols have been proved flawed several years after their deployment and use. The primary intent of this research proposal is to elaborate a practical framework that is based on solid theoretical foundations for the design and analysis of cryptographic protocols. This framework will be used to: (1) Describe security protocols, (2) Ascribe semantics to protocol behaviors, (3) Express security properties, (4) Verify cryptographic protocols against security properties, (5) Derive attack scenarios in the case of flawed protocols. The theoretical foundations of this framework will be rooted in a game theoretic setting.""336379,""DeBell, Keith"
"338360"	"Dehne, Frank"	"Coarse grained parallel algorithms"	"The Coarse Grained Multicomputer (CGM) model, which was proposed by the applicant, provides a simple and practical model to analyze the performance of parallel algorithms, in particular for processor clusters. The CGM model has attracted considerable attention (e.g. two special issues of Algorithmica) and the applicant has been at the forefront of research showing that significant speed improvements can be achieved, theoretically AND in practice, through the use of CGM algorithms. During the last funding period, we provided a general algorithmic solution for parallel external memories (solving a challenge posed at the ACM Workshop on Strategic Directions in Computing), and we presented efficient CGM algorithms for fundamental graph problems, Computational Geometry and dynamic programming. We built the first parallel software prototype that can build data cubes, a central data structure for data warehousing/OLAP, at a rate of more than one TB per hour. Our study of CGM algorithms for parallel k-vertex cover led to the first parallel software prototype that can identify erroneous genome or protein sequences in multiple sequence alignments for input data sets with more than 1000 sequences. The following are the main thrusts of the proposed research for the next funding period. (1) Parallel caches: We propose to extend our solutions for parallel external memories towards the study of efficient and scalable CGM algorithms that utilize multiple, parallel caches. (2) Parallel MDX queries: In order to provide parallel support for the analysis of large multidimensional data sets, we propose to add to our parallel data cube construction methods a full set of CGM algorithms and software prototypes that support parallel MDX queries on data cubes. (3) Parallel fixed parameter tractability for bioinformatics: We propose to conduct a comprehensive study of possible parallel CGM algorithms (and software prototypes) for NP-complete problems in bioinformatics that are fixed parameter tractable. (4) Parallel protein interaction prediction: We propose to develop an efficient, scalable and practical CGM algorithm and software prototype for the problem of predicting the probability of an interaction between two proteins.""334760,""Deibel, Donald"
"338413"	"Desharnais, Josée"	"Formal methods for labelled Markov processes"	"The general goal of my research is to establish formal methods for reasoning about concurrent probabilistic processes.  In particular, I am interested in processes that have a continuous state-space, a model that we call Labelled Markov Processes (LMP).  Possible application  areas include communication protocols and process control software.  An example of the latter is a controller for a system like a vehicle. I want to continue develop interactions of the theory of LMPs with other fields, but my main goal is to develop the theory along two generalizations of LMPs.  In a recent past, we have defined pre-LMPs, a generalization of LMPs by weakening a property on their probabilistic structure;  this resulted in working with notions from capacity theory, a theory developed for economic mathematics.  This model is very promising for other goals, as it may represent - a way to  automatically generate a model from a set of properties. - a way to reason about non determinism, in a probabilistic setting as well as in a more traditional one, without probabilities.   This is interesting since non-determinism has been  a very important subject that has raised debates more than once. - a way to complete a categorical view of LMPs which, until now, has been stopped by the problem of existence of pullbacks when considering relations between LMPs.  We believe that these pullbacks may exist in the more general category of pre-LMPs. Another generalization of LMPs is with domain theory and  functional analysis.  We want to propose an abstract version of LMPs, by taking the point of view of linear functionals rather than probabilities (following a suggestion of Lawvere which has since proved very efficient in the context of domains and valuations).""337516,""Desharnais, Jules"
"338957"	"Devabhaktuni, Vijaya"	"Computer-aided high-frequency modeling and design"	"The competitiveness of the high-frequency electronics industry including RF/microwave and wireless sectors can significantly contribute to our economy. For instance, Canada's wireless industry generated $8.2 billion in revenues in 2003, and today more than 25,000 Canadians work for the industry. Research in computer aided design (CAD) arena leads to key vehicles for design and optimization of high-frequency circuits, which find applications in communications, consumer electronics and satellite links. What makes CAD of these circuits challenging? Electronic circuits operating at Gigahertz frequencies emulate full-wave electromagnetic or EM behaviours. High-frequency effects, e.g. EM coupling, if not considered during the CAD phase, could lead to prolonged design-revision and time-to-market cycles thereby causing substantial economic fallouts. The proposed research is geared towards enhancing traditional CAD tools and their low-frequency oriented model libraries. Teaching artificial neural networks (ANN) to learn the EM behaviours, followed by elegant integration of EM-ANN models into the CAD-tool libraries will remain an active thread. Specifically, this proposal focuses on development of cost-effective high-frequency modeling tools using adaptive sampling, exploration of rapid interconnect optimization tools, formulation of flexible CAD models representing EM coupling between on-wafer passives, and investigation of CAD tools for design of filters used in satellites. The most direct outcome will be invention of novel computer aided modeling, design and optimization tools for circuit designers and CAD-tool users in industry. This work has a potential to increase design efficiency, shorten design cycles and time-to-market, enhance product quality, and decrease consumer costs. Personnel trained by this research will constitute highly skilled workforce for our industry. These factors could lead to maintaining the competitiveness of the Canadian high-frequency electronics industry in the global market.""386222,""Devabhaktuni, VijayaKumar"
"338432"	"DiPaola, Steve"	"Intelligent virtual character systems"	"Recent developments in multimedia hardware and software have made it possible for many systems to incorporate audio-visual components. On the other hand, the human face as a communication object plays a major role in our interactions. This demands a special focus on computer-generated face animation as a specific area of research in multimedia systems and computer graphics. Among possible applications, communication, web-based services, interactive agents, computer games, and visual effects in movies can be mentioned. Our primary area of research is social user interfaces, i.e. a certain category of human-computer interaction where computers can show socially meaningful, verbal or non-verbal, behaviours such as gestures and expressions. This Discovery Grant is to support our long range research project - a comprehensive environment for interactive 2D and 3D face animation which uses a knowledge-based approach. This approach gives us a multi-dimensional parameter space language of hierarchical facial expressions, emotions and behaviours; creating a facial environment that uses a natural and complex knowledge representation of the face (and face expressions). To achieve these goals our development system uses component-based streaming architecture, and Face Modeling Language (FML), an XML-based language compatible with MPEG-4 XMT framework, and is called iFace.  iFACE is based on the idea of Face Multimedia Object (FMO), designed for a variety of face-centric applications.""343506,""DiPenta, JillianCatherine"
"338871"	"Dodds, David"	"Communication electronics for telephony and internet access"	"The FIRST proposed work is to refine the wideband frequency domain reflectometer (FDR) method developed by former students.  This method will allow telephone companies to more easily identify and locate obstructions in telephone lines that interfere with Internet access. The new capability of measuring reflection angle can help characterize the fault and facilitate rapid repair in urgent cases such as water in the cable. This development has been done at TRLabs with the help of SaskTel (first in North America with commercial high-speed Internet). A Canadian company ( www.consultronics.com) has incorporated software FDR in their top-of-the-line instrument and plans to implement a much more capable hardware version. The SECOND proposed work applies a spread spectrum synchronizer, built in a VLSI chip by former students, to the problem of 911 cellular telephone location finding. Recent US regulations have triggered considerable research and there has been much consideration of GPS technology. Within buildings, some satellite signals are reduced by several orders of magnitude making detection difficult in the presence of high thermal noise and full-strength signals from other satellites. We anticipate that our synchronizer will be capable of meeting this objective. A THIRD proposed work uses the same VLSI synchronizer and aims to combine the efficiency of packet transmission with the efficiency of code division multiple access (CDMA). Current research on packet CDMA assumes a lengthy preamble for the essential codephase synchronization.  We propose to eliminate the packet preamble since our synchronizer can operate in the presence of both data modulation and Doppler shift.""329505,""Dodds, Shawn"
"338967"	"Doucette, John"	"Design and operation of highly reliable telecommunication networks"	"Physical failure of a single telecommunication network component such as a switching node, or even more likely of a fibre cable, can be disastrous, affecting a vast array of business operations and even emergency systems (911). Such failures are surprisingly frequent in large networks even in normal times, and recent concerns about terrorism and deliberate attack or sabotage only heighten the importance of methods to ensure network reliability and availability. Much of the current work done on the network survivability problem generally assumes that, although failures can not necessarily be predicted, potential failure scenarios are known and specifiable in advance (say, individual fibre cables, or in some cases, combinations of no more than two or three at a time). However, determining how the network can be optimally designed and then dynamically reconfigured in the event of arbitrarily widespread and catastrophic failure to maximize survivability, particular of critical services, is still very much an open issue, and is the topic of my proposed research. This research will extend current survivability principles to allow for disaster recovery where possible, and develop new methods to deal with aspects of disaster recover that cannot be accommodated with conventional protocols. More to the point, we will seek to develop and demonstrate a suite of survivability mechanisms that are able to recover from arbitrarily widespread network failures and can provide a coordinated network-wide reconfiguration response to maximize survivability of critical services, with minimal additional capital investment. Distributed self-organizing path restoration is particularly well suited to the type of global reconfiguration needed here, and several emerging network survivability approaches (node-inclusive span restoration, failure-independent path-protecting p-cycles, node-encircling p-cycles) also show promise in this regard. The focus of this research will involve the modification of those existing protocols and development of new ones to allow for a coordinated network-wide response to a catastrophic failure, particular so that critical or emergency communication are maximally restorable.""333279,""Doucette, Tracy"
"337814"	"Dssouli, Rachida"	"Communication software engineering based on formal methods"	"The quality of reactive systems is of prime importance to industry, public safety and security. Testing and validation activities represent a substantial percentage of the development costs of reactive systems. It is important to develop new methods, techniques and enhance, as much as possible, the existing approaches that are used in various activities that compose reactive systems' life cycle. The objective of my research is to enhance the quality of end-product and to reduce their development costs. The ultimate solution is to obtain the correctness of systems by construction, which means the derivation of specifications based on formal methods and the automation of subsequent development steps. Unfortunately, this solution is not yet available. Until then, the only way to deal with the quality of systems is to perform some partial modeling, partial validation and verification (partial proof using abstraction and inference) and testing.  More specifically, the aim of my research is to improve the development of reliable services over heterogeneous networks. There is a need for rapid development of services over advanced network platforms. The main challenge for service providers and network operators is the rapid provisioning of high quality services. The aim of the on-going research is to provide a service creation, specification methodology and prototype tools that assist designers with the creation, specification, validation, implementation and testing of new services in the context of their environment. The proposed methodologies will be based on the use of formal techniques. I believe that formal techniques are the key for fault detection, fault avoidance and automation of the development process. It is well known that design faults that are detected earlier in the development process will cost less than the same faults detected at the maintenance step. For these reasons, my research will focus on specification, design and test steps of complex systems. The keyword ""Quality"" is the link between all my research projects.""335386,""Du, Donglei"
"338958"	"Dumitrescu, Sorina"	"Algorithms for optimal design of multiple description source codes"	"The advent of modern communications raises new challenges for multimedia transmission. One of these challenges, encountered in both wired and wireless networks, is the necessity to alleviate the impact of packet loss. A paradigm to effectively solve this problem is multiple description coding (MDC). The basic idea in MDC is to create several descriptions of the signal such that the source can be reconstructed to a certain fidelity from each individual description, and when more descriptions are available, they can refine each other, leading to a higher fidelity reconstruction. The main objective of the proposed research is the development of efficient algorithms for the design of optimal multiple description source codes. The emphasis will be on two approaches for MDC: multiple description scalar quantization and uneven erasure protected packetization of scalable codestreams. Beside developing new solutions (faster and/or of higher accuracy) to previously formulated optimization problems, the applicant will propose innovations to the existing frameworks that offer the promise of higher performance. The proposed research will lead to new techniques for robust multimedia transmission over lossy networks, which more efficiently use the available resources in transmission rate. These techniques will be directly applicable in practice. Additionally, this research will bring new insights in the area of optimal MDC design, thus contributing to the advancement of knowledge and understanding in the area and laying background for further research.""330286,""Dummer, Timothy"
"337795"	"Eager, Derek"	"The design and performance analysis of distributed and parallel computer systems"	"A key challenge in the Internet is that of providing efficient content delivery, such as of web pages, video files, and software packages for example, to large numbers of heterogeneous clients.  A converse problem is found in the context of wireless sensor networks, in which potentially large numbers of interconnected, power-limited computing devices are embedded in the physical environment.  Here a key challenge is that of efficiently collecting and extracting from the network the data that is captured at these devices.  The proposed research concerns the design and performance evaluation of protocols for content delivery and for data collection in large-scale systems, and can be divided into three main projects.      The first project concerns content delivery in the Internet, specifically on-demand delivery of large, highly popular files that many clients concurrently request.  Of particular interest are scalable protocols that make use of file replication at multiple servers,  retrieval of data from other clients that have (all or a portion of) the requested file, and parallelism in the form of concurrent delivery of file data to a client from multiple sources.      The second project concerns data collection in large-scale sensor networks, specifically protocols for in-network aggregation of data as it is being forwarded across the network to the collection point(s).  Of particular interest are asynchronous protocols that determine when to form aggregates at each node adaptively rather than with fixed periodicity.  Issues include how to best construct aggregation trees, packet loss recovery, adapting to node failure, and rate control.      Finally, the third project concerns the advancement of performance evaluation methodologies, including both measurement studies and the development of new analytic modelling techniques, with particular emphasis on the application domains of content delivery and sensor networks.           Contributions from these three projects should significantly improve our understanding of, and ability to quantitatively assess, techniques for content delivery and data collection/aggregation in large-scale systems.""344192,""Eagle, Christopher"
"338904"	"ElGamal, Mourad"	"Systems and technologies for emerging wireless communications"	"The past 35 years have seen a very rapid advancement in technology on many fronts. This was constantly driven by increasing needs and expanding markets for higher performance and higher-speed products, applications, and services that we have come to depend upon for convenience and enjoyment. Integrated circuits (ICs), very large scale integration (VLSI) technologies, and microelectronics have played a key role in this process by providing very sophisticated, and often complex, signal processing capabilities at reasonable cost. These technologies will continue to be at the core of the growth of future and emerging systems for a wide range of applications, such as information and communications technologies (ICT), health care and pharmaceutical, aerospace, automotive, and energy and environment. New and emerging technologies such as very advanced wireless communication systems, micro electromechanical systems (MEMS), and nanoelectronics will augment existing technologies in order to enable the envisioned microsystems of the future. The objective of this proposal is to advance the state-of-the-art of the radio frequency integrated circuits (RFICs) technology, which is at the heart of the wireless revolution we are all experiencing, and is expected to be for years to come. To achieve this, we will capitalize on new and emerging technologies, such as RF-MEMS and ultra-wideband (UWB) communications schemes. The latter transmit information as very weak signals over a wide frequency spectrum. This is known to have low probability of detection by unintended radio systems, and to have excellent multipath immunity and less susceptibility to interferences, thus improving the quality of service. Also, it has potential for very low-power consumption and wider bandwidth, thus can enable longer battery lives and faster wireless communications.""327993,""ElGammal, Mohamed"
"338863"	"ElMasry, Ezz"	"Low-voltage and low-power analog integrated circuits"	"Analog integrated circuits (IC's) designers have been forced, primarily by economics, to develop circuits using the dominant digital Complementary Metal Oxide Semiconductor (CMOS) technologies. In CMOS the supply voltage continues to scale down and it is expected (in only couple of years) to be as low as 0.7 V in 70 nm CMOS technologies. Even if the process technology didn't require reduce supply voltages, the rapidly growing reliance on battery-powered microsystems (such as portable consumer electronics, computing devices, wireless communication systems, and biomedical implants) has led to an increase need for high performance ICs that operate under low supply voltage, consume low-power (LV/LP) and capable of high frequency operation. On top of this continuous drive for performance improvement, there is also the continuing march of Moor's Law, which drives future microsystems to be implemented in nanometer CMOS process. High performance analog circuits become increasingly difficult to implement in deep sub-micron CMOS due to the reduced intrinsic small signal gain and device headroom, making previously acceptable analog topologies and technologies obsolete. Therefore, new specialized design techniques, new topologies and new architectures are needed. This provides the motivation for this research project. It explores LV and LP analog IC's design techniques and develops new high performance analog devices for future microsystems implementations.""338531,""Elmasry, Mohamed"
"337855"	"ElSaddik, Abdulmotaleb"	"Peer to peer multimedia collaborative environments and web services"	"Multimedia Communications is the field that concerned with the representation, storage, retrieval, and dissemination of machine-process-able information that is expressed in multiple media, such as text, voice, graphics, audio, video, haptic, and 3D scenery. Collaborative Multimedia Communications refers to an area in Multimedia Communications where several participants are concurrently engaged in a multimedia session. In that session, users can either interact with each other, collaboratively manipulating the media in the session (conversational session), or they can simply receive the information that is broadcasted (playback) to all participants, without any interaction capabilities (presentational session). The proposed paradigm integrates the Service-Oriented Architecture (SOA) of the emerging web services community with multimedia communication protocols with real-time characteristics such as Peer-to-Peer (P2P) communications. It is therefore our vision to develop an integrated framework comprising these technologies for a new ambient intelligent multimedia collaborative environment that will help overcome the above mentioned limitations and that will at the same time open the door to a wide range of new functionalities and novel applications.""339150,""ElSakka, Mahmoud"
"338879"	"ElTanany, Mohamed"	"UWB channel sounding and signal processing"	"Following the recent approval by FCC and other regulatory bodies around the world for the unlicensed use of Ultra Wide Band (UWB) technology, UWB devices are currently under development for the mass consumer market mainly for use in personal entertainment applications and for use in short range wireless peer-to-peer pico area networks (WPAN). Due to the low power levels allowed for UWB transceivers, the range of a communications type UWB transmitter is limited to about 10 metres. To extend the range of UWB, it has been proposed to use fibre backbones to interconnect different piconets in a building. This program is directed at identifying the fundamental issues surrounding the use of UWB technology for imaging and for  high speed communications and the inter-connection of UWB piconets through backbone fibres. The emphasis will be on UWB channel characterization, and development of smart signal design, signal processing and measurement algorithms to avoid the introduction of unneeded interference into conventional systems such as 802.11 WLANs, and to enhance the performance of UWB receivers in the presence of narrowband interference. The research to be done may be summarized as follows: a) Carry out  measurements to characterise the wireless UWB channel for UWB communications applications in terms of impulse responses and propagation path loss subject to various conditions such as different building types and also different interference conditions. The measurement results will be used to generate reliable stochastic models for the UWB channel. b) Combined channel and signal-to-interference ratio (SIR) estimation and data detection algorithms for use in coded, cognitive UWB transceivers. Our developments will be based on the measured channel characteristics, typical phase noise masks for economical optical modulators and detectors and measured co-channel interference levels. The sought after algorithms will target UWB waveform design for interference avoidance and will be suitable for incorporation in software-based UWB transceivers. c) Investigation of the statistical characteristics of UWB interference as viewed by narrow-band receivers.""336880,""Eltis, Lindsay"
"338939"	"EnNouaary, Abdeslam"	"Incremental testing of real-time systems"	"Real-Time systems are widely used in many critical domains such as patient monitoring, plant control, and air traffic control. The behaviour of such systems is time dependent. Indeed, the computation of real-time systems does not depend only on the input/output event from/to the environment and the data conveyed by these events but also on the time at which these events occur. The violation of such timing behaviour might have catastrophic consequences on both human lives and environment. To develop reliable real-time systems, formal methods should be used during the development life cycle. One of these formal methods is testing. It consists of executing some test cases against the implementation of the system with the intention of discovering potential errors. Those test cases are generated from the specification of the system under test. However, the specifications of real-time systems are generally complex. To cope with this complexity, certain system development processes foresee that the designers develop systems specifications and implementations incrementally. Moreover, the specification usually evolves over time due to changes of the user requirements or system environment. A possible approach for generating test cases from the modified specification is to consider it as a new specification and apply an existing testing method on it. Another possible approach is to generate test cases only for the modified part of the specification. The latter method is referred to as incremental testing. Incremental testing helps in saving time and space required for testing complex systems such as real-time systems. The objective of this research proposal is to develop incremental testing methods for real-time systems specified by Timed automata. All related issues will be investigated.""333055,""Enns, James"
"338358"	"Enright, Wayne"	"Robust and reliable software for the numerical solution of ODEs"	"We plan to analyze and improve the reliability and robustness of numerical methods for differential equations -- both ordinary differential equations (ODEs) and partial differential equations (PDEs).We also plan to analyze and develop effective tools for the visualization and verification of approximate solutions to differential equations. We will identify and quantify the key trade-off that arises between efficiency and reliability. Where possible we will implement methods which allow the user to select from a small menu of available error control strategies. The more expensive strategies will produce approximate solutions that are more likely to have an accuracy that is consistent with the prescribed tolerance."
"337839"	"Esfandiari, Babak"	"Peer-to-peer service composition management"	"The goal of this proposal is to investigate the benefits of peer-to-peer networks and multi-agent systems for the management of service compositions, i.e. distributed applications built dynamically with third party Web Services. The promise of such applications can only be fullfilled if there is a corresponding management infrastructure that: - monitors the correct composition and execution of the components; - detects violations of service level agreements; and - helps with the dynamic replacement of a service in case of failure. A centralized system to accomplish such tasks could lead to problems including: the creation of a bottleneck and single point of failure, the risk of bias and the lack of proper instrumentation. We propose investigating a distributed infrastructure to address the above issues. Nodes would then rely on a peer-to-peer network for the discovery of services and the evaluation of their reputation. The work will build on our experience with Web Service composition, peer-to-peer networks, feature interaction detection and trust acquisition.""344775,""Eshoul, Maysoon"
"337871"	"Ester, Martin"	"A clustering framework for the process of knowledge discovery in databases"	"Knowledge Discovery in Databases (KDD) has been defined as the process of extracting valid, novel, understandable and potentially useful patterns from large databases. The KDD process involves several steps, in particular focusing, pre-processing, data mining and evaluation, that normally have to be iterated to achieve satisfactory results. So far, most KDD research has focused on the data mining step, developing efficient algorithms for tasks such as clustering, classification and association rule mining. Unfortunately, not much research has addressed the other steps and the process as a whole, which has seriously limited the usefulness of existing data mining methods. In this project, we want to explore support for the entire KDD process in the context of clustering, one of the most important data mining tasks. The lack of support for all KDD steps is especially problematic for clustering due to its unsupervised, exploratory nature and because most clustering algorithms do not generate explicit patterns, but return clusters simply as sets of objects. The objective of this proposed project is to develop a framework for clustering supporting all KDD steps. Most existing clustering algorithms exploit only attributes of the objects to be clustered, but in many emerging applications relationships among the target table and attributes from related tables play an important role in representing the objects of interest. In market segmentation, e.g., not only the purchasing preferences but also the social network among the customers is relevant for clustering. When clustering gene expression data, as another example, attributes of the related proteins and their further relationships must be considered. We plan to evaluate our clustering framework in close collaboration with domain experts in the applications of analysis of gene expression data, analysis of flow cytometry data as well as community identification and market segmentation.""324703,""Estey, Mathew"
"337857"	"Far, Behrouz"	"Intelligent decision support system for COTS based software development"	"Commercial Off-The-Shelf (COTS) Based Software Development (CBSD) process relies on identifying, evaluating and selecting COTS products that satisfy system requirements, and satisfy stakeholders' preferences. The methodologies that focus on the second target are mostly based on the consensus approach in which the stakeholders have to first agree on the solution evaluation criteria and their relative importance. Then, the developer team has to evaluate the functionality of different COTS candidates against the solution evaluation criteria to select the most suitable COTS. However, there are several problems with such approaches: (1) in many cases it not possible to harmonize concerns of the stakeholders and agree on the solution evaluation criteria; (2) means for trying out different scenarios to deal with conflicting stakeholders' preferences may be missing; (3) situations where one (or more) stakeholder changes his/her preferences during the negotiation process is not usually dealt with; (4) coalition formation among stakeholders is ignored; (5) uncertainty and interdependencies inherent in the evaluation process are sometimes left unaddressed; (6) repositories for reuse of information from previous evaluation processes are either centralized or left out; and (7) cases where stakeholders have differing degree of significance are usually ignored. In this research we suggest a radically different approach in which (a) initial consensus is not required; (b) the above deficiencies are fully addressed; and (c) experience repositories are decentralized and require little or no maintenance. The use of multi-agent systems (MAS) and ontology-based knowledge management (OKM) are unique points in this project. Implementing the system using MAS offers flexibility, expandability and hiding complexity from users and allows for intelligent message passing and knowledge level negotiation. OKM allows for dynamic classification and distribution of knowledge, adapting to diversified contents, representations and personalized styles; incorporates efficient retrieval mechanisms; and requires virtually less or no centralized management and maintenance.""339566,""Farah, Ilijas"
"337823"	"Feeley, Michael"	"Toward absolutely dependable, autonomic personal computing systems"	"A variety of technology trends are combining to revolutionize the way that people acquire, organize and manipulate information in digital form.   One important consequence of this revolution is a dramatic increase in the number and variety of computing platforms a person uses on a daily basis.  Currently, for example, a typical user might work on a single task using multiple desktop and/or portable computers in a number of different locations (e.g., at work, at home and on the road).  In the near future, similar interactions will involve an even greater diversity of devices --- television-set-top boxes, game consoles, media players (e.g., an iPod), digital cameras, video recorders and cell phones --- each enabled with massive data-storage capacity and network connectivity (e.g., 802.11, WiFi).  This environment presents considerable data-management, system integration, security, reliability and networking challenges that are not adequately addressed by current approaches.   This research program will develop novel system software to address a variety of issues that arise in this environment.  The goal is to present users with a single, simple and absolutely reliable view of their data, regardless of where it is stored or from which device it is accessed.  To achieve this goal requires tackling several open problems at the core of operating systems and distributed systems research.  For example, two key issues are that current systems are too unreliable and require too much human administration.  This research program will develop techniques that secure valuable data such as family photos from hardware failure, software bugs and malicious attack (e.g., maleware such as viruses).  It will also build mechanisms for automatic configuration, self tuning, re-configuration and recovery that require no human intervention.""327696,""Feindel, Kirk"
"337835"	"Fels, Sidney"	"Advanced human communication technologies"	"As technology is becoming ubiquitous, it is important for design to be human-centred. Computing machinery facilitates communicating experience instead of only exchanging knowledge.  At the Human Communications Technology (HCT) lab that I am leading, we consider the relationship that forms between people and machines as the core of developing technologies for interaction.  The positive feedback loop that can  occur when the device feels good to control and does what you expect develops intimacy with the device.  This  intimacy allows expression of experience, ideas, emotion, and knowledge easily either to another person,  mediated by the machine, or to the machine itself.  The HCT lab projects all involve developing technologies  and theoretical frameworks for understanding this process.    In the next four years, there are two key thrusts for  researching this experience-oriented framework proposed here. First, I am researching ways to understand large, interactive multimedia environments.  Specifically, this project involves the creation of ubiquitous computing environments and new techniques for interacting with large screen displays.  The approach we will use for ubiquitous computing involves developing context-aware active rooms using  a parallel distributed camera array coupled with additional tracking technology and multimedia capture.  This environment will be placed in our interactive multimedia laboratories, including one with a wall-sized interactive glass rear-projection screen.   The second thrust of the research considers creating output media that enhances the experiential phenomenon when people interact with machines.  We propose to extend our existing 6 degree-of-freedom (DoF) anthropormorphic jaw to a complete 21 DoF head + 3 DoF neck. This robotic head will be used to study non-verbal communication and perception and to develop human-like robotic interfaces for interaction.""337620,""Felty, Amy"
"338366"	"Ferrie, Frank"	"Visual models and applications"	"This research program deals with the problem of constructing virtual models of real-world scenes within a computer from information acquired by sensors.  Sometimes referred to as virtualized reality, the goal of this research is to develop novel algorithms that can extract visual information from one or more sensors, determine the 3D shape and texture of objects within the field of view,  and render a facsimile of the real scene from an arbitrary viewpoint.  In recent years my research has been about computing such representations and using them to provide situational awareness to human operators performing difficult tasks under poor viewing conditions, e.g., using textured digital elevation maps to produce synthetic images of the ground for a helicopter pilot flying in fog.  Here the research emphasized how these computer representations could be updated in real-time based on external sensory data (e.g. infrared cameras) to reflect the actual state of the world.   However, taking these ideas forward to a more complex domain such as an urban street scene invokes a far more difficult acquisition task (multiple, distributed sensors), more complex representations (e.g. dynamic, articulated 3D shape models), and the evolution from static rendering to dynamic simulation.  Towards these somewhat ambitious goals, the proposed research extends our current work in active model acquisition to the multi-sensor case and new representations for complex 3D scenes.   In addition to supporting visualization, our work will also investigate the interpretation of evolving 3D scenes, such as the recognition of events taking place.  As part of a related project, this research will be initially applied to the problem of monitoring changes to and events within urban environments.""340867,""Ferris, Grant"
"338416"	"Fevens, Thomas"	"Computational geometry and applications"	"My research programme is centred on using geometrically motivated techniques to solve basic problems in Computational Geometry as well as work on the adaptation and extension of such Geometric Computing solutions to applications primarily in the areas of Networks and Graphics/Image Processing. This research programme has extensive opportunities for the training of highly qualified personel as well as the potential for commercial applications in areas ranging from Networks to Computer Animation to Medical Imaging. In the area of Computational Geometry, my research deals with optimization and characterization problems for point sets, polygons and polygonal surfaces. In particular, these include the optimized partition of polygons in terms of area and shape of the sub-polygons, and characterizing polygons that may be partitioned into, for example, congruent pieces. Applications of such work include Pattern Recognition and Computer Graphics. In the application area of Networks, I am in the process of working on routing problems in position-based mobile ad-hoc networks (MANETs). The general goal of the proposed research is to devise algorithms for routing to destination nodes with guaranteed delivery (or high delivery rates) in MANETs, while limiting the length of the paths discovered by the routing algorithm with respect to the shortest path possible. In particular, we are exploring the use of new geometric sub-graphs (of the network model such as the unit disk graph) and randomization in our study of routing in two dimensional and three dimensional MANETs. In the application area of Graphics/Image Processing, the problems I work on are related to visual data processing of 3D Scientific Data and Medical Imagery. One project involves extracting and characterizing the dynamic component of time-dependent volume data, with application to Scientific Visualization and Computer Animation. A second project involves developing a Health Informatics tool for computer assisted grading of the malignancy of cytological images of cancer cell biopsies using Image Processing techniques.""337681,""Fiaidhi, Jinan"
"337819"	"Fiech, Adrian"	"Composite objects: dynamic representation and encapsulation by static classification of object references"	"A central technique of object-oriented software construction is the composition of several objects to a one, higher-level, composite object. Standard object-oriented programming languages do not ensure protection of composite objects. In particular, there is no guarantee that the composite object controls the changes to its own state. We extend a subset of the Java language by providing mode annotations at object references. These annotations guarantee that the state of composite objects cannot be changed from outside. The result is more secure software.""334530,""Fiege, Jason"
"338391"	"Flocchini, Paola"	"Algorithms for mobile agents, autonomous robots, and sensor networks"	"Originating from fields as diverse as AI, robotics, and software engineering,  the research area on  autonomous mobile  entities  focuses on computational entities capable of moving over  a network (e.g., mobile software agents) or a terrain (e.g., autonomous robots or mobile sensors). This area has seen a rapid growth of interest. On one hand,  the use of mobile agents is becoming increasingly popular when computing in networked environments, ranging from Internet to the Data Grid, both as a theoretical paradigm and as a system-supported programming platform. On the other hand, the theoretical and algorithmic  study of autonomous robots is now even more important considering  the link with a network of mobile sensors. From   computational and   algorithmic viewpoints, research on mobile agents, on autonomous robots (and especially on mobile sensor networks) is still at a very early stage. Theoretical frameworks are vague and sometimes missing, algorithmic techniques and tools   are primitive.  To start covering this lack of theoretical and algorithmic background,  this project focuses on computational and algorithmic issues arising  in systems of mobile entities. The main goals are: to reach a better understanding of the computational power of mobile entities (i.e., what tasks can be performed by the entities, under what conditions, and at what cost), and to design efficient algorithms for solving basic coordination, control, and security problems. Both the computational and the algorithmic tasks will be studied also in presence of possibly unsafe components in the environments (faults, harmful elements). In particular,  in the case of mobile agents  the project  considers situations when some  of the hosts are harmful (e.g., host attack), and when an unwanted intruder  is moving   in the network(e.g., a spyware or a virus).  In the first case,  the goal of the agents is to localize the harmful host trying not to be  damaged by it, in the second case, it is to capture the intruder. In the case of robots, the project focuses on computability issues and on the   design of coordination   algorithms, under different assumptions on the capabilities of the robots; particular emphasis and focus is on applications in the  context of  mobile sensor networks.""326674,""Flood, Heather"
"338365"	"Franek, Frantisek"	"String algorithms and their implementation, intelligent tutors"	"(1) Suffix data structures and related algorithms. The focus of the research is ""relaxed"" versions of suffix trees and arrays (i.e. without regards to lexicograhic ordering); their computation in linear time with hight memory efficiency, their transformation to ""regular"" suffix trees and arrays, their use for applications. The main objective is a fast linear non-recursive memory efficient algorithm for computing suffix trees and arrays. The research will explore the following path to such an algorithm: first compute a ""relaxed"" version of the data structure, then transform it to the ""regular"" structure. (2) The periodicity in strings. The investigation of mathematical and combinatorial properties of the function r(n) - the maximun number of runs (maximal non-extendible repetitions) over all strings of length n. The main objectives are: to prove that asymptoticaly r(n)<n, to find the structurual properties of strings with large number of runs, and to build a structural theory of periodicities. (3) C/C++ implementations of strings algorithms. This part of the proposal is concerned with building a library of robust and polymorphic string algorithms implemented in C/C++ usable for a variety of alphabets, with minimized memory use, and with parametric control of the size of working memory. (4) Intelligent tutoring systems. The main goal is a design of an intelligent tutoring system for ER modelling. The proposed architecture will allow problems and solutions to be generated rather than using a ""canned"" set of problems and solutions.""339189,""Frank, Brian"
"337875"	"Gagnon, Michel"	"Annotation automatique de ressources pour l'application du web sémantique à l'enseignement et l'apprentissage"	"Le but du programme de recherche proposé est le développement d'une architecture de site web d'appui à l'enseignement, basé sur les principes et les technologies du web sémantique. Un des principaux objectifs visés est la réalisation d'un module d'annotation capable d'établir de manière automatique les liens entre les ressources du site (notes de cours, banque d'exercices et d'examens, etc.) et les concepts, qui sont représentés par le biais d'une ontologie. Ce processus d'annotation sera basé sur une analyse syntaxique complète des phrases contenues dans les documents. La création d'ontologies pour la tâche d'enseignement et de méthodologies pour la gestion de ces ontologies, de manière à ce qu'elles soient facilement partagées par différents sites d'apprentissage en ligne, occuperont aussi une place centrale dans ce projet. Il n'existe pas actuellement de système d'annotation complètement automatisé. Or, étant donné la grande quantité de documents à gérer dans un environnement d'apprentissage en ligne,  toute approche qui repose sur une intervention constante des gestionnaires et des auteurs de ressources n'est pas réellement viable. Ce projet permettra donc de combler cette lacune. De plus, il vise l'obtention d'un procédé d'annotation plus puissant que ceux qui existent à l'heure actuelle, notamment en se basant sur une analyse syntaxique complète des phrases contenues dans les documents. Compte tenu de l'importance de la langue française dans le contexte canadien, les travaux porteront initialement sur cette langue qui jusqu'à maintenant a reçu peu d'attention dans les recherches sur l'annotation sémantique. Bien  qu'appliquée d'abord au français, l'approche développée sera faite dans un souci d'assurer son applicabilité à d'autres langues, comme l'anglais et l'espagnol. Ce projet permettra aussi de renforcer le leadership du Canada, tant dans les industries de la langue que dans le domaine de l'apprentissage en ligne.  ""341446,""Gagnon, Patricia"
"338429"	"Galinier, Philippe"	"Métaheuristiques et satisfaction de contraintes"	"Le programme de recherche proposé vise à contribuer au développement des techniques d'optimisation appelées 'métaheuristiques'. Il s'agit de techniques de résolution qui visent à trouver des solutions approchées (non nécessairement optimales) pour des exemplaires de grande taille de problèmes difficiles (NP-difficiles). Une métaheuristique (par  exemple, la recherche avec tabou, le recuit simulé, l'algorithme génétique, etc.) constitue un schéma d'algorithme générique, c'est-à-dire applicable à de nombreux problèmes. Le programme de recherche proposé comporte les trois axes suivants. Le premier axe consiste à proposer de nouvelles métaheuristiques, à compléter et améliorer les techniques déjà connues, à les hybrider et à étudier leur comportement. Il se rapporte aussi à la manière d'adapter une métaheuristique à un problème particulier ou une famille donnée de problèmes : ceci concerne la définition de fonctions de voisinage pour le problème cible et la spécialisation des opérateurs de recherche (par exemple, les opérateurs de recombinaison dans un algorithme évolutionniste). Un des projets proposés consiste à étudier le problème de k-partitionnement de graphe. Le deuxième axe porte sur la résolution des problèmes de satisfaction de contraintes (CSP) à l'aide de méthodes de réparation. Il s'agit de techniques de recherche locale fondées sur la minimisation de pénalités attribuées pour la violation des contraintes et qui utilisent des voisinages simples. Nous poursuivrons le développement d'un système logiciel générique de résolution de CSP - bibliothèque en langage C++. Un autre projet consistera à approfondir et appliquer l'approche originale que nous avons proposée récemment et qui permet de détecter, dans un CSP irréalisable (sur-contraint), un sous-problème lui-même irréalisable mais 'minimal'. Le troisième axe porte sur l'utilisation de métaheuristiques pour la résolution d'applications dans le domaine des réseaux de télécommunications.""326957,""Gallagher, Else"
"338439"	"Gallant, Robert"	"Algorithms for discrete logarithm cryptography"	"The widespread use of digital communications has much increased the possibility for their   interception and misuse by hackers and other adversaries.  Defending digital infrastructure requires understanding how to defend networks generally as well as understanding how to protect communications along individual links in the network. Cryptography uses mathematics and computer science to help protect network communications from adversaries.  The effectiveness of a cryptosystem in practice depends on the details of the implementation -- for example the speed and computational complexity of the algorithms available to implement or attack the system.  Recently several governments have mandated using elliptic curves for cryptosystems protecting high security communications.  These initiatives are resulting in large deployments of these systems, so better algorithms affecting these cryptosystems will  have practical significance.  This proposal discusses research into these cryptosystems specifically, and research into security in networks generally.   The general long-term objective of the proposed research is to further investigate uses of multiplicative maps in algorithms relating to discrete logarithm cryptography. Conceivable outcomes of this research include faster or cheaper communications equipment or more secure networks.""324118,""Gallant, Thomas"
"338451"	"Gao, Yong"	"Algorithms, heuristics and typical case complexity of hard problems"	"During the past decade there has been an extensive study of the phase transition phenomena in various combinatorial search problems such as Propositional Satisfiability (SAT), Constraint Satisfaction Problem (CSP), and graph coloring. These problems are computationally hard in the worst-case, but are of great importance in artificial intelligence and other application domains such as planning, scheduling, and communication networks. The growing interest in the study of the phase transitions in combinatorial search is due to the observation that the typical-case hardness of a random instance is closely related to the phase transition of the solution probability under some instance distribution. Far from the threshold, random instances are typically under-constrained (or over-constrained) and are very easy to solve. Around the threshold, however, extremely hard instances tend to appear with high probability. In the proposed research program, we study theoretically and empirically the interplays among the efficiency of heuristics in search algorithms, the typical-case complexity of problem instances randomly-generated from certain probability distributions, and the various problem structures such as problem representation (encoding), constraint consistency, backbones, backdoors, and structural symmetry. The objectives are (1) to gain insight on the impact of problem structures on the performance of different heuristics and to draw guidance regarding the design of efficient heuristics; (2) to understand the fundamental limitations of different classes of search algorithms; and (3) to develop algorithmic solutions to real-world problems. The algorithmic problems to be investigated fall into three categories: (A) Traditional NP-complete problems that are of interests in artificial intelligence; (B) Specific problems in graph theory with special properties such as the dominating clique problem and the tree-width problem; and (C) Problems from application domains such as the phylogeny reconstruction problem  and the multi-constrained path problem in communication networks.     ""339513,""Gao, Yun"
"338356"	"Garey, Lawrence"	"Parallel methods for solving large banded circulant systems"	"This research proposal is about solving very large systems. Some examples of problems which give rise to such systems include: problems of image restoration (with applications in space photo work) which can involve a 1024 by 1024 pixel image. This is  represented as a special matrix of size 1024^{2} x 1024 ^{2} which is very large and which can be solved using  parallel computers. Another example is the monitoring of heat units for the purpose of maintaining temperature control.This can give rise to very large systems as well. A portion of the support requested is to help fund highly trained people, including graduate students, and to top up support for undergraduate students in connection with support given to student research assistants through government grants. The novelty of this proposal will be in working with specially formed  systems which have easily stored forms and choosing from many available methods to compare performance. The implementation of methods will be of two different approaches. The first is based on the work of many authors' results leading to some or all of the components of the solution  being corrected. The second approach, known as a communicationless method, relies on parallel processing. This approach provides suitable approximations without the need to explicitly identify the correction expression.""345088,""Garfinkle, Michael"
"337798"	"Gentleman, Morven"	"Building robust software with fallible construction"	"Many of today's systems depend on large, complex software with the need to be built and deployed more rapidly and cheaper than traditional development methods can deliver. Moreover, because organizations depend on these systems, they must be more predictable and trustworthy than traditional development methods can deliver for the available time and cost investments. However this requirement is not quite compatible with the traditional project-oriented view of software development, which is prevalent in the acquisition methods of many organizations. Today's systems are typically integrated from components. These components may themselves contain flaws,originating in specification, design or implementation errors, or in miscommunication between different teams involved in the development. More seriously, the integration process itself may be flawed, as when pre-existing components are used for purposes that their developers had not envisioned, and the integrators misunderstand the detailed behaviour of the components. Interoperability failures between systems belonging to different organizations often are of this form. Attempting to build infallible systems with fallible construction methods may be too stringent an expectation to achieve, but the weaker objective of resilience to failure and outages may be more realistic. Experience with interoperating commercial products, especially in the context of the Internet, indicates that robustness to fallible components and fallible integration can be achieved without centralized predictive coordination. Appropriate software architecture, redundancy in functional alternatives, and enforcement of critical interface standards in middleware in particular appear to be key elements of success. Autonomic computing techniques, such as improved registry and plug-and-play concepts, can help automate integration and reduce configuration problems.""330289,""Gentner, Nicole"
"338874"	"Georganas, Nicolas"	"Ambient multimedia intelligent systems (AMIS)"	"""Ambient intelligence"" refers to pervasive computing environments that are sensitive and responsive to the presence and movements of people. This notion was proposed in 1998 by Philips, Netherlands, and first appeared in the literature in 1999. Ambient Intelligence opens up a world of very rich experiences. The interactions of people-with-people, or people-with-machines, using electronic devices will change as context-awareness, natural interfaces, and ubiquitous availability of information are realized. Distributed multimedia applications and their processing on embedded static and mobile platforms will play a major role in the development of ambient-intelligent environments. The proposed research program will focus on the design, implementation, testing and evaluation of multimedia technologies and applications running in ambient intelligence environments. Multimedia applications will be composed of combinations of media such as audio, video, text, graphics, animation, Virtual Reality, Augmented Reality and Haptics, where the media have space, time and/or semantic relationships. Natural interfaces, such as human gestures and body movements will be in the core of this research. User-centric computing, employing the user profile, preferences and context will be main points of interest. Multimedia content adaptation and delivery, according to user and situation context will be effected, using standards such as MPEG-7 (for indexing), MPEG-21 (for adaptation and stream control) and MPEG-4 (for content coding, transport and rendering). The role of intelligent algorithms in this research is apparent because it is the key enabling factor for realizing natural interaction. The applications of this research are plentiful, ranging from the smart-home and smart-workplace of the future, to entertainment, digital media in health applications and digital media in the arts.""342680,""Georgas, Michael"
"337870"	"German, Daniel"	"Understanding and visualizing fine-grained software modifications"	"Software maintenance and evolution is very expensive and knowing what changes need to be performed to complete a task takes time and experience. The objective of this research is to identify the characteristics of changes applied to software systems by their developers. This research project will define metrics to evaluate the amount of ""change"" performed by a modification, and create an ontology of the different types of changes that we observe.  Algorithms will be developed to automatically classify a given change within the framework of this ontology. This method of analysis will allow us to more clearly discern the relative importance of different changes in software evolution. We propose that previous studies using historical information can be improved through a weighting of the importance of certain types of changes.  Finally, we will continue our efforts to develop useful visualizations of the historical information of a software system. I believe that the research proposed herein will directly benefit software developers (and their companies) by reducing the cost (in time and money) of maintenance, and improve the overall quality of the software they produce.""330724,""Germida, James"
"338923"	"Geurkov, Vadim"	"Application of error-control codes to digital systems testing"	"Reconfigurable Computing Platforms (RCPs) based on programmable logic devices (PLDs) have been widely used in various areas: aerospace, flexible manufacturing, wireless communication, etc. The primary reason for this use is the ability of such RCPs to adapt their hardware to the algorithm and data structure of the application. This important feature ensures high cost-performance characteristics compared to conventional microprocessor based platforms. Increasing level of complexity of nowadays applications has made an SRAM based Field Programmable Gate Array (FPGA) the most suitable choice for a PLD. However, such FPGAs are highly susceptible to radiation and other environmental factors. With reduction of a VLSI feature size the susceptibility tends to increase inhibiting the use of SRAM FPGA based RCPs in nuclear power stations, aerospace, and military areas, which are the most demanding for RCPs. The main objective of the proposed research is to develop testing methods oriented to digital devices, which will be applicable to SRAM FPGA based RCPs and will increase their fault tolerance. To achieve this objective the following tasks will be accomplished: - Error-Control Codes (ECC) will be reviewed in terms of applicability to on-line testing of digital devices - Encoding/decoding algorithms and encoder/decoders with simple hardware implementation will be analyzed - Self-checking as well as off-line digital testing methods based on the most efficient ECC will be examined - Applicability of the testing methods based on the ECC to partially reconfigurable FPGAs (e.g. Xilinx Virtex family) will be scrutinized - Hardware recovery procedures based on relocation of data-processing micro-architectures in FPGAs (with possible performance degradation) will be investigated. The outcome of the research will be invention of new on-line and off-line digital testing methods, which will contribute to development of a high-performance dependable RCP with improved lifetime parameters.""339427,""Ghahramani, Fereidoun"
"338444"	"GhodsiBoushehri, Ali"	"Data analysis by learning manifolds from high-dimensional data"	"Dimensionality reduction (manifold learning) addresses the problem of dealing with complex data by mapping high-dimensional data into fewer dimensions. Many problems of scientific interest that require the analysis of very large and high-dimensional data sets can benefit from dimensionality reduction techniques. Most existing dimensionality reduction techniques ignore information about the sequence of observations and actions between data points.  I recently co-developed a new dimensionality reduction technique called Action Respecting Embedding (ARE) which exploits this additional information, successfully translating actions into meaningful and interpretable low-dimensional representations. In my proposed research, I intend to refine this new technique to make it faster and more efficient. This could lead to novel solutions to real-world sequential decision problems such as planning and localization for robots. Unlike existing techniques, an ARE approach would require no expert knowledge about the domain to find effective solutions. On the more theoretical side, I propose to explore and formalize non-linear dimensionality reduction techniques as probabilistic models. I will address the problem of how such models should be constructed, and how they should respond when data is missing. This has many potential uses in fields such as physics, economics, and medicine, where meaningful information must be extracted from large data sets.""344947,""Ghonaim, Nour"
"337860"	"Giroux, Sylvain"	"Pervasive cognitive assistants for smart homes/l'assistance cognitive dans les habitats intelligents"	"Governments aim at keeping at home and in their community people suffering from cognitive impairments (Alzheimer's disease, trauma, schizophrenia...).  In many cases, people would be able to stay at home if light assistance is provided. But resources are scarce. Thus most of the time, relatives take responsibility for care without access to appropriate resources. Too often, this situation then turns to an exhausting burden. Therefore informal and professional caregivers urge for help. Smart homes in particular and health telematics in general can foster independent living and enhanced life style for elderly and disabled people. This proposal addresses deficits of attention, initiation, memory and planning. An attention deficit occurs when a person has difficulties to focus on his current activity that may then be forgotten and never completed. An initiation deficit leads to inactive periods whereas the person is supposed to perform actions. Memory deficits could lead to difficulties to remember the activity he has to perform, the steps of the activity, the locations of the objects, and even which objects are involved in the activity. Finally a planning deficit leads to difficulties to perform a sequence of actions in the right order to achieve a goal. In the long run, this research proposal aims to - contribute to the theory and praxis of pervasive/ubiquitous computing and tangible user interfaces compulsory to create smart homes and smart environments, - foster the autonomy of cognitively impaired people thanks to cognitive assistance to activities of daily living, - investigate how pervasive services and tangible user interfaces can support tele-monitoring, tele-assistance, and communication between cognitively impaired people and their caregivers.""328410,""Gish, Douglas"
"337861"	"Goswami, Dhrubajyoti"	"Towards the enrichment of parallel systems skeletons"	"Despite the advancements of hardware technologies and consequently the availability of low-cost commodity clusters and high-performance computers, complexity of parallel application development remains one of the major obstacles. As one of the techniques for easing this complexity, researchers are actively investigating the patterns-based approaches to parallel programming. As re-usable components, patterns are intended to ease the design and development phases of a parallel applications, as compared to the traditional approaches like MPI, PVM and sockets. PAS (Parallel Architectural Skeleton) is one such patterns-based parallel programming model and tool, originated at the University of Waterloo and currently being actively researched at the Concordia University, which generically defines the architectural aspects of parallel computational patterns. Ongoing research has contributed to an extensible PAS (EPAS) model and system that supports extensibility and skeleton composition, enabling design and incorporation of new skeletons and creation of new composite skeletons from existing repertoire. In the course of this research, some of the other significant requirements and research issues for PAS have been identified, better understanding and incorporation of which could significantly enhance its usefulness and appeal to the parallel computing community. More specifically, the original intention of PAS (and most other patterns-based approaches) was towards programmability. However, being pure architectural abstractions which are decoupled from an application and programmer, PAS could encapsulate many useful systems-specific requirements which are otherwise rather difficult and time-consuming to implement from scratch. The proposed research in this direction addresses the following: (i) Dependability: dependable skeletons with reliable composition and communication-synchronization abstractions; pattern-specific strategic fault-tolerance; support for avoidance/detection of possible atomicity violation; maintenance.(ii) Performance: adaptable and re-configurable architectural skeletons; architectural skeletons with cost-performance models and support for performance profiling and re-structuring.""337018,""Goto, Natalie"
"338446"	"Green, James"	"Discovering protein structure and function through nonlinear system identification"	"Predicting protein structure and function from sequence is of fundamental importance to biomedical research. An effective solution has the potential to accelerate the drug discovery process and may lead to an increased understanding of cancer and other disease processes. Unfortunately, experimental methods for elucidating a protein's structure are very costly and are not always applicable. Computational prediction techniques provide an attractive alternative; however, the accurate prediction of 3D tertiary protein structure directly from amino acid sequence data continues to elude researchers when highly similar previously solved protein structures are not available or for longer domains. Even then, knowing a protein's tertiary structure may not be sufficient to fully understand its function: the majority of eukaryotic proteins are not functional in their original structure and must be activated through some form of post-translational modification (PTM). We have recently developed a novel approach to protein secondary structure prediction making use of Parallel Cascade Identification (PCI), a powerful method of nonlinear system identification. Resulting accuracy not only compared well with top contemporary prediction methods based on neural networks and hidden Markov models, but PCI was also shown to be highly effective when used in concert with such methods. While predicting secondary structure is a useful intermediate step, the ultimate goal of protein structure prediction is to predict the complete 3D structure of the active conformation(s) of a protein, including PTM information, given knowledge only of its amino acid sequence and its environment. Building on the established success of PCI as a proteomics tool, the research program proposed herein will make significant progress in these new and challenging areas, and make a real contribution towards in silico drug prediction of protein structure and function.""335895,""Green, James"
"338337"	"Green, Mark"	"Design and implementation of 3D user interfaces"	"Over the past decade considerable progress has been made on 3D graphics hardware, but similar progress has not been made on software and applications that take advantage of this hardware.  One of the main reasons for this situation is the lack of software tools supporting the development of 3D applications. The Grappl system is one of the first software tools that assists with the production of 3D user interfaces, and the research program proposed here extends this system to cover a wider range of applications and produce higher quality user interfaces.  This research program will investigate techniques for arranging user interface and application components in 3D space, incorporating multiple modes of interaction, and techniques for structuring large 3D user interfaces.  The resulting software tools will reduce the time required to produce user interfaces for a wide range of 3D applications.""331828,""Green, Mark"
"338417"	"Greenspan, Michael"	"Pose determination and tracking in sparse range data"	"The goal of this research is to quantitatively determine and track the position of objects that are moving through 3-dimensional space. This problem lies at the very core of computer vision research, and to develop a solution that is flexible, efficient and robust will require that I extend both the theoretical and practical aspects of my previous work. There are a number of important applications that will become immediately enabled once this capability is proven, such as autonomous robotic grasping, advanced human computer interfaces, and improved computer aided surgery. In the longer term, the results of this research will be significant and far-reaching, enabling robotic systems to ""see"" and interact with their environments, and transforming intelligent systems to the next level of capabilities.""343872,""Greenspoon, Philip"
"338884"	"Grenier, Dominic"	"Imagerie radar avec traitement d'antenne haute-résolution"	"On propose de combiner efficacement les algorithmes à haute-résolution de traitement d'antenne, la diversité  en fréquence et la diversité de polarisation dans des algorithmes d'imagerie radar à ouverture synthétique  inversée (ISAR) de cibles non-coopératives avec une antenne-réseau en réception (bi-statique). Les problèmes reliés à l'imagerie ISAR sont de deux ordres : la cible non-coopérative oblige un alignement estimé des données, ce qui constitue une étape crutiale; la transformation dans le domaine des pixels de l'image nécessite une double FFT qui limite la précision de l'image en portée selon la largeur de bande occupée par le signal émis.   Jusqu'à présent, on a cherché à modifier la transformation dans le domaine des pixels de manière à  simplement insérer une méthode à haute-résolution telle MUSIC, adaptée pour la circonstance. La corrélation totale des signaux complique la situation et la décorrélation implique une largeur de bande équivalente plus grande car le lissage spatial (``spatial smoothing'') s'avére impossible avec une seule épreuve virtuelle. Parmi les solutions, on cherchera celles qui permettront de décorréler les signaux issus de la combinaison des échos des diffuseurs. Certaines  pistes d'agent de décorrélation seront prévilégiées dont les cumulants d'ordre supérieur (non affectés par la  corrélation) ou encore, les diversités en polarisation ou en fréquence. Un pistage plus précis avec le traitement d'antenne aidera l'alignement des données (autofocus). Dans des solutions plus élaborées, il faut donc envisager une combinaison moins  directe et plus subtile des algorithmes de DOA et d'ISAR, une symbiose à tous les niveaux : pistage,  alignement, transformation dans le domaine des pixels. On souhaite aussi obtenir des résultats expérimentaux par   le biais d'une antenne-réseau existante qui devra être mise-à-niveau, de sources émettrices et de modèles réduits de cibles. Ces cibles mûes par des moteurs électriques, se déplaceront le long de trajectoires rectilignes.  Des essais préliminaires ont démontré la faisabilité d'un tel projet. Les résultats expérimentaux viendront confirmer  la validité des algorithmes développés.""341403,""Grenier, Emilie"
"338456"	"Gruninger, Michael"	"Ontologies for semantic integration"	"Many tasks require correct and meaningful communication and integration among intelligent agents and information resources. A major barrier to such interoperability is semantic heterogeneity: different applications, databases, and agents may ascribe disparate meanings to the same terms or use distinct terms to convey the same meaning. It has been recognized that the emerging technology concerned with the development and application of ontologies will play a central role in achieving semantic integration. An ontology is a computer-interpretable specification that is used by an agent, application, or other information resource to declare what terms it uses, and what the terms mean. The objective of this proposal is to conduct basic research into the design of ontologies using mathematical logic and the application of this work to support semantic integration of software systems. In particular, the research will focus on the ontology of the Process Specification Language, and the application of ontologies will focus on two keytechnologies -- the generation of semantic mappings and the specification of semantic web services.""324116,""Gruninger, Robert"
"338407"	"Guenin, Bertrand"	"Set covering polyhedra graphs, and matroids"	"My primary area of interest is combinatorial optimization, in particular the integer programming models known as packing and covering. A wide variety of practical questions can be formulated in this setting and include: scheduling problems, assignment problems, and the problem of shipping one or more commodities across a network. Sometimes, owing to the special structure of the problem, the natural linear programming relaxation yields an optimal solution. When this occurs the problem can be solved efficiently. The focus of my research has been to characterize when this phenomenon occurs. Although these models have their roots in real life, this leads to deep questions of a purely mathematical nature.""334466,""Guenther, David"
"338378"	"Gupta, Arvind"	"Bioinformatics and combinational algorithms"	"My research lies in the study of algorithms for combinatorial properties and the application of such algorithms. I have worked extensively in devising new and faster algorithms for many practical questions. Most recently this is in the area of bioinformatics. Working with students, we have devised faster and better algorithms for sequence alignment, a key tool in gene finding. In other research, I am considering the haplotyping problem. Haplotypes of individuls are the basis for genetic disease susceptibility and in-silico techniques are needed for haplotype determination. We are devising tools to make this more efficient and give better results. I am also considering the Inverse Protein Folding Problem. Here we attempt to construct a desired protein given structural information about the protein. This research has the potential to play a major role in drug design.""337341,""Gupta, Bhagwati"
"338431"	"Hamel, Sylvie"	"Recherche et comparaison de séquences et de motifs biologiques"	"Mes travaux de recherche se situent à l'interface entre l'informatique et les mathématiques avec des applications en biologie moléculaire. Les problèmes algorithmiques liés à l'analyse des données provenant du séquençage nécessitent des calculs exigeants sur des structures complexes. Il est donc naturel de chercher à les accélérer par une approche de calcul parallèle. Avec mes collaborateurs, j'ai mis au point des techniques qui permettent de déduire des algorithmes parallèles efficaces à partir de la construction d'automates possédant de bonnes propriétés. Rappelons qu'un automate permet d'analyser une séquence de lettres en se déplaçant dans un ensemble d'états. L'important de cette analyse est la suite des états visités, par l'automate, lors de la lecture d'une séquence. On dit d'une telle analyse que c'est la transduction de la séquence, via l'automate. En résumé, on cherche donc à produire des algorithmes parallèles (ou vectoriels) efficaces pour effectuer la transduction d'un séquence via un automate. Mon projet de recherche comporte deux volets: un volet théorique et un volet algorithmique. Le but du volet théorique est d'étudier de grandes classes de langages pour déterminer dans quelle mesure, le problème de la transduction via les automates associés à ces langages, est vectorialisable. Ces considérations théoriques ont comme objectifs de développer des algorithmes explicites. Ces algorithmes trouvent principalement des applications dans le domaine de la bioinformatique mais aussi dans d'autres domaines de l'informatique comme le génie logiciel. Mon approche permet donc de systématiser la recherche d'algorithmes efficaces dans ces domaines.""341281,""Hamel, Sylvie"
"338894"	"Hasan, Anwarul"	"Cryptographic computations:  algorithms, architectures, and assurances"	"In order to provide security for communications and computers, various cryptographic schemes can be used.  Many of these schemes require special type of computations, e.g., point addition on elliptic curves, which deal with very large and abstract operands, such as elements of finite fields. Such computations are not directly supported by main stream processors, and require special representations, algorithms and hardware architectures. For practical applications, these algorithms and architectures are to meet performance requirements under various constraints, such as limited area, narrow bandwidth, and low power. More importantly, they need to operate in a way so that they do not leak information related to the secret key of the cryptosystem through various side channels, such as power consumption signals, electromagnetic emanations, and erroneous outputs caused by faults induced deliberately by a malicious attacker. The main objective of the research is to investigate various aspects of cryptographic computations that extent from digital signature generation by elliptic curve point addition to basic field arithmetic, and are at the heart of many security schemes used in communications and computers. Under the proposed research, we plan to work towards the development of novel algorithms and hardware architectures for such computations. The algorithm development is envisioned to be quite challenging and is expected to be largely influenced by properties of the underlying mathematical structures and can be quite abstract for field arithmetic. Efforts for mapping of algorithms onto hardware will take into account implementation platforms, target applications, and more importantly, various architectural challenges that are quite significant owing to very large size operands used in cryptography. In addition, considerable efforts will be required so that critical cryptographic computations are resistant against various side-channel attacks.""331172,""Hasan, Mainul"
"337896"	"Hassan, Ahmed"	"Leveraging historical project information to understand and support software maintenance and change activities"	"Software repositories (such as source control systems, archived communications between project personnel, and defect tracking systems) contain a wealth of historical information about the progress of software projects. For example, source control systems store changes to the code as development progresses and defect tracking systems follow the resolution of software defects. Practitioners actively use such information in ad-hoc manners to effectively manage their projects and develop high quality code; yet few software engineering researchers investigate enriching their current research methods and techniques with historical information.     The long term goal of this research is to systematically uncover the potential and empirically explore the benefits of information stored in software repositories. This proposal will mine such historical information to understand and support software change activities. The recovered information will be integrated into tools, development environments and processes used by software practitioners to build high quality systems that are delivered within budget and on schedule.     In particular, this research will create models to understand, predict and explain code change patterns (such as refactorings) in large projects over the short (inter-release) and long (across releases) term. Since practitioners require immediate and timely access to the most appropriate and knowledgeable person related to a piece of code or feature, this research will develop techniques and tools to support practitioners in effectively locating code experts (i.e. other developers who are most familiar with particular code sections). This proposal will as well explore improving the learning experience of junior students by creating automated code tutors to assist students in producing bug free code and to inform instructors about any difficulties students are having in applying recently taught concepts. All the proposed research will be validated through large empirical studies.""328124,""Hassan, Assem"
"338862"	"Hayes, Jeremiah"	"Studies in wireless communication"	"There are two areas of investigation. The first of these is an analysis of multiplexers which are designed to handle a wide range of traffic types such as real-time videoconferencing, Voice over IP, real-time query, web applications and delay tolerant applications. Depending on the application, appropriate performance measures are connection delay, packet loss, maximum packet delay, throughput and response times. The results of this work will be applicable to both land-based mobile wireless and satellite networks. A second study is modulation and multiple access techniques in mobile ad hoc networks. These networks are composed of wireless links whose nodes operate as end hosts as well as routers.  Since the nodes are mobile, the network topology and the transmission properties of wireless links are constantly changing. The dynamic nature of the network calls for protocols, at all levels, that are adaptive in nature. Adaptive protocols take advantage of changing conditions to improve performance. The mobility of the nodes engenders the second major consideration: the need for power conservation in order to conserve battery life. Our goal is to develop modulation and multiple access techniques that improve performance through adaptivity and that are power frugal.""325286,""Hayes, John"
"338381"	"Hayward, Ryan"	"Discrete algorithmics with applications to combinatorial optimization"	"Many problems studied by computer scientists can be modelled by some discrete, as opposed to continuous, structure. For example, a graph is a mathematical object that can be drawn on a piece of paper by first drawing some dots and then drawing lines between some pairs of dots. The only relevant information in a graph is which dot pairs are joined by a line. Many kinds of problems can be modelled with graphs, including communication problems (e.g. routing calls through a phone network), transportation problems (e.g. routing delivery vehicles), location problems (e.g. finding the location of a gene in the human genome sequence). In many of these problems some aspect requires optimization. For example, with delivery vehicles travel time should be minimized; with gene location the number of matched gene sites should be maximized. Algorithms which find optimized solutions efficiently are desired. Understanding the underlying discrete structure of these problems often leads to efficient algorithms. Once these properties are understood, efficient optimization algorithms can be designed. This proposal concerns certain kinds of graphs for which some optimization problems can be solved very efficiently, and the algorithms associated with those optimization problems.""325382,""Hayward, Vincent"
"337900"	"Hengartner, Urs"	"Privacy in future computing environments"	"The goal of this research is to develop software techniques for increasing the privacy of individuals who want to benefit from emerging services that manage or exploit personal information. Many services that gather and provide personal information about individuals have been introduced recently. For instance, Web-based email or search services, such as those provided by Google, store a user's emails and search behavior.  Online galleries, such as Yahoo!'s ""flickr"" service, keep a person's pictures. Bell Canada offers a ""Seek & Find"" service, which tracks GPS-equipped cellphones so that, for example, people can locate their friends.  Privacy is a major concern for such services. Namely, the information stored by a service about an individual should not be released to third entities, unless the individual defines privacy preferences allowing this release. Another privacy-related concern for these new services is that they can be broken into. For instance, an intruder into the network of cellphone carrier T-Mobile read emails and personal files of hundreds of customers over seven months.   In this research, we investigate software techniques that limit the amount of personal information that a service (and any intruders into the service) learns during the specification and enforcement of an individual's privacy preferences. Our ultimate goals are to make the public aware of privacy issues surrounding emerging services and to give people technologies that allow them to benefit from these services without compromising their privacy.""343600,""Henkemans, Emily"
"337862"	"Hepting, Daryl"	"Democratization and personalization for information access"	"Computer software should conform to its users and not the other way around.  Although this phrase is often repeated, it seems to carry little weight today: users are regularly told of the need to use standard processes and software.  My research explores an alternative view: let software deal with each user on their own terms and then let it manage the translation of each user's work into the terms of the others with whom he or she needs to collaborate.  Democratization  means that any user will be able to have satisfying and meaningful interactions and explorations of their chosen problem domains.  Personalization  means that each user is able to interact with the tool in a way that mirrors his or her own concept of the problem.  Knowledge acquisition techniques will be used to discover each user's concept of a problem without requiring its explicit naming with a possibly unfamiliar vocabulary.  User experiences will be more satisfying, hence more helpful to democratization, if the personalization phase can be as short as possible.  I have developed applications in the areas of information visualization, recipe browsing, environmental decision support systems, eyewitness identification, and multimedia composition.  I am seeking techniques to understand, as directly and as efficiently as possible, how any given individual conceives of a problem.  I plan to take this understanding or knowledge and make it available for manipulation by that individual through my software, called cogito.  This interface is intended to allow and support a user's exploration of that information, enabling him or her to manage and even transform the space (by changing the order in which decisions are made, for example) until the desired results are obtained.  People will bring their diverse applications or problems to the cogito system and they will have satisfying experiences exploring them.  Technology can, I believe, be more powerfully used when it augments individuals seeking personally-creative solutions and helps those individuals to communicate those ideas to others.""331024,""Hepworth, Shelley"
"337800"	"Hoffman, Daniel"	"Test frameworks for network security devices"	"Along with the many benefits of the Internet come the well known dangers. Computers connected to the Internet face a steady stream of attacks. The first line of defense is the firewall. Home firewalls are available as PC software or inexpensive appliances sold as ""home routers"". Powerful commercial firewalls protect web servers and business LANs, as well as industrial networks used for manufacturing and power generation. Each firewall contains a set of filtering rules designed to permit legal Internet packets and to filter out all other packets. Unfortunately, it is difficult to develop rules which do this precisely. Today's Internet applications use complex protocols and hackers often make their attack packets look very similar to legal packets. As a result, firewall filtering rules are long and complex, and often contain errors. Systematic testing can help find the errors before the firewalls are installed. Voice over IP (VoIP) is an exciting new application: an Internet telephone service which is inexpensive and globally available. Unfortunately, the VoIP protocols are also complex. It is not yet understood how to modify existing filtering rules to allow VoIP traffic and still block illegal traffic. Without research in this area, the benefits of VoIP will come with significant security drawbacks. This research project will improve firewall effectiveness in two ways: (1) by developing software to automatically test filtering rules, revealing errors before the firewalls are installed, and (2) by developing and testing firewall filtering rules specifically aimed at the new VoIP protocols.""323919,""Hoffman, Michael"
"338349"	"Horton, Joseph"	"Applications of graph theory"	"I have two major projects.  One is to find as quickly as possible the minimum cycle basis of a graph.  Cycle bases are used to study cycles in networks, especially when the networks lead to sets of equations.  Smaller bases produce simpler equations to be solved.  Applications include electrical networks, structural engineering, scheduling subway systems, studying metabolic pathways, in topology.       The other project is the study of clause trees.  These give a way to represent a restricted set of logical proofs in simple diagrams, that can be represented in a computer.  Then computer programs can be written, starting from known axioms, to construct and manipulate clause trees to find proofs of theorems. Few new results have been found using such automated theorem provers yet, but such systems are used in many artifical intelligence applications.  Can clause trees be used to find proofs faster than other methods of reasoning with computers?""333018,""Horton, Keith"
"338882"	"Huang, WeiPing"	"Modeling and design of high density photonic integrated cricuits"	"Photonic integrated circuits (ICs), first envisaged by Dr. S. E. Miller at Bell Laboratories in early 70s, have a long history but few applications. In reality, most optical systems are still built on  discrete photonic devices made of different materials by different processes. The photonic ICs are pale in terms of the density and complexity in comparison with the electronic counterpart and suffer from  compromised performance and poor yield. One of the fundamental issues in the convetional photonic ICs is the weak confinement of the optical field in  waveguiding and resonant structures with weak index contrast/variation. As such, high-density integration and strong interaction between light and media become extremely difficult. The integration and interaction are essential for realizing advanced optical signal processing functions for different applications. Therefore, the abilities to confine and manipulate optical fields on chip and to couple lights into and from chip are the key enablers for realizing robust, complex photonic ICs. The proposed research will investigate and develop high-density photonic devices and integrated circuits to perform advanced optical signal processing functions. These high-density photonic devices and ICs differ from the currently popular photonic crystal (PC) structures in the sense that the confinement, guidance and resonance of the optical fields are not due to the periodic index variation (i.e., the ``crystal effects''), but to the strong index contrast (i.e., the ``defect effects''). First of all, the research will devote substantial effort for development of efficient and accurate theoretical models and numerical techniques for simulation of the vectorial properties of the light and its strong interaction with media. In this respect, most of the existing models and simulation techniques are not sufficient;  new and improved models and algorithms need to be proposed and implemented. Secondly, basic waveguiding and resonanting structures will be examined based on consideration for absorption and scattering losses, ease of fabrication and characterization, etc. By using structures of large index contrast, we will be able to confine and manipulate light on chip by shrinking the feature size of the optical devices by a few orders of magnitude. Several strong confinement structures, e.g., the silicon wire, the deeply etched InP ridge waveguide, and the micro-ring resonator, etc., have been proposed and demonstrated. A major focus of the research is to investigate and develop novel circuit architectures to connect these waveguide and resonant elements to form complex photonic ICs. Further, we will develop efficient and accurate modeling and simulation tools for investigation of coupling between the PIC and off-chip elements. Efficient and robust mechanisms for coupling in and out of the chips will be examined. Finally, we will investigated ways for control and manipulate lights. Electrical, thermal and nonlinear effects pertinent to the materials used will be studied. The research will link closely with the on-going research in the areas of integrated optical transceivers, all optical regenerators etc. Strong collaboration with researchers at McMaster and external industrial and academic institutions will be sought and strengthened, especially in the experimental aspects to complement the expertise of the applicant. The research, if successful, will shed lights on some of the salient features of the high-density photonic ICs and advance the state-of-the-art PIC technologies towards practical applications. Several Ph.D students and PDFs will be exposed and trained in this emerging technology with good knowledge and experience on modeling, design as well as perhaps fabrication and characterizations of these PICs for different applications.""338278,""Huang, Xiangji(Jimmy)"
"337817"	"Hughes, Larry"	"The research development and application of self and neighbour awareness protocols for emerging network architectures"	"The past decade has seen a dramatic increase in the number of devices and applications for wireless networks.  During this time, wireless networks have undergone a revolution, from fixed-infrastructure networks to mobile ad hoc networks to sensor networks. Not surprisingly, as the network structures have evolved, so have the problems, most notably, bandwidth utilization, power consumption, dynamic reconfiguration, and quality of service. Collectively, these problems add to the complexity of the design and implementation of both connection-oriented and datagram protocols. The objectives of this research are to develop solutions to the problems of bandwidth utilization, power consumption, dynamic reconfiguration, and quality of service in mobile ad hoc and sensor networks through the use of adaptive protocols that allow a node to be both self-aware and neighbour aware. In addition, the research will continue to focus the development of the novel, Cartesian network architecture, as our past experience with this network has lead to the design of an FPGA (Field Programmable Gate Array) router, and several wireless protocols, all of which demonstrate the viability of this type of network design.""342856,""Hughes, Mark"
"337810"	"Hutchinson, Norman"	"File and directory services"	"As the number of computers involved in every facet of daily life continues to grow rapidly, the problems in the area traditionally known as distributed systems become increasingly important.  In particular problems such as harmonizing the configurations of many machines, reliably storing important data, ensuring the consistency of that data, and replacing and updating the physical machines as they age take up an increasing amount of effort.  My research into file and directory systems for distributed systems addresses these concerns by inventing and refining techniques for storing and synchronizing vast quantities of data. An old technique called virtualization has recently been rediscovered and is being widely used to help manage and secure the increasingly large number of computers in use.  Virtualization affects the functionality required in computer storage systems as well as the demands placed on these storage systems, and so I am also looking at storage systems that are designed and optimized for virtualized environments.  In these environments the ability to quickly create copies of a file system that can share storage resources with other concurrently written copies is very important but is not supported by current technologies.  I will be looking at system structures that can provide this support very efficiently.""325381,""Hutchinson, Robin"
"338906"	"Ilow, Jacek"	"Dynamic spectrum management in packet networks"	"New communications technologies and services have created an explosive demand for electromagnetic spectrum. The radio spectrum is a limited resource, the use of which is licensed by governments. Traditionally, spectrum allocations are based around the paradigm of fixed long-term frequency band assignments and do not necessarily ensure spectrum is being used most efficiently at all times, in all areas. To exploit unused spectrum more efficiently, this proposal seeks to design communication systems that adapt to rapidly changing network conditions. Different levels of cooperation and competition in the network will be considered. Innovative use of spectrum investigated in this proposal will aid regulatory bodies as well as service providers in supplying the public with increased capacity for wireless Internet access. The strategic plan for the proposed research involves the development of distributed, multi-layer algorithms for joint medium access control and resource allocation in networks with variable capacities subject to a global resource constraint. Protocols will be devised that jointly optimize end-to-end rates and resource allocation to maximize total network and spectrum utility. Specifically, this proposal aims to address technical issues in radio networks, such as (i) multi-access fading channels; (ii) random packet arrivals and time-frequency allocations; (iii) energy/delay tradeoff in real-time transmissions; and (iv) end-to-end performance management. The multi-wavelet packet modulation (MWPM), introduced recently by the applicant, generalizes orthogonal frequency division multiplexing (OFDM) schemes proposed for adaptation in cognitive radio systems. The investigations will build on the MWPM as the multiple access scheme which provides flexible time-frequency tiling of the temporal spectrum. The research will impact the direction of the development of new radio network architectures and will be applicable to any radio system, from wireless to cable modems, implementing adaptive modulation schemes and the cross-layer network protocol design.""342381,""Ilton, Mark"
"337837"	"Inkpen, Kori"	"Supporting collaboration in multi display environments"	"As display costs fall and users' needs for additional display space grows, multi-display environments (MDEs) are increasing in popularity. Whether it is an individual user with tiled monitors, or a large, technology-rich conference room, equipped with wall and tabletop displays, very little is known about how to effectively design multi-monitor environments and how users may ultimately interact with these workspaces. As displays become ubiquitous in our environment, we need to think carefully about how information is managed across these displays and how users (in particular, co-located collaborators) will interact with these displays. The focus of this research is on the use of MDEs to support small-group, face-to-face collaboration. We are interested in developing a better understanding how users interact in MDEs and building new interaction techniques to enable users to collaborate in a natural, seamless manner. Over the next five years, the prevalence of multi-display environments will increase dramatically, both in the workplace and in the home. These environments have the potential to be significantly underutilized without an appropriate understanding of how to effectively design multi-display systems which support co-located collaboration. Current computer technology constrains our collaborative interactions and forces us to adapt to inferior but available interaction styles available. However, the underlying architectures for MDEs do not currently exist. Therefore, research in this area will help shape the systems that are ultimately developed and influence the way systems and application designers think about MDEs.""334693,""Innes, David"
"338355"	"Jackson, Kenneth"	"Numerical solution of differential equations with applications to computational finance and numerical weather forecasting"	"I am engaged in several research projects to develop, analyze, test and evaluate numerical methods and to construct robust mathematical software.  I hope these projects will ultimately lead to improvements in mathematical software, enabling scientists, engineers and financial modelers either to solve previously intractable problems or to solve problems more efficiently and/or reliably than is currently possible. Previously, my research focused primarily on initial-value problems (IVPs) for ordinary differential equations (ODEs), but it has since broadened to include boundary value problems (BVPs) for ODEs as well as the application of ODE techniques to partial differential equations (PDEs) and to the solution of practical problems in science, engineering and finance.  I have a secondary interest in numerical linear algebra, particularly problems arising from differential equations.  Over the next few years, I will concentrate on the following two major projects: (1) the study of guaranteed error bounds for the numerical solution of IVPs for ODEs, with the long term goal of developing a robust, reliable, easy-to-use package incorporating these schemes; (2) the study of numerical methods for finance, with particular emphasis on effective computational techniques for computing the price of convertible bonds and the price of credit derivatives.""342861,""Jackson, Kevin"
"338406"	"Jagersand, Martin"	"Vision-based supervisory control and predictive display for uncalibrated tele-robotics"	"My research is in the intersection of computer vision, robotics and graphics. In particular I'm interested in how to use visual information and coordinate frames to encode and communicate spatial information, motion, and interactions. Among applications I work on are visual servoing for robot motion control and predictive display for operator visualization. Both of these combine into tele-robotics systems.         A key idea is to work with visual (camera) spaces instead of Euclidean world coordinates as in conventional engineering. Deictic motions and gestures of a human in a scene can be viewed by cameras and interpreted in camera space w.r.t. that scene. Similarly, in tele-robotics, a human operator is immersed in a visualization of a remote scene and can gesture instructions w.r.t. the shared visual space between the remote and local sites. Using sensory frames instead of a conventional world coordinate frame brings up new issues in task and trajectory planning. Furthermore, robots operating in human settings face more varying tasks than currently in manufacturing. Achieving full autonomy in such challenging situations is difficult, and may not always be desirable. Instead our thrust is to endow the robot with the ability to carry out motions and small subtasks semi-autonomously, while under the guidance of a human, so called supervised control. The motions and tasks are built bottom up, from simple motions, reaching and fine manipulation to more complex subroutines, e.g. pick-and-place. Tasks we can implement include a light bulb exchange subroutine, setting a dinner table and solving a child's shape sorter puzzle. The routines are currently built by scripting, but we plan to look into learning from observing a human. A general way of visually instructing a robot has many applications from the high end e.g. in surgical assistive robotics, where the human surgeon can't interrupt the surgery to use a conventional keyboard interface, to everyday service robotics e.g. for the handicapped and elderly.""344757,""Jahanbakhsh, Sanaz"
"338351"	"Jeffrey, David"	"Computer algebra for engineering and industry"	"This project develops algorithms so that computer algebra systems can be used more effectively for the solution of mathematical problems. The algorithms developed  will be implemented in widely available computational platforms and distributed to the scientific and engineering communities. The mathematical problems considered will be drawn from important areas of engineering and science. Industrial manufacturing processes and engineering design are important areas of scientific computation. The performance of a piece of engineering equipment is best analysed using quantitative modelling. Quantitative is not the same as numerical, however; for design purposes, an analysis containing symbols might be better. An example might help non-scientists. Suppose you are designing something that uses a piston, and your job is to select the length of the piston rod. One way to do this would be to select a definite initial length, for the sake of an example let us say 10 cm, and calculate the behaviour. Then you might repeat the calculation using a length of 9.5 cm. Instead of recalculating every time that there is a change, you might try instead to call the length L and derive the modelling equations just once. The software products that help perform such a task are often called computer algebra systems. This project will strengthen computational systems, especially in applications, by speeding numerical calculations, increasing the number of types of algebraic manipulation possible and controlling expression swell. This last item refers to the fact that symbolic expressions tend to grow constantly throughout a calculation, and even moderate-sized problems can generate expressions that completely fill computer memory, thereby effectively stopping the calculation. This proposal builds on earlier work to develop techniques for controlling very large expressions in industrially significant calculations.""336335,""Jeffrey, Kenneth"
"338937"	"Johanson, Robert"	"Electrical and optical properties of chalcogenide alloys and photoconductors"	"Diagnostic radiology in hospitals and dental clinics is increasingly making use of flat-panel, digital X-ray detectors.  This technology has many potential advantages over conventional film-based imaging such as instant imaging, digital image enhancement, and lower patient dose and higher resolution.  There are a number of basic designs for digital detectors; one of the most promising uses selenium as the active layer.  The selenium absorbs the X-rays and directly converts them into charge that can be collected and measured using associated electronics.  The performance of the detectors depends on many factors--one of the most critical is the electrical noise.  One source of electrical noise is the selenium itself because all materials that conduct produce random fluctuations.  My recent research has produced the first measurements of the electrical noise in selenium.  These measurements provide the data that design engineers need to correctly model the performance of the detectors.  One goal of the proposed research is to extend these measurements to newer selenium alloys and multilayer structures that are being considered for direct-conversion X-ray detectors again with the goal of aiding the proper modeling and design of such detectors.     The other major component of the proposed research involves the optical properties of compounds containing a chalcogenide--the elements sulphur, selenium, and tellurium.  Chalcogenide alloys are already in use in various optical devices including writable CD-ROM and DVDs.  In my lab, we can produce thin films of multi-component chalcogenide alloys.  We will do fundamental materials research especially on the optical properties of these alloys and assess them for potential use in future photonic devices.""331095,""Johari, GyanPralash"
"337828"	"Jutla, Dawn"	"Client-side multi-agent system for contextual private data management for e-commerce and m-commerce on the semantic web"	"Existing private data management and related privacy enhancing tools (PETs), aimed at increasing the users' control over their private information, are fragmented, limited in capability and usability, and thus are far from suitable for wide scale user adoption. There is also an urgent need for client-side technologies to complement, and thus strengthen the goals, of the enterprise technologies that organizations' employees use to protect and manage customers' private data, in accordance to recent legal guidelines in many countries. Currently, technology-based privacy solutions take a one-size-fits-all approach. This research's goals are to provide a high-level of customization around user decision-making depending on the contexts in which the user is being asked to release private information. Contexts and customization are important and useful mechanisms, as privacy, ethical, social, and economic concerns regarding data handling differ according to user profiles, customer, and other stakeholder groups. Such concerns differ too according to physical (spatial and temporal) situation and types of data. This research proposes a client-side private data management system to support users' contextual and privacy-aware data release to organizations when conducting commerce over the Internet and mobile networks.  Scientific problems that will be investigated include suitable socio-technical privacy architecture, knowledge representations to support privacy-aware user contexts in e-commerce and m-commerce, intelligence-based algorithms and agents for supporting these contexts, intelligent algorithms and personal agents for increasing privacy software usability, and novel agent-based privacy Web services and infrastructure. While the objectives of this research address scientific problems, the user-visible outcomes are technologies that enable users to make informed and highly-nuanced contextual decisions around releasing private data to organizations. The technologies will also alert users as to potential privacy problems, remind them of socio-economic contexts when conducting e- or m-commerce transactions, and allow users to organize and view reports on the private data they have given to organizations under particular circumstances.""332413,""Jutras, Pierre"
"338386"	"Kabanza, Froduald"	"Goal driven integrated intelligence for autonomous systems intelligence planifiée pour les systèmes autonomes"	"Developing autonomous systems--whether hardware (e.g., robots) or software (e.g., computer-aided education tools)--that can handle novel situations for which they have not been explicitly programmed remains a complex and challenging task. Most of the current tools and methodologies are, at best, in the experimental stage. Reacting to unanticipated events, planning and monitoring the steps to accomplish a task, and learning from experience all remain substantial problems that can be further complicated by the system's intrinsic limitations in perceiving or acting on its environment. One objective of this project is to develop new automated planning algorithms for complex robotics processes that are beyond the capability of today's planning systems and to integrate these algorithms with other robot decision-making processes. This will enable robots to accomplish tasks that are difficult with current approaches, such as assisting people in everyday life activities, performing surgery or inspecting and repairing space stations. Another objective is to develop new planning algorithms for tutorial dialogues that are impossible with current approaches and to integrate these algorithms into simulators to support the learning and control of robot manipulation procedures. This will increase the safety of robot manipulations and reduce training costs (simulations are less expensive than the actual robotics equipment and there is no risk of injury or equipment damage). These simulators can be installed anywhere and anytime to provide ""just-in-time"" training to support continuing education necessary for operators to maintain their skill levels (e.g. during long missions). This project primarily targets the application domain of robotics, but the research results have potential applications in many others.""341218,""Kabashin, Andrei"
"338948"	"Kabbani, Adnan"	"Interconnects aware design methodology for high-performance nanometer CMOS circuits"	"With technology down scaling, the number of transistors on-chip has increased dramatically and reached up to 250 million transistors in 65nm technology as predicted by Semiconductor Industry Association Roadmap. This caused interconnect lengths, density and complexity to increase. Combined with higher operating frequency it has become increasingly difficult to ignore the effect of interconnects on the design performance such as delay, power, bandwidth, and noise. This effect has shifted the performance influential factors from gates to interconnect despite major changes in materials such as copper and low-dielectric-constant insulators. The focus of the proposed research work is to study the increasingly important role interconnects play in determining the performance of the current and future generations of CMOS high-speed ICs. The investigator will use these results to develop interconnect aware design methodology for high-performance nanometer CMOS circuits. This methodology will reduce the turn around time and achieve fast timing closure, which is very important to lower the design cost and shorten time-to-market.""337292,""Kablar, Boris"
"338373"	"Kaczynski, Tomasz"	"Computational topololgy and its appliations (topologie computationnelle et ses applications)"	"My current research interest is focused on the new domain of mathematics called Computational Topology.  The objective of research in this domain is to develop a combinatorial and algorithmic approach to abstract structures of topology with the aim of applying them to sciences and engineering.  The topological structures I am presently working with are cubical complexes, their homology, and homology of continuous maps on cubical sets. The applications that motivate my research fall into two categories. One is in the area of nonlinear analysis and dynamical systems, for example computer-assisted proofs in dynamics. The main topological tools involved in this category of applications are homology, fixed point index, and Conley index. The other category of applications is related to qualitative analysis of numerical data in the area of imaging science and in a variety of engineering problems such as automated search for defects in the microstructure of materials. The main topological tools involved in those applications are cubical and cellular complexes, relative homology, classical and discrete Morse theory.""341140,""Kadla, John"
"338891"	"Kadoch, Michel"	"Reliable multicast and related issues in mobile ad hoc networks"	"Mobile ad hoc network (MANET) is an economic solution for wireless communications, because it doesn't require any prior investment in fixed infrastructure. At the same time, the popularity of group computing has grown rapidly. Multicast is a very useful and efficient means of supporting group-oriented applications, especially in mobile/wireless environments where bandwidth is scarce and equipment has limited power. However, more work is needed to bring these two technologies together. Our research proposal focuses on reliable multicast in mobile ad hoc network, which is crucial to military or disaster rescue wireless multicast applications. To obtain reliable multicast in mobile ad hoc network, a recovery scheme must be employed to ensure end-to-end delivery of unreliable multicast packets for all group members. Moreover, mechanisms coping with node movements are also indispensable. A combined analysis with the hybrid FEC/ARQ scheme and the mobility prediction Aided Dynamic Multicast Routing algorithm will be investigated. More work will be undertaken on the nature of the multicast routing protocols while considering the Minimum Hop Tree (MHT) approach and taking into account the maximum route life. Specification, Validation, Mathematical Analysis, Simulation, and Testbed Experiment are the five stages in any implementation of a protocol.""343626,""Kadosh, Jesse"
"338411"	"Kégl, Balazs"	"Developmental and applications of efficient and reliable machine learning methods"	"In this proposal my primary goal is to study, analyze and develop novel efficient and reliable machine learning methods that are capable to process information coming from diverse and loosely structured data sources such as the Internet, large biological databases, or other heterogeneous databases. My research is grouped around the three major areas of machine learning. My main objective in learning theory is to continue to explore the possibility of using data-dependent probabilistic bounds of the generalization error, and to define and analyze nonparametric probabilistic models of unsupervised learning. On the algorithmic side, my main goal is to further broaden the scope of boosting methodologies in order to develop a modular, transparent, and interactive learning system that can learn from data as well as from user interactions. Finally, I actively participate in applying machine learning techniques to solve software engineering, bioinformatics, and music processing problems.""336860,""Kehl, Steven"
"337815"	"Kerherve, Brigitte"	"Metadata management for quality-driven delivery"	"The recent advances in distributed systems and wireless technology, as well as the development of sophisticated applications, now allow users to access any information, from anywhere with any device. In the context of such complex environments, and more specifically for continuous media delivery, Quality of Service (QoS) management has been introduced to support service provision with quality requirements, traditionally focusing on varying communication conditions. If we consider the notion of QoS from a broader perspective, we have to position the user at the center of the process and allow him to express non-functional requirements specified according to high-level quality dimensions that are later mapped onto low-level dimensions. For that purpose, we propose a quality-driven delivery (QDD) approach, aiming at providing user oriented QoS management through the integration of different mechanisms, such as quality information management, system components interoperability, distributed execution of QDD activities or multi-criteria optimization.   In the framework of our research program, we will focus on quality information management to support QDD for distributed multimedia applications. Quality information can be any type of metadata describing the computing or communication resources, the data to be delivered, the technical infrastructure or the user characteristics. Such information is essential to support QDD activities and decisions, and mechanisms should be provided to store, access, share, transfer, integrate or exchange quality information. We will investigate model management and model engineering approaches to propose, design and implement such mechanisms. Model management and model engineering have recently attracted a lot of interest in database, software engineering and semantic web research communities. We believe that they are particularly adapted for the design and development of flexible and extensible metadata management strategies. We plan to validate our approach with specific applications in the fields of media arts, health-care and video delivery.""339337,""Kerman, Ronald"
"338917"	"Khan, Gul"	"Hardware-software co-synthesis of high performance embedded computers and development of smart home embedded systems for the elderly"	"An embedded computing system is part of a larger system that supports the functioning of overall system. Applications of embedded computer systems range from sophisticated aircraft flight control to consumer products. Traditionally, embedded computer have been developed as a two-stream process, where hardware engineers deliver special-purpose computers that are programmed by software engineers. Optimal performance is achieved when hardware and software are properly 'tuned' to each other. Hardware/Software co-design methodologies combine the hardware and software design efforts to produce efficient embedded systems with lower time to market. The co-synthesis is one of the problems encompassed by hardware/software co-design that deals with automatic (or semi-automatic) design of hardware and software to meet the system specification. Co-synthesis process starts with task assignment from a task graph to processing elements (allocation) followed by confirmation of whether the system meets requirements (usually through scheduling). We are focusing on distributed-memory embedded computers, where some distributed memory topologies (e.g. hypercube, binary and quad-tree) provide flexible and reliable architectures.    This research is motivated by the need to automate the embedded system design process to produce high performance devices. Embedded applications that would particularly benefit from high levels of performance and fault-tolerance include aerospace, medical instrumentation and safety critical systems. The co-synthesis techniques are being employed to design high performance embedded systems for medical devices, smart home, and industrial embedded applications. We are investigating active-RFID (Radio Frequency Identification) tags based embedded systems for object monitoring and tracking the occupants of smart homes. One could attach RFID tags to shoes or clothes of the elderly persons, and then monitor them in real-time. Health care provider can use such systems to monitor elderly as well as people with dementia. It would help the elderly to live at home longer, contributing to a better quality of life for the Canadians.""342459,""Khan, Jameel"
"338409"	"Kharma, Nawwaf"	"Evolvable pattern recognition systems"	"BACKGROUND. Project CellNet (Kharma et al., 2004) is one of a handful of research projects that aim at producing generic Pattern Recognition systems, which can be configured for particular recognition tasks, using Genetic Programming (GP) and some measure of training.   MOTIVATION. The current version of CellNet (CellNet-CoEv) is still an intermediate step on the path of automatic synthesis of a complete multi-classifier. The features are still hard-coded: we intend to replace them with a parameterless GP-based feature creation module. Also, the next version of CellNet will produce complete multi classifiers. METHOD. We intend to maintain the overall emphasis on the use of Genetic Programming for the purpose of feature creation and selection, as well as classifier synthesis. However, we will  introduce other techniques to the next version of CellNet. Some serious pre-processing will be carried out to create a prototypical sum image per class of patterns. This will be followed by the automatic creation via GP, evaluation via statistical techniques, and employment of feature detectors. Finally, the selected features will be used by a classifier, which will be thoroughly assessed using standard pattern databases. CONTRIBUTIONS. The main anticipated contributions are: (a) The production of a fully evolvable multi-class pattern recognition system, which can deal with a range of real-world symbol/signal classification tasks; (b) The development of some leading-edge Genetic Programming techniques, suited to the problem of automatic Feature Creation, in the area of Pattern Recognition; (c) A better understanding of how complexity can arise from simpler forms, given the right mechanisms and initial conditions. REFERENCES. Kharma, N., Kowaliw, T., Clement, E., Jensen, C., Youssef, A., and Yao, J. ""Project CellNet: Evolving an Autonomous Pattern Recognizer"", International Journal of Pattern Recognition and Artificial Intelligence, Vol. 18, No. 6 (2004) 1-18.""330025,""Kharouba, Heather"
"338449"	"Kherfi, MohammedLamine"	"Indexation de la vidéo et intégration des différents types de médias"	"Avec la multiplication rapide des chaînes de télévision et l'apparition de la vidéo sur demande, nous assistons aujourd'hui à la production d'une quantité gigantesque d'enregistrements vidéo de différents types : films, documentaires, émissions sportives, bulletins d'information, etc. Gérer cette quantité phénoménale d'information et l'exploiter d'une manière optimale est cependant devenu plus difficile que jamais. À titre d'exemple, au moment de la réalisation d'un nouveau documentaire, on a souvent besoin d'y inclure des passages extraits d'émissions existantes. Retrouver ces passages est cependant une tâche fastidieuse et longue qui requiert parfois plusieurs jours de visualisation séquentielles des émissions existantes. Le système que nous comptons réaliser se veut donc un outil d'aide pour tous ceux qui travaillent dans le domaine audiovisuel. Sa tâche principale serait d'organiser les enregistrements vidéo, de les indexer, de les résumer, et d'aider à retrouver les passages voulus en un temps raisonnable. Les applications de cet outil sont aussi nombreuses qu'importantes. Ceci inclut l'extraction des faits saillants des différents types de matchs et la production de leurs statistiques, l'indexation et le classement automatique des enregistrements existants, l'aide à retrouver rapidement des passages dans des émissions existantes pour des fins de réutilisation, la vidéo sur demande où l'on permet au téléspectateur de retrouver facilement un passage qu'il vu auparavant, le filtrage de la vidéo et l'aide au contrôle parental, ainsi que le résumé automatique des bulletins d'information. Ce projet s'inscrit dans le cadre global d'un système de gestion de bases de données multimédia que nous comptons développer. Une fois achevé, ce système devrait permettre de gérer différents types de média incluant le texte, les images, la vidéo, et le son.""339798,""Khesin, Boris"
"337876"	"Khriss, Ismail"	"Une nouvelle approche de rétro-ingénierie basée sur les transformations"	"Les compagnies font face à d'énormes coûts pour maintenir leurs applications informatiques. Au fil des ans, les codes de ces applications ont accumulé des connaissances corporatives importantes (règles d'affaire et décisions de conception). Mais après plusieurs années d'opération, d'évolution du code et de roulement de personnel, ces connaissances deviennent difficiles à récupérer. Les développeurs doivent consacrer beaucoup de leur temps à essayer de comprendre le code des applications, une activité connue sous le nom de  « compréhension de programmes ». Comme il a été estimé que cette activité accapare entre 50% et 90% du travail d'un développeur, simplifier le processus de compréhension de programmes peut avoir un impact significatif dans la réduction des coûts de développement. L'objectif de cette proposition est de construire une nouvelle approche de rétro-ingénierie qui, à partir de l'analyse du code source d'un système logiciel (en Java pour commencer), permettra d'en extraire une représentation abstraite en deux composantes : - un modèle décrivant les règles d'affaire que contient le système logiciel - un modèle décrivant les décisions d'architecture et de conception du système. Cette représentation abstraite facilitera la compréhension, et par conséquent l'évolution, du système logiciel.""332101,""Kianoush, Reza"
"338379"	"King, Valerie"	"Algorithms and data structures for network problems"	"The widespread use of the Internet has made us all vulnerable to the spread of worms and viruses, and resulted in a cost to the economy of billions of dollars. The goal of the research proposed here is to enable processors in a network to coordinate their efforts to perform such tasks as automatic worm detection in a way which is robust against a malevolent attacker who  controls some portion of the network. The problem of coordination which is robust against attacks has been long studied, but the methods were not designed for the large networks which we have today. The work proposed here emphasizes techniques which are feasible for use in large networks including the peer-to -peer networks that exist today.""387075,""King, WilliamAllan"
"337863"	"Kirkpatrick, Arthur"	"High-fluency haptic interfaces"	"This research is intended to improve the productivity of experts working with computers.  Designers, planners, and analysts routinely use computers to create their work products (designs, plans, and so forth).  Their productivity is partly limited by the input technologies they use.  They cannot manipulate ideas as fluidly on a computer as they can with a pencil and paper.  This project explores methods of working with computers that use the human sense of touch as a supplement to vision.  By making use of a larger range of human senses, user productivity can be improved.""337935,""Kirkpatrick, David"
"338441"	"Kobti, Ziad"	"Evolutionary learning in complex social networks"	"Complex systems and agent-based modelling are attracting a growing interest from experts from various disciplines to simulate and study real world applications. The implications are of significance because they broaden our understanding of little known complex social phenomena, including reasoning about the causes of observed system behaviour and predicting the outcomes of tested case scenarios or interventions. Exciting systems that this field encompasses include: terrorist networks, prehistoric civilizations, and vehicle safety. An agent for instance can represent an individual, a household, or a driver. System designers often build a state based configuration for agents to react in a dynamic environment. Hence, the model provides a sandbox approach for the researchers to test various hypotheses. For example, an ancient civilization can be modelled using real field data where an archaeologist can then examine the effects on the emerging population size and movement under drought conditions compared to normal conditions in order to test a hypothesis suggesting that a drought caused the population to leave the study area. The research program proposes applying new techniques in agent learning when developing agent behaviour in a complex system. Beyond the basic finite state machine that an agent can use to react to its dynamic environment, a more realistic approach for agent intelligence proposes reactive agents that can accumulate knowledge at the individual and population levels, learn new plans and evolve new strategies on their own to fulfill their goals. Human behaviour however, is distinguished from other animal behaviours and consequently the conventional evolutionary frameworks need to be extended in order to build more realistic artificial agents. Through the proposed methods, the outcome of this research program will enhance the learning theory in artificial models and researchers working with agent-based simulations would be able to create more accurate and intelligent models using the algorithms from this work.""333666,""Kocabiyik, Serpil"
"337824"	"Kontogiannis, Kostas"	"Model driven software development and evolution"	"Large software applications are designed and developed in distinct phases. These phases pertain to specifying the requirements and the business objectives, designing the system, implementing the source program, testing the individual components and the application as a whole and finally, deploying the system. In the software engineering literature we consider that once software systems are deployed on a site then they enter a new phase which is referred to as, the maintenance phase. In this respect, designing a new system pertains to forward engineering while understanding and maintaining existing systems pertains to reverse engineering and evolution. Traditionally, forward engineering and application evolution were considered as two distinct areas of software engineering research that share very limited, if any, common models, processes, techniques, and tools. It is only recently that the research community and the software engineering practitioners view software development as an iterative and incremental process, where models and artifacts are slowly and incrementally designed, developed and, refined until the full application is completed. Such approaches consider that software development should take the form of consistent, systematic and incremental understanding, manipulation, transformation, evolution, and synchronization of software models. These models co-exist in specialized tools called Integrated Development Environments or IDEs. In this context, the objective of the proposed research is to investigate, design, and implement novel techniques to support the representation, transformation, and synchronization of software models in order to facilitate the semi-automatic, iterative and, incremental design, development and, evolution of large industrial systems. To date, there is only limited research on generalizing such model transformation frameworks that can be used to support the next generation of Integrated Development Environments. This research is expected to provide insights on how to design the next generation Integrated Software Development Environments.""331588,""Kontopoulou, Marianna"
"338418"	"Kotsireas, Ilias"	"Algorithms in algebraic combinatorial design theory"	"Combinatorial Designs are used in every day life, even though this is not always visible. Satellite transmissions use Error-correcting codes which are based on certain special kinds of combinatorial designs. Wireless communication protocols use combinatorial designs as well. The theory of combinatorial designs uses extensively tools from Mathematics (from areas such as Linear Algebra, Group Theory, Number Theory) and Computer Science (from areas such as Algorithms, High-performance computing, Metaheuristics). The main focus of our project is the study of combinatorial designs from a Computational Algebra point of view. Formalizing combinatorial designs via Computational  Algebra allows one to employ powerful techniques to study them. An important outcome of our project is the creation of databases with inequivalent designs. These databases are available to statisticians and other researchers.  ""340938,""Kott, Laima"
"338942"	"Krishnamurthy, Vikram"	"Stochastic sensor scheduling and optimization"	"Several statistical signal processing applications (e.g. radar systems and civilian sensor networks) require dynamic allocation of a finite amount of resources  due to physical, communication or computational constraints. In this project we address the issue of how to dynamically schedule sensors and control signal processing resources to extract signals from noisy measurements.  This project comprises of three inter-related parts: (i) Development and analysis of novel stochastic scheduling and optimization algorithms using structural results in dynamic programming and simulation based optimization (ii)Applications of these algorithms and analysis in radar resource management and sensor networks. (iii) Applications of these simulation based stochastic optimization algoritms in Brownian dynamics and Molecular dynamics simulation of  biological membrane ion channels. The outcomes of this project are novel algorithms and analysis that permit the design of computationally efficient high performance sensor management algorithms and efficient stochastic simulation algorithms for ion channels.""386602,""Krishnamurthy, Vikram"
"338945"	"Kumar, Shiva"	"Design of high speed optical transmission systems"	"The objective of the proposed research is to study the transmission impairments in fiber optic systems and design new components and systems that enhance the quality of transmission. In this research program, we propose to develop new system designs and multiplexing schemes that provide the best  transmission performance. We also develop analytical expressions for the error probability of systems based on differential phase shift keying (DPSK) in different propagation regimes. These analytic expressions are important because the calculation of error probability of a nonlinear fiber-optic system by numerical simulations is a formidable task.  In this research program, we will develop novel time division multiplexing(TDM) schemes based on soliton interaction to reduce the time-bandwidth product of a signal. Such a scheme increases the spectral efficiency and hence, it would enable to transmit more data in a channel with limited bandwidth. For cable TV applications, it is desirable to design optical fibers that can carry large powers without back-reflection due to stimulated Brillouin scattering (SBS). Large optical power launched into the optical fiber causes lattice vibrations resulting in acoustic waves. The acoustic waves reflect the incident optical signal due to acousto-optic interaction. In the proposed research program, simulation tools to model acoustic propagation and acousto-optic interaction in fibers will be developed. With the help of these tools, we explore  new optical fiber designs that suppress back-reflections. We also develop new techniques to suppress the back-reflection due to SBS using isolators and dissimilar fiber geometries. The system designs investigated during the research programs are benefecial to Canada because several Canadian industries are interested in these designs in which it is possible to launch high optical powers to their system/device.""335426,""Kumar, Shrawan"
"338878"	"Kwan, HonKeung"	"Advanced digital filters without and with neural techniques"	"Filters are core sub-systems in an expanding spectrum of products in communications, consumer electronics, and others. Emerging applications have placed an increasing demand on the robustness, speed, and power of a filter to be accomplished within a shrinking product cycle. This research attempts to provide practical solutions to the associated problems. The robustness of a filter refers to its coefficient sensitivity, roundoff noise, and stability properties, and to its abilities to handle overflow. Different filter structures have different robustness properties. For a specific application, what will be the optimal structure to achieve desired robustness? Real-time functionalities and applications demand a filter to operate extremely fast and with low delay. Different filter structures have difference speed and delay properties. For a specific application, what is the best structure for real-time operations? Nowadays, portable applications impose an increasing demand on battery with a longer operating time, while overheating reduces processing speed and life. There exist different techniques for reducing the power consumption. For a specific application, what will be the best strategy for low-power solution? Once a filter structure is decided, we need to design its coefficients for applications. Recursive filters are more compact but their designs involve nonlinear problems that have no global solution. For a specific application, what will be the best design strategy? To answer the above questions, we propose to continue research on optimal structures and design algorithms for digital filters, Gabor filters, and adaptive neural/fuzzy-neural filters. Some of the research ideas are summarized here. In the search of an optimal filter structure, what is the relationship between the robustness of a filter structure and its passivity and stability properties? The most time-consuming and power-consuming element in a filter is multiplier. Is there a smart way to reduce or eliminate multipliers in a filter structure? If feasible, it would indeed provide a high-speed and low-power solution. What is the best way to solve the difficult nonlinear design problem of a recursive filter? How about a combination of techniques such as a combined global and local search?""343642,""Kwan, Jonathan"
"338875"	"Kwok, Harry"	"An investigation into the electronic transport properties of organic semiconductors and thin film devices"	"Organic thin film semiconductors offer a competitive alternative for the making of electronic devices and circuits. Our key reasons for studying these materials arise from low cost, that some deposition processes are amenable to large area fabrication, and the variability in the physical properties of the deposited films. Currently, light-emitting diode displays using organic thin films are being manufactured and numerous prototype organic thin film transistor circuits have been reported. Unlike the case of single crystal semiconductors, our understanding of the organic thin film semiconductors is limited. Many important issues linked to energy band structures, impurity states, charge excitation and recombination, etc. remain to be resolved.  As far as the electronic circuits and devices are concerned, the important parameter affecting performance has been the charge mobility. This parameter generally controls the speed of operation of the device, the gain when the device operates in an amplifier circuit and the charge dynamic during switching. Our proposed research addresses a number of related issues and attempts to examine theoretically (in conjunction with experiments) how charge mobility can increase beyond its current achievable value. Our earlier work suggests that this is very possible. The other significance of this work is to train technologists and engineers in this upcoming field to cope with foreseeable demands particularly in areas such as flexible electronics and sensory circuits. These are areas where the industries in Canada are in a good position to excel given the type of prevailing demands in health care and environmental engineering.""343385,""Kwok, Samantha"
"338930"	"Labeau, Fabrice"	"Robust multimedia transmission through generalized multiple descriptions"	"This research project is focused on enhancing the reliability of multimedia transmission over unreliable network. The target applications are in emerging services in the area of information and communications technologies (ICT), that will demand high volumes of data (such as video) to be transmitted over heterogeneous networks with good reliability, with a seamless integration of wired and wireless technologies. Typical examples of these emerging services include telemedicine (and in particular remote diagnosis and monitoring), security (and in particular video surveillance of large or geographically distributed areas) and intelligent home system (in particular in-home multimedia streaming, broadcasting and delivery). This project aims at developing new compression paradigms to transmit multimedia data, around the principle of multiple-description (MD) coding; this amount to sending several approximate versions (descriptions) of the original multimedia signal across a channel, so that a receiver having all descriptions available will be able to reconstruct a very good reproduction of the original; on the other hand, if some descriptions do not reach the receiver, decoding is still possible with a lower quality, so that the higher the number of available descriptions is, the better the reproduction will be. The main topic of this proposal is to develop further the concept of MD coding, to generalize it to new channel models, including a study of the possibility of decoding from several partially corrupted descriptions, and finally to adapt this whole system to the predictive nature of recent video coding standards.""343057,""Labelle, André"
"337847"	"Labiche, Yvan"	"Class (integration) test orders: further research and pratice"	"Any incremental class testing approach for an object-oriented software system has to answer the two following questions: What integration process, indicating in which order classes are (integration) tested, should be selected? Which test design techniques should be used to unit and integration test classes? Although there is a fairly large number of reported works on both questions, much remains to be done. On the one hand, class (integration) testing techniques have mostly been described without any consideration for any integration process. On the other hand, integration processes have been suggested without much consideration for how they could be used in practice, along with class (integration) test design techniques. It appears that there is a lack of research and practical results on how a class integration process can be used to conduct class (integration) testing. Therefore, it is often difficult to answer any of the two aforementioned questions: We do not know how the two kinds of strategies can be applied together, and what kind of benefits would be revealed by such combination. This research proposal aims at providing more trustworthy (research) results to test engineers and managers on how, and to what extent, class (integration) testing techniques benefit (or not) from using an integration process. This research may also lead to the definition of new class (integration) testing techniques and new class integration processes, as well as prototype tools supporting the (semi-)automation of these strategies. The overall objective is to facilitate class (integration) testing in object-oriented software systems, thus likely leading to reduced testing costs.""324090,""Labonté, Jessica"
"338979"	"Labun, Andrew"	"Analytical models for complex, heterogeneous structures"	"Large semiconductor integrated circuits such as microprocessors are the most complex engineered structures known.  Paradoxically, they are composed of very simple components designed according to very simple rules.  This simplicity makes it possible to use computer software to automate their design and to check the layout drawings for errors and problems.  This project aims to spur development of complex structures on chips that aren't simply electronic, but might also include small machines or gas channels for all sorts of applications such as ""labs-on-a-chip.""  The focus in this project is on the ""layout,"" the drawing which shows the shapes and placement of all the transistors, interconnecting wires, and all other aspects of the chip, as they are to be manufactured.  The purpose of this project is to develop software tools to automatically analyse the layout to determine the sensitivity of the chip's properties to variations in the manufacturing process.  These variations can not only affect chip performance but also the ways that the wires embedded within chips heat up as electric current passes through them.  The project will go on to apply the same rapid, automated analysis to other physical effects such as diffusion of gas and mechanical deflection.  Automating the analysis of non-electrical aspects of a chip layout will mean that engineers can analyse chips which combine electrical and non-electrical functions.  This will enable them to design more complex structures on chips than ever before and to check their designs to make sure they'll work without having to build them first.  This will make the development of new products based on these complex structures faster, less risky, and more feasible.""339619,""Labute, John"
"338873"	"Lacroix, Suzanne"	"Dispostifs photoniques tout fibre nouveaux concepts nouvelles applications"	"Dispositifs photoniques tout-fibre Nouveaux concepts, nouvelles applications Au cours de la dernière décennie du XXème siècle, la photonique a donné un essor spectaculaire aux télécommunications.  En dépit des apparences, la demande n'a cessé de croître dans ce domaine et il est essentiel de continuer la recherche et le développement de nouveaux produits pour répondre à ces besoins.   Par ailleurs, le potentiel de la fibre optique dépasse le domaine des télécommunications à haut débit : d'autres types de réseaux de fibres optiques et d'autres applications des composants tout-fibre voient actuellement le jour.  Par exemple, certains besoins (en particulier, de diagnostics ou de transport de puissance laser à des longueurs d'onde  inhabituelles) que le domaine biomédical  manifeste, peuvent être comblés par les composants photoniques tels des capteurs tout-fibre.  Un autre domaine d'application encore plus «exotique» est celui de l'information quantique qui comprend la cryptographie quantique et l'ordinateur quantique.  Dans ce cas, il s'agit d'utiliser les fibres pour transporter un photon à la fois tout en gardant ses propriétés optiques (telle sa phase) qui portent l'encodage. La proposition de recherche vise à  apporter des éléments de réponses à ces besoins.  Quelques exemples d'applications sont donnés: un convertisseur de fréquence  tout-fibre pour le domaine des télécommunications à haut débit, un modulateur de phase tout-fibre pour la cryptographie quantique, une porte logique pour le calcul quantique, une «micropipette intelligente» pour le domaine  biomédical.""329447,""LacroixVachon, Benoit"
"338896"	"Lakhsasi, Ahmed"	"Dynamique thermique pour l'analyse des microsystèmes électroniques"	"Ce projet de recherche introduit une nouvelle orientation pour la caractérisation de la dynamique thermique dans les microsystèmes électroniques. Le projet de recherche reconnaît l'importance grandissante des microsystèmes sur puce qui constituent la convergence de plusieurs dispositifs, systèmes et technologies comme solution d'intégration du futur pour l'industrie de la microélectronique. Cependant, l'ampleur de l'intégration de microsystèmes électroniques complets et l'augmentation de la vitesse d'opération amènent des problèmes de contrainte thermomécanique insurmontables. Si ces aspects ne sont pas traités convenablement cela représentera une menace sérieuse pour le design des microsystèmes du futur. Plus spécifiquement, ces aspects deviennent critiques lors du design des circuits VLSI (Very Large Scale Integration) concernant la sécurité et la fiabilité de ces nouveaux dispositifs microélectroniques. Pour ces derniers, l'idée principale de cette recherche couvre les aspects de la caractérisation de la contrainte thermomécanique. Bien que les microsystèmes sur puce couvrent différents domaines physiques, la caractérisation de la dynamique thermique représente le coeur des préoccupations actuelles et futures de l'industrie de la microélectronique. De plus, ces domaines partagent différents aspects dont la température est la principale préoccupation commune et qui pose des défis énormes pour les méthodologies de design des microsystèmes. Dans cette recherche, la méthodologie adoptée pour la caractérisation de la dynamique thermique des microsystèmes sur puce sera basée sur la généralisation des concepts de couplage électrothermique. Cette méthodologie a donné des résultats très encouragent pour la caractérisation des aspects de la dynamique thermique dans les circuits VLSI. Ces concepts seront alors généralisés aux microsystèmes pour créer une nouvelle méthode de caractérisation de la dynamique thermique dans les microsystèmes électroniques à haute densité dans une perspective de développement d'une unité de contrôle intégrée de la contrainte thermomécanique.""326040,""LakinThomas, Patricia"
"338405"	"Langer, Michael"	"Appearance models for computer vision"	"My research is concerned with fundamental questions about the visual appearance of the natural world.  How can we mathematically model visual cues such as shading, shadows, mirror reflections, texture, defocus, and motion parallax ?  How can we develop algorithms that are based on these models and that allow a vision system to make reliable inferences about 3D world, namely inferences about object layout and shape, surface material, and illumination.  The physics (i.e. optics) of image formation has been well studied and many models exist at  varying levels of simplicity.  The challenge from a computer vision point of view is to develop models that support reliable inferences from images.  The main approach I take in my research is to identify particular kinds of scenes and vision problems that have been neglected in the past.  For example, I am addressing the appearance of surface texture on highly slanted planes, such as at the boundary of a smooth object or at the horizon, and in particular I am examining how the appearance of texture depends on focus. Another problem is the relationship between shading and texture and how these cues can be distinguished in images.  A third problem is motion parallax which can occur in a variety of guises, included clutttered scenes such as forests, but also on curved mirror surfaces such as glass or metal.   In addition to addressing each of these problems in isolation, my research will also examine mathematical and computational relationships between these problems.   ""335267,""Langevin, André"
"337897"	"Lank, Edward"	"Inferring user intention in stylus input systems"	"The personal computing revolution of the past two decades has moved to a new phase where the keyboard-mouse interface no longer dominates humans' interactions with computers.  While the long-term future may involve conversational interfaces, retinal projection, and computer vision recognition of hand gestures, the hardware technology currently being adopted most rapidly is directed toward pen or stylus input, on both bibliographic surfaces (i.e. personal-sized, normally horizontal, e.g. PDAs, Tablet PCs) and epigraphic surfaces (i.e. shared, normally vertical, e.g. SMARTBoard, iRoom). One attractive aspect of stylus-based interaction is the fluidity of interaction it appears to promise. Interactions between a pen and a computer application can include data input, annotations, free-hand drawings, the selection of content, and the interaction with user interface objects such as menus and buttons.  Users, using a stylus-based entry device such as a tablet computer, want an interaction experience that moves fluidly between these operations.  Pen-tablet computers, however, have typically not delivered the fluidity of interaction expected by users. These programs require frequent mode switches when switching tasks, and result in slow, pedantic input for all but the simplest of tasks. This research seeks to add intelligence to the activities performed by a stylus-based computer system when capturing and analyzing the actions of the user.  To do this, this research aims to build models of both the content created by the user with a pen and current actions performed by the user with the pen.  Given these models, the research aims to then develop computer programs that analyze these current actions in context to reason about the intention of the user when performing actions in interfaces.  The end result will be tablet interfaces in which users move fluidly though actions in the application interface. Onus is placed on the software application to understand users' goals, rather than on the user to understand the software application.""342538,""Lankadurai, Brian"
"337864"	"Leduc, Ryan"	"Extending the hierarchical interface-based supervisory control platform"	"The proposed research  investigates the use of discrete-event systems (DES) as a software design, specification and synthesis tool. The advantages of this approach is that it provides an intuitive graphical representation  of the systems behaviour as well as a precise formal description immediately usable for verification and synthesis. The automata can then be used for code generation for the target software language.    The research will target  three of the main obstacles to this approach: 1) a design, verification, and synthesis method capable of handling industrial strength problems,  2) a reliable real-time implementation method for DES supervisors, and  3) a graphical software tool to implement the algorithms and allow users to design, verify, synthesize, and  simulate DES based systems, and automatically generate code based on the results.      The hierarchical interface-based supervisory control (HISC) method from the applicant's doctoral thesis will be used as a foundation. This approach develops well defined interfaces between components to provide the structure to allow local checks to guarantee global properties such as controllability  or nonblocking. Additionally, the development of clean interfaces facilitates parallel development and re-use of the component subsystems.  This approach will be extended to be more flexible and scalable. The next step will be to adapt timed DES, DES fault diagnosis, and a discrete optimization method to the new framework. An implementation method that uses the framework will then be developed, based upon the applicant's masters thesis.    In addition, the DES results will be extended to more general and complex software verification approaches such as model checking and predicate logic based descriptions verified using theorem provers. This should enable these approaches to scale significantly better.    Finally, a software tool will be developed to implement the above theory. The software tool will be developed open source in order to encourage other academics to use, test, and contribute to the software.""341902,""LeducHamel, Mathieu"
"338981"	"Lee, Daniel"	"Extremely fast spreading factor adaptation in spread-spectrum networks"	"Wireless communication and networking services are penetrating Canadian society at an explosive rate, resulting in the engineering community setting ambitious goals to rapidly advance their capabilities.  Two aspects of future wireless services present particular challenges.  First, future wireless systems will need to service a wide variety of applications, including voice, high-speed internet access, real-time multimedia, etc.  Second, the multimedia will exhibit a high degree of temporal variation in its traffic intensity.  These challenges, together with the inherent limitation of wireless resources and the time-varying nature of wireless channels, call for efficient resource allocation and adaptation schemes. The objectives of the proposed research program are: 1) to design, analyze, integrate, and optimize extremely fast transmission rate adaptation schemes in wireless communication systems, and 2) to integrate these schemes into wireless networks and multimedia applications.       Dr. Lee's recent publications presented a spreading-code-based method by which a wireless link can adapt its data transmission rate extremely fast to the wireless channel condition. His preliminary work also shows that the rapid adaptation of data transmission rate to the channel condition can increase the data throughput, for a fixed bit error probability, by many orders of magnitude over the existing transmission technologies. Therefore, it is anticipated that the proposed research will lead to a wireless transmission system with dramatically increased data throughput, and the integration of other components of communication systems such as symbol content detection, error correction coding, and chip synchronization. Joint design and optimization of these components will result in a system that will dramatically improve network applications' quality of service, with particular focus on real-time multimedia in a wireless mobile network.""335618,""Lee, Donald"
"338969"	"Lee, Ivan"	"Distributed multimedia streaming with visual sensor networks"	"Distributed multimedia streaming over ubiquitous networks has gained tremendous momentum in recent years. However, the streaming of multimedia content under the current Internet infrastructure faces rigorous challenges in term of scalability issues. The collaborative streaming technique proposed in this project is an answer to such a challenge. The research will be led by the applicant who has had extensive experiences in multimedia signal processing, wired and wireless communication, and embedded system development, in both industrial and academic environments. Distributed Multimedia Streaming with Visual Sensor Networks will focus on the following key research areas: (1) visual sensor network, (2) multimedia signal processing, (3) on-demand region-of-interest zooming, (4) distributed multimedia streaming over ubiquitous networks, and (5) peer-to-peer overlay content routing and security. In addition, two sub-projects will be investigated: (1) multimedia gateways, and (2) smart projectors. This proposed infrastructure represents a complete end-to-end solution for media content acquisition, communication, and display. This project aims to develop enabling technology to make the best and innovative use of today's network infrastructure for multimedia content distribution. The generic nature of the proposed streaming techniques makes it suitable for live broadcasting, e-entertainment, telemedicine, distance education, and multi-site videoconferencing. The short-term goal of this project is to decentralize the computation and bandwidth loads for multimedia applications over the existing internet infrastructure, while long-term goal is to promote Canada as a global leader in multimedia communication.""332510,""Lee, James"
"338864"	"LeNgoc, Tho"	"Broadband communications systems"	"We are proposing a research plan to develop collaborative multi-dimensional transmission and resource allocation techniques, as applied to the current and next-generation broadband access systems. The communications systems are generally power- and bandwidth-limited while the communications channels are characterized by their frequency-selective fading and the presence of noise and interference. Due to the dynamic nature of both transmission media and multimedia traffic, re-configurable and adaptive schemes are of particular interest. Furthermore, in most broadband communications systems, interference is the major performance-limiting factor and interference mitigation can be more efficiently done by coordination of the transmission and resource allocation between users to minimize mutual interference in a collaborative manner. Cooperation between users in transmission and resource allocation can also be extended to obtain broader coverage and to mitigate channel impairments without the need to use large power at the transmitter and hence to achieve substantial gain in system throughput. Multi-user cooperative transmission concept coupled with multi-hop relay approach promises a power-efficient solution for future wireless communications systems. Performance gain can be particularly significant in multi-input, multi-output (MIMO) systems when cooperative transmission concept is used in conjunction with multidimensional (space, time, frequency, and spreading code) coding and/or pre-coding schemes. The proposed research is significant to both wireline and wireless broadband communications systems such as xDSL, WiFi (IEEE802.16), WLAN (IEEE 802.11), PAN (IEEE 802.15), Ultra-Wideband (UWB), and should be undertaken to enhance Canada's competitive position in providing advanced communications systems. Our industrial contacts are a definite asset for the eventual application of this research in new operational systems.""341554,""Lenk, Teegan"
"337883"	"Lhotàk, Ondréj"	"Static analysis of object-oriented and aspect-oriented programs"	"Program analysis is a technique for proving properties about the possible behaviour of programs when they are executed.  It is used by software development tools such as compilers, verification and testing tools, and program visualization tools. The first objective of this research is to make program analyses more precise (that is, to enable them to prove some properties that existing analyses cannot) using a technique called context sensitivity. Context sensitivity is widely expected to improve program analysis particularly of object-oriented programs, but it is currently too expensive to be usable in practical tools. The proposed research is necessary to reduce its cost. An improvement in program analysis precision will enable compilers to generate more efficient code, and software engineering tools to provide programmers more information about their programs. The second objective is to develop program analyses specifically for the features of aspect-oriented programming languages. Aspect-oriented programming is an emerging style of software development which allows programmers to organize and modularize their code more thoroughly, in order to make it easier to understand the interactions between related concerns. New program analyses are required to analyze the new features introduced in aspect-oriented programming languages, so that compilers for these languages can produce efficient code, and development tools can provide information about aspect-oriented programs to programmers.""341889,""Li, Aimee"
"338918"	"Li, Baochun"	"Self-evolving and high-performance overlay architectures"	"The current Internet faces many fundamental challenges, including the lack of support for emerging applications such as multimedia streaming, the lack of trust and security, and the emergence of mobile and sensor networks at the edge.  The Internet has increasingly become a hybrid network, with a myriad of networking technologies connecting mobile devices, sensors, and set-top boxes.  In this proposal, we argue that most of such challenges in the current Internet are amenable to the treatment of a new self-evolving, self-healing, and high-performance overlay architecture, that is well designed and responsive to changes at all time scales.  Such a new self-evolving overlay is to be implemented in the application layer, without changes to the Internet Protocol (IP) architecture. Specifically, we propose to design a new overlay that bring higher performance, better scalability, better robustness, better immunity to malicious traffic, and more flexibility.  This new overlay will serve as the foundation of the new wireless Internet.  The new overlay not only takes advantage of the idle upload capacities at the edge computer systems, but also constantly evolves its network topologies to adapt to network dynamics, and to converge to the highest possible performance.  To provide better immunity to malicious traffic, we apply principles of machine learning and bayesian networks to design a mesh of filters in the application layer, such that malicious traffic and attacks may be recognized, learned and filtered over time.  To accomodate edge heterogeneity, we propose to study new overlay mechanisms to provide seamless integration and optimal resource allocation in a hybrid Internet with mobile and sensor devices.  To support a new array of multimedia applications, we construct a set of overlay algorithms specifically tailored to the needs of large-scale multimedia streaming and conferencing.  We plan to realize our vision of such a new self-evolving and high-performance overlay with large-scale emulations and a real-world implementation.""338425,""Li, Ben"
"338374"	"Li, Ming"	"Bioinformatics software tools and Kolmogorov complexity"	"This proposal is in two areas: bioinformatics and Kolmogorov complexity. Part I: Building a comprehensive biological sequence annotation system. This system consists of homology search tools (PatternHunter), motif finding and protein family classification tool (COPIA), protein threading tool (RAPTOR), coding region detection tool (ExonHunter), and protein-protein interaction literature mining tool (PathwayFinder). In developing these tools, we have invented the optimal spaced seeds for homology search and linear programming formulation for protein threading. We plan to investigate new ways of studying the following problems: (a) Linear programming formulation for loop modeling, side-chain modeling, refinement, and folding transmembrane proteins. (b) New indexing structure for large scale DNA databases. (c) Theory of optimal spaced seeds. Our invention of optimal spaced seeds has started a new trend of research. We will study the theory of spaced seeds in depth. (d) Further development of the widely used PatternHunter software in order to better serve the bioinformatics research community and industry. Part II. Kolmogorov complexity. We will do research in two directions: (a) Incompressibility method and average case analysis of algorithms. Over the past 15 years, we have been developing the incompressibility method and used it to solve a number of open problems. For example, recently we have successfully applied the method to analyze the average case complexity of Shellsort, an open problem for 40 years. We plan to further develop this methodology and use this method to analyze more algorithms. (b) Information distance, introduced by us in 1991, is now an accepted concept for measuring information distance between two objects. Our normalized information distance (2001) has found applications in whole genome phylogeny, English document classification, plagiarism detection, google distance, and various time series classification. This research will further generalize such concepts and develop more applications. Our research has directly created a part of Canadian bioinformatics industry (BSI); our research results are, and will continue to be, used in proteomics reseaerch and pharmaceuticals to help speed up drug discovery.""327152,""Li, Ming"
"338454"	"Li, Yuying"	"Computational optimization in finance and beyond"	"My research combines numerical methods with advanced computing technologies to develop efficient and robust tools for modeling financial risk. Complex financial instruments such as derivative contracts have become increasingly important in capital markets. Accurate pricing and risk management tools for these instruments are essential. They enable investors to take advantage of market opportunities while shielding them from undue risk. My research will focus on developing such tools for new financial innovations where standard methodologies cannot be readily applied. Hedge fund investments, for example, have recently attracted tremendous interest from institutional and individual investors. As a result, risk analysis and modeling of hedge funds are of crucial importance. However this is very difficult for several reasons: a lack of transparency, the many dynamic trading strategies that the managers employ, and the fact that only a small amount of data is available to model a complex relationship affecting returns. In addition, robust calibration of an appropriate model for asset price evolution from the traded market option prices is crucial for accurate pricing and effective hedging of exotic derivative products. I intend to utilize novel mathematical formulations and to creatively combine numerical methods with advanced computing technologies to solve some of these problems. I will investigate appropriate risk models and develop reliable and efficient tools for hedge fund risk analysis which will be of use for financial professionals. In addition, my investigation can potentially lead to discovery of new mathematical models and numerical methods, which can potentially be applicable to similar problems in other areas of finance, insurance, and resource management.""324103,""Li, Yvonne"
"337856"	"Lin, Guohui"	"Algorithm design and analysis, and software tool development, in bioinformatics and computational biology"	"Algorithm design and analysis plays a central role in computer science. Bioinformatics and computational biology are emerging interdisciplinary areas of study that require expertise from computational sciences and biological sciences. On one hand, biology has become a high-throughput science where its applications are desperate for advanced computing techniques to digest the huge amount of biological data to provide further understandings leading to new biological discoveries. On the other hand, the practice of algorithm design and analysis on the computational problems formulated out of the biological applications helps improve our understanding of the nature of computation, and perhaps triggers new revolutions in computer science. The proposed research has exactly this two-fold significance, where we focus on two biological applications: protein Nuclear Magnetic Resonance (NMR) spectroscopy and whole genome phylogenetic analysis. Protein NMR spectroscopy is one of the two key technologies that determine protein three dimensional structure at the atomic resolution, where in order to obtain accurate structure a nearly complete resonance sequential assignment is a prerequisite. Our research focus in this project is the automated spectral data analysis to guarantee nearly complete resonance sequential assignments. Upon success, we would be able to transform the currently time-consuming and expensive technology into a high-throughput one. The impacts of phylogenetic analysis using whole genomes are that individual genes might suggest controversial evolutionary pathways and that whole genomes are believed to contain the richest evolutionary information. The key thing in this line of research is to represent whole genomes in a way that the phylogenetic analysis can be efficiently and effectively done. We have recently proposed some new representations, and with them, we are interested in the phylogenetic analysis on avian flu viruses to picture their global migration and evolutionary patterns. The expected results will help in fast identification of emerging avian flu virus and fast determination of vaccination strategies.""332945,""Lin, John"
"338403"	"Linney, Norma"	"Development of computer vision algorithms for tissue identification and segmentation in medical ultrasound images"	"Ultrasound is one of the more patient friendly medical imaging modalities. It is the modality of choice for infants as the equipment can be brought to the bassinette or incubator and administered with minimal disturbance or side effects to the infant. Similarly the equipment can easily be brought to adult patients and the procedure is well tolerated by the extremely ill. However, ultrasound images contain speckle and noise making the extraction of diagnostic information from these images more challenging than for other imaging modalities. The goal of this research project is to develop automated software tools that will help the physician extract relevant diagnostic information from these ultrasound images. In particular to develop an automated technique for the identification and characterization of tissue types so that ultimately healthy tissue can be reliably distinguished from unhealthy tissue. Improving the viability of using a more patient friendly imaging modality while still achieving the same or better clinical results will have a significant impact on patient health and health care costs.""323663,""Linte, Cristian"
"338461"	"Liu, Jiming"	"Development of autonomy oriented computing (AOC) systems for distributed computational problem solving"	"Autonomy oriented computing (AOC) is a bottom-up computing paradigm for solving computationally hard problems and for characterizing complex systems. Drawing on our previous work on AOC (Liu et al., 2002, 2004,2005), in this proposed research program, we aim to systematically model and develop autonomy oriented self-organizing systems as well as to empirically validate the capability and robustness of the AOC systems in solving large-scale computationally hard problems, such as distributed constraint satisfaction and optimization problems. In so doing, we will formally characterize the basic AOC elements, their local interaction and self-organization processes, and thereafter gain an in-depth understanding of the AOC performance from the points of view of systems optimization and topology-based computational complexity studies.   This research will have direct impacts on several important systems applications: (1) ( megascale) The development of e-transformation (e.g., e-learning) solutions, in which the service related problems are large-scale, highly-distributed in nature, and can readily be formulated, solved, and evaluated using the AOC approach; (2) (microscale) The development of autonomy oriented sensor networks that perform the tasks of collective spatial mapping (or world modeling) while satisfying their resource and energy constraints; (3) (nanoscale) The design and development of amorphous computing devices such as nanomachines and cellular computers, in which computational or programming means for generating coherent behaviors from vast numbers of unreliable parts, called ""computational particles"" (whether microfabricated particles on a chip or engineered cells for assembling nanoscale structures), are essential that are locally-interacting, irregularly-placed, and asynchronous.""328264,""Liu, Jing"
"338368"	"Lowe, David"	"Object category learning and recognition"	"Computer vision systems are starting to be used for many important applications, such as warning drivers of impending accidents, diagnosing disease from medical images, or allowing robots to perform useful tasks in household environments.  However, the capabilities of human vision remain far beyond what computer vision systems can do.  We are able to look at a scene, photograph, or drawing and immediately recognize the objects that are present.  Computer vision has made dramatic advances within the past few years, but has only reached the point at which small well-defined object catgories (such as ""faces"" or ""cars"") or larger libraries of specific objects (such as a database of particular buildings) can be reliably recognized.  The research in this proposal aims to expand the capabilities of computer vision systems so that they can generalize to much broader categories of objects and scenes, and thereby open up a broad range of new applications.  The best current approaches to object class recognition are based on using many local features corresponding to scale-invariant patches of an image.  We propose to extend the useful set of features to a much broader class of image descriptions.  We will develop features based on local contours, shapes, and textures.  A particular area of investigation will be feature classes that are robust to background clutter because they respond only to features that are likely to be part of the object while ignoring regions outside its bounding contours.  We intend to examine a range of learning methods, based upon AdaBoost, SVM, or sparse logistic regression, that can provide feedback on selecting appropriate image features for particular object categories.  We will also work on new methods for pose clustering and verification.  Our goal is to combine all of these new components into a complete system for object class recognition, opening up important new applications for computer vision.""334540,""Lowe, Robert"
"338893"	"Lynch, William"	"Error resilience and quality in compressed video"	"The objective of this grant is to study issues surrounding video compression. The main issues I will study are error resilience for compressed video, and automatic measures of quality and how they can be used in coding/transcoding and joint coding/transcoding. The work here will be on algorithmic development. The error resilience of video is an important topic because video is often transmitted over unreliable communication channels (wireless, networks where data may be dropped or delayed due to congestion). Compressed video is fragile in the sense that small errors can propagate over large portions of the uncompressed video. Methods that alleviate the effects of transmission errors would have broad future applications. Transcoding is the translating of the video from one format to another. Here it is from a higher bit rate to a lower bit rate. This could have applications in a video on demand server, or as an alternative to packet dropping in a network. Joint coding/transcoding is the compression of several bit streams onto a single channel. Coding or compression methods usually aim to respect a Constant Bit Rate (CBR) constraint with the aid of an appropriate buffer. Within this constraint the compressor attempts to give the highest possible quality. If what one guarantees to a user is a constant bit rate then this is appropriate. However, it might be more useful to guarantee to the user a constant quality, as is done for telephone voice service. Peak Signal to Noise Ratio (PSNR) is usually the quality measure. This grant will build on previous work, in developing utomatic quality measures.""344860,""Lynes, Rebecca"
"338402"	"Ma, Bin"	"Bioinformatics algorithms and applications"	"Bioinformatics has become an essential part of genomics and proteomics research.  Almost every type of biological and biochemical data needs to be analyzed using computers and special software tools.  The success of the wet lab experiments depend greatly on the sensitivity and accuracy of the algorithms and software employed for data analysis.  The proposed research program concerns the algorithm design and software development in bioinformatics. Because of the complexity of the data and unavoidable experimental errors, bioinformatics algorithm design and software development are challenging, yet very necessary.  The proposed research program will allow the applicant to continue his successful work into bioinformatics algorithm and software development.  The research will be focused on, but not limited to, the analyses of genomic sequence data and mass spectrometry data. Specifically, the research project will develop full-sensitive and high-speed homology search algorithm and software tools.  Currently the full-sensitive homology searches are performed with costly parallel computers.  The research potentially replaces the costly hardware solution with highly efficient software and inexpensive PCs.  The project will also develop software tools for identifying proteins and their modifications using tandem mass spectrometry data.  Protein identification is a fundamental step for almost all proteomics research and tandem mass spectrometry is a standard experimental method.  However, the existing software is insufficient.  This research has impact for drug development and other human-health related research.""385755,""Ma, Bin"
"338977"	"Ma, Yuan"	"Controllable, continuous and large tilt optical micromirror device integrated with VLSI technologies"	"The objective of this proposed research program is to develop an integrated technology platform where a micro optical mirror can be tilted continuously over a range most desired by telecom industry. Standard CMOS/BiCMOS processes will be used for the fabrications and novel concepts of micro-scale actuations will be explored.      Electrostatic actuation is favored for many MicroElectroMechanicalSystem (MEMS) applications because of its low power consumption. An important property (or limitation) of the electrostatic actuators is their pull-in voltage, beyond which the electrostatic torque overcomes the mechanical torque, and the movable plate snaps abruptly to the fixed electrode plane. Only about 30% of the full tilt range of a typical electrostatic actuator is controllable. One major goal of this project is to increase the controllable tilt of movable plate substantially. It will be achieved using ""intelligent"" electrodes and hybrid actuation scheme.      In another aspect, complexities of optical network component operation require sophisticated electronics operations. The proposed efforts will be based on VLSI platform and an electronic control algorithm will be developed to address the snapping effect of the electrostatic mirror. Supporting circuitry will include power supply, D/A and A/D conversion, data acquisition, signal conditioning, and digital signal processing.         The applications for the proposed systems include a variety of display subsystems, bar code scanning, adaptive optics systems, microfluidics and telecommunications. In optical network field, it can be used for products like attenuators, equalizers, multiplexers, cross-connects, data transmitters and optical switches. The integration of optical MEMS and micro-electronics offers enormous potential for not only significant performance and cost improvement, but also opening the doors for many novel applications which are not feasible for discrete components based system.""341335,""Ma, Yuan"
"338363"	"Mackworth, Alan"	"Constraint-based computational intelligence"	"This research program is focused on constraint-based computational intelligence. We have developed a formal theory for the design and implementation of constraint-based intelligent systems. Our Constraint-Based agent (CBA) framework is a set of theoretical and practical tools for designing, simulating, building, verifying, optimizing, learning and debugging controllers for agents embedded in an active environment. The agent and the environment are modelled symmetrically as, possibly hybrid, systems in the Constraint Net (CN) dynamical systems language, as developed in this research program. We intend to develop and apply the CBA framework, emphasizing the important special case where the agent is an online constraint-satisfying device. Here it is often possible to verify complex agents as obeying real-time temporal constraint specifications and, sometimes, to synthesize controllers automatically. The CBA framework demonstrates the power of viewing constraint programming as the creation of online constraint-solvers for dynamic constraints.Our objectives over the next five years are to develop the theory further and to validate and improve it using experimental implementations on varous robotic systems in our lab. This research program is focused on constraint-based computational intelligence. We have developed a formal theory for the design and implementation of constraint-based intelligent systems.The experimental environment for this work includes the soccer-playing platforms pioneered by us, a CBA Java environment and a set of mobile robots with on-board real-time stereo and colour vision. We propose to use CBA models for the control, perception and coordination algorithms required for multi-agent real-time collaborative activities such as soccer, navigation and manipulation. The scientific novelty of this approach stems from its formal foundation, its practicality and its simplicity as a unitary theory of hybrid intelligent systems. The symmetry of the agent-environment pair model is necessary and unique. The technical significance is that it is a foundation for a new design framework for perceptual, collaborative, constraint-based agents.""386533,""Mackworth, Alan"
"337848"	"MacLean, Karon"	"Physical user interfaces communication of information and affect"	"Touch is crucial to communication in the real world. In contrast, our exchanges with everyday modern technology are haptically impoverished; we are insulated to an extent that physicality has come to feel unnatural, with practical and aesthetic consequences. In a related evolution, the spread of complex and pervasive human-machine systems into areas like automobile cockpits and handheld devices has made information overload and the constant interruptions a way of life.       Using psychophysically grounded design to embed haptics in the world is a path both to restoring what has been lost in the computerization of daily life, and to a new generation of interfaces designed with the goal of respecting and preserving that increasingly precious resource, human attention. The unifying goal of my research program is thus to consider haptic feedback in the broader context of communication, humans to machines or to other humans mediated by a machine. The proposed research has three parts:      1) Methodology for Perceptual Design of Haptic Icons.  My group's past and ongoing psychophysics experimentation has centered on revealing the foundation that will support the design of a new, abstract haptic iconography. We will use this data to develop a new procedural experimental/analytical technique to efficiently generate distinguishable and meaningful sets of haptic icons.      2) Haptic Guidance for Shared Control of Systems. Building on our recently gained knowledge in techniques for haptic guidance in driving-like contexts, we will continue to develop and explore possibilities and mechanisms for haptically-mediated, user/system control sharing in a new testbed environment based on musical instrument control.      3) Bidirectional Haptic Communication of Affect. We will use recently completed test environments and initial results to experimentally establish essential attributes of different communication modalities.""343455,""MacLean, Kenneth"
"338389"	"Maheshwari, Anil"	"Design, analysis and implementation of discrete algorithms for graph and computational geometry problems"	"The proposed research is in the field of design, analysis and implementation of algorithms, a subfield within theoretical computer science. We focus on  problems arising in computational geometry and graph theory. The problems include (a) Geometric path problems - how to find a path between two points in a geometric domain (surface, planar region, three dimensional space amidst obstacles) under various constraints (shortest, monotone descending, minimizing the number of links, multiple criteria) (b) Graph separators - computing vertex/edge separators of planar (or planar like) graphs (c) Visibility optimization problems (with applications in radiation therapy) - need to see a polyhedra (tumor), avoiding/minimizing a set a polyhedra (healthy organs), from a minimum set of locations (possible position of laser guns placed outside the body) (d) Developing generic external memory algorithmic techniques for solving graph and geometric problems (e) Developing algorithmic technqiues for solving problems in variety of computational models - external memory model, parallel computing model, priced information model, streaming and models with very limited extra space (f) Approximation algorithms based on discretization - we have proposed discretization based schemes to solve geometric path problems. Most often spatial problems arising in practical settings are solved by heuristics using some form of discretization. We propose to study these problems, in light of our results, and hope to provide approximation algorithms with provable bounds in terms of their accuracy and complexity (g) Analyzing complexity of geometric algorithms using geometric parameters - traditionally the complexity of geometric algorithms is analyzed with respect to input parameters (number of segments, vertices, faces, etc.), but often in practice, the run-time is very sensitive to geometric parameters - angle, distance, fatness. So the design of the algorithm needs to take these parameters into account; this came up in our work on shortest paths. We propose to expand the design and analysis of geometric algorithms to include geometric parameters (h) Finding ways to make geometrical concepts presentable on the web in an interactive manner.""337605,""Maheswaran, Muthucumaru"
"338959"	"Maier, Martin"	"Design, control, and performance evaluation of all-optically integrated access and metro WDM networks"	"Very recent research results show that currently widely deployed network topologies such as bidirectional rings and hierarchical networks are poorly suited to support unpredicted shifts in future traffic loads and require topological modifications in order to improve their network lifetime. The ability of a network to sustain and efficiently support unpredicted changes and shifts in future traffic loads is becoming increasingly important due to emerging services and applications such as file sharing, on-line games, and grids. These novel applications have specific traffic characteristics which are expected to change today's network traffic loads significantly and already start posing severe challenges on existing access and metro network infrastructures. Apart from alleviating the bandwidth bottlenecks in the first/last mile and at the metro level of today's telecommunications networks infrastructure, novel network architectures and protocols have to be developed and examined which not only efficiently support conventional voice, web, and streaming traffic as well as recently emerging applications, but also support unpredicted shifts in traffic loads due to future, not yet existing applications and traffic types in a cost-effective, pay-as-you-grow, and future-proof manner. The proposed research program aims at alleviating both aforementioned bandwidth bottlenecks. We extend current Ethernet-based single-channel networks to multichannel systems by using multiple wavelength channels in the existing fiber infrastructure. We study the all-optical integration of both access and metro networks and develop novel control paradigms. Apart from voice, video, and data traffic, the resultant access-metro network will be able to efficiently support emerging applications such as file sharing and on-line games. In addition, we will investigate how the proposed architecture efficiently supports unpredicted traffic shifts due to future, not yet existing applications and traffic types. The obtained results will lead to novel next-generation low-cost access-metro networks with a dramatically improved lifetime.""325110,""Maijer, Daan"
"338946"	"Majedi, AmirHamed"	"Integrated superconducting optoelectronics for single photon quantum sensing"	"Technology allowing the combination of photonic and electronic devices in an integrated circuit or module to take advantage of quantum phenomena is the key to providing a promising platform for practical realization of integrated quantum optoelectronic systems. Superconductivity as a macroscopic quantum phenomenon demonstrates the exploitation of quantum effects such as quantum tunneling, quantum interference, quantum phase transition and quantum photoresponse in electronic and optoelectronic devices. SIS (Superconductor-Insulator-Superconductor) junctions, SQUIDs (Superconducting Quantum Interference Devices), superconducting bolometers and superconducting single-photon optical detectors are paramount examples of such devices. Combining these quantum characteristics with their unique electromagnetic behavior (ultra-low electrical resistivity and magnetic flux insulation and quantization), superconducting electronic and optoelectronic devices can be employed for ultra-low-noise/ultra-low-power and ultra-fast/high-frequency electronic and optoelectronic functions such as detection, mixing, amplification and switching. Superconducting quantum electronic and optoelectronic devices are now in high demand for applications in quantum sensing, quantum metrology, on-chip circuit cavity quantum electrodynamics and quantum information processors.The general research proposal will involve the utilization of integrated superconducting passive and active optoelectronic devices for ultra-sensitive quantum photonic sensing. The research program will involve theoretical analyses, development of design methodologies, and computational simulation with recommendations for fabrication layout and methods of performance evaluation.""340609,""Majewski, Marek"
"338414"	"Makarenkov, Vladimir"	"New algorithms for the classification and visualisation of evolutionary and biomedical data"	"My research proposal includes four major parts: 1. Species evolution has long been modeled using only phylogenetic trees. In such a tree, each species has a unique most recent ancestor, whereas other interspecies relationships, such as those caused by horizontal gene transfers or hybridization, cannot be represented. First, I propose to develop two methods, based on the distance and probabilistic approaches respectively, for the prediction and visualization of possible horizontal gene transfer events. The complete and partial gene transfer models allowing for a number of important evolutionary constraints will be considered. 2. Second, I intend to develop a method for the analysis and representation of reticulate evolution by hybrid networks. I will design an algorithm and corresponding software for computing the smallest number of hybridization events required to explain conflicting gene trees and develop an algorithm to determine whether an arbitrary rooted network can be realized by contemporaneous reticulation events. 3. Third, I propose to continue the investigation of the problem of phylogenetic inference from sequence data containing missing bases. A new effective approach allowing one to estimate unknown nucleotides prior to computing the evolutionary distances between them will be designed and tested in simulations. 4. My fourth objective consists of designing and implementing methods for studying and correcting high-throughput screening (HTS) data. High-throughput screening is an effective technology for drug discovery. I plan to develop two methods for adjusting the hit selection procedure during HTS experiments. The proposed techniques will allow researchers to minimize the impact of systematic error affecting the selection of potential drug targets.""337618,""Makaroff, Dwight"
"337811"	"Malton, Andrew"	"Strategies for software source migration"	"Proven software systems are established value in business and in the software industry; but in the fast-moving world of software development, proven systems must constantly adapt to changing external conditions.  The competitiveness and health of Canadian software industry depends on this. When an existing system is adapted to a new environment, a new compiler or programming language, or different support software and operating system appear around it.  One way to manage this adaptation is by ""source migration"", which involves substantial changes to the source code, in order to maintain it properly in the new environment. In the proposed research, we develop software design recovery from source, and automated source transformation strategies, under four headings: multi-layered software architecture, dialect (compiler version) migration, object-oriented framework migration, and rapid migration plans using dynamic and static analysis.""325889,""Maly, Kenneth"
"337827"	"Manjikian, Naraig"	"Architectures, components, and software tools for system-on-chip design and implementation"	"Improvements in microelectronics fabrication technology continue to enable the integration of many components onto a single chip for use in a variety of applications such as computer systems, portable telephones and other consumer products, and sophisticated biomedical devices. This type of integration has many benefits including higher performance, reduced power consumption for longer battery life in portable and embedded systems, and physically smaller devices and systems. In order to aid the designers of single-chip systems in addressing the complexity of the task of developing such systems, the proposed research will investigate novel architectures, components, and software tools for implementing highly-integrated chips. Architectures define how the processing, memory, and other components are logically organized for implementation within a single chip. The components themselves can be customized to provide the appropriate functionality to suit a particular application. Finally, sophisticated software tools can aid designers in selecting, configuring, and combining the appropriate components into the most suitable architectures for implementation. Given the high complexity of single-chip systems, the aim is to automate as much of the entire process from architectural conception to detailed implementation as possible. Because the actual implementation of highly-integrated custom chips to characterize the benefits of new architectures, components, and software tools is an expensive undertaking, the proposed research will rely on the use of prefabricated, programmable chips and the associated software tools for converting design specifications into hardware configurations for these chips. Advances in microelectronics fabrication technology have given these programmable chips the same benefits as customized chips, hence the growing capacities of programmable chips are sufficient for prototype implementation of large single-chip designs that will be generated in the pursuit of this research. The experience and results that are obtained from this approach can then be applied to the implementation of custom chips.""340112,""Manjunath, Puttaswamy"
"338380"	"Marchand, Mario"	"Learning algorithms and risk bounds for sample-compressed Bayes classifiers"	"This research project belongs to the subfield of artificial intelligence known as machine learning. More specifically, we work on the well known classification problem (also called the pattern recognition problem). The success of many applications, such as those arising in bioinformatics, computer vision and natural language processing, is often strongly limited by the efficiency of the learning algorithm used to solve an underlying classification problem. Many learning algorithms have been proposed over the past forty years but, too often, none of them are efficient enough to meet the requirements of the application. Consequently, our main long-term objective is to understand better the circumstances under which efficient learning is possible and find the most efficient algorithms that can learn in these circumstances. To progress in this direction we plan to continue our exploration of learning algorithms, such as the set covering machine, that produce sample-compressed classifiers. These are classifiers that are identified only by a small subset of the training data and for which we have a good performance guaranteed which is specified in terms of a risk bound. A risk bound is a bound on the probability that a classifier makes a classification error on a new (unseen) example. Furthermore, a risk bound is computable from what a classifier achieves on the training data. In our quest to obtain better classifiers, we now want to design efficient learning algorithms for producing a weighted majority-vote of sample-compressed classifiers. For this task, we first propose to search for a tight risk bound for this type of classifiers and then propose efficient algorithms to find a classifier achieving a low risk bound. We also want to extend this approach the the common cases where part of the training data is unlabelled and to the cases where the loss incurred on misclassifying an example depends on the example's class. Tight risk bounds should provide new optimization problems and, hopefully, new and more efficient learning algorithms.""326779,""Marchand, Maryse"
"338909"	"Mason, Ralph"	"High data rate wireless transceiver ICs providing quality of service (QOS) for streaming media"	"The main focus of this effort is directed at getting good performance from low-cost integrated circuit technologies.  Three novel technologies being developed are on-chip filters, low power oscillators and low jitter subsampling circuits that are used as part of a wireless receiver. The significance of this work will be the ability to produce low cost highly integrated CMOS wireless integrated circuits with capabilities to support multimedia applications such as streaming audio and video.""344157,""Mason, Ross"
"338342"	"Matwin, Stanislaw"	"Topics in machine learning and data privacy"	"My research is in the area of machine learning/ data mining, known also as knowledge discovery. I am interested in increasing the capacity of the existing learning systems, and in making them more suitable for using text data. I am also working on responsible ways of using this technology: hence my interest in data privacy.  Computer's ability of using data about individuals to learn new information about them is an important factor in their privacy. My main idea here is to develop tools that will give people control over their data: everyone should be able to decide who can use their information and for what purpose. One way to achieve this is to control the applications software, and allow it to access and process the data only according to the authorizations of individuals who own the information. I propose to use formal methods, i.e. proving program properties, to guarantee that software that claims to respect user authorizations, indeed does so. The second topic of my proposed research are new ways in which documents should be represented so that they can be efficiently categorized by a computer. My approach is to further explore linguistic properties of documents by looking at their structure, terminology, and the meaning of the terms they contain. I am particularly hopeful that this will improve learning performance of specialized categorization tasks, particularly when data is sparse, e.g. in sentence classification. My final area of interest is scalability of relational learning systems: enabling them to learn from large, real-life datasets. To cope with the information overflow phenomenon, we must employ knowledge information extraction and discovery tools. An identified gap in the knowledge discovery research is the scalability of tools, especially the relational techniques. And yet relational data abounds in the socially most relevant applications: systems biology and pharmacology, and social networks. Hence the importance of relational scaling-up research for knowledge discovery.""335520,""Matyas, John"
"337877"	"McCuaig, Judith(Judi)"	"Adaptive information systems"	"There is great demand for automated processes capable of selecting and presenting information that is both reliable and precise, gathered from sources that are massive and complex. The proposed research will investigate techniques to select and execute contextually appropriate adaptations to information acquisition algorithms based on models of user, domain, task and goal.  Information-based applications, such as information retrieval, are obvious beneficiaries of this research however, adaptive information selection algorithms can also be used for intelligent tutoring, information summarization and fusion, as well applications for which the goal can be modelled.  One such application is the composition of web services, an application that will be the example domain for much of the proposed research. The long-term goal of this research is to develop algorithms to adapt the process for achieving a goal.  These algorithms will be instrumental in creating adaptive information systems that can respond to changes in context not only with changes in information presentation, but also with adaptations to system behaviour.""344311,""McCulloch, Amy"
"326098"	"McHugh, John"	"Computer security research and analysis facility"	"This proposal describes an experimental facility to support a program of research in Computer and Network security to be carried out at Dalhousie University under the auspices of the newly established Privacy and Security Laboratory. The proposed facility supports both observational and experimental studies.  Over the years, the internet has grown to the point that it exhibits emergent behaviors, i.e. aggregate phenomena that cannot be predicted from the behaviors of the individual systems or groupings of individual systems from which it is composed.  These manifest themselves in a variety of ways, including the collapse of parts of the core routing structure during scanning worm outbreaks, and a highly visible disturbance of an otherwise power law connection statistic by a relatively small pool of machines infected with the Welchia-B worm.  By aggregating and analyzing massive amounts of network data, we develop global situational awareness of the network that allows us to place local observations in perspective. The analytical component of the facility supports this kind of investigation. At the same time, the defense of individual enterprises and systems requires local detection and defense measures.  There have been a few attempts at performing evaluations of defensive mechanisms such as Intrusion Detection Systems, but these have been largely unsatisfactory for a number of reasons.  The second major component  of the proposal is  an isolated test facility that supports such evaluations. It can also be used for a variety of projects involving malicious code such as worms and for exploring new ways to both detect and mitigate attacks on end systems and networks. This test facility will also support research projects in a number of related areas, including the generation of realistic background traffic for use in security testing, and better techniques for evaluating intrusion detection and prevention systems.  It can also serve as a dedicated simulation or emulation facility to explore selected aspects of network behavior for larger networks. The facility also supports the training of both researchers and practitioners in computer security practices.""327454,""McHugh, John"
"337894"	"McHugh, John"	"Research program in computer and network security"	"This proposal describes a program of research in Computer and Network security to be carried out at Dalhousie University under the auspices of the newly established Privacy and Security Laboratory. The proposed program covers both observational and experimental studies.  Over the years, the internet has grown to the point that it exhibits emergent behaviors, i.e. aggregate phenomena that cannot be predicted from the behaviors of the individual systems or groupings of individual systems from which it is composed.  These manifest themselves in a variety of ways, including the collapse of parts of the core routing structure during scanning worm outbreaks, and a highly visible disturbance of an otherwise power law connection statistic by a relatively small pool of machines infected with the Welchia-B worm.  By aggregating and analyzing massive amounts of network data, we develop global situational awareness of the network that allows us to place local observations in perspective. At the same time, the defense of individual enterprises and systems requires local detection and defense measures.  There have been a few attempts at performing evaluations of defensive mechanisms such as Intrusion Detection Systems, but these have been largely unsatisfactory for a number of reasons.  The second major thrust of the proposed research plan is the creation of an isolated test facility that can be used for a variety of projects involving malicious code such as worms and for exploring new ways to both detect and mitigate attacks on end systems and networks. This test facility will also support research projects in a number of related areas, including the generation of realistic background traffic for use in security testing, and better techniques for evaluating intrusion detection and prevention systems.  It can also serve as a dedicated simulation or emulation facility to explore selected aspects of network behavior for larger networks. The program also provides opportunities for the training of both researchers and practitioners in computer security practices.""388125,""McHugh, John"
"338443"	"McQuillan, Ian"	"Computational modelling of genetic processes"	"There are many parallels between computer science and genetics.  The structures in both store information as lengthy sequences, sometimes in the form of zeros and ones or perhaps as DNA.  In both fields, we perform operations on those sequences through different mediums, such as electronic circuitry or proteins.  We now have an ever increasing wealth of genetic information at our disposal.  Consider that the genome of a single human being is over three billion nucleotides in length.  It is essential that we find interesting patterns amongst the plethora of data in order to understand the inner-workings of even a cell.  Computer science allows for a natural way to study these patterns, based on its similarity to biological structures and its ability to process massive amounts of information quickly.  By establishing new connections between the two fields, we can then use methods from computer science towards the study of analgous processes in biology.  Furthermore, we could uncover new methods of computation through the guise of biology.  For example, stichotrichous ciliates, a group of single-celled organisms, perform a descrambling process on their DNA to create genes capable of generating proteins.  Descrambling of strings is a mechanism well studied and understood in computer science.  By modelling this process in computational terms, we can create algorithms for studying ciliate genome data.  Moreover, we can attempt to comprehend the exact manner and machinery used for descrambling in a cell.  In parallel, this provides us with a new model of computation found in nature. My short term goals include the comprehension of the genome gymnastics of ciliates as well as the process of frameshifting, a twist on protein translation that allows for multiple proteins to be produced from a single gene. My long term goal is to use computational analysis to converge towards a deep scientific understanding of various biological processes.""345442,""McRae, Hannah"
"337882"	"Mellouli, Sehl"	"Ontologies for fault-tolerant multi-agent systems"	"This research program focuses on multi-agent systems interoperability. Multi-agent systems interoperability means that agents should be able to understand each other. Each agent has a mental state in which it stores the information required for its well functioning. Hence, agents must be able to exchange information between their mental states. However, information could not be described by the same way in each agent. Consequently, agents should refer to a same vocabulary in order to map received information with their own description of that information. This vocabulary is called an ontology. An ontology is a vocabulary used to refer to concepts manipulated in a particular domain. Furthermore, there could be several multi-agent systems in which there are agents which have to interact with each other. If there is at least one ontology defined for each multi-agent system, then we face the problem of integrating several ontologies related to several multi-agent systems. Consequently, in the context where multi-agent systems require interoperability, this research program proposes: 1)to define a methodology to develop ontologies for multi-agent systems based on the context in which agents are, the events to which the agents must respond, and the actions the agents has to perform, 2) to develop new techniques for ontologies mapping by identifying the concepts that will be mapped, and how they will be mapped, 3)to develop approaches which will ease the identification of common and different concepts of ontologies, 4)to propose new algorithms for merging ontologies and detecting inconsistencies when merging ontologies.""326706,""Melnichuk, Svetlana"
"338960"	"Messier, Geoffrey"	"Design and evaluation of ambient wireless networks"	"The goal of this research is to realize novel physical layer designs for nodes in an ""ambient"" wireless network (AWN).  An AWN will consist of 100's up to 1000's of wireless nodes all communicating with each other.  This dramatic departure from traditional wireless network architectures has triggered an equally dramatic change in the types of services AWN's can support.  AWN's are being considered for applications as diverse as patient monitoring in a homecare setting, tracking wildlife or fires in a forest setting, identifying and tracking livestock and automating the tracking and sale of retail goods.  As a result, AWN research is a very active area.  However, much of the effort has been concentrated on algorithms and protocols that operate at the higher layers of the AWN protocol stack.  These algorithms are primarily responsible for network organization and control.     Physical layer design is another important aspect of AWN research that has received less attention.  The physical layer is the lowest layer of the protocol stack and deals with the algorithms and hardware required to transmit digital information from one node to another.  AWN nodes require a physical layer that can reliably communicate digital information while consuming extremely small amounts of energy.  The purpose of this research is to find new ways to meet this challenge.   The first step in this process is to characterize the unusual conditions that AWN's operate under.  A wireless channel measurement program will determine how various AWN operating environments affect the wireless signal.  This data will then be used to develop numerical models for interference between nodes.  This knowledge will serve as a foundation for developing AWN physical layer designs that will minimize energy consumption using adaptive techniques.  The end result will be AWN wireless node hardware that can operate for a year or more on a single battery.""324601,""Messier, Julie"
"338401"	"Mignotte, Max"	"Bayesian statistical models based on global constraints in computer vision and image processing"	"My research program investigates the use of new unsupervised probabilistic or energy-based models including, as novelty, specific geometrical, functional, physiological or spatial adaptive global a priori constraints for restoring, reconstructing and understanding still and moving images.  In this way, this research program will attempt to propose a statistical framework to integrate multiple (low or higher level) image cues (e.g. edges, segmentation map, velocity field, depth map, etc.)  to improve the accuracy of the solution to be recovered or  to fuse multiple image cues coming from  multiple and complementary medical imaging modalities (SPECT with MRI, angiographic with Xrays, etc.) for the efficient restoration or the recognition or 3D reconstruction of anatomical objects or for the overall understanding of certain medical phenomenons. More generally, this research program aims at designing automatic recognition reconstruction and understanding systems of the next generations""338280,""Mihailidis, Alex"
"337852"	"Miller, James"	"Constructing and testing dynamic web-based systems"	"Web sites are becoming increasingly sophisticated, no longer are simple static, HTML centric web sites the norm. At the same time many organizations are becoming increasingly dependent on these sites as vital mechanisms to allow them to interact with their customer base. For example, in 2003 Dell's U.S. home & home office division generated $2.8 billion or 50% of its revenues through its online storefront. These systems present different challenges for traditional desktop applications, and these differences are only likely to grow as industry enters fully into an e-commerce and m-commerce universe. While these systems may only require limited functionality, they are highly demanding upon many non-functional characteristics, including some, which are rarely encountered in traditional systems -such as persuasiveness. In addition, their production period is much shorter in duration. They also tend to evolve at a much faster rate - the ""shelf life"" of a web-based system may be only a few months, and hence their production, or evolution, is, by requirement, highly iterative in nature. These time frames almost mandate the extensive use of a COTS centric production methodology. The implication of all of this is that the production of these systems may be beyond the capabilities of many organisations and yet these organisations are racing headlong to adopt these systems because of their perceived economic advantage and necessity. This proposal plans to investigate various aspects of the hypothesis: How do we construct and test the e-commerce and m-commerce systems of the future? This direction is currently being investigated via an empirical angle, as the proposer is currently heavily involved in the construction and validation of a number of systems with broadly similar characteristics; including systems which seek to provide, en-mass, monitoring to subject physiological parameters via wearable devices and their interaction with various health organizations and authorities; and a I.T. based smoking cessation system based around the integration of peer reinforcement, Social Cognitive theory, and popular theatre storytelling to be delivered via a video game metaphor via cell-phone technology. These projects are providing rich sources of information on the challenges in successfully producing these types of systems. These insights will then be transferred into the development of new production, verification and validation processes, methods and tools to ease the issues associated with these constraints. Despite the significant differences between desktop and dynamic web-based systems, a limited number of specific verification and validation (evaluation) techniques have been produced for this new, and growing, domain. The principal objective of this application is to remedy this deficiency. For example, increasing 'cyberattacks' target the application layer rather than the infrastructure of web-based systems. Hence, these systems need specific techniques aimed at producing secure systems while abiding by the 'business' requirements outlined in the opening paragraphs. Without these techniques, consider the potential issues that Dell, and other companies, face in the future is massive and the potential economic impact if these companies were subject to a successful 'cyberattack' (or other type of functional or non-functional requirements failure) given the volume of finance that they process. To complete the process, these novel techniques would be empirically explored in the projects outlined in the previous paragraph.""328530,""Miller, Jodi"
"338376"	"Mineau, Guy"	"From text mining to knowledge extraction"	"The Semantic Web promises a wealth of ready-to-use information to the world. Companies like Google recently engaged in such an endeavour to provide more than just textual information to its users, but some semantic content as well. Since it is a colossal work to manually encode that information into some formal knowledge representation format, automatic information extraction from text becomes therefore relevant for that purpose. Extracting information is too context sensitive to be applicable on the Web per se. Therefore, concise domains, like corporate Intranets, may prove to offer more realistic application grounds. Also, since queries to such corporate libraries are usually goal (or task) oriented, as opposed to general browsing queries, it may be useful for the query-answering system to benefit from a classification of the texts that compose these libraries. When manually built, these classifications are also too costly to be achieved. Consequently, in that context where documents are (partly) classified with regard to precise subject matters, this research program proposes: 1) to study the integration of various feature selection algorithms into the knowledge extraction process, based on some of our previous work in feature selection, text classification and knowledge extraction, 2) to study the impact of automatic ontology construction tools on the knowledge extraction process itself, 3) to propose some probabilistic assessment algorithm for the relevance of the extracted knowledge structures pertaining to recall and precision, and 4) to propose some knowledge indexing schema that would speed up the look up time needed to filter out the irrelevant documents from the relevant ones.""334773,""Mineau, Pierre"
"338929"	"Miri, Ali"	"Secure infromation transmission and data storage in distibuted systems"	"Deployment of large numbers of ubiquitous wireless communication devices, and distributed databases offers new opportunities and challenges. In this paradigm, users may use their cellphones or PDAs to form impromptu networks. They will authenticate themselves to each other using their biometric traits, and access information gathered by a network of sensors and stored on distributed databases across conventional and new types of networks. The proposed research will address security issues in such a paradigm. It is comprised of the study of problems in three main areas of secure, distributed database and information systems. First, we are interested in security issues related to how information is gathered in distributed systems such as sensor and ad hoc networks. Second, we will examine how information is stored and retrieved securely over distributed databases.  Third, we will study the use of biometric cryptosystems and resource constraint devices for secure information access.""328671,""Mirkovic, Tihana"
"338457"	"Mohr, Jonathan"	"Multiple wavetable synthesis"	"Multiple wavetable synthesis is a music synthesis method that can efficiently produce high-quality musical tones from a compact data representation.  Multiple wavetable synthesis, including variants known as ""vector synthesis"" and ""wavestacking,"" can be used in a variety of contexts and devices, including digital electronic music synthesizers (""keyboards""), cell phones (to produce musical ring tones), and software programs that have an accompanying audio track produced by real-time tone generation (such as game software).    My research program focuses on both the quality and generality of multiple wavetable synthesis, seeking to improve the fidelity of synthesis on devices that have limited resources (such as memory and number of digital oscillators) and to support the generation of a wide variety of tones from a common bank of waveform data.""327452,""Mohseni, Madjid"
"338400"	"Moura, Lucia"	"Covering arrays and combinatorial algorithms"	"This project centers around constructing covering arrays, which are used in component testing of software and hardware. In fact, covering arrays can be used for testing any system for which the following problem arises: select scenarios that specify the level of various factors in such a way that interactions among factors are identified. Components of a software system may have been individually tested, but faults often result from the interaction of certain components. To avoid this problem, we could test every possible combination of components. However, this results in test suits of enourmous sizes even for small number of components. Instead, we can use a covering array, which produces reasonable sized test suits while guaranteeing to test a specified degree of interaction (pairwise, 3-wise, 4-wise, etc). This is a compromise that has been very successfull in fault detection for software interaction. It has been verified in several experimental studies that a large number of errors can be detected by testing  pairwise or 3-wise interaction of components. This project aims at the development of mathematical and computational constructions of optimal covering arrays. The project also covers other topics in the general area of combinatorial algorithms.""337005,""Mourez, Michael"
"338455"	"Murua, Alejandro"	"High dimensional data clustering and pattern recognition"	"Exploratory statistics plays a key role in fields such as bioinformatics, medical imaging and pharmaceutics. Data derived from these fields are complex, large and high-dimensional. Thus most classical statistical methods are inadequate for their analysis. Kernel-based methods often yield good results. However, they have two drawbacks: lack of inference, and too much generality. The former drawback is related to common statistical issues such as model selection, outlier detection, and multiple testing. The latter is related to their being general-purpose methods, and thus they cannot accommodate for specific data point or variable relations. Our long term goal is to develop sound statistical methodology for the exploratory analysis of high-dimensional large size data sets that take into account the two drawbacks mentioned above. We aim at developing models: (A) for finding intrinsic data structure (clustering), patterns (e.g. variable selection), and anomalies (multiple outliers); that at the same time (B) are amenable  for statistical inference, and (C) take into account structural constraints in the data. Short term tasks targeted are: (i) clustering of high-dimensional data using Potts model-like methods; (ii) estimating the clusters and the number of clusters by consensus; (iii) variable selection within clustering; (iv) biclustering; and (v) uncovering strong associations between variables. To account for (B), and to profit from the good performance of kernel-based methods, we seek Bayesian models, or probability-based models that entail non-parametric kernel density estimators and efficient use of sampling mechanisms (e.g. MCMC) to generate samples of the quantities of interest. Some applications guiding this research are discovering groups of genes associated to certain diseases or tumors, finding the role of proteins through association with those whose functions are known, detecting relevant association between drugs and adverse reactions, and discovering patterns in brain responses.""328220,""Muruganathan, SivaDharshan"
"338398"	"Musilek, Petr"	"Artificial immune system for data analysis"	"The current explosion of data and information both in volume and diversity brings about an urgent need for the development of new techniques of data analysis. Data mining (DM) is one of the most important approaches to intelligent data analysis as it can provide insightful summaries into a vast amount of data by extracting more general and thus easily comprehended entities such as patterns and trends. Over the last few years, DM has attracted considerable attention due to the demand for data analysis in various fields of biology, medicine, decision making, information technology, and marketing, to name just a few of the most representative. Although DM represents only about 10-25% of effort spent in Knowledge Discovery and Data Mining projects, it is a crucial step with a great potential for improvements. Major challenges in DM include distributed mining, incremental mining and mining of hierarchical rule systems. Across these types of DM tasks are underlying problems of dealing with heterogeneous data and minimization or elimination of parameters for DM algorithms. These tasks and problems will be tackled in the proposed research project with artificial immune systems as the main source of methodology. Immune systems display a number of interesting characteristics potentially desirable in a variety of artificial systems. From the DM perspective, the most attractive characteristics of immune system are its flexibility, decentralization, and error-tolerance. It is expected that flexibility, directly associated with the ability to adapt and change, can be used to develop algorithms for incremental DM; decentralization can contribute to development of systems for distributed DM and generally alleviate problems of scalability by operating simultaneously on different segments of enormous data sets; and error-tolerance can help handle other major problems of many databases - large amounts of missing and/or noisy data.""326566,""Muslimi, Besa"
"338861"	"Muzio, Jon"	"Methods for the design and testing of digital systems"	"The major area of study concerns the design and uses of Finite State Machines as generators to build onto a chip to provide built-in self-testing of the system on the chip. We have proposed Cellular Automata Registers (CAR) as alternatives to linear feedback shift registers in a number of applications, since, while they are similar to linear feedback shift registers from a theoretical perspective, they generate bit sequences that lead to improved testing behaviour. Our earlier work has led to the only practical algorithm for the synthesis of these machines. However, it is clear that modifying them to be slightly non-linear, gives improved performance for testing. The design and analysis of such machines is considerably more complex, since we lack some of the analytical tools, and the proposed research will continue both with the development of CAR and underlying research into other non linear machines. Our second direction is a new one developed over the last year in the area of reversible logic.  It picks up on some of our earlier work from 25 years ago on restricted primitives for the design of circuits. This are has become of rapidly increasing importance rcently because of the von Neumann/Landauer principle that normal (non reversible) logic operations incur a fundamental minimum energy cost. This cost threatens to end improvements in practical computer performance within the next 15 to 20 years. However, it has been proved that computers based on reversible logic operations can reuse a fraction of the signal energy (in theory, this fraction can approach 100%) that gives us the possibility of continuing to arbitrarily improve computer performance within a specified level of power dissipation. For this to be feasible, we have to develop algorithms for the design and testing of highly optimized reversible logic circuits. This is the goal of our second research area.""326294,""Muzychka, Yuri"
"337805"	"Müller, Hausi"	"Methods and tools for evolving autonomic application software"	"Our long-term objective is to investigate infrastructure, methods, and tools to support the evolution of long-lived software systems. Over the next five years, we propose to concentrate on developing methods and tools to evolve autonomic or self-managing application software effectively. Over the past decade we concentrated on selected research issues in the areas of program understanding, software reverse engineering, software visualization, software migration, and tool adoption. With this experience we are ideally positioned to conduct software evolution research. We will validate and evaluate our research results in collaboration with industry using real-world software systems. As in the past, we anticipate training a significant number of graduate students. The complexity and costs of developing, evolving and managing software keep spiraling upwards since many tasks involved remain mostly manual, time-consuming and error-prone. In response to these trends, researchers in academia and industry investigate techniques that lead to self-managing software (i.e., self-configure, self-heal, self-tune or self-repair) thereby reducing the need for human intervention throughout its operational lifetime. Such self-managing software is often called autonomic. We will concentrate on several dimensions on how to evolve autonomic application software effectively. In particular, we will concentrate on how to inject autonomicity into existing software systems and on how to evaluate architectures for self-managing applications in the context of evolution over long periods of time. To facilitate this evaluation process, we propose to develop a software analysis and reasoning framework for autonomic computing architectures. The reasoning framework will involve quality-attribute specific models for specific functional and non-funcational requirements. Quality attributes include not only traditional quality criteria such as variability, modifiability, availability and security, but also autonomicity-specific criteria such as dynamic adaptation and upgrade support, support for detecting anomalous system behaviour, or support for how to keep the user informed.""335915,""Müller, Jens"
"337838"	"Nascimento, Mario"	"Data management for sensor networks"	"Sensor networks, can be broadly defined as a network of sensing devices capable of processing, storing, sending and receiving data.   A typical sensor is now a minuscule device capable of processing several data entities simultaneously and autonomously as well as communicating  wirelessly. Sensor networks are typically deployed to monitor physical phenomena,  e.g., temperature and humidity levels, within a geographical area and over a time window (limited by the  network's lifespan).  A few application domains where the use of sensor networks is suitable are: environmental  monitoring, biological contamination detection, warehouse management, traffic organization and battlefield  surveillance.  Being autonomous, sensor nodes have a limited power supply, hence, a main constraint in such an  environment is the energy cost for operating the network.  Given the cost of sending/receiving data -typically  orders of magnitude larger than processing the same in memory- a key issue is to minimize data  communication among nodes while at the same time taking advantage of potential cooperation among nodes in the network when processing queries.   Overall, the aim of this proposal is to investigate, from a database management perspective, techniques for storing and querying data within a SN while minimizing energy cost.  To accomplish this goal  several alternatives, including new algorithms and data structures, will be designed and evaluated.""338841,""Naser, Hassan"
"337885"	"Nasser, Nidal"	"Management techniques and architectures in heterogeneous wireless networks"	"Future wireless and mobile networks will rely on multiple radio access technologies, including cellular networks, Wireless LANs (WLAN), e.g. IEEE 802.11 WiFi (Wireless Fidelity), personal area networks, e.g. Bluetooth, and multi-hop/ad hoc variable topology networks. Seamless intersystem mobility across such access heterogeneity will be the capital feature in next generation, labelled Fourth Generation (4G), wireless networks. This change in the technological landscape further challenges today's radio systems by the increasing amount of capacity-demanding services, which span from traditional conversational audio to conversational video, voice messaging, streamed audio and voice, interactive gaming, Web browsing, file transfer, and emailing. No single radio system can effectively provide multi-service delivery if Quality of Service (QoS) is to be maintained. This poses serious issues to networking researchers on both the provider and the user side. Recent interest has been in interworking the different but complementary radio systems that together can provide an unparalleled level of service delivery. The objective of this research is to distinguish and devised effective resource management techniques and architectures for 4G wireless networks. A further inherent objective is to realize design methodologies that emphasize mindful considerations of the mobile terminal's limited capabilities, including battery lifetime and processing power. Moreover, we will propose architectures for end-to-end QoS across such platforms and evaluate alternative technological or methodological approaches for QoS service delivery. Research in this area will offer personalized service delivery over the most efficient/preferred network, depending on the user profile and the type of data to transmit. The research findings of this project represent a definite step towards meeting the ""anywhere, anytime and any form"" communication concept. More importantly, the findings will benefit the Canadian telecommunications industry and business with means to plan, implement and deploy heterogeneous wireless networks in a manner that is both time- and cost-efficient.""339051,""Natarajan, Krishnamoorthy"
"338395"	"Ngom, Alioune"	"Computational intelligent methods for the analysis of molecular data"	"Computational Molecular Biology has emerged as a strategic discipline at the frontier between biology and computer science, and has impacts in medicin, biotechnology and society in many ways. The current research drive in the fields of bioinformatics is to be able to understand evolutionary relationships in terms of the expression of protein function. However, conventional computer science tools are being increasingly unable to address many of the most interesting sequence analysis problems. This is due to the large sizes of the molecular data sets as well as the structural and functional complexities of the genome. My research aims at developing machine learning algorithms that will extend the range of existing applications in bioinformatics. Issues to be addressed are as follows. Current machine learning (as well as feature extraction and selection) approaches to DNA and protein information (such as sequence, structure or function) analysis will be parallelized onto parallel, distributed or grids computing architecture to allow fast analysis. Intelligent methods for DNA and protein information identification, classification, prediction, mining and recognition will be studied in order to build more efficient analysis systems. Biologically inspired heuristics will be applied for the solutions of selected computationally hard bioinformatics problems. Finally, new sequence information retrieval methods that combine multiple DNA and protein information will be investigated. Applications will be conducted on different bioinformatics problems domains.""330230,""Nguyen, BinhDinh"
"338335"	"Oppacher, Franz"	"Machine learning with enhanced evolutionary algorithms"	"Most Machine Learning (ML) work to date has tackled problems of supervised learning where the data are presented in the form of vectors of 1-place predicates and their values. A supervised learning system attempts to find concept descriptions for classes that are, together with preclassified examples, supplied to it by a teacher. Recently, we have developed and tested two evolutionary ML systems. One of these systems is based on a modification of Genetic Programming (GP) and compares favorably with other supervised learners such as C5 but, unlike the others, also learns when given relational data. The second system is a nearly parameter-free evolutionary algorithm that evolves populations of decision trees. This system significantly outperforms several other supervised learners on commonly used test datasets. We intend to extend our algorithms to enable them to perform unsupervised learning as well. Unsupervised learning tasks cannot rely on a target class and prespecified examples, and range from the discovery of concepts through conceptual clustering to the further discovery of empirical regularities relating concepts constructed by the system. Besides being intrinsically more difficult than supervised learning, unsupervised learning also poses serious problems in the post-analysis of the results because testing the discovered relationships is subject to personal bias. We will develop a testing methodology to address this problem. On the basis of our experience with applying evolutionary algorithms to supervised learning tasks we expect that progress with relational learning from richer representations and with unsupervised learning will require several improvements to the standard GP approach. For this part of the research we intend to strenghten GP by introducing a divide-and-conquer approach to problems, i.e. to investigate how building blocks can be automatically composed into useful higher-level modules.""336458,""Opps, Sheldon"
"337887"	"Osgood, Nathaniel"	"MOdel-based approaches for enhancing wireless sensor network usability, programmability, reliability and efficiency"	"While wireless sensor networks (WSNs) offer great potential for use as a monitoring tool, they are currently dauntingly complex to program and use.  Building on the observation that many types of sensor data require incorporation into a model before they can become useful, I propose to reduce complexity and improve power economy and reliability in WSNs by integrating them with explicit models of physical and decision context.  Model-based specification of the physical environment and a WSN's response to it will greatly reduce application implementation complexity.  Incorporating explicit models into WSN application development also permits a model compiler with the information required to automatically distribute model-related computation to nodes across the WSN.  Rather than relaying all sensor data out of the network for external processing, such model-based WSNs will instead be responsible for internally tracking and reporting on a ""best guess"" of the state of the physical environment in a distributed fashion.  In-network model execution will improve power economy by reducing communication, and increase monitoring applications' robustness by lessening reliance on communication to an external central system.       We plan to provide user-oriented database interfaces, programming languages and associated compilers for popular classes of applications, and supporting libraries needed to support model-centric applications development.  The TinyDB WSN database system will be adapted to allow ad hoc or periodic user queries regarding model state and output using dimensions drawn from the model domain and natural concepts such as time and likelihoods.  In the later stages of this project, we will expand the models involved and introduce models of decision context.  By incorporating an understanding of the implications of data for decision-making needs, this integration will permit improved tradeoffs between energy and sensor data.      We expect this work to benefit society by substantially lowering the barriers to cost-effective deployment of wireless sensor-based monitoring in facilities, industrial processes, farms, and civil structures. By enhancing the usability, programmability, and power efficiency of important subclasses of WSNs, these projects should facilitate the development of effective monitoring applications.  Opportunities for contribution are particularly strong in applications involving the monitoring of facilities, industrial processes, farms, and civil structures.""341112,""OShaughnessy, Douglas"
"338394"	"Ozell, Benoit"	"Synthèse d'images réalistes pour la modélisation graphique des flammes de la fumée et d'autres phénomènes physiques"	"L'objectif principal de cette recherche est d'être en mesure de produire facilement des séquences d'images de synthèse réalistes, de qualité photographique, représentant les résultats de simulations numériques du feu (flammes+fumée). D'autres phénomènes physiques pourront ensuite être représentés par les mêmes principes. Pour débuter, nous étudierons les applications des nuanceurs de sommets et nuanceurs de pixels des cartes graphiques récentes à l'affichage des flammes et de la fumée. Nous appliquerons ensuite les techniques trouvées à des séquences d'images et des animations vidéo. En parallèle, nous explorerons les possibilités des blobs (ou métaballes) en tant qu'éléments clés pour la représentation géométrique des surfaces dynamiques complexes comme la surface d'une flamme ou d'un fluide. Par la suite, pour un réalisme plus grand, nous étudierons et intégrerons des méthodes pour: la représentation du degré de carbonisation et de disparition des objets de la scène; l'intégration de photos dans la scène; la visualisation de l'ensemble ou d'une partie de la scène en temps réel et l'utilisation d'un environnement de réalité virtuelle. Dans le contexte du génie informatique, cette recherche se veut appliquée et vise des applications concrètes en génie.  La méthodologie présentée produira des outils pour d'une part rendre plus crédible et plus réaliste le rendu graphique des incendies et d'autre part permettre une meilleure interprétation des résultats des simulations numériques d'incendies.  Une approche visant la création en temps réel de séquences d'images de qualité photographique pour présenter les résultats numériques trouve des applications dans la prévention d'incendies et l'analyse des dommages autant que pour l'industrie des jeux vidéo au Canada.""330798,""Ozga, Jocelyn"
"338362"	"Paige, Christopher"	"Numerical linear algebra: algorithms, analysis and applications"	"In order to solve many scientific, engineering, business and other problems using the computer, we usually convert the problems to subproblems involving matrices --- rectangular arrays of numbers.  Because of this, matrix computations and its theory (often called numerical linear algebra) lies at the heart of most scientific computing.  The key to solving many smaller matrix problems is to find some appropriate factorizations of matrices --- so for example a given matrix can be described as the product of two or more such factors.  However many problems are large or complex, and the resulting matrix problems are extremely large --- a million rows and a million columns is not at all unusual.  And such large matrices are nearly always very sparse --- the vast majority of elements being zero.  Factorization methods can then become too costly, or sometimes simply impossible, and iterative methods may be necessary for solving such problems.  This research develops numerically reliable and efficient algorithms for solving both classes of problems.  It carries out analyses of such algorithms to prove their efficiency and reliability, and sensitivity analyses to show what effects changes in the data (for example caused by finite precision computation, or uncertainty in the data) will have on the final results. These analyses lead to greater understanding of individual problems and their computed answers, and improved general purpose and specific area algorithms. The results of the research are useful for all practitioners in scientific computation.  Some application areas of particular importance in Canada that use these techniques are the aerospace industry (via global positioning systems, aircraft body design, etc.), power generation and distribution, and resource discovery, extraction and transportation.""344466,""Paige, Lisa"
"337905"	"Pan, Jianping"	"Networking a dependable cyberinfrastructure"	"The daily life of ordinary people, ranging from banking to shopping and from learning to entertaining, is increasingly dependent on the Internet-centric cyberinfrastructure. In the last decade, a vast amount of research and investment has been devoted to improving the performance and reliability of the Internet, primarily in its backbone. However, there are still many obstacles left in providing such reliability in access networks and among Internet Service Providers. Denial-of-Service attacks, email spams and web scams have caught many Internet users, both consumers and businesses, off-guard. The control-plane vulnerabilities in Domain Name System (DNS), Border Gateway Protocol (BGP) and Session Initiation Protocol are even more alarming. Existing schemes such as DNS Security and Secure BGP have high complexity and are rarely deployed. For such a complex system as the Internet, it is very surprising to find that there are no systematic and online testing capabilities embedded. In summary, the Internet is still much less dependable than what it should be and most people have taken for granted. Our research program on a dependable cyberinfrastructure focuses on the fundamental networking issues in order to considerably improve the end-to-end reliability, security and testability of the Internet and its accessories. The research explores three highly correlated topics: (1) We want to leverage the reliability at the core Internet with that of wireless personal, local, metropolitan and even wide area networks in ad hoc or hybrid mode. (2) We want to enhance the control plane security for the Internet, particularly in its naming, routing and signaling infrastructures. (3) We want to embed the systematic and online testability into the cyberinfrastructure, rather than being another add-on. The research is also expected to produce highly qualified personnel for academia and industry with growing demands for such talent to meet this grand challenge during the next major transformation of the Internet.""323793,""Pan, Leilei"
"338902"	"Paranjape, Raman"	"Self-organizing mobile agent systems"	"Self-Organizing Mobile Agent Systems The scientific approach adopted is to investigate unique mobile agent capabilities using specific and important problem domains. We will examine the effectiveness of using mobile agents to support electronic health records. We have done some interesting preliminary work on using agents to move health records autonomously through intranets which model an electronically connected health care system including components such as: hospitals, physicians, pharmacies, and medical test laboratories. It is very informative to see how data clusters and identify bottlenecks in the system by modeling various scenarios as to how this system will function. We are also looking at the scheduling problem from the perspective of mobile agents as complete units within the system. Agents-to-agent negotiation and information exchange is used to allow the system to evolve to acceptable and then optimal solutions. Scheduling is a very important problem and we believe by using the fundamental attribute of autonomy of the mobile agent, we can illicit unique and effective self-organization of agent systems to create rapid, dynamic (real-time and changing requirements) solutions. We are initially applying these methods in the relatively simple problem domain of university course scheduling and later we are looking at applying these approaches to form dynamic solutions in situations such as hospital resource allocation and usage. The approach of using agents as self-organizing mechanisms in real world scheduling problems such as university timetabling and the data movement in electronic health records has not been investigated.""336324,""Paranjape, Vireshwar"
"338384"	"Parizeau, Marc"	"Algorithmes évolutionaires, apprentissage incrémental et reconnaissance des formes"	"Les algorithmes évolutionnaires («Evolutionary Computations») sont des techniques émergentes dans le domaine de l'intelligence à base de calcul («computational intelligence»). Dans le contexte d'un problème déterminé, elles consistent essentiellement à faire évoluer un ensemble de structures de données représentant des solutions pour ce problème, on parle de population d'individus, grâce à un processus de sélection naturelle inspiré de la théorie de Darwin et d'une série d'opérateurs, tels les croisements et les mutations, inspirés de la génétique cellulaire. Le processus de sélection engendre une nouvelle génération d'individus alors que les opérateurs génétiques transforment ou combinent ceux-ci pour créer de nouveaux individus que l'on espère meilleurs que leurs prédécesseurs. En apprentissage incrémental, on suppose explicitement que l'ensemble des données devant servir à l'apprentissage n'est pas totalement disponible a priori. Le système doit donc apprendre de façon incrémentale, c'est-à-dire au fur et à mesure que de nouvelles données deviennent disponibles. Il doit améliorer ses performances au fil du temps, mais sans recommencer systématiquement son apprentissage à partir de zéro. Il doit donc être en mesure de: 1) acquérir de nouvelles connaissances sans requérir la mémorisation de toutes les données d'apprentissage; 2) retenir ce qu'il a appris précédemment, dans la mesure du possible, tant que les nouvelles connaissances n'entrent pas en conflit direct avec les anciennes; et 3) accepter de nouvelles catégories de connaissances (classes de formes), qu'il n'a jamais observées auparavant. Pour cette recherche, nous proposons de développer de nouvelles approches méthodologiques basées sur les algorithmes évolutionnaires pour résoudre des problèmes de reconnaissance des formes dans un contexte d'apprentissage incrémental.""325354,""Park, Andrew"
"338459"	"Perkins, Theodore"	"Genetic networks: Dynamics, inference, analysis and evolution"	"Genetic networks can be viewed as the control systems of living cells. Inferring regulatory relationships between genes, which implement this control system, is one of the fundamental problems in molecular biology. Advances in technologies for measuring the abundances of biomolecules, and interactions between them, are providing revolutionary new data for solving this problem, yielding detailed pictures of individual genes as well as cell-wide snapshots of gene activity and interactions. However, the quantity of the data, and uncertainties in it, necessitate new computational approaches for extracting meaningful information. The overall goal of my research is to advance both the theory and practice of the analysis of high-throughput molecular biology data, ultimately providing mathematical and computational tools that are efficient and robust enough to become part of the standard analysis toolkit of molecular biologists. My efforts will focus primarily in three areas: (1) Developing theory and algorithms for inferring regulatory relationships between genes, particularly differential equation models. (2) Applying modeling methods to important biological systems. (3) Developing tools for the functional interpretation and genetic networks.""344568,""Perley, Danielle"
"337901"	"Pfahl, Dietmar"	"The Virtual Software Production Laboratory(VPSL): an innovative approach to accelerate theory building in software engineering"	"The objective of the proposed research is to accelerate theory building in software engineering by complementing and integrating current empirical research by a Virtual Software Production Laboratory (VSPL). A VSPL consists of a systematically developed set of reusable software process simulation modules which can be composed and tailored in order to run goal-oriented simulation experiments. In particular, the multitude of variations of an experiment, which is often necessary to cover different impact factors when exploring technology alternatives, can easily be performed in a controlled manner by simulation. Consequently, learning cycles can be shortened, as different starting conditions, external influences, or process characteristics can easily be generated. The main purpose of the VSPL is to produce (generalized) hypotheses about the effectiveness of software development techniques and tools with regards to defined product quality goals. Based upon the collected (individual) pieces of empirical evidence, hypotheses will be created by systematic variation of context factors (i.e., the set of all other relevant methods, tools and techniques in place, combined with people competencies and skills) through combined Monte-Carlo and process simulation. The results of the simulations will be aggregated into effectiveness profiles which serve as well-defined hypotheses that can be exposed to systematic empirical testing. Since the proposed research aims to deliver a proof-of-concept, it will focus on supporting theory building linked to a restricted class of techniques and tools, e.g., focusing on requirements engineering and architectural design.""333115,""Pfaus, James"
"338372"	"Poole, David"	"Automated decision making with rich representations"	"Computers promise to change the world in the future, even more than they have done in the past, by being able to make decisions.  What is a good decision for an agent (computer, human, or organization) depends on the agent's beliefs, its preferences and its abilities. One of the great results of 20th century science is Bayesian decision analysis: rational agents can make decisions measuring belief by probabilities and preferences by utilities. Another strand of artificial intelligence research considers rich representations of objects that are described at multiple levels of abstraction and detail, using complex ontologies to describe the vocabulary. The aim of this proposal is to allow probabilistic reasoning and decision making with rich representations of multiple objects. Thus we expect an agent to have beliefs and preferences and be able to observe and condition on multiple objects with complex relationships. This includes reasoning about the existence of objects and reasoning efficiently about sets of objects we don't need to distinguish. The challenges in this project are to find representations that are epistemically adequate (are rich enough to solve the problem), can be learned from data and/or acquired from people, and can be reasoned with efficiently. I plan to leverage the advances in probabilistic reasoning that have been made over the last 20 years, and more recent work showing, for example, how to do probabilistic reasoning without grounding. This is important as it's difficult to imagine an application that does not involve reasoning and decision making about multiple individuals.""336404,""Poole, Peter"
"338371"	"Popowich, Frederick"	"Information extraction with knowledge-based linguistic analysis"	"With the ever increasing amount of on-line textual information available to individuals and organizations in numerous languages, it is becoming even more important to provide better techniques to manage, retrieve and analyze this text.  While a wide range of techniques have already been developed that allow computers to automatically detect patterns and relationships, tasks involving the processing of human languages can benefit from additional knowledge relevant for the domain in question, and general knowledge about language and its use. The proposed research will deal with fundamental issues in natural language processing research, examining how linguistic and domain knowledge can be applied to improve the performance and maintainability of applications that make use of information extraction. In dealing with this long term goal, the key short-term objectives will include the development and evaluation of algorithms and resources for the following tasks: 1. anaphora resolution - specifically, techniques to determine the named entity associated with an occurrence of a pronoun like ""he"", ""she"" or ""it"" in a text, as required by information retrieval and information extraction applications 2. Automatic and semi-automatic construction of linguistic and domain-dependent knowledge resources, with an initial focus on medical texts, and algorithms that can be used for information extraction and information retrieval 3. Automatic and semi-automatic construction of multi-lingual resources for information extraction and retrieval, with an initial focus on English and French, and development of algorithms for evaluating the effectiveness of these resources""342102,""Popp, Gregory"
"338870"	"Pulfrey, David"	"Design tools for nano-electro-photonic devices"	"The work proposed here seeks to build compact models to aid in the design of devices  for applications in the strategically important areas of nanoelectronics and nanophotonics. Principally, nano-scale devices built from the carbon nanotube molecule will be investigated. Carbon nanotubes have compelling properties, which will enable applications in electronics and photonics that are far beyond the scope of today's, non-molecular semiconducting devices. Examples of these properties are: diameter-dependent bandgap, near-ballistic transport, ability to be self-assembled via biological recognition, exceptional mechanical strength and thermal conductivity. The main device to be studied is the carbon nanotube field-effect transistor (CNFET), both as a high-frequency electronic device, and as a voltage-controlled light-emitting transistor. Our approach is vertically integrated from the bottom-up, namely: we seek to model each device at the deep, quantum-mechanical level, and then proceed to distill the physics of operation into a set of compact expressions that can be used by designers of circuits and systems for applications in nanoelectronics and nanophotonics. At the fundamental level, the emphasis is on improving the Schroedinger-Poisson solver that we have developed over the last 3 years, so that it can capture the quantum features of practical CNFET structures involving planar gates, finite-size contacts, and nanotubes in which recombination, electron-electron collisions, and non-uniform azimuthal charge distribution, are all present. A time-dependent solution is to be attempted to provide information on AC operation. As the details of operation at the fundamental level become clear, equations of the simplicity required by circuit- and system-designers will be developed to provide tractable models for practioners in the emerging fields of nanoelectronics and nanophotonics. The emphasis will be on improving our models for the high-frequency figures-of-merit for CNFETs, and for creating models for the new, light-emitting transistor, in which the position of light emission along the nanotube can be controlled by the gate voltage.""327227,""Pulichino, AnneMarie"
"338970"	"Qiu, Dongyu"	"Modeling and analysis of peer-to-peer networks, wireless networks, and TCP/IP networks"	"Communication and networking technologies have undergone rapid development in the past decade. However, the rapid development also poses many problems. In this research program, I will investigate how to improve the performance and security of various communication networks, such as peer-to-peer, wireless (including ad-hoc and sensor networks), and TCP networks. In broadband networks, scalability is a very important issue, especially when the user population is getting larger. Peer-to-Peer (P2P) technology is a very promising solution to this problem. I will study the efficiency and incentive mechanisms of P2P systems. In wireless networks, the performance of current systems is still far from being satisfactory. Cross-layer optimization has been believed to be the key technology for good performance in future wireless networks. I plan to apply it to the problem of joint distributed scheduling and congestion control in multi-hop wireless networks. Security has always been an important issue for networks. I will investigate the security problem of how to deal with selfish or malicious users in a wireless ad-hoc network. In the traditional queueing networks, I am interested in the queueing analysis of networks under TCP congestion control, which in return will help us to improve the TCP protocol for next generation networks.""328947,""Qiu, Jie"
"338889"	"Raut, Rabin"	"Analysis, design and implementation of reliable, robust, low power, wideband analog IC and VLSI systems"	"Electronic devices are part of our day to day lives. An electronic device or system is meant to handle information in the form of electrical signal. In the real physical world, the signals change continuously with time. Such signals are referred to as analog signals.The current research work will involve inventing new electronic devices that can be built using modern sub-micron (smaller than one-millionth of a meter) integrated circuit technologty. The device and any bigger system built using these devices should operate reliably under varying enviromental conditions. The emphasis will be on to save the bettery power and to process analog electrical signals that changes very rapidly with time. Thus the work will involve analysis, design and implemenattion of low power, high frequency, robust, reliable anlog integrated circuits and systems. Our research will concentrate on devices which can work with electrical signals that change several hundred million times (or more) in a second. These signals are referred to as high frequency signals. These devices may find use in variety of electronic systems such as fiber optic communication, computers, personal communication devices, wireless communication and so on. It is known that devices operating at high frequencies consume substantial power from the battery. However, special optimization technique can be used to reduce this power consumption. Thus our work will involve innovations for low-power high-frequency electronic devices. All electronic devices working with analog signals exhibit changes in functionality with changes in environmental conditions i.e., temperature change, battery voltage change and so on. By applying special feedback techniques, such changes can be minimized. Thus our task will involve research for creating reliable and robust analog electronic devices and systems. Europe and Asia have very strong base in analog and radio frequency electronics. We hope that our research shall provide Canadians with a group of very highly skilled technical personnel in this competitive area of electrical engineering.""343052,""Raven, Bradley"
"337872"	"Rice, Jacqueline"	"Representation and classification of switching functions in digital logic design"	"It is important when designing a computer chip to aim for three goals:  faster, smaller, and requiring less power. One way to try and reach these goals is to apply a mathematical transform to the function and perform analysis on the resulting values. This technique has been shown to highlight important properties  that can then be used in the design process. My research involves the use of the autocorrelation (AC) transform.  My previous work used this transform to identify properties such as symmetries and different types of decompositions.  I now intend to extend the existing results from Boolean logic, limited to only 0's and 1's, to multiple-valued logic.  Viewing a problem as a multiple-valued problem can often shed light on previously unconsidered solutions. Another approach for reaching the three goals is to classify the functions representing the computer circuits into groups with similar properties.  I propose to use the AC coefficients in this application as well.  Preliminary work has shown that the use of AC coefficients provides a unique classification technique, and this can also be extended to multiple-valued logic. An additional area of research involves the use of a new type of logic circuit that is reversible.  The advantage of reversible logic is that circuits created with this property can theoretically use almost no power!  This would mean, for instance, that batteries for cell phones could last almost forever.  However, we must still optimize for area and speed and so many of the same questions as above must also be addressed for reversible circuits.  This area is so new that questions such as how to build clocked circuits and use known tools such as decision diagrams still must be answered.  These are a major focus of my continuing research.""334430,""Rice, John"
"338914"	"Ricker, Laurie"	"The lexicon of decentralized discrete-event control"	"A discrete-event system is a special class of systems where the behaviour of interest is how particular instantaneous actions or interventions to drive various technological processes. For example, such actions include pressing a key on a keyboard or a traffic light changing colour. Other event-driven systems include communication networks, manufacturing facilities or the execution of a computer program. As event-driven systems get more complex, for example, electronic commerce applications, security protocols, computerized steering modules in cars, to name but a few systems, our understanding of how these systems interact and require intervention is aided by the construction of mathematical models. This proposal will investigate appropriate mathematical models for ensuring that these systems operate in predicatable ways, preventing any undesirable or dangerous behaviours from occurring.""344083,""Ricketts, Chelsea"
"338883"	"Roberts, Gordon"	"On-chip instrumentation for analog / mixed-signal silicon debug, diagnosis and manufacturing test"	"With the growing complexity of integrated circuits reaching the tens-of-million-plus transistor scale, electronic component and system manufacturers are moving towards a commence in third-party electronic component wares (called cores).  Besides the obvious need for standards to assist in their integration, the need for individual validation and final test is paramount to the overall product success.  From a digital design perspective, the appropriate design and test methodologies are, for the most part, already in place.  Scan chain techniques will continue to be used to move digital test information about a complex IC, regardless of the core origin.  In contrast, no simple method exists in which to move analog test information around an IC without serious degradation of its signal-to-noise ratio and high-frequency spectral content.  At the moment, on-chip sampling-scope-type circuits and time-to-digital converter circuits are showing great promise for observing signals on-chip at very high signal frequencies, as well with acceptable precision and accuracy. As the test information is sampled and converted into digital form, test information can be easily moved around the IC without loss of information and easily integrated into existing digital test structures, buses and protocols. Another important reason for the practical success of the on-chip sampling scope and time-to-digital converter is the fact that they require very little silicon area to implement. Furthermore, the impact on the performance of the circuit-under-test is minimal. Following similar considerations, this research project will consider developing other types of embedded circuits that can be used to observe signals directly on-chip.  As process variability affects all analog circuits, this project will further consider how such embedded instruments can be calibrated directly on-chip in real-time or off-chip through external digital control. The objective will be to construct circuits that require little silicon area and have little impact on the building block's power and performance. In all cases, experimental ICs will be constructed in state-of-the-art CMOS fabrication technologies made available through the Canadian Microsystems Corporation (CMC).""344635,""Roberts, Janet"
"338867"	"Robertson, William(Bill)"	"Algorithms and implementations for internet communications"	"The Internet is becoming an increasingly useful means of conducting business, and delivering education and services.  The growth in Internet traffic continues unabated in spite of the readjustment which started in 2001 and lasted for a number of years.  The confidence of businesses in the Internet must be fostered through increasing the ability of the service providers to meet their Service Level Agreement (SLA) while extending the underlying technologies into emerging areas of use.  This proposal targets research to improve the Quality of Service (QoS) of networks, implement  control paradigms for real time process control, and to establish new methods for the efficient implementation of wireless networks both for mobility and sensor networks.  This work will improve the performance and usefulness of the  Internet in a number of application areas, and hence will contribute to Canadian business and research efforts.""345136,""Robichaud, AaronScott"
"338880"	"Rouat, Jean"	"Robust signal analysis and recognition techiques for man-machine interaction"	"New applications have been developed like automatic speech translation, internet search tools, multi-modal computer interactions. These systems can not adequately process corrupted, noisy signals. They also use pattern recognition techniques that require supervision during training. I propose robust signal analysis techniques and to develop bio-inspired and unsupervised pattern recognizers. Results of the proposed research can be used  to assist in the creation of new intelligent systems that can interact with the environment. Electroneurophysiology can also benefit from the new signal analysis and recognition tools that will be developed for this research. I will work on the development of new systems in sound source separation and intelligent signal recognition with capabilities of being unsupervised and trained on limited data sets. Current state of the art statistical systems usually use supervised training with very much data. Therefore, the design and training of such systems is heavy and very costly. The dynamical, non linear, space and time filtering of spikes combined with neurotransmitters diffusion  along with the topographic organization of neurones yields simultaneous signal processing and recognition. Moreover, spiking allows the encoding of information into a second time scale that is different from the usual time. This second time scale encodes the relative timing of firing neurones. It is a key feature that allows unsupervised training. Synchronization or the generation of specific spiking temporal sequences becomes an auto-organization criteria. On a long term time scale (5 years), I propose to explore the integration and interaction of two fields: the conventional signal and pattern recognition paradigm (discrepancy between analysis and recognition) and the continuum (in organization) and dynamical (in times) analysis/recognition made by the peripheral nervous system with emphasis on signal processing. On a shorter time scale  (2--3 years per project), I will focus on applications like sources separation, signal and speech recognition.""341154,""Roué, Lionel"
"337884"	"Ruecker, Stan"	"Meaningful and useful variation of representation in browsing interfaces"	"The goal of my research is to make browsing interfaces useful for people interested in carrying out research tasks using online digital collections. This project builds on my PhD dissertation on the use of browsing strategies for collections that have been encoded at an interpretive level with a text markup language, and on my postdoctoral research on the design of interfaces for working simultaneously with multiple electronic documents. It also extends work previously done by researchers such as Pirolli (1996), Shneiderman (2000), and Bederson (2002). In this project, the user is provided with a visual display that includes some meaningful representation of every item in a collection, coupled with tools for manipulating the display. I refer to this configuration of items and tools as a rich-prospect browsing interface. The innovative aspects include the ability to change the form of meaningful representation, so that depending on the nature of the browsing task, the user is able to select or even generate an appropriately meaningful and useful representation. Users of online document collections are often provided with a single representation of collection items, which reduces the opportunities to carry out useful synthetic, associative, and other visual organization activities. Representations can take the form of text labels such author names or document titles, keywords, metadata items from document description schemes such as the Dublin Core, or automatically generated text fragments. They may also take the form of visual representations such as photos, icons, or illustrations. Or they may be hybrid forms that derive from combinations of text and image, as advocated by Horn (2001). In our pill identification prototype, the meaningful representations are photos of the pills: the variations could be photos of the other side of the pills, or of the profiles; or vectors to make the text clearer; or 3D versions. By studying variations in representation of individual collection items at the level of the browsing interface, this project will contribute to our understanding both of information browsing behaviors and task-associated representation.""324645,""Ruel, Alexandre"
"338857"	"Salt, Joseph"	"DSP algorithms for carrier and timing recovery"	"The general objective of the applicant's research is to increase the rate at which digital information can be communicated over analog media like twisted pair, coaxial cable, optical fiber and radio  channels. Examples of where  digital information is sent over an analog channel are cellular telephone, digital TV and the internet. Unfortunately, digital data can not be sent directly over an analog medium. It must be converted to an analog signal that is suitable for transmission. The device that converts the digital data to appropriate analog signal and visa versa is called a modem.   The operation of a receiving modem is quite complex, but it basically has three parts. The first part samples the incomming analog signal and changes the continuous analog signal into a sequence of numbers. The sampling rate is usually quite high, for example this part of the modem could convert the analog signal into stream of 20 million numbers per second. The second part of the modem is critical and requires sophistacted processing. It must synchronizes a clock in receiving modem to the clock that was used in the transmitting modem. It does this by analysing the stream of numbers.  The third part of the modem uses the synchonized clock to help decipher the stream of numbers and reconstruct the digital data that was sent by the transmitter. The applicant is interested in the second part of the modem. The intent is to find a faster and yet cheaper way to synchronize the clock in the receiver modem to the clock in the transmitter modem.  In doing so, modems could communicate over analog channels at higher information rates and at a lower cost.""344393,""Salton, Grant"
"338972"	"Samadi, Saed"	"Automatic generation of signal processing algorithms and architectures using nature-inspired paradigms"	"The purpose of the proposed research program is to study, design and implement an intelligent algorithm- and architecture-generating system for a slew of targeted signal processing tasks with an embedded search engine working based on the principles of evolutionary computation and other nature-inspired optimization paradigms. The evolutionary design of algorithms and dedicated architectures will be carried out by focusing on discrete-time digital systems consisting of an array of interconnected modules. The modules will be designed internally, and their interconnection is optimized externally, to perform various dedicated signal processing tasks including, but not limited to, digital filtering, digital watermarking and signal compression.  The final design candidates are obtained as the solutions to an optimization problem.  The optimization is based on the principles of genetic algorithms, genetic programming, coevolutionary algorithms, and other nature-inspired meta-heuristics.  These methods, because of their effectiveness in solving difficult combinatorial optimization problems, have the potential of generating innovative digital signal processing systems from scratch, only based on the prescribed specifications.  A distinguishing characteristic of the proposed system is that it will be designed to possess the  capability to reuse, accumulate and extract knowledge about the design space. We will also study and propose the formal description methods for encoding the target signal processing system as a data structure.  This is a key element in the success of this research program and plays a specially significant role when we intend to harvest potentially new theoretical knowledge from what the system has discovered as the candidate solutions to our problem.  Alternatively, a human expert may also analyze the final design candidates generated by the evolutionary process and reveal the theoretical underpinnings of the successful solutions.  This process is significantly enhanced by the effectiveness of the formal description method.  The formal description of the system will be treated in a higher level at the initial phase of the research.  It will be enahanced to give rise to a system specification for implementation using field-programmable gate arrays.""333890,""Samani, Abbas"
"337866"	"Sander, Jörg"	"Data management and data mining in spatio-temporal databases"	"Spatio-temporal data is being collected at an increasing rate, e.g., through satellite images, GPS systems, mobile communication networks, medical imaging technology, sensor networks, or video observations etc. In this proposal, we consider spatio-temporal data in its ""raw"" form to be data that is (partially) described by locations and/or extensions of objects in a spatial frame of reference over time. Spatial frames of reference can be, for instance, parts of the earth's surface, a city's street network, a model of the brain of humans, a building, parts of an underwater area explored by a remotely operated vehicle, etc. Spatio-temporal data often represents moving objects (e.g., people moving around with a GPS enabled device) or moving and evolving geometries (e.g., forest fires). The increased availability of this type of data presents a great opportunity for advanced services and data analyses in application areas such as mobile computing and commerce, fleet control, monitoring and analyzing of environmental and socioeconomic phenomena, traffic control, animal tracking, radiation treatment planning for tumours, epidemiological analyses, etc. However, advanced data analysis and querying techniques for large volumes of spatio-temporal data are still in its early stages. On the one hand, geographic information systems (GIS), although they provide sophisticated analysis and display methods for spatial data, are not yet ready to deal with large data volumes and the complexity of exploratory spatio-temporal analysis. On the other hand, object-relational database systems, which provide efficient access methods for non-spatial and to some degree spatial data, do not yet support spatio-temporal data in an effective and efficient way (although research in recent years has advanced significantly in the area of modeling and indexing trajectory data for basic query processing). The global objective of this research program is to bridge this gap, and to devise general database technology to support a wide range of advanced applications, analyses, and services by developing effective and efficient methods and algorithms for representing, storing, querying, and mining of spatio-temporal data at different scales.""336694,""Sanders, Barry"
"337906"	"Saroiu, Stefan"	"Addressing internet security vulnerabilities"	"An important problem that Internet users are facing today is their vulnerability to a variety of attacks and malicious programs. Although Internet computing has seen tumultuous change over the past decade, we are still relying on decade-old security mechanisms to protect Internet hosts. In the past, a few million early adopters occasionally dialed into the Internet, software was shrink-wrapped and purchased from stores, few applications used the network, and viruses were the dominant threat to end users.  Today, hundreds of millions of technologically unsophisticated users have permanent broadband Internet connections, they acquire and upgrade their software over the Internet, their applications depend on the network to function correctly, and as a result of all this, they must contend with spyware, worms, denial-of-service (DoS) attacks, and viruses. My long-term research objective is to make Internet user's desktops less vulnerable in the face of Internet attacks and malicious programs. We are planning to first address two problems in this area: the proliferation of spyware programs and the examination of the impact of Internet attacks to network traffic ""bystanders"". Spyware programs are software programs that collect information about a computer's use and relay it to a third party surreptitiously. Our approach is to create counterfeit information to such a large degree that we devalue the gathered information to the point that the incentive to collect it in the first place is eliminated. Internet attacks have received a great deal of interest from the research community. However, an essential aspect of these attacks has not been examined: their impact on ''bystanders'', that is users that are not directly targeted but happen to share the same infrastructure as the victims. We would like to quantify the extent of this problem and characterize current solutions from the perspective of network bystanders, rather than the victims. 1. The proliferation of spyware programs. Spyware programs are software programs that collect information about a computer's use and relay it to a third party. Spyware exists because the gathered information has value. This information is useful to vendors, Internet advertisers, as well as malicious entities. Although spyware is a recent addition to the growing list of Internet security problems, spyware poses many risks: it can compromise users' privacy, it can expose the infected host to additional security vulnerabilities, and it can degrade the performance of infected hosts. My previous research has characterized the spread of spyware within the University of Washington, a large organization with over 60,000 students, faculty, and staff. We propose a novel approach to address the proliferation of spyware: creating counterfeit information. This technique does not prevent spyware installations, nor does it recover from them; instead it focuses on decreasing the value of information spyware collects. While information about real users is valuable, counterfeit information is not. To be successful, our approach must generate enough counterfeit information to devalue the gathered information to the point that the incentive to collect it in the first place is eliminated. 2. Characterizing the impact of Internet worms and DoS attacks on network traffic bystanders. Current Internet attacks, such as worms and DoS attacks, have received a great deal of interest from the research community. However, an essential aspect of these attacks is currently poorly understood: their impact on ''bystanders'', that is users that are not directly targeted but happen to share the same infrastructure as the attacks. We believe that the frequency of worms and DoS attacks is so high that a large fraction of legitimate traffic is being affected by it, but masked by the network and applications congestion control mechanisms. We are interested in quantifying the extent of this problem and characterizing current solutions from the perspective of network bystanders, rather than the victims. We plan to develop a network tracing infrastructure to capture worms and DoS traffic. This new system will be deployed at the Internet border of a large user population (e.g., all Internet users at the University of Toronto) and it will observe all traffic flowing through the border. We plan to collect network traces of worms and DoS traffic and to analyze their impact on bystander traffic. Finally, we plan to investigate the differences of the network traffic characteristics in the presence of an Internet worm or DoS attack as well as in the absence of such attacks.""328161,""SarraBournet, Christian"
"338973"	"Schriemer, Henry"	"Engineered electrophotonic nanosystems"	"Maintenance of economic growth across diverse market sectors requires continued evolution in applications that exploit increasingly sophisticated materials technology.  Using a complex systems design approach, based on nanowire fabrication process technology, we are developing a multifunctional materials system and design paradigm exploiting optoelectronic nanostructures coupled to photon transport architectures for bi-directional control of device operation.  We use quantum dots grown inside nanowires, whose positions are controlled through a templating process.  By mixing nanowires of various compositions and size, we can place quantum dots as integral parts of microstructured arrays that form, e.g., a planar photonic crystal.  This permits the local photonic density of states to be tailored through numerical design and simulation.  Our present focus is on the ability to all-optically manipulate the flow of light through an enhancement of the nonlinear properties of the system via interacting quantum dot chains.  The control aspects come in at a later stage though manipulation of the electronic densities of state.  Our design and simulation information is used to inform fabrication process development, which we are engaged in collaboratively.  The device prototypes are analyzed in our nanosystems characterization facility to fully close the design loop, thus permitting us to realize ever more sophisticated applications of this technology.  Our approach is predicated on a ""smart systems"" perspective for exploitation of technology that cannot arise within the current Moore's law paradigm.  It will hopefully render irrelevant the distinction between hybrid and monolithic integration in future technologies that meld the electronic and the photonic at the chip level.""335868,""Schrobilgen, Gary"
"338357"	"Scott, Philip"	"Polarized logics, geometry of proofs, and semantics of computation"	"Linear logic and proof nets, introduced by Girard in 1987, gave rise to new ways of thinking about computation and the geometry of proofs. For example normalization of proofs (which corresponds to evaluation of functional programs) can be modelled in terms of travelling along paths in the nets (graphs) representing proofs. These travels are governed by algebraic laws on operators associated to edges of proof nets. This led to  dynamical semantics of proofs in terms of functional analysis, to a new semantics of algorithms and to dynamical invariants for normalization.   This is Girard's Geometry of Interaction (GoI).    In this work, we shall examine the program of Geometry of Interaction as well as so-called Polarized fragments of linear logic, trying to find models for dynamics of information flow and connections with complexity theory and the theory of quantum programming languages.""336216,""Scott, Robert"
"337833"	"Sekerinski, Emil"	"Languages and tools for the construction of verifiable programs"	"As we become more dependent on software, the reliability of software is of increasing concern to society. In the US alone, the Department of Commerce in 2002 estimated that the cost to the economy of avoidable software errors is between 20 and 60 billion dollars every year. Over half the cost is incurred by the users. The long-term goal is to contribute to the ideal of ""correct software"". The focus is on making the design phase of software development more reliable. The thesis of this project is that further groundbreaking progress can only be achieved by an integration of several techniques for improving reliability. The techniques that we consider are object-oriented programming, specification techniques, concurrency, exception handling, theorem proving, and documentation techniques. The approach is to integrate an automatic theorem prover (a decision procedure for first-order logic) in the compiler and allow programs to contain specifications. This allows the compiler to check if an implementation meets its specification. It also allows the compiler to detect where errors (specification violations) can occur and make the programmer write exception handlers in those cases. Action-based concurrency is easier to express and verify than thread-based concurrency. On the other hand, action-based concurrency is difficult to implement efficiently. The integrated decision procedure allows for optimizations, like avoiding evaluations of action guards. Thus, the approach gives programmers multiple benefits for writing specifications: checking the correctness, better exception handling, and more efficient code.""325791,""Sekuler, Allison"
"338974"	"Shahbazpanahi, Shahram"	"Channel estimation and tracking for point-to-point and multiple-access MIMO communications"	"Traditional point-to-point wireless communication systems consist of a single-antenna transmitter and a single-antenna receiver. In such systems, the Single-Input Single-Output (SISO) channel between the transmitter and the receiver is subject to ""fading"", i.e. the channel behavior changes with time and with locations of the transmitter and receiver.  Such channel variations can adversely affect the quality of the communication.  Recently, Multiple-Input Multiple-Output (MIMO) techniques have been studied as powerful tools to combat fading. In MIMO communication systems, both the transmitter and the receiver are equipped with multiple antennas. In this way, the MIMO channel becomes more reliable because it is very unlikely that all individual links between the transmitting and receiving antennas go into ""deep fading"" at the same time. This fading resistance characteristic of MIMO systems can lead to an increase in the data transfer rate. To benefit from potential advantages of MIMO systems, sophisticated techniques for transmitting the information over multiple antennas are required. Also, at the receiver, advanced methods are required to efficiently combine the received signals from multiple antennas and to retrieve the transmitted information. In order to design efficient MIMO reception schemes, one needs to have precise knowledge of the underlying MIMO channel. Hence, channel estimation plays an important role in MIMO communications. Moreover, as the channel changes with time, tracking and predicting the channel behavior is essential to reliable MIMO communication. This project aims at developing more efficient and reliable channel estimation and tracking methods for MIMO communications. This study will consider both point-to-point and multiple-access MIMO communication schemes. This research program aims at developing practically useful results that will improve the communication reliability within the Canadian wireless infrastructure. It will also have a direct impact on training highly qualified personnel who can become the leaders of Canada's future wireless industry.""345541,""Shahi, Arash"
"338458"	"Shaw, Christopher"	"Highly interactive 3D user interfaces"	"This proposal concerns the research and development of new techniques for the design and understanding of free-form surfaces. Examples of free-form surfaces include car bodies, ship hulls, and fanciful creatures in video games. The proposed work is aims to improve the design and understanding of free-form surfaces through the use of hand-held 3D position and orientation trackers. In MeshSculptor, our experimental free-form editor, a real-time graphics system continuously animates the edited surface as the user sits in front of the screen manipulating the surface using the two 3D trackers. Each hand has a distinct role to play, with the dominant hand being responsible for picking and manipulation, and the less-dominant hand being responsible for context setting of various kinds. Employing a user-centered design approach, we plan to develop new two-handed techniques for designing surfaces and for displaying surface properties using specialized textures rendered on the surface. Our approach to surface editing is to warp the highlighted part of the surface using the 3D trackers, and this project seeks to develop new warp operators that are powerful and easy to use. We also plan to develop a new abstraction for warps that will allow warps to be re-done and adjusted after they have been completed. We plan to apply this work to the domain of pediatric cardiac surgery planning. In this domain, surgeons reroute blood flow in patients whose hearts have only a single ventricle. We are working with researchers who evaluate the flow efficiency of the rerouting surgery using fluid dynamics simulations. These researchers are using our free-form surface editor to aid understanding of the current blood vessel shape, and to edit the shape in hopes of finding a more efficient configuration.""332417,""Shaw, Cliff"
"338901"	"Shen, XueminSherman"	"QoS routing and medium access control in ultra-wideband wireless communications networks"	"Ultra-wideband (UWB) is an emerging wireless communication technology with unique potential merits such as high rate, low transmission power, immunity to multipath propagation, and capability in precise positioning. It has been considered as one of the most promising candidates for short range indoor and indoor-to-outdoor wireless communications. It is expected that UWB wireless networks should be capable in supporting multimedia traffic which presents coexistence of diverse services. However, the diversity of services results in the need for provisioning heterogeneous quality of service (QoS) in the networks. This is a very challenging task because of the limited transmission power, user mobility, hostile channel conditions, short range and high rate transmission, interference from other co-existent systems, and stringent constraints on computational complexity and system overhead. As a relatively new technology for commercial applications, significant R&D efforts have been devoted to UWB wireless technologies, most of which focus on UWB transmission issues at the physical layer. More recently, there is increasing attention to the design of protocols and algorithms at the UWB network higher layers since efficient and effective routing and medium access control (MAC) mechanisms are essential to QoS satisfaction and high network resource utilization. One of the promising approaches is the cross-layer design which considers the UWB transmission characteristics. The proposed research will focus on 1) development of an efficient MAC protocol in the avenue of multiple access, overhead reduction, packet scheduling, and QoS provisioning; 2) development of a routing scheme for UWB mobile ad hoc networks, which should ensure QoS satisfaction and adapt to user mobility and traffic load variations, taking advantage of the positioning capability of UWB; 3) investigation of the vertical coupling between the protocol stacks, so that the resource allocation algorithms at the link and network layers can adapt to dynamics of the wireless channels and characteristics of UWB transmission and user applications to achieve a better tradeoff between QoS support and high resource utilization.  ""332459,""Shen, Yanan"
"337816"	"Singh, Ajit"	"High level abstrations for parallel computing"	"Nowadays, even though there is an abundance of parallel computers around us in the form of multi-CPU PCs, shared-memory multiprocessors, and message-passing processor-networks, a majority of the programs are still being written for single processor computers. The current state of the art lacks parallel programming models and systems that are flexible, easy-to-use, and applicable to most of the common computer architectures. Researchers have been experimenting with high level parallel computing models that rely on new or extended languages, GUI based environments, pragmas, parallel patterns and skeletons for addressing these issues. This research proposes  a generic pattern based parallel programming model, called GPE, that: (a) supports parallel programming on shared-memory as well as message-passing computers; (b) is based on simple expressions, called Generic Pattern Expressions (GPEs), for instantiating common parallel structures and behaviors; (c) provides the required trade-off between parallelization effort and performance; (d) can be implemented in a common programming language (e.g., C++) without requiring any new language or language extensions; (e) supports user-written shared-memory or message-passing based new patterns. Our recent research work on parallel programming systems PAS and EPAS demonstrates that it is possible to design and implement an extensible pattern-library based parallel programming model and system while our work on Active Expressions shows the feasibility of instantiating common parallel behaviors without requiring any language extensions, preprocessing, or GUIs. Even though satisfying above-stated objectives (a) to (e) in a single research project may be a challenging proposition, it is worth pursuing and we believe, given certain recent advances in this research area, these may be feasible.""343712,""Singh, Alamjit"
"338424"	"Singh, Karan"	"Art and anatomy based techniques for interactive character modeling and animation"	"Character modeling and animation has limitless possibilities with important applications in medicine, education, training, telecommunication, ergonomics, human computer interaction and last but not least entertainment. Current animation techniques, however, are suboptimal for animators, clinicians and users in general since they are driven by the mathematics of digital geometry and the physics of motion rather than an understanding of anatomy, aesthetics and the specific interactive skills of the users. This project will explore techniques for interactive character modeling, animation and reuse based on foundations of anatomy, aesthetics, timing and leveraging their specific user skillsets towards the modeling and animation of digital characters. The state of the art in the computer representation of a character resembles a marionette, allowing an animator to emote through puppet-like controls on the character rig. Rigging a character is a complex process and anatomic approaches to character and facial rigging have shown promise for character reuse and in capturing subtleties in animation that are difficult to capture with other techniques, that this project hopes to explore. The bigger challenge is controlling the animation of a rigged character, where we hope to devise techniques driven by the high level manner in which directors would interact with and specify the motion of actors. The complexity of motion and gestures of the hand makes it not only a challenging animation problem but a pervasive part of human function, communication and aesthetics. Repetitive strain injuries, which are hard to diagnose early, greatly impair productivity in the workplace. The development of an anatomically accurate computational model of the hand is an important part of this project. The end goal of the anatomic hand model is to capture the functional relationship between numerical inputs to the muscle actuation sites (conceptually equivalent to contraction impulses) and the motion of the skeletal joints of the hand. The mapping will be both forward, where impulse values can be mapped to a resulting joint motion, and inverse, where we compute the many possible contraction impulses in various muscle combinations that produce some given motion of the joints. Such a mapping for a specific patient can be a valuable motion analysis tool in a clinical setting as well as a tool for the creation and validation of realistic hand animation.""331969,""Singh, KripaShankar"
"338940"	"Sit, Jeremy"	"Microdevices based on nanoengineered porous thin film"	"One central goal in the emerging, but already very broad, field of nanoscience and nanotechnology is the desire to understand, and eventually control, the structure and properties of materials at the nanometre size scale.  This research program will investigate structure and properties of materials with nano-scale features, with a view to applications in micro-devices such as sensors and catalysis, and in the longer term, exploring potential applications for biomedical devices and energy storage and conversion.  Work by this researcher and his collaborators has laid some of the groundwork for these studies in recent years and is an ongoing topic of interest.  This work has shown that by combining novel, nano-porous thin film materials with chemical functionalisation techniques, a high degree of control over both structural and surface properties can be achieved. The level of control afforded by the thin film deposition process over the morphology of these nanostructured materials is complemented by the fact that it should be possible to attach nearly any desired chemical functional group to the surface.  This will allow the derivatised material to be tuned both structurally and chemically to the specific application.  For instance, an optical filter based on our porous films needs to be desensitised to changes in humidity for optimal environmental stability.  On the other hand, if we want to create integrated humidity sensors, we need to improve the sensitivity of the device toward water vapour.  With control over both morphology and surface chemistry, we will be better able to accomplish our goals of functional nano-materials and devices.""324741,""Sitarski, Joanne"
"337813"	"Slonim, Jacob"	"Privacy mechanisms in databases"	"This research proposal addresses the problem of preserving privacy in the knowledge economy.  Modern applications, notably e-commerce, e-health, and e-banking, have a strong requirement on distributed transaction processing management systems that emphasize personalized security, privacy, and trust.  Current Distributed Database Management Systems (DDBMS) contain mechanisms such as schema privileges, administrative authorities, and access control that help to ensure the security of data.""328445,""Sloss, Craig"
"337849"	"Sodan, Angela"	"Intelligent dynamic scheduling in clusters and computational grids"	"Computational grids play a major role in the Canadian research landscape due its highly distributed nature and are supported by several organizations and funding agencies like GridCanada, C3, Canarie, and CFI - e.g., via the Sharcnet network of clusters, serving highly diverse application areas like chemistry, engineering, and economics. Computational grids provide the option to join and share HPC resources for tackling large-scale problems that go beyond the local site's capacities or for balancing hot spots in local workloads among sites. For scheduling of large-scale jobs, either coarse-grain workflow or simultaneous resource allocation may be chosen, of which this proposal addresses the latter. Finding matching scheduling slots on different sites, then, is difficult. Furthermore, schedulers have to deal with heterogeneity, new multi-processing features available in modern multi-core CPUs like the AMD Opteron, and new I/O-intensive applications like datamining. These trens make job-scheduling approaches necessary that are flexible and dynamic to improve machine utilization and to find simultaneous time slots without harming local workload. This research addresses such dynamic approaches in the form of adaptive resource allocation and time-shared job execution. Some of these approaches also require middleware to make the applications adaptable. Altogether, innovative solutions are required at the grid, job, and application level. This research proposes solutions for adaptive job scheduling, special time-shared job scheduling on multi-core CPUs, an integration of these approaches into grid scheduling, special grid schedulers with economic models, and adaptive middleware with high-quality data distribution for an important class of communication-critical numeric applications. The solutions will be practically supported by a tool environment and theoretically by advanced cost modeling. Furthermore, the solutions will be designed configurable to be application- and resource-aware, and suitability ranges will be extracted.""324237,""Sodomsky, Matthew"
"338421"	"SoltysKulinicz, Michael"	"The proof complexity of matrix algebra"	"Proof complexity (PC) is an area of theoretical computer science that deals with the computational complexity of formal reasoning.  PC is intimately connected to major open problems in complexity theory (notably the famous ""P vs NP"" problem), and to the field of automated theorem proving.   My research is particularly concerned with the application of fast parallel algorithms for matrix computations to upper and lower bounds in propositional proof systems.""339857,""Solymosi, Jozsef"
"337850"	"Somé, StéphaneSotèg"	"Use cases based requirements engineering"	"Use cases are black box views of a system behavior. They are expressed in a user oriented and intuitive way. As such use cases have been proved useful to capture and document external functional requirements of systems. Different software development approaches use use cases as part of requirements. However, very few work have been done on use cases formalization and automated support. The proposed research program is a continuation of an ongoing work that aims at providing an approach and tools to support use cases based requirements engineering. We focus on abstract use cases represented in a textual natural language. Results obtained so far include: - a formal definition of use cases, - a restricted natural language for use cases, - an incremental algorithm for synthesis of hierarchical state machines (statecharts) from use cases, - a methodology for use cases capture and elaboration in conjunction with a domain model. We implemented our results in an open source tool for use cases capture and validation by simulation. In the proposed research program, we plan to work on: - extension of use cases for non-fonctional and crosscutting requirements, - the application of formal languages such as the OCL, and model checking to use cases based requirements validation, - the transition from the requirements phase to the design phase through use cases refinement to scenarios, - development of approaches to effectively derive test cases from use cases, - validation and improvement of our methodology using feedback from teaching and case studies.""325811,""Somers, Christopher"
"337826"	"Song, Shaowen"	"Residential multi-service access networks and photonic computing"	"My current research is subdivided into two major programs: Program-1: Broadband Residential Integrated Services Networks, and Program-2: N-nary Digital Photonic Computing.         The objective of program-1 is to develop the technologies for broadband residential integrated services networks. With the advancement of the DWDM (Dense Wavelength Division Multiplexing) technology, it becomes sensible now to combine existing network protocols, namely the Ethernet or the SONET, with DWDM to achieve a new generation of metro networks for broadband multi-service to homes. In this Program, we are currently researching, in parallel; two access network models, one is based on the SONET and the other is based on the Ethernet. The goal is to compare the two architectures in order to develop an efficient and low cost network system for future broadband residential communication services. We have designed the architectures and developed the access protocols for both network models. Our current focus is on developing a low cost residential gateway (RG) for each of our network systems.      Program-2 is based on my inventions of Photonic Semiconductor Transistors (PST) and N-nary Digital Photonic (NDP) System (US patent #67783030) and Optical Memories (US patent #6647163). Similar to electronic semiconductor transistors, photonic semiconductor transistors are the fundamental elements for digital photonic circuits. The PSTs are used to construct logic gates and then logic gates are used to build digital photonic devices, in the same manner as digital electronics. The NDP system uses N (N>=2) number of wavelengths to encode information, which leads to high memory capacity and computing speed. An N-valued digital logic system has been developed which enables circuit designs by using the N-nary photonic logic gates (US patent #67783030). We are currently using the Optiwave software to design the microstructure of PSTs. The design has been submitted to NRC Canadian Photonics Fabrication Centre (CPFC) for prototyping.""328624,""Song, XiaoGeng"
"338422"	"Stege, Ulrike"	"Parameterized complexity in computational biology and cognitive psychology"	"In many application areas, such as Computational Biology or Bioinformatics, there is a tremendous need for practical algorithms for NP-hard problems (i.e., problems where no polynomial time algorithm is known and most likely no such algorithm exists). Several approaches to tackle NP-hard problems exist including approximation algorithms and fixed-parameter-tractable algorithms. My expertise is in the latter one, which belongs to the area of Parameterized Complexity. We distinguish between parameterized problems which are fixed-parameter tractable and fixed-parameter intractable (i.e., hard for the class W[1]). For parameterized problems that are fixed-parameter tractable, there exist algorithms that have a polynomial-time behaviour for small parameters, that is such an algorithm may be of exponential time in the parameter, but of polynomial time in the non-parameterized input size. In Bioinformatics, I am especially interested in (1) analyzing the relationship between biological sequence data and (2) modeling the evolution of a genome. Other important applications I am investigating are in Cognitive Psychology, that is, the modeling of cognitive theories in general and understanding of human problem solving and decision making strategies in particular. Finally, my principle objectives are to design tractable algorithms for NP-hard problems and to extend the parameterized tractable algorithms design toolkit with new techniques (e.g., kernelization); the modeling of and algorithm design for genome rearrangement problems for given biological sequence data; the modeling of cognitive theories and their validation; and the investigation of human problem solving strategies when confronted with NP-hard problems.""327420,""Steger, Debra"
"338961"	"Stepanova, Maria"	"Comparative analysis, modeling, and optimization of nanocrystal synthesis for magnetic storage and field emission"	"We are getting used to enjoying high-tech products at our hand, such as cell phones, laptops that fit a pocket, window-size flat TV screens, and many others. However, do we always remember that these things would not exist without such mandatory milestones as discovery and design of new technological processes? The new ideas enabling new process design depend, in the basis, on the depth and strength of our knowledge and develop as our understanding grows. Thus nanotechnology, which is a technology employing objects sized between 1 nm and 100 nm, resides on our ability to understand, manipulate, and characterize those small things. As these are frontier challenges in many disciplines, nanoscience makes headlines in scientific and engineering press. This project contributes to understanding, and therefore to development, of optimal processes to synthesize arrays of nanosize crystals on a flat base (substrate). Many various technologies require this, for example, fabrication of flat displays and sensors, higher-density information storage, or more powerful electronic chips. The project focuses on methods of synthesis known as physical vapor deposition (PVD), which use physical processes (for example, evaporation) to obtain atomic fluxes of material that are then deposited on a substrate in vacuum. PVD does not employ or produce hazardous chemicals, is clean, and can be efficiently controlled. What is missing so far is a clear enough understanding of which particular crystals, of which shape, size, and area density, will grow under particular conditions. This project provides theoretical models and numerical simulations detailed enough to answer these questions. The project is designed as the initial step in a long-term program of creating a comprehensive knowledge base on synthesis of functional nanostructures. The knowledge base will contain the necessary simulation software, analytical reviews, and relevant databases, which are operated through an interactive management system designed to help users to select methods, materials, and process conditions to synthesize desired nanostructures.""334292,""Stepanova, Natalia"
"338361"	"Stewart, Lorna"	"Structured graph classes:  characterizations, algorithms, and complexity"	"Many problems in graph theory are known to be NP-hard, which means that they likely cannot be solved by efficient algorithms. However, it is sometimes possible to construct an efficient algorithm for such a problem, if only a subset of all graphs needs to be considered. The study of particular classes of graphs is usually motivated by theoretical considerations, relationships to previous work, and/or applications. Such graph classes are typically defined in terms of forbidden substructures,  decomposition schemes, vertex orderings, or as intersection (or other) graphs of subsets of a set. The definition usually implies additional properties and structure, which in turn provide clues about how problems may be solved efficiently for those graphs. Most of my research involves characterizing properties of graph classes, and using those properties to design polynomial time algorithms and to prove complexity results, for various problems on structured graphs. The goal is to understand the interplay between problems and graph properties, and to identify relationships that lead to efficient algorithms.""337961,""Stewart, Neil"
"338452"	"Swamy, Chaitany"	"Algorithms for planning under uncertainty and in the presence of selfish users"	"Technological advances over the past few years have led to the development of systems of rapidly increasing size and complexity. The design, maintenance and management of such systems, especially high-speed telecommunication networks and decentralized networks like the Internet, which are the important computing and communication platforms of the future, present several challenges. In this proposal, we shall focus on two important issues that arise in the design of such systems: (a) Uncertainty in the data or specifications. Uncertainty is a facet of many practical decision environments, and to function effectively one needs to anticipate uncertainty and take measures to be robust in the presence of uncertainty. We will investigate the key underlying algorithmic problems in models that  abstract these settings, and develop techniques and efficient algorithms for effectively tackling these problems. (b) The presence of uncoordinated components in the system with ``selfish'' interests. In an environment with self-interested users, like the Internet with its autonomous sub-networks, one can no longer assume that the users will act as stipulated by the system designer to optimize performance. We shall study, when, and to what extent, does selfish behavior affect performance, ways to mitigate the resulting inefficiencies, and ways to devise algorithms in such selfish environments that resist manipulation. In many cases, the algorithmic problems encountered are computationally intractable (e.g., NP-hard) optimization problems, so it is unlikely that exact solutions can be computed in polynomial time, and our approach will be to devise an approximation algorithm for the problem, that is, a polynomial-time algorithm that always delivers a provably near-optimal feasible solution. Approximation algorithms constitute a unifying theme of this research, and our research will also have impact on this rich and exciting field by furthering the development of general principles for the design and analysis of these algorithms.""338872,""Swamy, Srikanta"
"337801"	"Szafron, Duane"	"Generative patterns - higher level programming"	"The goal of my research is to provide non-programmers with the ability to generate computer programs in specific application domains, without writing a program in the traditional sense. I am not proposing to solve the difficult problem of automatic program generation from specifications in arbitrary application domains. Instead, I want to solve a similar problem in a more limited scope.  In this research, I will try to apply automatic program generation to the creation of computer game adventures. Programmers write a single game engine that can be used to ""play"" many game adventures. However, custom scripting code must be attached to game objects for each game adventure that uses the game engine. The research goal is that a story writer with no programming skills can generate this scripting code, without learning how to program or finding a programmer to write the scripting code.The scientific technique that I will use to try to solve this problem is generative design patterns, based on the well-known idea of design patterns. Traditional design patterns are descriptive. Each design pattern describes a set of known solutions to a recurring design problem, by providing a design lexicon, solution structures and the reasoning behind the solutions. Since each pattern is a family of solutions, it must be adapted to a specific context during program construction. An adapted design pattern is a detailed specification that can be translated to code quickly by experienced programmers who have implemented the same design pattern in other contexts. I have successfully generalized descriptive design patterns to generative design patterns in the domain of parallel programming, by replacing the final step of manually translating the solution to code by an automatic code generation step. However, it can be argued that giving sequential programmers the ability to generate parallel code is easier than giving non-programmers the ability to generate scripting code. Therefore, this research will establish whether non-programmers can use generative design patterns to generate scripting code in a specific application domain - computer games. If this research is successful, future research could try to use generative design patterns in other specific application domains.""329475,""Szajnfarber, Zoe"
"338907"	"Tait, Niall"	"MEMS device integration"	"Micro-electro-mechanical systems (MEMS) are miniaturized mechanical structures manufactured using the same techniques that have made highly functional low cost electronics so widely available.  These miniature devices have been successfully used in automobiles, health care systems, and communications systems.  The design program proposed here will take advantage of new materials and MEMS manufacturing techniques developed at Carleton University, and enable integration of these new devices with electronics.  This will enhance performance and enable smarter MEMS devices.  Improved design techniques are planned to make devices available to a larger group of design engineers.""340467,""Taivassalo, Tanja"
"337867"	"Talim, Jerome"	"Filter with learning and exploration capabilities to support documents analysis and controversial material detection"	"Email users receive an increasing number of spam messages selling some products, advertising some services, or requesting some personal information. The authors of such messages or spammers collect email addresses from personal web pages, or messages from newsgroups. Email servers are facing a larger and larger volume of spam messages; and commonly, they use spam filters to discard the unsolicited messages. Unfortunately, spammers keep changing their strategy of communication to defeat the filters. The objective of the project consists of designing an adaptive spam filter that uses the message content and the sender identification to discard more efficiently spam emails. Since the proposed filter has the task of processing a collection of  emails, it can be extended to filter a collection of documents, such as messages posted in newsgroups or web documents, in order to detect controversial material (related to child pornography, criminal or terrorist activities). Thus the designed methodology may assist administrative agencies in their investigation of illegal activities on the Internet.""339371,""Tall, Franklin"
"338462"	"Tardos, Gabor"	"Complexity of geometric arrangements"	"Arrangement of simple geometric shapes (lines, triangles, hyperplanes) can lead to very complex arrangements. The complexity of many geometric algorithms crucially depend on the complexity of the these arrangements, therefore it is important to understand what requirements ensure a bound on the complexity of the arrangements and therefore on the running time of certain algorithms. In certain computational or combinatorial problems that are seemingly not geometric in nature, for example in some optimization problems, one can represent the data in as an arrangement of surfaces or other shapes and apply geometric arguments in the solution. The proposed research is a continuation on similar research carried out by myself and other researchers on complexity of geometric arrangements. We propose for example to extend results about the complexity of the union of fat objects from two to higher dimensions. Geometric graphs proved to be very useful in modeling spatial arrangements and relations. We propose to work on the extremal theory of geometric graphs and related theory of ordered graphs.""386696,""Tardos, Gabor"
"337895"	"Terry, Michael"	"Supporting community-driven, open source usability"	"Widespread adoption of open source software (OSS) by individuals, educational institutions, and governments has gradually broadened the open source user base from the technically sophisticated to the everyday user. As a consequence, OSS developers are increasingly faced with the need to address usability issues in the design of their software.  However, user-centric design and evaluation require expertise, resources, and infrastructure above and beyond what volunteer-driven OSS projects currently possess. This work will investigate how human computer interaction (HCI) methodologies can be transformed to operate solely in the loosely-coupled networked cultures of open source projects. More specifically, we will research and improve one of the most fundamental problems OSS projects face today as they adopt user-centric practices: User profiling, or characterizing end-users, their goals, their typical tasks, and any special needs they may have. We will pay particular attention to how the open source community's ""economies of scale"" can be applied to this problem: How the larger community can assist in identifying critical interface design flaws and how they can actively participate in the design of an application's interface and modes of interaction.""343739,""Tesche, Cassandra"
"337854"	"Thulasiraman, Parimala"	"Parallel algorithm design for processor in memory (PIM) architectures"	"Parallel computing involves using  many computers to cooperatively solve a given problem fast. Parallel computing has played a significant role in application areas such as weather forecasting, physics and chemistry.Applications  as broadly based as finance, biology and medicine are bringing new to high performance parallel computing research.One of the fundamental problems in parallel computer architecture is that while processor speed has increased dramatically over the past few years, increaes in memory speed have been only modest.To bridge the processor-memory gap,computer architects have proposed integrating memory logic, processor and interconnect support on a single chip, called Processor-in-Memor(PIM) chips.Many such PIM chips can be connected creating a cellular architecture to achieve the next generation of supercomputing machines.The question one asks is what are we going to do with these powerful supercomputers?One of the most important applications that can profit from leveraging the powerful resources of parallel computing is medical imaging. Determining the internal structure of an organism with sufficient detail to yield diagnostic information of a patient,is a first step before administering any procedure.Medical imaging is the process by which physicians evaluate an area of the patient's body that is not normally visible.There are many imaging techiniques such Computed Tomography (CT), magnetic resonance imaging (MRI) and positron emission tomgraphy (PET). This research concentrates on (CT).These techniques detect abnormalities in organs such as breasts.  The images in any of the imaging  techniques are large requiring large amount of storage space and need to be processed fast to make real time decisions on the patient. Initially, the proposed research will focus on CT and expand it to other imaging techniques. Therefore, the focus of this proposed research will be on the design and performance evaluation of sophisticated algorithms for medical imaging techniques on PIM arcitectures .""345042,""Thuot, Arthur"
"338858"	"Todd, Terence"	"Wireless LAN mesh networks and protocols"	"Wireless local area networks (WLANs) have rapidly appeared in homes, businesses and in public Wi-Fi hotspots. Hotspots are now being extended to form contiguous hotzones, spanning tens of kilometers in expansive outdoor areas. In these types of deployments, WLAN mesh networks can be used to reduce the cost of large-scale hotzone deployment.  WLAN mesh network nodes do this by functioning as multihop backhaul relays in addition to their role in providing radio coverage for end user stations. This project will fund the applicant's research in the area of sustainable-energy WLAN mesh networks. For the past several years the applicant has developed a solar powered WLAN mesh network, referred to as SolarMESH. A solar powered solution has an advantage over conventional WLAN mesh networks in cases where continuous power sources are difficult or impossible to provide. The work will include various activities that are motivated by reducing the power consumption of the WLAN mesh nodes. Reduced power consumption leads to lower node costs due to a reduction in the resources needed for continuous operation. Research is needed into ways of including this functionality in a manner that supports the quality of service required for real-time services and that will support fast real-time handoff for mobile stations. WLAN mesh networks are very new and commercial markets for these networks will be world-wide. For this reason the proposed work has a very strong potential for generating results of benefit to companies working in this area.""327040,""Todd, Terence"
"337907"	"Topaloglou, Theodoros"	"Biomedical data management technology"	"Biomedical researchers have now access to more data than ever before. For example, a graduate student today can have complete genomes of many organisms stored in her personal computer, access to countless data resources over the internet, and mounts of experimental data flowing out of sophisticated laboratory equipment. In order to make meaningful use of the data, she needs to shift through it for information, categorize it and otherwise structure it appropriately, also maintain it current. Biomedical research increasingly depends on information management. As the amount and the variety of the available data continue to grow, the management of data will become even more demanding and even more critical.     The basic hypothesis of this research is that advances in data and information management research have made possible to develop integrated technologies along with methodologies which can greatly facilitate the management of biomedical data and turn it to actionable scientific advances.     My research in the next five years will focus on three applied topics. The first is semantic approaches to biomedical data integration and specifically the role of domain-specific ontologies in deriving correspondences of related concepts between biological databases. This work will result in a formal methodology and tools for the integration of biological databases. The second is scientific workflow management, a synthesis of issues from data management and process modeling and simulation with foundational importance to laboratory information management and implementations of bioinformatics analyses. This work will result in methodologies for managing workflow execution in a web-services environment, and a formal treatment of provenance in scientific workflows. The third is modeling and effective management of non-standard and evolving data types in biomedical research, in particular focusing on issues related to the collaborative electronic notebook application. The result of this work will be methodology for distilling and organizing data from experiments and an implementation of a collaborative electronic notebook.""333469,""Topper, Timothy"
"326097"	"Tory, Melanie"	"Display technology for collaborative visualization"	"Visualization tools, or visual representations of data, are used to aid data analysis in many applications. This process is often conducted collaboratively over large screen displays, where two or more users work together to interpret and analyze data. However, current visualization and interaction techniques for large screens do not support fluid collaboration.  For example, users can get in each other's way. Hence, the collaboration process can be awkward and inefficient. We will design, build, and characterize usable and effective interactive workspaces for collaborative data analysis, including both the hardware arrangements and visualization techniques. Some specific research questions related to this goal are: 1. What display arrangements and input technologies are best suited to different users, tasks, and visual data representations? 2.What display arrangements best support conflicting information needs of collaborators? 3.How can visualization techniques be better designed for collaborative use?""337903,""Tory, Melanie"
"338949"	"Tremblay, Christine"	"Next-generation optical transmission technologies, systems and networks"	"So far, optical fiber technology has provided the capability of transmitting huge amounts of information.  This has lead to the current generation of high capacity dense wavelength division multiplexing (DWDM) systems operating at 10 Gbps bit rate with 50 GHz optical channel spacing.  The demand for capacity is more than doubling every year, even since the telecom bubble burst, as an effect of the continuous growth of combined Internet, data and voice traffic.  Thus it is only a matter of time until the existing infrastructure will run out of capacity and new capacity will have to be added to meet the ever-increasing demand for bandwidth.  Furthermore, cost effective transmission at higher bit rates and tighter channel spacing has been found to be very difficult because of dispersion and nonlinear effects.  Advanced modulation combined with forward-error correction and digital signal processing are technologies that can be used to reduce fiber impairments and to provide extra capacity and flexibility to optical networks.     Communications technology is one of the key directions in Photonics Engineering that have been identified as a result of the 2000-2002 NSERC Reallocation Exercise.  The objective of the proposed research program is to investigate how the capacity and flexibility of current optical transmission systems can be enhanced by exploiting a combination of coherent and signal processing technologies, and to study the impact of these technologies on the architecture of future optical networks.  The short term objective consists in exploring how these novel technologies can provide cost effective and flexible high capacity solutions in optical networks.  The long term objective will be to develop novel transmitter/receiver technologies better adapted for specific applications and fiber mitigation tasks. The result will be a contribution to the development of next-generation optical networks.  ""329504,""Tremblay, Deanna"
"337820"	"Tremblay, Guy"	"Spécification et vérification formelles de services web"	"Les services Web émergent de plus en plus comme une infrastructure fondamentale pour le développement et la mise en oeuvre du commerce électronique et, plus généralement, des processus d'affaires électroniques.  De nombreux langages et standards existent pour décrire et spécifier de tels services et processus, et ce à plusieurs niveaux, par exemple, orchestration décrivant le fonctionnement interne d'un processus, interface comportementale d'un processus, chorégraphie régissant la coopération entre divers processus et services. Bien qu'on reconnaisse que les méthodes formelles de spécification et de vérification peuvent jouer un rôle important dans le développement de logiciels de qualité, ces méthodes sont encore peu utilisées dans le cadre des processus d'affaires et services Web. L'objectif de notre recherche est de concevoir et développer des outils de support aux méthodes formelles qui permettront de développer des processus d'affaires basés sur les services Web qui soient corrects et fiables.  Plus spécifiquement, notre recherche vise tout d'abord à définir un langage de spécification d'assertions et de contrats pour les opérations de services Web ainsi qu'un engin d'exécution de processus d'affaires permettant la vérification dynamique de ces contrats. Notre recherche vise ensuite à développer des outils d'analyse de processus d'affaires qui permettent d'effectuer diverses vérifications formelles, par exemple, vérifier que l'interface comportementale d'un processus possède certaines propriétés, vérifier qu'un groupe de services sont compatibles entre eux et peuvent effectivement collaborer tel que requis par une orchestration de référence.""342396,""Tremblay, Hugo"
"337892"	"Truong, Khai"	"Architecture & techniques for automatically capturing, sensing, and sharing information from daily life"	"One major theme of research in ubiquitous computing is capture and access, that is the recording of details from daily life and then providing interfaces for reviewing that information at a later time. Much of the existing work in this area has investigated issues relevant to the capture and access of information for individual users. The capture and sharing of information between users within a community, however, has not been explored to the same extent and presents novel research questions. In particular, collaborative capture and access can result in a compilation of collective knowledge that can assist in addressing problems that individual users may not have enough information to solve. For instance, a traditional capture and access application for the car may involve instrumenting an automobile to capture images when in operation. This enables the owner to later review drives between locations she has visited. Users looking up directions to places they have not yet visited, however, may want access to similar levels of detail in the form of textual directions coupled with a preview of the images and landmarks for the drive. This level of detail can be collected over time if each individual in a community of users captures and shares images from their daily experiences on the road. Furthermore, a user may want to preview images for a drive that no individual user has previously made, but information captured by multiple users can be combined to provide the desired result. Such requirements present unique challenges above and beyond those already present in systems supporting the capture and access of information for individual users. The proposed research involves design, development, and evaluation of interaction and sensing techniques and an architecture for automatically sensing, capturing, and annotating information from daily life in ways that afford collective use by a community of users. Subsequent application development can be used to validate the generalizability of the architectural solution and enable evaluation of the usability and effectiveness of the techniques in real use settings.""328706,""Truong, VanAnh"
"338428"	"Turcotte, Marcel"	"Combinatorial algorithms for pattern discovery in RNA sequences"	"In recent years, we have seen a rapid growth of the number of known RNA families.  For a significant fraction of them, the mechanisms of action remain unclear. Their signature combines structure and sequence information. In most cases, they are difficult to identify from sequence alone.  Traditional approaches to identify RNA motifs seek to find conserved structures with minimum free energy in an ensemble of aligned sequences.  Often, an alignment is not readily available because of the difficulty to build a reliable alignment without prior information about the structure. Accordingly, comparative analyses are mostly done by hand, iteratively, starting with the most conserved sequences. We recently developed two prototype software systems for the simultaneous alignment and structure prediction of three RNA sequences (eXtended Dynalign), as well as for the inference of RNA secondary structure/sequence motifs (Seed).  Our research suggests that using several input sequences allows to circumvent limitations of the nearest neighbour free energy model --- as the number of input sequences increases it becomes less likely that all of them simultaneously fold into a bad free-energy minimum.  We showed that the use of three input sequences greatly improves the accuracy compared to predictions made from one or two input sequences.  We have also shown that support and exclusion constraints are sufficiently powerful to allow for our combinatorial algorithm to enumerate exhaustively the search space of all conserved motifs. We propose several extensions to the software systems eXtended Dynalign and Seed. Our primary research objective is to develop and compare new objective functions for ranking RNA secondary structure motifs.  Our work on eXtended Dynalign suggests that the accuracy of the nearest neighbour model improves as the number of input sequences increases.  Accordingly, we will explore schemes that are based on this model. In parallel to this work, we will also develop objective functions inspired from models that have been successful for the discovery of sequence patterns. In particular, we will develop objective functions based on the minimum description length encoding principle.""342127,""Turcotte, Martin"
"337868"	"Tzerpos, Vassilios"	"Understanding software systems"	"A large number of science graduates are employed in software development projects all across Canada. These projects range from small, one-person ventures to large enterprises involving hundreds of developers in multiple geographical locations.     When software projects become large and complex, it becomes impossible for developers to possess a complete understanding of the whole system.  At the same time, time-to-market pressures result in the system documentation not getting updated. When system architects or seasoned developers are transferred to different projects or find employment in a different company, important knowledge about the project is lost. This makes system maintenance more expensive in terms of time and money, since most maintenance tasks require a good understanding of the system at hand.     The research outlined in this proposal aims to help increase the productivity of software companies in Canada by recovering knowledge from large software systems automatically. This will be accomplished by:     a) decomposing the software system into meaningful subsystems, so that project newcomers can get a clear view of the system's high-level structure.     b) identifying design pattern instances in the source code, thereby providing clues about the rationale behind the system's design.     c) creating a mapping between testing and application code in order to identify application components that require further testing.     We believe that the software industry stands to benefit in increased productivity and system quality from the development of techniques such as the above.""325267,""Tzoganakis, Costas"
"338377"	"Urquhart, Alasdair"	"Logic and computational complexity"	"The main aim of the research is to understand why certain problems are difficult to solve using computers.  Typical of such problems are those in which a certain number of simple constraints must be satisfied, as in constructing schedules.  The characteristic feature of such problems (known by the technical name of NP-complete problems), is that a solution, if found, is easy to check, but deciding whether or not such a solution exists in the worst case requires an exponentially long time relative to the size of the input.   The principal aim of the theory of complexity theory is to decide whether or not such problems, that seem to require exponentially long computation times in the worst case, really do require such times. My own research aims to work towards showing that exponentially long times are in fact required in the worst case by proving lower bounds on the length of certain kinds of proofs.  Since a computation showing that a solution doesn't exist can be considered as a proof of a kind, lower bounds on the length of proofs show that certain kinds of algorithms (including the algorithms most often used in practice to solve such problems) cannot be efficient in the worst case.""325347,""Urquhart, Stephen"
"337869"	"Vachon, Julie"	"Méthodes, techniques et outils pour la conception d'applications fiables basées sur la composition de services web"	"Le Web met à disposition des personnes, des entreprises et des gouvernements une diversité de données et de services. Pour pouvoir en profiter pleinement, des méthodes et des outils sont nécessaires pour en faciliter l'intégration. De nouveaux processus d'affaires pourront alors être élaborés par la composition et l'intégration de ces ressources. D'autre part, l'essor du Web et de l'informatique orientée service dépendra de la confiance que les utilisateurs pourront accorder aux services offerts. Un des moyens pour accroître cette confiance consiste à offrir des méthodes et des outils pour le développement de services (simples ou composites) fiables et capable de garantir certaines qualités de service. Des techniques de tolérance aux fautes, de recouvrement et de vérification formelle sont vivement souhaitées. En l'occurrence, nous croyons qu'il est important de travailler au développement de méthodes, de techniques et d'outils qui faciliteront (A) la composition des services et l'intégration des données échangées sur le Web; (B) la vérification et la garantie de la qualité des processus d'affaires qui dépendent de la composition de services Web. Notre projet de recherche propose donc la réalisation des objectifs suivants : (1) La définition d'un modèle de composition de services Web, incluant un modèle transactionnel, pour la conception de processus d'affaires fiables. (2) La modélisation orientée aspect et la vérification formelle de la composition des services Web. (3) Le développement d'un système multistratégique et adaptatif pour l'alignement sémantique de sources de données et la mise en oeuvre d'une plateforme pour l'intégration des sources de données géographiques.""329880,""Vachon, Martin"
"338423"	"Valtchev, Petko"	"Lattice-based confrontation of ontologies"	"Interoperability of heterogeneous systems on the Web, e.g., two applications exchanging electronic documents, will be admittedly achieved through a confrontation and the subsequent agreement between the underlying ontologies. The most basic confrontation problem, called ontology alignment, reads as follows: given two ontologies each describing a set of entities (concepts, relations), find the relationships (e.g., equivalence or generality) that hold between these entities. Most alignment methods use either similarity or machine learning as tool for detecting plausible alignments and only a small number rely on semantic-based reasoning. They all suffer on specific shortages, ranging from inefficiency to output limitations. As a remedy we shall investigate a novel alignment approach, based on the joint use of concept lattices and similarity, the former as support structure for alignment hypothesis generation and the latter as a ranking mechanism for hypotheses. Concept lattices provide a hierarchical and contextual representation of ontology entity similarities based on shared binary attributes. By binding those attributes to the structural and the terminological aspects of entity descriptions, we expect to approximate the existing semantic relationships between entities and make them appear in the concept lattice as distinctive ""patterns"". Similarity will then be used to filter the most plausible instances of such patterns before turning them into alignment cells. The ultimate goal is the design of an operational framework for ontology confrontation based on lattices and similarity.""337069,""Valvano, Miguel"
"338975"	"Van, Vien"	"Optical nanocavity devices and structures"	"Nanophotonics - the study of light interactions with matter at the nanoscale level -  has recently emerged as an important research discipline in the broader field of nanotechnology. Technological breakthroughs in nanophotonics bear significant impact on a wide range of areas, from optical communications to signal processing, from quantum information to nanobioscience. At the fundamental level, many of these applications rely on the engineering of the interaction between light and matter to produce strong, useful optical effects. In this respect, nanophotonics offers significant advantage by enabling tiny optical structures to be fabricated that can concentrate intense amount of light energy into extremely small volumes of material, thereby greatly enhancing the strength of light-matter interactions. The proposed research program seeks to design and fabricate silicon nanophotonic devices based on ultrahigh-quality nanocavities with dimensions of only a few wavelengths of light. The large resonance-enhanced optical interactions achieved in these nanocavities will be explored for use in a wide range of novel applications such as low-power optical nanotransistors, nanophotonic logic elements, silicon nanolasers, and ultra-sensitive biochemical nanosensors. Improvement in the cavity's quality factor will also lead to potential applications of cavity quantum electrodynamic effects for quantum information processing. In addition, the extremely small sizes of these devices will enable ultra-large-scale integration of nanophotonic circuits and the fabrication of massive nanosensor arrays for rapid biochemical analysis, with important applications in DNA sequencing, drug development, food safety and environmental monitoring. Optical nanocavities are a fundamental building block of nanophotonics technology and an enabling element for a diverse array of novel applications. The proposed research program seeks to unlock the tremendous technological potential of these nanoscale devices through their innovative applications. The innovations made and new knowledge gained will help strengthen Canada's expertise in the areas of nano- photonics and nanotechnology, which are of increasing strategic importance to the nation's future economy.""330831,""VanAcker, Rene"
"338445"	"Vardy, Andrew"	"Visual robot navigation"	"The objective of this research program is to develop methods for robots to navigate in unstructured environments.  Unstructured environments are those which have not been specifically designed for robot operation.  The approach taken here is partially inspired by biological models.  The current view of navigation in insects such as bees, ants, and wasps is that these insects capture ""snapshot images"" throughout their environment.  They then use a procedure called visual homing to move between snapshot positions.  This research will apply a similar methodology to robots. Robot navigation is a difficult problem for two fundamental reasons:  (1) Using odometric information to estimate position incurs a cumulative error.  In other words, the further a robot travels, the more unsure it becomes of its position.  (2) Places that have the same appearance cannot be distinguished.  Interestingly, most existing methods can address one of the above difficulties, but seldom both. This research program will utilize visual homing to allow a robot to travel between places where it had previously captured snapshot images.  We will begin with an existing model for navigation based on homing, and incorporate additional insights so as to resolve the problems described above.  The desired end product of this research will be a set of methods which allow a robot to travel robustly between known places.  The challenge is to make these methods truly robust so that available visual cues are utilized while noise is ignored. The applications for robustly navigating robots are both numerous and diverse.  Some example applications are autonomous driving, planetary exploration, search and rescue, surveillance, remote sensing, and household cleaning.""328045,""Vardy, Stephen"
"338393"	"Vassileva, Julita"	"Encouraging participation in online communities"	"Online communities (OC) engage users in various forms of sharing: discussion forums and blog systems for sharing ideas, peer-to-peer systems for swapping songs and videos, shared repositories of digital photos, expertise sharing systems, online learning communities, communities of practice, and communities of volunteers collaboratively developing open-source software. The existence and sustainability of an OC requires the active involvement of a wide pool of end-users. Most large communities (e.g. for sharing music) are able to achieve the redundancy and diversity of resources required for their sustainability. However, in small communities, like an online learning OC or an expertise-sharing OC, it is hard to ensure the ""critical mass"" of participation and contributions that would make the community self-sustainable. The proposed research will address the problem of motivating participation in online communities by developing appropriate community infrastructures. First, there must be either material or social incentives for sharing. Second, since the users are volunteers, they should only need to make minimal effort to find and to share resources. The community infrastructure has to provide both rewards for sharing and tools for search / annotation of shared resources that are powerful, easy to use and respect the autonomy of the participating users. The research will involve developing theoretical models, implementation, and extensive evaluation. The practical nature of the problem will provide both a test bed and a challenge to the existing techniques developed in several areas of computer science: multi-agent and peer-to-peer systems, personalization and user-adaptive systems, machine learning, and AI in Education. In addition, the research involves issues related to the usability and social aspects of online community infrastructures.  ""345134,""Vassileva, Svetla"
"337818"	"VonBochmann, Gregor"	"Requirements engineering and design methods for distributed applications"	"This project aims at developing and/or improving techniques for describing the dynamic behaviour of system components, especially at the requirements level, in order to facilitate the building of complex distributed systems out of smaller components, which may be re-used or developed for the particular application.  It deals with methods and tools for partially automating the development of a distributed system architecture and high-level design from given requirements, and for building distributed applications that are able to select appropriate partner applications for providing required services, and to adapt to the constraints given by the execution environments of the different system components. In this context, non-functional properties like response time, reliability, security and cost are important selection criteria. The objectives of the research include the development of methods and tools for deriving distributed system designs from a requirement  specification given in the form of UML Activity Diagrams or Use Case Maps, the consideration of distributed transactions, the integration of the ""publish-subscribe"" paradigm in order to allow for adaptable applications taking various non-functional requirements and trust relationships into account, and the improvement of methods for automatic submodule construction and their application to component re-use in the context of evolving system requirements. The methods and tools developed within this context will be evaluated by applying them to the development of system prototypes for various applications of realistic size and complexity. Possible areas of application are management of optical networks, electronic commerce, Grid computing and distributed multimedia applications, possibly using proposed Internet protocol standards.""340334,""VonderWeid, PierreYves"
"338908"	"Wang, ChunyanCW"	"Low-power VLSI circuits for signal sensing and processing"	"The rapidly growing needs for high signal processing capacity and low power dissipation in electronic systems call for innovative design and implementation of VLSI circuits. The reduction of power dissipation is an important issue which allows a larger scale of integration of processing units that are needed to improve the computation capacity of the circuits. If the circuits are battery-operated, lowering power dissipation means an improvement in their utility. Also, applications of VLSI systems in emerging areas, such as bio-engineering, may impose more restrictions on electrical signal and power supply sources, which put more challenges to VLSI designers. Furthermore, to better explore the emerging technology and devices, new computation schemes for signal processing and design methodologies for integrated circuits need to be developed. The overall objective of the proposed research is to develop VLSI sensing and processing circuits with constraints of a very restrictive power dissipation and high computational capacity. This work requires research for innovative design methodology in the device, circuit, and system levels, and low-cost implementation as well. It includes developing (a) low-cost integrated sensor units, (b) analog-mixed circuits operating with sub-nA currents, (c) digital circuits with ultra-low power dissipation, and (d) computation schemes and architectures tailored for target signal/image processing tasks. Integral to the above objective will be the inclusion and training of a large number of graduate students to become highly qualified personnel needed in our industries.""344374,""Wang, Danyang"
"337807"	"Weddell, Grant"	"Database technology for embedded software systems"	"Commercial relational engines are a multi-billion dollar industry, and are testimony to the reductions in the cost of developing and maintaining information systems that derive from the use of the SQL language and the consequent deployment of these engines. The proposed research program is guided by the hypothesis that comparable reductions in the cost of developing and maintaining embedded software would also ensue from the use of the SQL language, which in turn requires a number of new or improved capabilities with the underlying technology.    First, there is a need for improved query optimization for cases involving much richer forms of both conceptual and internal database schema since query plans generated by query optimizers must be comparible in performance to low-level code written directly by expert programmers. This places much greater demands on the sophistication of various facets of query optimization, including physical data models and data encoding, view integration capabilities and conceptual data modeling.    Second, there are a number of properties of embedded systems that affect the indexing and encoding of data. For example, decisions on the selection of appropriate indexing in this setting can now be driven by threshold requirements dictating that a particular query must run in logarithmic or even constant time. Other consequences are the need for more general and reliable data integration capabilities, and on alternative local data encoding in main or flash memory.    And third, there is a need for alternative transaction models in which a database server can be much more client aware in terms of the dynamic aspects of client interactions with the server. This can enable, for example, the use of deadlock free locking protocols and a consequent decoupling of concurrency control from data recovery, an essential requirement for many embedded systems in which logging and general abort capabilities would otherwise lead to unacceptable runtime overhead.""335297,""Wedley, William"
"338412"	"Wei, Ruizhong"	"Topics in network security and data security"	"This proposal includes several selected important research topics in network security and data security. Topics in network security consist of sensor network secure key management and access control in hierarchies. Wireless ad hoc sensor networks offer certain civilian applications as well as assisting against potential terrorist threats. Security is a very important issue in those applications. Nodes in a network should use secret keys for communication and authentication. So how to distribute keys in sensor networks is very important. In this proposal, we will consider secret key schemes for general sensor networks and  consider key management schemes for more specific sensor networks to fit various real world applications. In an organization hierarchy, a node at the higher level can access the node at the lower level, but not the inverse. How to distribute secret keys for access controls in that situation is a challenge. We willfind an efficient way to do that in this proposal. Topics for data security include inference problem in a database and digital fingerprinting. To simplify things, we may think that an inference problem in database means that one can infer some secret information from public databases. So we need methods to decide what kind data can be put in to public databases. Recently we proposed some schemes for inference problems in a single database. Next we will consider those problems in distributed databases. Digital files, such as software, music, movie, etc., are easy to copy and redistribute. Digital rights management(DRM) is an emerging area that deals with the copy right protection of digital files. Digital fingerprinting are used to protect against ownership hijacking and unlawful redistribution. Cryptography plays a key role in this research. In this proposal, we will undertake furtherinvestigation to find more efficient and easily implemented methods. We will then consider some specific digital files, such as image and music files, and modify the methods to fit the special requirements. All the solutions in this proposal will be explicit and easy to implement.""328352,""Wei, Wanxia"
"337899"	"Whitehead, Anthony"	"Investigations into new multimedia and video modalities"	"Since Edison Labs developed the first video camera (the Kinetograph), society has embraced video cameras through home movies, entertainment, education and security. From the use of cameras to create home movies, to our daily use of television, to blockbuster films created purely for entertainment value, video plays an important role in our every day life. Just two decades ago, we can easily remember having only a few television channels, whereas today we have access to hundreds of stations from around the world. This glut of video information introduces unique problems and research opportunities in the manipulation, management, indexing, browsing and searching of the volumes of video data we have at our finger tips.  However, unlike its textual counterpart since the advent of hypertext and the World Wide Web, there is little interactivity in video and it remains a static medium from the viewer standpoint.  This is to say we watch video; we do not interact with video! The core objective of this research program is to develop systems to enable radical changes in the new generations of computer based television (such as HDTV and digital television) so that it becomes an interactive medium.  Through continuing research into computational video techniques and the novel application of hyper-media methodologies to video data it is expected that the research results will help to make video become as ubiquitous and easy to access an information source as text and the World Wide Web is today.  This research represents the beginning of a new modality through which we will experience video. It will facilitate changing users from simply being static observers of video on a television set or monitor to being a dynamic and interactive part of a system where input and feedback is applied to adapt the system to meet current user whims.""334739,""Whitehead, Henry(Hal)"
"338336"	"Whitesides, Sue"	"Robotics and geometry"	"This proposal focuses on developing ways to use computers to answer questions in geometry and its application areas.   When designing a method that a computer can use to solve a general geometry problem, it is important to understand how the computer resources the method will use depend on the particular problem to be solved;  obtaining this knowledge   is part of the design process for the computer method itself. Application areas that can benefit from the geometry problems studied here include computer graphics, motion planning of robots, layout and visualization for graphs and networks, emerging technologies for manufacturing objects at small scale, and protein modelling.""332832,""Whiteway, James"
"338348"	"Wong, Andrew"	"Discovery, representation and analysis of statistical and structural patterns"	"The proposed research extends the current NSERC Discovery Grant Project entitled: ""Discovery, Representation and Analysis of Statistical and Structural Patterns"". Its objectives are: 1) to automatically discover statistical and structural patterns from various types of data in the presence of noise; 2) to develop a unified framework of pattern representations for analysis, interpretation and manipulation and 3) to apply the developed technologies to real-life problems which require intelligent solutions. Through collaboration with Pattern Discovery Software Systems Ltd (PDS), the theories and algorithms of pattern discovery (PD) have been developed into a software system known as discover*e with great success in solving complex real world problems in areas covering pharmaceuticals, oil sand, gas and coke processing, bioinformatics and healthcare, etc. My recent NSERC research focus is on the scientific fundamentals of PD and its extensions including: 1) clustering of statistical patterns and discovery of their associated data groups in various data subspaces; 2) discovering the structural relations of data groups; 3) fuzzy partitioning for continuous attributes; 4) attribute clustering, feature selection and re-pooling for classification and interpretation; and 5) PD for samples with unbalanced distributions. Fundamental works have been documented in journal publications and conference proceedings. In structural pattern and knowledge representation, a US Patent was granted. Its attributed hypergraph representations (AHR) and manipulation will be extended to cover a broader range of data and models. The upcoming research will focus on: 1) the generalization of the discovery process and knowledge representation of statistical patterns and data groups in various data subspaces (in  AHR), moving towards a more general PD framework for large database and systems;  2) the discovery of resolution-dependent patterns in images for segmentation, object/scene representations, categorization and recognition; 3) applications of PD to a) oil exploration, extraction and refining; b) healthcare in diagnostics and prognostics; c) gene expression data and d) genomic pathways.""343802,""Wong, Andrew"
"338934"	"Wong, Kainam"	"Diversely polarized smart-antennas algorithms for next-generation wireless communication technology"	"      This proposed inter-disciplinary research program offers an uncommon link between ""Electromagnetic Physics"" and ""Statistical Signal Processing"" to advance new exploitations of polarization in wireless communication systems and defence electronics.       Wireless telecommunication equipment requires brainpower (""signal processing"" algorithms) to direct their antennas and electronics hardware to intelligently handle complex and shifting situations. This algorithm / hardware synergy is hampered by the routinely unconnected development of the algorithm and the hardware--- each typically designed while accounting for little of the other's attributes. One reason for this ""left-hand-knows-not-what-the-right-hand-does"" predicament is the unavailability of MATHEMATICALLY PLAIN descriptions of the ELECTROMAGNETICALLY COMPLEX phenomena of mutual coupling and depolarization --- mathematically uncomplicated yet reliable enough to develop optimal ""smart-antennas signal-processing"" algorithms to handle various non-ideal antenna / channel effects. Through advancement of innovative antenna / channel models and algorithmic approaches, this research will resolve the abovementioned critical yet overlooked issues on software-hardware synergy over polarization.       Integration between ""statistical signal processing"" and ""electromagnetic physics"" will lead to the following notable competitive advantages for the Canadian telecommunications equipment industry in the international marketplace: (1) Increase in the wireless communication system's transmission capacity. (2) Increase in the wireless communication system's user load; increase in the wireless communication system's bandwidth efficiency; and decrease in the wireless communication system's hardware costs --- by reducing required density of base-stations per unit of geographical area. (3) Extension of the mobile handsets' battery life; and reduction of the mobile handset' radiation effects & associated health hazards.""341414,""Wong, Kelvie"
"338396"	"Wong, Tony"	"Optimisation multicritère par méthodes évolutionnaires distribuées"	"L'optimisation multicritère est une formulation généralisée et naturelle des problèmes d'optimisation. L'utilisation des algorithmes évolutionnaires dans l'optimisation des problèmes multicritères permet la prise en considération directe des objectifs à optimiser, sans l'aide de pondération a priori. Toutefois, l'application des algorithmes évolutionnaires, dans ce contexte, n'est pas simple. En effet, il existe un nombre d'obstacles qui rendent ces algorithmes rébarbatifs pour leur déploiement concret. Les principaux obstacles sont la nature stochastique et la variété des algorithmes existants; la détermination des paramètres de contrôle et la charge de calcul de ces algorithmes. Les objectifs de ce programme de recherche sont: (1) Établir de façon automatique l'intervalle utile des paramètres de contrôle des algorithmes évolutionnaires, par une nouvelle méthode d'estimation, malgré les incertitudes et la nature stochastique des variables. (2) Classifier statistiquement la performance de ces algorithmes en utilisant un cadre de comparaison multiple non paramétrique, un ensemble de problèmes de référence et trois problèmes d'application : la génération des horaires, la prédiction de la volatilité financière, l'extraction des caractéristiques dans la reconnaissance d'écriture manuscrite. (3) Dégager une politique de balancement de la charge de calcul en arrimant le modèle d'exécution des algorithmes évolutionnaires distribués avec le modèle de communication d'une grappe d'ordinateurs. À terme, la combinaison de l'estimation des paramètres de contrôle, de la classification statistique de la performance et de la politique de balancement de calcul permettra la création d'une plateforme informatique capable d'automatiser le choix et la configuration des algorithmes évolutionnaires distribués dans l'optimisation des problèmes multicritères.""342036,""Wong, TszHo(Chris)"
"338344"	"Woodham, Robert"	"Analysis and interpretation of visual motion"	"The goal is to make machines see.  Research consists of selecting a task and implementing proof-of-concept systems to study performance in both real and controlled experimental situations.  The current objective is to understand how motion supports high-level interpretation tasks related to an object's identity, to its non-visual physical properties and, for an object that is an active agent, to its state and intentions.      Most often, recognition is taken to be the 'sine qua non' of computer vision.  In this context, motion becomes the tracking of high-level features of objects already recognized statically.  The study of biological vision systems suggests that motion itself is the more primitive cue.  But, what are the low-level features to track?  How might the tracking of low-level features drive recognition (rather than follow it)?  These are the basic scientific questions.      We have developed specific tools and experimental environments to explore these questions.  Our Active Measurement Facility (ACME) is an integrated robotic measurement facility.  ACME is used to acquire images of objects in motion using controlled illumination and sensing.  ACME, by design, supports careful, repeatable scientific experimentation.      Recent work interprets the motion of active agents, both cooperative and competitive, in the context of hockey.  A proof-of-concept system has been demonstrated. The system takes as input player motion trajectory data tracked from real game video and supported by knowledge of hockey strategy, game situation and specific player profiles.  The system describes what happened in each identified situation, assesses the outcome, estimates when and where key play choices were made, and attempts to predict whether better alternatives were available to achieve understood ""coaching"" objectives.  This is an important first step toward generalizable, motion-based ""situation analysis.""""331242,""Woodhouse, Kimberly"
"338385"	"Xiang, Yang"	"Multiagent uncertain reasoning and knowledge discovery with graphical models"	"Due to the scale and complexity of today's software and the reliability required, and due to the advance in computer network and reduction in processor cost, it has become both necessary and possible to solve a complex problem in a large domain by a set of cooperating computers, called agents.  A common task of agents is to determine what is the state of their domain in order for them to act accordingly. I will study fundamental difference between alternative ways to perform such reasoning: tightly coupled, where agents exchange limited beliefs, and loosely coupled, where an agent treats everything from another agent as observations.  Constructing such agents is challenging to software developers. I will investigate ways to automate some steps of construction, e.g., to enhance agent interfaces automatically for improved reasoning efficiency. One application area of such agents is collaborative design in supply chains.  No existing method enables agents to construct an optimal design efficiently.  Continuing my progress in the last period, I will analyze the conditions under which an optimal design can be efficiently computed by agents and develop such a method. The agent that I study is equipped with knowledge represented as a probabilistic graphical model.  Learning such a model from data is one way to automate agent development.  My research found that a class of models, called PI models, cannot be learned by commonly used methods.  If an agent is equipped with an incorrectly learned model, it may make mistakes in carrying out its tasks.  In order to learn such models effectively, a deep understanding is needed on how they are composed from independent parameters. Such understanding on some subclasses of PI models has been obtained in my last period of research. I will further investigate to achieve thorough understanding for the entire class of PI models.  Once the composition of the entire spectrum of PI models is understood, a new generation of learning methods can be developed which will improve the reliability of the agents built from the learned models.""326399,""Xiang, Yingwei"
"338947"	"Yam, SzeHong(Scott)"	"Optical waveguide fabrication by hot embossing technique for cost-effective optical interconnect"	"Current wavelength division multiplex systems offer 32-64 wavelengths at 2.5-10Gb/s/wavelength, with capacity approaching 1Tb/s. As the total data rate of a single fiber-link is increasing at a rate faster than electrical switching capacities, end-to-end system performance will not be limited by point to point fiber links, but rather the switching and routing equipment that terminates and switches traffic at a carrier's central office. In fact, it is the interconnect (backplane) that provides communication between the line cards and hence supports the entire capacity of the switching fabric. The technology has traditionally been all-electrical, with copper traces on printed circuit board (PCB), and this does not scale with the electromagnetic parasitic effect at high data rate. Optical waveguides based on PCB compatible polymer have been proposed, as it circumvents issues such as electromagnetic interferences and impedance mismatch at connectors. However, its manufacturing process to date involves complex deposition and edging steps. This proposal investigates the incorporation of the hot embossing process into optical interconnect fabrication. It is the stamping of patterns into a polymer softened by raising the temperature of the polymer just above the polymer transition temperature, and has been used to produce micron-scale size features in microfluidic devices. Once defined, the stamp can be used repeatedly to mass-produce low-cost waveguide devices such as wavelength division multiplexer and de-mutliplexer to take advantage of WDM in optical interconnect. Moreover, this will provide a future platform to integrate photonics with low-cost polymer-based microfluidic chips as bio-chemical sensors. Highly qualified personnel (at least two graduate students, M.Sc,/ Ph.D.,) will be closely supervised to actively participate in this research. The training promises a versatile workforce, highly skilled in such diverse areas as polymer device fabrication, optics, high-speed data communications, and bio-chemical sensing applications.""339671,""Yampolsky, Michael"
"338900"	"Yang, EnHui"	"Joint compression and watermarking:  theory, algorithms, and applications"	"Personal multimedia communications, which involves the seamless integration of the telephone, television, and computer and is concerned with efficient, reliable and secure distribution and storage of digital media to people and machines through different channels, is a major area of growth in engineering today. However, before multimedia communications can happen successfully in various environments such as wireless and mobile environments and will change our lives and economy for many years to come, many challenging problems have to be resolved. Among them are data compresson to efficiently utilize existing limited storage and bandwidth resources, and digital watermarking to provide techniques and tools for digital right management. So far, to a large extent, data compression and watermarking have been investigated and designed separately. Since in personal multimedia communications, watermarked signals will be likely stored and/or transmitted in compressed format, instead of treating watermarking and compression separately, it is interesting and beneficial to look at joint design of watermarking and compression schemes. In this research, we will investigate joint compression and watermarking systematically. Our scientific objectives are to further extend existing information theoretic results on joint compression and watermarking, to establish new theoretic frameworks for new configurations of joint compression and watermarking, to design efficient and robust joint compression and watermarking algorithms by extending source coding techniques to include watermarking capability, and to apply our results to current compression standards such as JPEG for still image compression, MP3 and AAC for audio compression, and MPEG-2 and H.264 for video compression, to see how they can be optimized and modified to support watermarking capability while maintaining or even improving their compression performance.""385697,""Yang, EnHui"
"337880"	"Yee, George"	"Developing secure software"	"Software has become common place in our daily lives. However, having software that works as intended is still a difficult challenge, especially where security is concerned. Today, security measures are added to software systems after they have been created, often resulting in performance and reliability issues. Rather than adding security measures this way, software needs to be designed for security from the start. This is a proposal for research in software security, i.e. how to develop secure software from design conception. Since there is no such thing as perfect security, secure software is software for which the risks of compromise have been minimized. The improvements to developing secure software will be accomplished by using highly effective formal and automated approaches to improve the areas of software threat modeling, security requirements, and security testing. This work is novel and highly practical, as it has application not only to new software but also to existing software, hardware, and even the insurance industry (to help determine the risk of threats to an insured system). It will have an enormous impact on everyday computer systems that people use by improving their confidence that their software will not be compromised. For example, it will make it more comfortable for people to participate in e-commerce and e-health, thereby benefiting both the economy and society.""334551,""Yee, Howard"
"338854"	"Yongacoglu, Abbas"	"Application of basis selection algorithms to wireless communications systems"	"Efficient signal representations require efficient signal decomposition algorithms. The problem of basis selection for signal decomposition consists of determining a small subset of vectors chosen from a large redundant set of vectors to match a given data. The basis selection algorithms are generally treated as a part of the specific application for which they are used. These applications include time/frequency representations of speech coding, spectral estimation, video coding and channel estimation. Finding an optimal solution using an exhaustive search is infeasible, suboptimal methods that require lower computational complexity have been proposed. Among these suboptimum techniques we have focused on sequential basis selection (SBS) in particular matching pursuit algorithms which have lower levels of computational complexity than the conventional least squares methods and work well especially in applications with sparsity properties.   The objective of our proposed research is to improve the performance of these SBS algorithms and extent their applications to more complex wireless applications. We are working on optimizing the associated dictionary structures. This corresponds to a training sequence design for channel estimation, array structure design for angle of arrival detection and code structure design for multi-user detection. By optimizing the dictionary, the requirement of a combinatorial search in the SBS algorithm is eliminated and the  complexity can be further reduced. Extending the wireless applications which utilize SBS algorithms will be done for multiple input multiple output (MIMO) systems, and for radiolocalization systems. MIMO systems are of interest due to their ability to provide substantial gains in capacity and quality. Similarly, accurately radiolocating a terminal is essential for efficient network operation of modern wireless systems. Many important results can be obtained by applying the overcomplete signal expansions to such systems due to the low complexities of the SBS algorithms. In addition to estimation of MIMO channels and geolocation, other potential applications we will investigate are higher dimensional angle of arrival estimation, and decoder design in channel coding systems.""337226,""Yoo, Dongwan"
"338448"	"Yu, Gwoing(Tina)"	"Analyzing population diversity of modular program evolution through interactive visualization"	"This research program addresses the question of how to move applications of the machine learning technique of genetic programming from small-scale examples to large-scale real world problems, such as open-ended design. The working hypothesis is that a learning system that is capable of composing modular solutions can better adapt to a complex problem. To verify this hypotheses, we have developed a prototype genetic programming system that produces solutions in modular form. The system has demonstrated some success in the task of learning financial trading rules. The objective of this research is to understand the learning process of the system by analyzing diversity of the solutions population using an interactive visualization tool. With that knowledge, we will be able to apply the system to large scale problems effectively.""326862,""Yu, Haitao"
"338387"	"Yu, MinLi(Joseph)"	"Routing in networks and graph labellings"	"Over the next few years, I intend to focus my effort on the following two areas. 1. Routings in Survivable WDM Networks We study the problem of designing a survivable WDM network for All-to-All communication in a network based on covering the physical network with sub-networks that are protected independently from each other. In the case when the sub-networks are cycles, this problem can be modelled as finding a cycle partition or covering of a complete graph (each edge corresponds to a request) such that for each cycle, its request edges can be routed in the physical network by a set of vertex disjoint paths. We are interested in designing shortest path routings which give a uniform or near uniform load. We investigate the problem for different physical network and subnetwork structures. 2. (d,1)-total labellings A (d,1)-total labelling of a graph G is an assignment of integers to all edges and vertices such that (i) any two adjacent vertices receive distinct integers, and (ii) a vertex and its incident edge receive integers differ by at least d in absolute value. The span is the maximum difference between two labels. We are interested in the minimum span of a (d,1)-total labelling and as well as the complexity aspect of the problem. Study this parameter is motivated by the applications in the channel assignment problem.""339794,""Yu, Pei"
"338887"	"Zaccarin, André"	"Algorithms for video compression"	"The objective of this research program is to advance compression technology by 1) investigating fundamentally different approaches to frame prediction, and by 2) investigating means to choose encoding parameters to increase visual quality. The first objective will be achieved by identifying in each video frame scene features that can be found in other frames despite differences in viewpoint, scale or illumination. By matching these features, we will then establish a correspondance between frames that will improve compression ratios. The second objective of this proposal will be achieved by integrating, into a video encoder, means to objectively evaluate the visual quality of video, and adjust encoder parameters to maximise the visual quality. Both objectives will require computationally complex algorithms. However, for offline compression, and taking into account the continuing increase in processor computational power, this will not be an impediement to the advancement of compression technology.""325362,""Zaccarin, André"
"337809"	"Zhang, Chang"	"Design methodologies and modeling of reconfigurable computing and multilayer access control"	"The proposed research program is comprised of the following two research projects: 1. Reconfigurable Computing Design Methodologies with Application to Hybrid Video Codec. The reconfigurable computing is able to re-write itself into different architectures in order to meet specific application and performance requirements. It provides an excellent platform to build system on chips (SoC) that require higher performance, low power dissipation and at the same time a high degree of programmability. In this research, we will investigate the design strategies and methodologies for reconfigurable computing and apply them to design reconfigurable architectures for hybrid video codec that is able to encode (decode) a video into (from) multiple formats according to different video compression standards (MPEG-2, MPEG-4 and H.264). The study will be conducted at three different levels: algorithm and process analysis at system design level, hardware-software co-design at the architecture level, and reconfigurable circuit design at the logic circuit level. 2. Multilayer Access Control Modeling for MPEG-7 Multimedia Applications. A multimedia document in a digital library could be huge and may contain private, sensitive or other restrictive information. A user may allow to access certain part(s) of the document depending access rights and needs. We will develop a generic multilayer access control model for various MPEG-7 multimedia applications including digital libraries, e-education, and multimedia entertainment such that the same model can be integrated by all existing access control systems. The proposed model will integrate with major features of the MPEG-7 standard to achieve efficient and effective secure multilayer access control.""333609,""Zhang, Chao"
"338943"	"Zhao, Dongmei"	"Quality-of-service provisioning and resource management in wireless mesh networks"	"Wireless mesh networking based on IEEE 802.11a/b/g, 802.16a or other technologies has been drawing significant attention in both industry and academia recently, as it demonstrates some extremely important characteristics, such as easy deployment, low cost, and potential high capacity. However, it also faces a variety of challenges relating to quality of service (QoS) provisioning, efficient radio resource usage, scalability, and other issues. The long term objective of this research is to study QoS provisioning for multimedia traffic in interconnected wireless and wireline networks where users can move freely to anywhere at any time. The objective of the research during the next five years is to propose and investigate performance of scalable radio resource management schemes in wireless mesh networks that can provide satisfactory QoS for more mobile users. In the proposed research, the radio resource management will consider how to best exploit the available capacity based on specific network conditions to support both the link layer and network layer transmissions. The resource management will also consider user mobility and handoffs in order to support mobile user's services seamlessly. A cross-layer approach allows the resource management to fully access to the benefit from different network layers. Various approaches will be studied to make the proposed resource management schemes scalable to work in large mesh networks. The proposed research is expected to achieve more effective and efficient radio resource utilization. It will also make wireless mesh networks easy to be deployed in not only local areas, but also city-wide and larger areas.""341287,""Zhao, Dongmei"
"338343"	"Ziarko, Wojciech"	"Rough set-based empirical modeling with applications to control, pattern classification adn data mining"	"The proposed research program is concerned with the fundamental research, algorithms and applications of rough set theory and its probabilistic extensions. The theory of rough sets and the related technologies deal in the most part with automated learning of classification algorithms from data. The rough set-based methods include techniques for the analysis of suitability of data for the learning purpose, techniques for the elimination of  useless information from data, while focusing on the most dominant classificatory factors, and   the techniques for automated formation of classification rules and decision tables from data.      The current proposal is focused on probabilistic approaches to rough sets, in particular on variable precision and probabilistic  rough set models. The probabilistic extensions allow to model stochastic relationships existing in data and to develop classifier systems operating with controlled  degree of uncertainty.      The application aspect of the proposal is focused on three major application domains. The first one is related to the theory, algorithms and experimental investigation of the problem of control algorithm acquisition from data through  machine learning. The key objective in this process is to substitute the complex mathematical modeling steps with automated generation of control algorithm from operation data.      The second application domain deals with the  methodology, algorithms and prototyping of selected pattern recognition applications. In particular, experiments will be conducted with  automated image classification such as medical image interpretation, face image recognition, text contents classification and  speaker-independent recognition of isolated spoken words.      The third broad application area is data mining in which the rough set-based algorithms will be used for analysis and modeling of probabilistic dependencies existing in data, detection of fundamental factors in the relationships and optimization of the derived models. The main focus will be on analysis and interpretation of medical data in cooperation with medical research institutions.""327721,""Zibin, James"
"338404"	"Zima, Evgueni"	"Accelerated computational schemes for symbolic and numeric algorithms"	"The objective of this research program is to develop a set of algorithms, software and hardware tools for acceleration of some standard symbolic and numeric algorithms. Our target symbolic algorithms are in the area of symbolic summation. The target numeric algorithms focus on high precision evaluation of elementary and special functions. Our approach to the acceleration is based on -- symbolic preprocessing of computational problems, including the choice of an appropriate representation, for example, closed form formula, annihilating linear differential or difference operator, chains of recurrences, integral representation of combinatorial sums and code generation based on chosen representation; -- use of alternative arithmetics for high precision computations such as modular representation of arbitrary precision rational numbers with special choice of moduli, adaptive precision floating-point arithmetic; -- distributing computationally hard problems over a network of workstations or through the web; -- VHDL prototyping of specialized hardware for acceleration of a particular computational task, including prototyping of specialized floating point units based on the technique of multidimensional chains of recurrences and adaptive precision floating-point arithmetic.""327209,""Zimmerling, Jeffrey"
"337802"	"Zuberek, Wlodzimierz"	"Timed petri nets in modeling and analysis of concurrent systems"	"The long-term objective of this research is the development of efficient methods for modeling and analysis of concurrent systems, i.e., systems in which some events can occur concurrently but there are restrictions of precedence and frequency of such occurrences. Multiprocessor and distributed systems, database systems, communications protocols and networks are examples of such systems. Because of very simple representation of concurrency and synchronization of events, Petri nets have been gaining popularity in modeling and analysis of concurrent systems. Several types of Petri nets ""with time"" have been proposed on order to take into account the durations of modeled activities, and to use the same models for qualitative and well as quantitative analyses of the modeled systems. However, net models of real-life systems tend to be too complex and too difficult to be dealt with directly (the so called ""state explosion problem"" of state-based models). Therefore special techniques (and software tools implementing these techniques) are needed for analysis of many realistic models. Short-term objectives of this project include: (i) automatic (or semi-automatic) derivation of net models from specifications of the modeled systems, (ii) efficient analysis methods of derived models, and (iii) development of software tools for generation, analysis and evaluation of net models. All these objectives are complementary in the sense that they refer to different stages of the modeling and analysis process. It is expected that more efficient algorithms and, especially, more user--friendly software tools (which will hide the more advanced aspects of Petri nets) will increase popularity of Petri nets in modeling, analysis and evaluation of complex systems.""338015,""Zucker, Jeffery"
