"id"	"researcher_name"	"application_title"	"application_summary"
"364234"	"Abid, ZineEddine"	"Defect-tolerant design techniques for nanotechnology integrated circuits"	"Silicon technology will be reaching its last node in year 2016.  This is due to the scaling down of the Silicon transistor towards its fundamental physical limitations of operation, causing a real barrier to further advancement.  New emerging nanotechnologies, based on new non-Silicon transistors and devices, are needed beyond the year 2016 to meet the projected performance of Very Large Scale Integrated Circuits and system-on-a-chip.  However, a large number of defects is expected due to the fabrication imperfections and non-deterministic self-Assembly process of these emerging nanotechnologies.  Such high defect density, although not as severe, is also expected for conventional Silicon technology once it reaches its sub-50 nm dimensions.  This causes a major limitation for sustaining the growth of Silicon technology requiring extensive work at the circuit level to overcome the presence of high density of defects.This research program focus is on the development of novel circuit techniques robust to the presence of large number of defects in fabricated electronic chips.  The objective is to reach the needed defect-tolerance level by using initially hardware redundancy, at the transistor and small gate levels, to mask defective devices and components in conventional Silicon technology.  Then more robust circuit techniques will be developed for non-Silicon nanotechnology along with new logic circuit configurations based on emerging nanometer devices.  Canadian Microelectronics Corporation provides the design tools and access to advanced Silicon technologies.  Such sources will be used for the development of novel circuit techniques to overcome some of the limitations of emerging nanometer devices.  Circuit models of these nanoscale devices, especially Carbon Nanotube transistors and diodes, will be developed.  This research work is critical to maintain the desired manufacturing yield and the continuous advancement of semiconductor technology.  This research will help reduce the industrial need for circuit designers in the nanotechnology field, by providing excellent training opportunity for students at the device- and circuit-level.""383582,""Abidali, Asraa"
"365686"	"Abidi, Syed"	"Semantic web framework for designing personalized information and adaptive web services"	"The sheer volume of information available over the Web has created the much-cited information overload problem whereby users are finding it cognitively stressful and difficult to find 'relevant' information. We are pursuing an Information Personalization (IP) research program that seeks to personalize the users web-based information mediation experience guided by a profile that characterizes their demographics, knowledge, interests, preferences, needs, goals and behavioural attitudes. Our IP approach extends beyond the prevailing techniques for personalizing the interface-level presentation of web-based information. Instead, we address the more complex issue of personalizing the actual information content delivered to users.Our research strategy is to work in concert with advances in web technology, in particular the Semantic Web. We recognize the potential of the Semantic Web to advance IP research, and therefore we propose to investigate the next generation of semantically powered reusable, shareable and interoperable IP methods and tools.  The objective of the proposed research program is to develop a web-based personalized multimedia information and services delivery system for the Web. The proposed system will dynamically personalize the information content and service functionality with respect to both the user's context and profile. The automated personalization capabilities will be context-aware-i.e. the system will sense the user's current task in order to respond with relevant information and services; pervasive-i.e. the system will provide personalized responses whenever the user interacts with the Web; and proactive-i.e. the system will invoke the appropriate personalization mechanisms. The personalization environment developed will serve as a workbench to custom-design web applications with context-aware, proactive and pervasive personalization capabilities.""369584,""Abielmona, Rami"
"364538"	"Abolmaesumi, Purang"	"Medical image processing and computer-assisted surgery"	"The applicant's research focuses on the deployment of advanced medical imaging in computer-assisted surgery and cancer diagnosis. In this research, real-time ultrasound imaging will play a significant role. In contrast to other conventional imaging techniques, ultrasound is a non-ionizing tool (which is especially important when performing operations on children and pregnant women), has significantly lower cost, and allows real-time visualization of the anatomy throughout the procedure. A probabilistic framework that could efficiently and automatically segment and register bone surfaces from ultrasound data is proposed. This framework will be utilized in the development of novel techniques in computer-assisted orthopaedic trauma surgery. The framework will be extended to generate statistical anatomical atlases and atlas-based registration techniques, as well as deformable registration of soft tissue for adaptive radiotherapy.Another aspect of the applicant's research involves applying computing techniques to ultrasound-based prostate cancer diagnosis. The innovative hypothesis is that there is a chaotic dynamic governing the interaction of ultrasound and the tissue. This hypothesis will be verified by extracting fractal and statistical features from raw ultrasound echo signals, collected from prostate tissue.  The extracted features will be correlated with the existence and staging of cancer using advanced classification techniques.This interdisciplinary research will be performed in close collaboration with leading clinicians. The algorithms will be designed with direct clinical applications in mind, expected to greatly impact Canada's healthcare sector. The research program has significant training potential for graduate students, where they will learn to translate their technical knowledge to high-impact medical problems with practical implications.""364330,""Abolmaesumi, Purang"
"364336"	"Aitken, Victor"	"Evidential reasoning for adaptive state and parameter estimation in nonlinear systems"	"State and parameter estimation is fundamental to control and modeling in many disciplines. Parameter estimation is needed to implement system models that, together with estimates of the current state, are used to formulate intelligent decisions based on sensor information, to compute feedback control signals, to adapt the model or control algorithms and/or parameters, and to build a representation of the operational environment. Classical approaches to state and parameter estimation, such as Kalman or related Bayesian methods, are largely based on linear Gaussian assumptions under which it is sufficient to estimate the mean and covariance of prior and posterior density distributions of the estimates. More recently, new methods have been developed for nonlinear systems based on sequential Monte Carlo particle filtering and its many variants. Particle filters carry more complete information using a point mass representation of probability distributions rather than just the mean and covariance. Our recent research has evaluated particle filtering methods for nonlinear applications and has contributed new techniques to improve performance and robustness of the methods and also to introduce evidential reasoning for representation and accumulation of knowledge based on the Dempter-Shafer theory of evidence. Resources requested under this proposal will fund graduate student research to develop, implement, and test new methods for evidential reasoning in particle filtering for parameter and state estimation. In our research, we will target applications in vehicular robotics, but the results are expected to be applicable to many diverse applications.""378140,""AitMohamed, Otmane"
"366203"	"Alexandrov, Dimiter"	"Excitons of the structure and their application for design of electron devices"	"This research proposal deals with the design of a new field effect transistor (FET) and the investigation of the properties of nitride semiconductors. The FET applies a new principle for formation of the channel conductivity based on tunnel injection of charges (electrons or holes) through a hetero-junction. These charges are a product of destroying of excitons of the structure located in the region of the hetero-junction. The proposed configuration includes a high resistance layer of GaN that acts as a FET channel. The hetero-junction is formed between this layer and an InGaN layer. The excitons of the structure are located within the InGaN. They are being destroyed if an electrical field of sufficient strength is applied on the GaN/InGaN hetero-structure. If the positive pole of the electrical field is connected to the FET-gate, the electrons of exciton origin penetrate into the GaN layer forming electron-type conductivity. If the negative pole is connected to the FET-gate, the holes of exciton origin penetrate into GaN forming a hole-type conductivity. In both cases the charge penetration is based on tunnel injection through a hetero-junction and the charges penetrate into the semiconductor with wider energy band gap (GaN). N-channel FET and p-channel FET are subjects of both design and technological performance. The objective of the project is to create a fast FET with potential application in both high frequency circuits and fast VLSI circuits based on nitride semiconductors. The dielectric function and the life-time of the excitons of the structure of several semiconductors related to InN are subjects of theoretical and experimental investigations according to the proposal. The electron band structure and the properties of MnGaN will be studied as well, because MnGaN is a promising semiconductor with application in both conventional and cryogenic electronics.""366969,""Alexson, Tania"
"366298"	"Allili, Madjid"	"Computational topology methods for data analysis and shape modeling"	"Topology is a branch of mathematics that deals with spatial properties of objects that are preserved under continuous deformations (such as stretching or squeezing an object without tearing or gluing any of its parts). In this branch, we do not ask how big an object is or what is its extension in space but, rather; is it all connected together, or can it be separated into many parts? Does it have any holes or cavities in it? It turns out that these types of properties allow us to design very powerful and flexible methods to distinguish between objects in space and to analyze high dimensional data sets. Many of these methods rely on the extraction of content descriptors capturing global and qualitative properties of the objects and the  data sets. In each case, the goal remains to extract features that enhance our understanding of the shapes of objects and the measured data and allow us to represent them in structures that are easy to manipulate for subsequent processing.      The main objective of this proposal is to utilize a combination of a theory from topology and the power of computing to solve problems in imaging and other fields where analysis of data sets is a key issue.  We will develop topological and algorithmic tools that allow automatic identification, location, and classification of qualitative features of data sets, such as the presence of corners, edges, peaks, pits, passes, and ridge and ravine lines, and their higher dimensional analogues. Once these features are extracted, different techniques are used to analyze relationships between them and represent their configuration in structures such as graphs which allow us to handle this information in an abstract way. We also aim at developing efficient topological shape descriptors for the representation of three-dimensional objects. 3D models are used in the design of objects and components, and are archived to enable reuse and rapid prototyping of new products. The World Wide Web enables access freely or commercially to these digital archives. Thus, it becomes imperative to develop efficient methods for comparison and matching of 3D shapes to enable automatic 3D shape retrieval.""371199,""Allili, MohandSaïd"
"363530"	"Amirfazli, Alidad"	"Surface engineering through chemical & topographical manipulation and drop dynamics on such surfaces"	"Nature provides examples of surface engineering in Lotus leaves that are self-cleaning when rain drops roll on these leaves. This amazing self-cleaning property is due to surface topography, chemistry and drop motion. Understanding the physics of drop motion on solid surfaces and what can facilitate or hinder this motion at early stages of movement along with how and what can be done to a surface chemically or topographically to manipulate drop movement are the subjects of this proposal. The implications of understanding above issues is not only of academic interest, but also of industrial applications; e.g., engineered surfaces for fuel cell can allow drop shedding in channels to avoid their flooding resulting in enhanced efficiency, or condensers that operate in drop-wise rather than film condensation mode, which will have much improved performance.             To develop tools to manipulate surface chemistry, we will further advance the direct laser patterning (DLP) technique for alkanethiol self-assembled monolayer surfaces. DLP will be used to produce surfaces with chemical gradients by modulation of laser power during scanning of the beam. We will also study the fundamentals of DLP, e.g. scaling of the pattern feature sizes with velocity of the scanning beam, and extension of DLP to alkylsilanes on silicon. Hot embossing will be used to engineer desirable surface topographies, e.g. hierarchical micro/nano-textured surfaces seen in Lotus leaves. The precise control of surface texture afforded by the novel approach of hot embossing allows us, to thermodynamically using free energy analysis, study apparent contact angle and contact angle hysteresis for textured surfaces. The incipient motion of drops subjected to an airflow for drops of various size and intrinsic contact angles on homogenous, and on textured or pattered surfaces with chemical gradients will be studied. These studies ultimately provide the toolboxes for creating smart surfaces with engineered surface texture and chemistry needed for above applications. This research enables training of 11 students in the highly interdisciplinary area of surface engineering to support Canada's needs for technology driven economy.""363529,""Amirfazli, Alidad"
"364124"	"Anantram, Manjeri"	"Quantum mechanical modeling of nano and bio-nano devices and sensors"	"Research activity in Electrical Engineering is experiencing a paradigm shift in focus, from submicron to nano electronics. This has the potential to revolutionize Electrical Engineering, with the creation of an entirely new class of devices, based on novel materials and device physics. The laws of quantum mechanics will greatly influence the behavior of these nanodevices, which will lead to lower power dissipation per logic operation, and higher functionality per unit volume and weight. Owing to their large surface to volume ratios, nanodevices are also opening up new solid-state technologies for chemical and biological sensing, with unprecedented single molecule sensitivities. It is not clear as to which of the emerging technologies will eventually come to stay. To enable rapid progress, there is a need to design devices with optimal performance. This can be achieved through theory and realistic modeling that accounts for the underlying device physics and properties of nanomaterials. As fabrication of nanodevices and nanomaterials are expensive, we believe that modeling can play a leading role, in isolating the winning nanotechnologies in a cost-efficient manner, and exploring new nanodevice concepts. The overall thrust of the proposed research activities is to build a strong focus in the computational study and design of nanodevices, at the University of Waterloo. This will be materialized by building upon our expertise in modeling nanodevices and nanosensors, based on the equations of quantum physics and atomistic methods.  The proposed work will aid in the creation of HQP in the area of computational nanoelectronics in Canada, and help create a center of excellence in computational nanotechnology at the University of Waterloo.""377263,""Ananvoranich, Sirinart"
"361041"	"Andrews, Robert"	"Removal of endocrine disrupting compounds and pharmaceuticals in drinking water treatment"	" In Canada, an increased demand for high water quality and stringent regulations are the main drivers for seeking alternatives to conventional water treatment processes.  In addition, the detection of endocrine disrupting compounds (EDCs) and pharmaceutically active compounds (PhACs) in wastewater effluents as well as drinking waters has brought increasing attention with respect to potential treatment methodologies for their removal.       This research will examine three methods that may potentially be easily implemented in water treatment facilities.  Powdered activated carbon (PAC) has been used for decades for taste and odour removal, however it is also known as a means to remove organic micropollutants.  The proposed research will examine the ability of PAC to remove a range of EDCs and PhACs.  In addition, it will provide data that may be used by other researchers in assessing whether this technology should be considered for emerging compounds.  Ultrafiltration, a practice that is now being adopted as an alternative to conventional filtration, will be examined as a means of micropollutant reduction by combining it with PAC.  Various methods of PAC addition prior to membranes will be studied.  Finally, the use of granular activated carbon (GAC) will be assessed using both methods that are currently avilable and also simpler ones that are being developed and may be more easily adopted by water treatment utilities and consultants.""361040,""Andrews, Robert"
"364991"	"Areibi, Shawki"	"A unified congestion driven power aware methodology for VLSI physical design automation"	"The layout of integrated circuits on chips and boards (physical design) is one of many interrelated complex tasks in VLSI circuit design. A major portion of the research in the area of design automation has been devoted to the development of efficient and easy-to-use systems that support circuit layout. Today's embedded systems contain various combinations of different components in the form of microprocessors, memories and application specific integrated circuits (ASICS). All this cannot be achieved without advanced and suitable CAD tools including physical design automation. The interconnect delay in VLSI circuits has become a critical determiner of circuit performance. It seems to become more and more important to know the impact of interconnection on speed and power consumption early in the design trajectory of high-performance processors and Systems-on-Chip. As a  result, circuit layout is starting to play a more important role in today's chip designs.Advances in process technology are driving the convergence of communication, computing, and sensor technologies onto a single silicon substrate -- Systems on a Chip (SoC). SoC will change the way future civilian/military systems are conceptualized and designed. Opportunities created by this development for electronic companies are clear, but with these possibilities also come significant design issues that have to be addressed and grand challenges that must be solved. The proposed approach seeks to achieve the following milestones: (i) integrate mathematical based methods with advanced meta-heuristics to solve the underlying hard optimization problems in circuit layout for Systems-on-Chip effectively and efficiently, (ii) reduce thetremendous complexity associated with System-on-Chip design by utilizing hierarchical/multilevel optimization approaches, (iii) tackle the low power design problem by considering it early and within physical design sub-problems, (iv) speedup the algorithms developed using special hardware accelerators for partitioning, placement and routing.""369785,""Arès, Richard"
"362642"	"Asif, Amir"	"Robust detection and tracking algorithms for time reversal matched field processing"	"In time reversal, matched field processing (TR/MFP), the backscatter of a signal transmitted into an environment is recorded, delayed, energy normalized, and retransmitted into the medium. TR/MFP offers an alternative to traditional detectors, e.g. the matched filter in one dimensional signal processing and the matched field processing (MFP) in multidimensions. The difficulty with MFP is that the Green's function used to model the channel is not known and, therefore, has to be computed numerically.  By its inherent nature, TR/MFP intrinsically derives the Green's function of the channel as long as the receiving transducer array provides an adequate sampling of the channel.  A second advantage of TR/MFP lies in the productive treatment of the channel multipaths. As is generally known, multipath significantly affects the performance of traditional detectors and is considered to be a detrimental and a negative whose effects should be minimized. TR/MFP presents the opposite opportunity: multipath as a positive, the more the better.  Though the technique of TR/MFP is not new, but a thorough theory of detection for this setting is lacking.  Our initial work addresses this gap and proves analytically, we believe for the first time, the phenomena of super resolution focusing observed with TR/MFP. In this proposal, we seek to extend this analysis further to provide quantitative answers to the following questions: How can TR/MFP be used in classical signal processing applications, such as: (i) target detection in a rich cluttering environment; (ii) channel capacity enhancement and equalization in wireless communications by some form of space division multiple access (SDMA); and (iii) inverse tomographic problems to determine channel characteristics, to name a few. We will also investigate how good TR/MFP is for target localization, tracking, and identification?  What are the fundamental limits of TR/MFP?  How stationary does the channel have to be for accurate TR/MFP?""381218,""Askew, Christopher"
"362053"	"Atlee, Joanne"	"Semantically configurable software modelling notations and tools"	"Errors in operational software significantly impact both users and software developers, in terms of rework to undo the effects of errors, extra work to avoid or mitigate errors, lost business due to shutdowns or delays in services, and increased customer support. Most of these errors are due to omissions, ambiguities, inconsistencies, or mistakes in the initial specification of what the software is to do, as opposed to design or coding errors introduced during development. Moreover, specification errors become more expensive to correct the later they are detected.Much of my research focuses on modelling and analyzing software specifications to detect errors in the early phases of software development, when errors are cheaper and easier to fix. For the past few years, my research group has been working on developing semantically configurable tools that support multiple modelling notations.  We have developed a way of structuring the definition of a modelling notation such that the definition can be interpreted and used to configure modelling tools, such as model editors and model verifiers.Continuing this research, I plan to study how to ease the task of configuring the definition of a new modelling notation.  I also intend to investigate whether our work can be adapted to accommodate heterogeneous models, thereby enabling specifiers to combine models written in different modelling notations.  In addition, I plan to explore ways of optimizing the performance of semantically configurable tools.This research will enable software specifiers to create their own modelling notations - that match the problems they are modelling and that domain experts can read and validate - but still have access to traditional tools for editing, manipulating, and checking their models.  Such technology will ultimately lead to improved software.""369220,""Atoyan, Hasmik"
"361735"	"Aubanel, Eric"	"Graph partitionong for high performance computing"	"Computationally intensive computer programs often decompose their data into parts to enable concurrent execution on multiple computers. Many of these programs employ a graph data structure to describe the computation and communication requirements of an underlying mesh, such as for finite element solvers for partial differential equations (PDE). Graph partitioning is the technique used to produce a balanced decomposition of these graphs and achieve a minimization of the communication requirement when theprogram is executed on a parallel computer. A number of graph partitioning techniques have been developed, including work by my research group which has created the PaGrid partitioner. This project will contribute new approaches to graph partitioning, both for traditional homogeneous parallel computers and for heterogeneous computer clusters and computational grids. It will also contribute tools to enable the comparative analysis of graph partitioners.Mesh-based parallel programs form a large part of the increasing role of simulation in science and engineering. Heterogeneous computational platforms are increasingly unavoidable, with the increasing hierarchical levels of computational resources, from multicore processors to clusters of multiprocessors and mini-grids (clusters of clusters). Heterogeneous graph partitioners such as PaGrid are vital in order to ensure the efficient execution of mesh-based programs on these platforms. This work will strengthen the work already accomplished on PaGrid, and will demonstrate its qualities using benchmark programs.""374094,""Aubertin, Michel"
"361686"	"Audet, Yves"	"Integration of surface plasmon into CMOS circuits"	"CMOS integrated circuits (ICs) are now prevailing in numerous electronic applications like computing, processing of images, video, music, and voice, wireless communication and sensors. The demand for low cost ICs with more functionality and greater processing speed forces designers to exploit state of the art CMOS technologies to their limits. Years ago, the microelectronic community identified metal wire used for interconnects in ICs as a key limiting factor for further speed  improvement of signal propagation in submicron CMOS processes. The resistivity and the capacitance associated with these wires, especially long buses used in digital circuits, degrades the high frequency switching signal transmitted on the bus.In order to compensate for this limitation, researchers have proposed replacing metal wires with optical interconnects in which the signal is transmitted by photons instead of electrons. Photons do not have a charge and thus interact less with matter, allowing them to travel at high speed with minimum perturbation from external noise sources. Waveguides currently used in micro-optics can not be fabricated from standard CMOS processes, which presents a major barrier to the integration of optical interconnects in CMOS integrated systems. Recent work on the propagation of Surface Plasmon Polaritons (SPPs) on metal lines has provided another alternative for optical interconnects on CMOS integrated circuits. SPPs are electromagnetic waves propagated at the interface of a thin metal line and a dielectric. This research proposal aims to study SPPs propagation on metal lines and dielectric materials used in state of the art submicron CMOS processes. In the long term, integration of SPPs as signal carriers into CMOS technology is expected to fulfill the performance requirement of future integrated systems with minimal or no change required in the next generation of CMOS fabrication processes.""363514,""Audette, Gerald"
"365490"	"Babin, Gilbert"	"Medigen - a mediator generator"	"The proposed research program aims to study the problem of data exchange from a B2B e-commerce perspective. In order for two enterprises to conduct business electronically, they must exchange data. For these exchanges to occur, both enterprise information systems must be compatible. This can only occur if there is a shared, common understanding. Should no such understanding be available, some mediation must be done for the exchange to be successful.Building on previous results, this research program will study the problem of adaptable mediators, i.e., software that will act as an interface between information systems. In particular, we are interested in the automatic adaptability of these mediators, without human intervention. We seek to answer questions pertaining to change detection in information systems and knowledge management.The results should enable the development of mediators to support the deployment of the Service Oriented Architecture, minimizing the adaptation costs between systems to integrate. They will also help identify what knowledge is required, nice to have or useful for adaptability to occur, hence providing some requirements to enterprise knowledge management.""382974,""Babin, Mathieu"
"363735"	"Balakrishnan, Ravin"	"Multi channel and multi degree-of-freedom user interfaces for heterogeneous display environments"	"Our computing environments are increasingly diversifying to include displays with a variety of sizes, resolutions, and form-factors. In terms of size and resolution, these range from tiny low-resolution displays found on watches to gigantic high-resolution wall-size displays driven by multiple tiled projectors. Form factors are also increasingly varied, ranging from highly portable displays that can be used in any location, position and orientation, to more static displays used in either a horizontal (e.g., tabletop) or vertical format. In addition, projectors are getting increasingly smaller, allowing users to create dynamic displays of variable size at any location. Despite this diversity in displays, the user interfaces we currently use differ very little regardless of the display's characteristics, and tend to rely on one or at most two channels of input (e.g., keyboard and mouse/pen) with limited degrees-of-freedom. Furthermore, current user interfaces do not support the seamless use of multiple heterogeneous displays in a coherent holistic manner. The net result is that users are unable to utilize the multiple displays in their computing environment to their fullest potential. The proposed research intends to develop multi channel and multi degree-of-freedom user input and interaction techniques to allow users to work effectively in computational environments consisting of a variety heterogeneous displays. The research will investigate user interfaces for single person use, as well as interfaces to support multiple people working collaboratively in such environments. In addition to explicit interaction, we also intend to focus on techniques for more ambient and subtle interaction in these spaces. The methodology will include exploratory user studies better understand the human user's capabilities when working in these environments, developing predictive models of interaction performance, and the iterative design of new interfaces.""363734,""Balakrishnan, Ravin"
"362659"	"Baljko, Melanie"	"Unimodal and multimodal variants of voice output communication aids"	"Individuals with little or no functional speech, gesture, or writingdue to underlying physical disorder may instead use a computationaldevice, called a Voice Output Communication Aid (VOCA), to producesynthetic speech (synthesized or digitized).  This device affordsanother, albeit aided, mode of communication.  Empirical evidenceshows that this mode is a complement to, and not a replacement for,the other, unaided modes, such as vocalization, facial expression andgaze, and gestures of the hand, head and torso.  Thus, the evaluationand design of VOCAs must take into consideration not only the issue ofrate of output (e.g., words-per-minute) but also issues that relate tothe utility of the individual's entire repertoire of mode strategies,of which synthesized speech is but one component.The goal of this program of research is to develop better VOCAsthrough the development of software techniques and modelling toolsthat take into account this ""global"" context.  The research will focuson the contrastive evaluation of unimodal and multimodal interfacevariants, and the use of design principles based on informationtheory.  The formal and theoretical analyses will be validated byempirical studies.""362756,""Baljko, Melanie"
"363712"	"Bandyopadhyay, Subir"	"Design of robust WDM networks"	"Optical networks use interconnections of high-speed broadband fibers to transmit information using optical transmitters and receivers. Wavelength Division Multiplexing (WDM) allows multiple communicationchannels to operate at different carrier frequencies on a single fiber. The rapidly growing WWW is crucially dependant on fast and reliable communication. The growing complexity of WDM networks makes the possibility of faults in networks a very important problem. Even brief interruption of data communication represents the loss of a huge amount of data.  In this investigation, we will look at the following problems on network survivability and the design of optical networks:- When designing a network, how to ensure that any single fault can be handled using as little additional resources as possible?- How to design networks in such a way that, when a fault occurs, communication using an backup path can be resumed very fast?- What is an economical way to allot low-speed user requests for communication to the high-speed capability of optical communication?""363894,""Banerjee, Neil"
"361962"	"Baniasadi, Amirali"	"Performance and power optimizations for modern processors"	"During the past six years my research has focused on developing power and performance optimization techniques for modern processors. I have introduced several power and performance optimization techniques for both high-performance and embedded processors.My future research includes both short-term and long-term goals. In the short-term, I will continue investigating eliminating redundant instructions and the potential power reduction benefits.  I will investigate methods that eliminate unnecessary computations and study their effect on power dissipation. In addition, I will extend our previous work and investigate the effect of eliminating redundant operations in simultaneous multithreaded (SMT) processors. My second short-term goal is to extend our previous work on dynamic resource allocation and investigate reconfigurable cache structures. My goal is to predict cache utilization and code behavior and to reconfigure the cache dynamically and according to the program requirements. I will evaluate our solutions for single thread and multithreaded processors.My first long-term goal is to investigate power and performance optimization techniques in the area of chip multiprocessors (CMPs). I will investigate techniques aiming at reducing power dissipation in both interconnect and memory units in CMPs. My second long-term goal is to investigate temperature-aware architectures. As cooling solutions continue to become more expensive finding new solutions to reduce temperature has become a major design concern. In addition higher temperatures increase chip failure rate and impact reliability. Power-aware design alone has failed to address these issues. As a result, temperature-aware design at all system levels, including the microarchitecture has become a necessity. I will investigate solutions to improve temperature by focusing on both localized heating and chip-wide heating.""377940,""Banihashemi, Amir"
"364872"	"Banzhaf, Wolfgang"	"Genetic programming and self-organization"	"The goal of the proposed research program is to study the relation between a particular method in the area of Evolutionary Computation, namely Genetic Programming (GP) , and the field of Self-Organization. The need for such a study comes from the realization that GP in its traditional form has reached a plateau of development. The technique is presently being exploited in many fields of Science and Engineering, yet its limitations become clearer every day. One key obstacle for GP is the problem of scalability, i.e. the difficulty to evolve large programs using present-day representations of programs. Another difficulty is the interaction of various non-intended, emergent phenomena of evolutionary runs with the primary goal of the artificial evolution.We aim at studying the relationship between GP and self-organization by introducing various innovative representations of programs (together with corresponding operators producing new variants of programs). Some will be non-sequential (like in our recent work on random sequences), some will be self-evolving representations, some will be otherwise complex. A representation with considerable promise is one consisting of a genome with regulatory elements (like regulatory networks in nature). Other representations will have elements of development added.The research proposed here will contribute to a vibrant research field of Computer Science, including Evolutionary Computation. In applications, the areas of Bioinformatics  and Computational Finance hold many problems that are suitable for exploitation with Genetic Programming, with substantial benefits to Canada.""376814,""Bao, Xiaoyi"
"361150"	"Barker, Kenneth"	"Privacy preserving database system"	"A Privacy Preserving Database Management System (DBMS) provides the core functions expected of any DBMS while guaranteeing data provided by a supplier is only used for its intended purpose. This research will develop a DBMS that includes privacy as a core feature rather than as an ""add-on"" to an existing non-privacy aware DBMS. The goal is to develop a system where data providers supply information with explicit privacy requirements, possibly different requirements for different kinds of data, while the DBMS itself can collect the data at any level but only use it in concordance with the privacy specified by the provider. Thus, the data provider's privacy is preserved by the DBMS.Once the privacy model is developed and demonstrated to be correct and complete, the approach is to take an existing DBMS, probably open source, and add these privacy features by wrapping an existing DBMS with them. This will not require changes to the DBMS engine itself and will provide insights into how to add privacy retroactively to legacy systems. Based on the lessons learned from this extension, the model will be incorporated directly into an open source DBMS engine to create a privacy-preserving one that is native in that all features are implemented within the engine itself. Thus, the native PP-DBMS will incorporate the privacy model in all components of a DBMS engine including the user interface, query processor, and transaction manager. This will ultimately lead to a system that can provide stronger security and provide users with increased privacy protection.The work will be undertaken in collaboration with several other academic researchers and industrial partners will be sought to develop commericialization possibilities.""372761,""Barker, Nicole"
"363258"	"Basir, Otman"	"A multi-modal sensor fusion architecture for audio-visual speech understanding"	"Modern speech recognition systems remain challenged in noisy environments despite all the technical advances that have been made to enhance their capabilities. There has been significant research work to address the effect of noise and word ambiguities on recognition performance. For speech recognition systems to be of practical use in noisy environments, such as in automobile, crowded areas and/or simultaneous human computer discourse applications, the issue of robustness must be addressed. The applicant maintains that this can possibly be accomplished by utilizing other sensing modalities to complement the acoustic signal of the speech. An example in line with this strategy is to fuse visual lip movements and expressions with the acoustic signal of the speech so as to maximize information gathering about the words uttered and to minimize the impact of acoustic noise.Understanding speech from visual information is an attractive approach that has captured the interest of many researchers to improve speech recognition performance. Not only does this approach have the potential to improve speech recognition performance but it may also be possible for speech impaired people to interact with devices in human machine interfacing applications.In this research project the applicant investigates the integration of audio and visual lip movement features (during speech signal production) to detect and recognize spoken phrases. A multi-modal fusion system will be designed. The system should be able to acquire visual and acoustic signals of speech and fuse them to detect and recognize words spoken by the speaker in a noisy environment. The system will also be tuned and tested in situations where the speaker is speech impaired. It is expected that the proposed research project will result in the training of 6 highly qualified personnel in human machine interfacing- an area of strategic importance to the Canadian economy.""369933,""Basl, PeterAtefWadie"
"362976"	"Beaubrun, Ronald"	"Protocoles d'interopérabilité et de gestion de mobilité pour les réseaux mobiles de la 4e génération"	"Les réseaux mobiles de la 4e génération (4G) réfèrent aux réseaux sans fil ultra-rapides qui seront déployés vers 2015, offrant un débit supérieur à 100 Mbps et regroupant une multitude de systèmes sans fil différents (GSM, GPRS, UMTS, 1xEV-DO, HSDPA, WLAN, ad hoc, hot spot). Ces systèmes formeront une infrastructure hétérogène qui offrira des services personnalisés dépendant à la fois du temps, du profil de chaque utilisateur, de sa position et du type de terminal utilisé. De tels systèmes auront les caractéristiques suivantes: 1) Support des services multimédias à faible coût ; 2) Utilisation élevée, car ce réseau sera indépendant de la technologie et, de ce fait, sera accessible à tous et à tout moment ; 3) Personnalisation, car les utilisateurs pourront directement configurer le mode d'opération de leurs terminaux et présélectionner le contenu des services désirés en fonction de leur profil; 4) Hétérogénéité des terminaux et des réseaux, car les terminaux seront différents (PDA, ordinateurs portatifs, téléphones portables, etc.) en termes de dimension, consommation d'énergie, portabilité, complexité, alors que les réseaux d'accès disponibles auront chacun sa propre couverture, son propre débit et son propre délai. Dans ce programme de recherche, nous visons à proposer et à mettre en oeuvre une série de protocoles qui facilitent l'interopérabilité des plateformes hétérogènes et, du coup, améliorent la gestion de la mobilité dans les réseaux mobiles 4G. L'objectif ultime consiste à faciliter la communication à travers tous les systèmes hétérogènes des réseaux mobiles 4G, en garantissant aux utilisateurs ou aux noeuds mobiles de bonnes performances, en termes de connectivité, de mobilité et de services. La principale retombée de cette recherche demeure dans la perspective d'une meilleure rentabilité des infrastructures des réseaux mobiles 4G.""369686,""Beauchamp, Benoit"
"361387"	"Belzile, Jean"	"Security aspects of partial reconfiguration in software defined radio"	"As FPGAs become more powerful and more power efficient there acceptance grows in the world of software defined radio (SDR). In SDR efficient sharing of resources is critical to the implementation of multiple waveforms. Partial Reconfiguration (PR) enables such technology. PR has been shown to work on large scale problems and is now well supported by some FPGA manufacturers.PR opens not only the way to the dynamic sharing of resources but also permits the implementation of security sensitive algorithms. Secure applications have many requirements that can be addressed by PR technology such as boundary monitoring and enforcing. PR however also raises a number of important security aspects due to the possibility of dynamically altering key portions of various algorithms.This project will focus on two distinct but similar problems. It will introduce and validate mechanisms to monitor and police the boundary conditions of secure algorithms such as to prohibit unwanted communications in and out of the secure box. It will also introduce mechanisms to monitor the dynamic download of code into the FPGA. A security framework test bench will be build in order to investigate these mechanisms."
"361774"	"Benlamri, Rachid"	"Framework for Context-Aware Mobile Learning"	"Context-aware mobile learning is an important requirement for next generation intelligent e-learning systems. However, context-management in real world applications is a complex information processing task. In this research program, context reflects timeliness and mobility to nurture pervasive learning throughout an enterprise ecosystem. The aim is to dynamically provide useful learning resources on demand, anywhere, in a learner-driven context and on a learner's schedule. This is particularly important for corporate learning providers in order to make it possible for organizations to transform learning into a strategic lever. As learning material scale up and longer instruction times become common, knowledge acquisition become time-dependent and is increasingly acquired on the fly due to considerable changes in today's know-how economy requirements. However, the process of inferring, communicating, and presenting context-aware learning resources to mobile learners poses new challenges to the research community. This is mainly due to the limitations of mobile technology (i.e. limited bandwidth on wireless networks, and limited resources on mobile devices), and to the lack of cognitive mobile-learning models (i.e. problems related to domain-knowledge representation and contextual information retrieval, time constrained instruction, and human-computer interaction). This research program contributes towards solving some of the challenging problems in cognitive mobile-learning by making use of the progress in Semantic Web theory, e-learning standardization, and mobile computing. The approach is to develop a knowledge-centered framework based on ontological formulation that structures knowledge into meaningful content and creates an environment where software agents roaming around peer-to-peer learning object repositories can readily carry out sophisticated learning services discovery and compositions. The use of ontology aims at providing customized learning patterns which are delivered on-demand to the learner and/or combined with other patterns to form a self-adjusted learning route according to the learner's needs and available learning time.""379666,""BenMahfoudh, Majed"
"365509"	"Berenbrink, Petra"	"Analysis of Randomized Algorithms"	"The proposed research program is in Theoretical Computer Science.  It is mainly concerned with the design and analysis of algorithms, and here, in particular,  randomized algorithms, and algorithms for dynamic systems (like dynamic load balancing processes and dynamic routing problems in networks).The main goals of this program are (i) to apply formal methods and techniques to real-world problems, e.g., in Networking (computer-related or otherwise), or in Biology, in a rigorous way, and(ii) to advance the formal (i.e., mathematical) methods and techniques used to study these models.  One main vehicle that is going to be used is that of randomized algorithms and their probabilistic analysis, or sometimes ``just'' the probabilistic analysis of structures used to model certain phenomena/scenarios.Randomized algorithms are typically a very elegant and efficient way of solving problems that would otherwise be very hard (or even impossible) to solve.  An example would be Property Testing, which is about devising and analyzing algorithms that verify whether or not a certain object (e.g., a database, or a DNA string) has a certain property. Randomized algorithms can do that in time that is sub-linear in the size of the object, i.e., even without looking at all parts of the object (one must allow for a certain error probability though).""370458,""Beres, Elzbieta"
"362518"	"Beyer, Dirk"	"Automated formal methods for reliable software components"	"In my research program I work on software verification, which is a method that takes as input a program and gives the user feedback about possible bugs in the program. We already use such techniques today, in our compilers: they check if we use all variables and functions properly, according to their types and signatures, respectively. As a next step towards the ultimate goal of error-free programs, we develop the technology needed for checking semantical properties, e.g., if data structures on the heap are consistent, or if some forbidden variable value is possible during program execution. Theoretically, these checks are undecidable, but in practice, due to several recent breakthroughs in program verification, there is a strong hope that we can actually verify many interesting properties. The problem is identified and widely acknowledged as a grand challenge in computer science, and only an enormous effort of many research groups together can develop all the techniques that at the end make it possible to achieve the ideal of software engineering: verified software.     However, it is not always possible to obtain the source code for all components of our system. This phenomenon naturally occurs if we build information systems based on web services. Different components, developed by different development teams, are running at different locations and on different computer platforms. Neither the source code nor the executable program is obtainable for inspection or in-house testing of certain components. In the proposed research, we are working on a new formalism, called web service interfaces, to formulate specifications of web services in a concise way, and develop a tool platform that supports modeling and analysis of web services based on our language. A provider of a web service can then offer to the clients a formal interface of its service. The provider can use the interface to check if the actual implementation fulfills all promises made by the interface. The client of the web service can use the provided interface to check if the client component is compatible with the interface. If both is given, then the overall composition is guaranteed to work. We will provide automatic procedures for both tasks.""382334,""Bezaire, Marty"
"363834"	"Bhavsar, Virendrakumar"	"Parallel and distributed intelligent systems"	"PARALLEL AND DISTRIBUTED INTELLIGENT SYSTEMSArtificial Intelligence (AI) has both engineering and scientific goals: (a) to develop systems that have some form of intelligence, and (b) to understand intelligent behavior in systems and living organisms. Since biological systems embody a marked degree of parallel and distributed functioning, we believe that the architectures of intelligent systems and/or their implementations should also embody some forms of parallel and distributed computing. The systems should also exhibit some fundamental features of evolution in nature.The long-term objective of our work has been to develop efficient models, algorithms and implementations of parallel and distributed intelligent systems. Our goal has been not only to develop and experiment with new algorithms, but to develop operational software systems that can be employed in the real world as well. Whenever opportunities arise, the outcomes of our research will be commercialized and integrated in products.We propose: (a) to devise algorithms and software systems for efficient, effective, precise, semantic matching in e-Business, e-Learning and social networking environments, (b) to utilize knowledge (e.g. gene expression data, protein-protein interaction data and regulatory elements) about existing pathways of model organism to construct partial or complete regulatory networks for non-model organisms, and (c) to develop a semantically enriched dynamic knowledge delivery system for personal learning services.""374847,""Bherer, Louis"
"366072"	"Borodin, Allan"	"Design and analysis of algorithms, mathematics of information retrieval, complexity theory"	"This proposal primarily concerns a number of topics relating to the design and analysis of algorithms incluing potential application to the areas of information retrieval, algorithmic mechanism design and bio- informatics. My current focus is on the design and analysis of ""simple"" and efficient classes of algorithms, such as greedy algorithms, local search, dynamic programming and local ratio/primal dual algorithms. These general techniques are the starting point (and sometimes the most efficient algorithms known) for many combinatorial optimization and search problems. While the meaning of such algorithmic paradigms is intuitively understood, establishing a precise and ""useful"" model of such algorithms is a challenging area of research. We consider precise models which capture known algorithms but moreover allow us to prove limitations on a particular approach as well as possibly suggesting ideas for deriving new algorithms.""374686,""Borowsky, Ron"
"364418"	"Bouguila, Nizar"	"Learning detailed models from images and videos using machine learning techniques and applications to graphics"	"Synthetic images and animations of real scenes are now common place in different domains such as medicine, computer games, feature films, TV advertising, homeland security, and fine arts. Creating these images and animation is the first goal of computer graphics.  Traditional computer graphics approaches start with geometric models and then generate and display virtual representations. Many computer graphics approaches have been successful. However, it is clear that synthetic images and animation still look artificial and that the cost and time have to be lowered.  In the past, the field of computer graphics has been considered  as the inverse of computer vision. Indeed, computer vision starts with input images and videos and process them to understand the geometric and physical properties of objects and scenes. The objectives of my research are the integration of computer vision and computer graphics techniques, and the creation of a framework in which these two domains collaborate through machine learning techniques to model the world around us (e.g. human body and motion, natural scenes, rigid and non-rigid objects) directly from measurements. These measurements will be learned from real images and scenes.  Indeed, recent years have seen a significant technological development in the areas of high-quality sensors which have simplified the acquisition of the world content.This unified view could make a leap of cost and quality. For instance, it should be less costly to make a 3D model of  a human directly from images and videos, than to have an artist construct such a model by hand. This unified view will also eliminate the need for many of the heuristics generally used,  and will create major new opportunities and sub-problems that it is our objective to address.""379810,""Bouillon, François"
"362957"	"Bouslimani, Yassine"	"Composants optiques à base de fibres à cristaux photoniques"	"Les fibres à cristaux photoniques appelées aussi Fibres Microstructurées Air Silice, fibres à trous, fibre creuse ou fibres à bandes interdites photoniques sont considérées comme une solution clé pour des communications optiques parfaites. Cette nouvelle génération de fibres optiques est aussi considérée comme le futur support d'information pour les systèmes de télécommunications. Dès que les installations actuelles de fibres atteigne la durée de vie de leurs projets et bien sûr après quelles soient bien rentabilisée on s'y attend à une migration vers ces nouvelles générations de fibres. Cette nouvelle tendance a vu le jour car la fibre optique monomode utilisée actuellement commence à montrer beaucoup de limitations particulièrement pour les très grands débits (>40Gb/s) et ceci malgré les performances dont elle a fait preuve jusqu'à présent. Plusieurs phénomènes physiques liés à la propagation optique sont à la origine de ces limitations et les fibres PCF permettent de contrôler une grande partie de ces phénomènes.Le présent sujet de recherche est centré sur l'étude, la réalisation et la caractérisation des composants de fibres optiques dédiés à des applications de télécommunications. Ce sujet de recherche comporte une partie consacrée à l'étude théorique des phénomènes liés à la propagation optique et qui causent des problèmes pour les communications à très grands débits. Le sujet comporte également une partie expérimentale qui va nous permettre de valider notre étude et de mettre en évidence la possibilité de concevoir de nouveaux composants optiques qui permettent aux systèmes de télécommunications optiques d'avoir de meilleures performances.""365072,""Bousmina, MostaphaMosto"
"361825"	"Bowling, Michael"	"Data-driven statistical agent modelling"	"Decision making in multiagent environments is a critical challenge for deploying artificially intelligent agents in real-world settings. Applications involving or requiring interaction between independent agents -- human, robotic, or software -- are rapidly increasing.  The rise of inexpensive robot platforms and the pervasiveness of distributed agents make the assumption of isolated decision making less justifiable.  Agent modelling is a key component for coping with the presence of other decision makers.  Applications as diverse as assistive technologies, autonomous driving, electronic commerce, and interactive entertainment all require or could benefit significantly from accurate models of other agents (artificial or human).One approach to constructing such agent models is to consult domain experts and build generic, knowledge-intensive models of the typical human or artificial agent.  The use of these entirely generic models, though, prevents tailoring the interaction to the particular participants.  In addition, many tasks require models that are not easily elicited from a domain expert due to domain complexity and the difficulty in codifying expert intuitions.This research examines a data-driven, statistical approach to agent modelling.  In particular, it focuses on the development of techniques for constructing statistical agent models from large datasets of previous interactions, both of the agent being modelled and past agents.  In the same way that large corpuses of written text transformed the practice of natural language processing, it is expected that a similar statistical approach can overcome the significant limitations of knowledge-based agent models.""362283,""Bowman, Jeffrey"
"363140"	"Boyd, Sylvia"	"Applying Methods from Combinatorial Optimization in Solving Real World Problems"	"Many practically important problems are combinatorial in nature.  Some examples of such problems include the design of reliable communication networks, fast printed circuit board production, scheduling problems and routing problems.  Many such problems are known to belong to a class called ""NP-hard"", which is strong evidence that these problems are extremely difficult to solve, and that it is unlikely that efficient techniques for finding the optimal (i.e. the best) solution exist.  The lack of efficient methods for such problems is of real importance as there exist large scale, real world instances requiring solutions.  Fortunately, in practice, it is often satisfactory to obtain solutions for these problems which, if not optimal, are at least guaranteed to lie within a certain satisfactory percentage of the optimal solution.Our research objective is to enlarge the set of such problems which can be practically and satisfactorily solved.  To achieve this objective we will use the following three approaches:1)  Investigate using closely-related relaxations of a problem to develop new, efficient methods which provide solutions which are guaranteed to lie within a satisfactory percentage of the optimal solution.2)  Exploit the underlying combintorial structure of a problem to provide the necessary tools for improved problem-specific implicit enumeration schemes for certain problems of interest.3)  Investigate constructing and applying combinatorial optimization models for applications arising in other areas of computer science.""374894,""Boye, Sandra"
"361366"	"Buro, Michael"	"Planning and state evaluation in real-time decision domains"	"Artificial intelligence (AI) research applied to games has a long tradition thatreaches back at least 70 years with Alan Turing's work on computer chess. Theadvantage of studying AI algorithms in this area is that games are preciselydefined, relatively small when compared to real-world decision domains, and yetsufficiently complex to pose tough research problems whose solutions can help uscreating machines of human-level intelligence. AI research has had its successesin games like chess, Othello, backgammon, and checkers - where machines now playon par with or better than the best human players. However, big challenges remain- most notably in the ancient Asian board game of Go for which to date no machinecan match the strength of good beginners. Likewise, video game AI systems arelacking learning, planning, and reasoning capabilities which results in weakperformance compared to human players.  The objective of this project is to improve the decision quality of AI systemsin real-time decision domains by means of abstractions, look-ahead search, andmachine learning of search heuristics. Our application focus will be on populargames in which beginners still outperform the best programs. The results of thisproject will increase our understanding of fundamental AI problems and will haveconsiderable impact on the real-time control domain in general and the computergames industry in particular, which is in need of credible computer controlledagents.""368647,""Burrell, Robert"
"362216"	"Cai, Jun"	"Efficient relaying and routing in practical wireless mesh networks"	"Future wireless communication networks are expected to effectively support large coverage area, heterogeneous multimedia applications, seamless roaming, and quick adaptation to network dynamics resulting from user mobility. As one of candidates for the next generation wireless communication networks, wireless mesh networks are considered to be promising due to their abilities in providing scalability, flexibility, resource efficiency, and quality of service (QoS) provisioning (in terms of network capacity, transmission latency, call dropping/blocking probability, etc.). The significance of wireless mesh networks has been supported by the current standard activities, such as IEEE 802.11s and IEEE 802.16. However, the practical applications of wireless mesh networks are strongly limited due to hostile wireless channel conditions, heterogeneous QoS requirements, non-infrastructure network topology, and multihop transmitting. Therefore, designing practically feasible wireless mesh networks has become a critical and emergent topic in the current telecommunications research field. In order to enhance the research in this field, the proposal focuses on designing effective and efficient network technologies for facilitating practical implementation of wireless mesh networks. Specifically, the proposal will focus on a) the distributed user relaying algorithms for network capacity improvement, b) the routing algorithms for high resource utilization and end-to-end QoS provisioning, and c) the analysis on network performance and the evaluation on the proposed algorithms. The integration of cross-layer design, heterogeneous traffic characteristics, and distributed and scalable requirements will be the major design principle. The research results will be disseminated through publications in journals and conference proceedings, technical seminars, and invention disclosures and patents.    The research is important and has close connection with commercial exploitation. It is expected that the research outcomes will have significant contributions to the cutting edge research and the state-of-the-art technology in the next generation wireless communication networks.""361028,""Cai, Jun"
"365657"	"Cameron, Helen"	"Lock-free data structures"	"The results of this research will help programmers write code that allows many machines to work on the same data at the same time yet not interfere with each other.Sometimes the amount of data can be very large and sometimes the correct piece of data may be urgently needed.  To help searchers find what they want in the data quickly, programmers have designed clever structures for storing data that allow the searcher to do something faster than ``search through all the data until you find what you want''.  Associated with these data structures are methods of keeping the data up to date (inserting new items and deleting old items) that maintain the clever search structure.Programming becomes much more complicated if new data can be inserted at the same time as old data is being deleted at the same time as searches are under way.  Unless great care is taken, the data structure can become corrupted and searches may return incorrect results.  At the same time, programmers want as many searches and updates to happen simultaneously as can be allowed without causing havoc.  Thus, simplistic schemes that lock all but one updater out of the data structure at a time can cause problems.  Lock-free schemes that allow as many concurrent updates and searches as possible are preferred.The aim of this research is to take complex data structures and use lock-free schemes to allow concurrent updates and searches.  The research benefits Canada by providing tools (concurrent data structures) to make the complex task of concurrent  programming easier.  As parallel shared-memory machines become more prevalent, the impact of this work will increase.""377720,""Cameron, Kathleen"
"363137"	"Carenini, Giuseppe"	"Generating interactive multimedia summaries of evaluative text."	"Many organizations and individuals are faced daily with the challenge of managing large corpora of text data. One important application is evaluative text, i.e. any document expressing an evaluation of an entity as either positive or negative. Evaluative information is strategic to decision making.  For instance, many websites collect large quantities of online customer reviews of consumer electronics. Although this literature can be of great strategic value to product designers, planners and manufacturers, the effective processing of this information remains a complex and time-consuming task. As another example, before traveling abroad, many users browse through travel-logs and online user reviews on hotels before they make reservations. However, there are so many evaluation web sites that users are awash in reviews.In these and many other similar situations, an automated solution has the potential to greatly reduce both the cost and time necessary to keep up with the growing amount of evaluative literature. Instead of asking the user to read through one review at a time, we propose to summarize the text reviews faithfully in a multi-granularity fashion, so that the user can pick the appropriate granularity level depending on her needs and time constraints.While there has been substantial work on how evaluative information can be extracted from text, little attention has been paid to how the extracted information can be effectively summarized and presented to users. In previous work we have started to explore this problem and in this project we propose to extend our work in different directions. In general, we aim at removing simplifying assumptions, at increasing the sophistication of the summarization process input, and at improving the quality of its output. A unique, distinguishing aspect of our approach is that it is multimedia and interactive. We aim to convey all the knowledge extracted with graphics, to point out the most important findings in natural language, and also to provide support for the interactive exploration of the corpus from which the knowledge was extracted.""379655,""Carette, Amanda"
"365075"	"Carette, Jacques"	"New methods for building computer algebra systems"	"The main objective of this research is to fundamentally re-examine the methodology currently used to build Computer Algebra Systems, aiming to make substantial improvements.  Computer Algebra Systems (CAS) are well-known to be at the forefront of many new programming language technologies (generic programming, dependent types, first-class types, etc), as well adopting advanced software engineering techniques long before they are popular in the mainstream.  While the development of new, faster algorithms in the area of Computer Algebra has proceeded at a breakneck pace in the last few years, engineering issues have received less attention.  However, recent progress in such areas as type theory, denotational semantics for nominal languages, generative programming and automated testing can all be profitably applied to the construction of CASes.We propose to re-examine, the current state-of-the-art in CAS development, and adapt current techniques in adjoining fields.  Just as simple type theory was too weak for Computer Algebra, it is expected that similar leaps will be needed in many areas of programming language theory and software engineering to properly address the very complex needs of a CAS.  First, we intend to concentrate on ``generic efficiency'', or how to write algorithms once using generative techniques to produce highly efficient specialized versions.  In our MetaOCaml experiments, the correctness guarantees are much higher than in traditional settings, while obtaining very efficient results at a fraction of the effort of developing new optimizing compiler techniques.  Secondly, current notions of polymorphism are not sufficient to appropriately express the kinds of ``genericity'' found in mathematics.  Approaches that frankly embrace developments in category theory will hopefully bear fruit.  This approach has worked very well for the development of powerful specification languages, we hope it can do something similar for programming languages.""367738,""Carew, Adam"
"364452"	"Carpendale, Sheelagh"	"Interactive information visualization"	"Terms like information society, information overload, information explosion, and information anxiety have become commonplace. We are generating information at an ever increasing pace and yet, even though most people want to be informed, all this information is often experienced as stress. I suggest that it is not the information itself that is the problem, but rather that we are bombarded with information in forms that are often hard to interpret.My research agenda is to produce interactive visualizations that enhance people's cognitive abilities, not only presenting information visually, but also providing people with capabilities for manipulating and exploring this information. An effective visualization provokes interpretation, exploration and appreciation, inviting direct interaction that reveals the information's contents. My overall objective is to: design, develop and evaluate interactive information visualizations so that they support the everyday-world practices of how people view, represent, manage, share and generally interact with information.Information visualization investigates the possibilities the digital world affords for people's exploration of dense and complex information spaces. By increasing the focus on the interactive aspects of information visualization, I will be considering both how various interaction techniques can support an individual's information exploration as well as how small groups of people share, exchange and collaborate during information exploration.  I will work towards developing embodied interaction for information exploration, investigating how technology can be designed as a truly integral part of the real world environment. The challenge is to design information presentations so that they become an integral part of the everyday social practices of the people who use them. Success occurs when these systems support and maintain people's everyday activities within their real world context. Interactive information visualization considers the fundamental nature of information and how people can effectively interact with it through technology.""385066,""Carpendale, Sheelagh"
"362023"	"Champagne, Benoit"	"Digital signal processing for broadband wireless communications"	"Emerging wireless broadband technologies promise to deliver high-speed data to multitudes of people in various geographical areas. This formidable revolution in wireless technology is due in a large part to the discoveries of ever more powerful techniques and algorithms for the processing of information signals.Funding is requested to support a program of fundamental and applied research in the field of digital signal processing for broadband wireless communications. Two specific research areas are targeted:(1) Channel parameter estimation in broadband multiple antenna systems and(2) Speech signal enhancement based on auditory modeling.The main objective of the work on channel estimation is to investigate new subspace-based algorithms for the blind estimation of unknown channel responses in multiple antenna broadband wireless systems, including so-called multiple-input multiple-output (MIMO) systems. The research will focus on the development of new estimation techniques, their theoretical performance analysis and their efficient adaptive implementation for use in dynamic radio environments.The proposed research on speech enhancement aims to improve the performance of current algorithms, in terms of speech quality and intelligibility, by exploiting relevant signal processing functions performed by the human auditory system. The targeted application will be that of voice communications in fixed and mobile wireless systems. New techniques for speech/noise classification will also be studied.The new channel estimation algorithms are needed in the application of advanced coding and detection techniques to multiple antenna wireless systems to achieve the high data rates promised by these technologies. The new speech enhancement algorithms will aim to provide high-quality voice communications under a variety of noise and interference conditions, making it easier to engage in a phone conversation over wireless in public places or noisy transportation systems.""366074,""Champagne, Pascale"
"363618"	"Chang, XiaoWen"	"Algorithms, analysis and applications of numerical linear algebra"	"Numerical linear algebra is at the core of most scientific computing. Many scientific, engineering, and other problems are most effectively represented and then solved on computers, as matrix problems, or linear algebra problems. One of the important applied areas of numerical linear algebra is estimation. This research aims to develop numerically reliable and efficient algorithms and software for solving general and specific estimation problems which may arise in practical areas such as communications, signal processing, global navigation satellite systems, and statistics etc. These problems can be either small scale or large scale. This research also carries out the analyses of  the algorithms to show their reliability and sensitivity analyses of the problems to show what effects changes in data caused by finite precision computation or uncertainty in the data will have on the final results.The outcome of this research will be valuable not only to the study of scientific computing but also to industry, particularly to the communications technology companies and GNSS receiver manufactures.""380739,""Chang, YuLing"
"365050"	"Charbonneau, Alain"	"Hybrid and adaptive methods for statistical mechine translation"	"The main concern of our proposal research program is the development of new methods and new approaches for known methods in the field of statistical machine translation (SMT). These methods are named adaptive and hybrid methods.Adaptation in SMT refers to the tuning of the log-linear model weights (LMs, TMs, ...) such that a sentence can be translated in a better way by software implementing this model. In previous experiments, we have already shown the existence of those desired sentence-dependent weights but have also shown the huge difficulty to exhibit them. Some improvement can be done in this direction.In SMT, one will call hybrid methods those that are combination of two or more methods. The difficulty here lies in finding criterions allowing to decide in an automatic way which method should be applied to translate a specific sentence. We have already developed such criterions but a lot of improvements and attempts need still to be done in this direction too.Hence the development of these general techniques in combination with already known methods, methods to be refined (like the new method we developed called back-translation) and methods still to be discovered is the core of our proposal and is also certainly a long-term research. Some of these methods are new and promising and the ideal expected goal of our works is certainly to improve translations provided by SMT systems, though this problem is an extremely complex one.""383208,""Charbonneau, Karyne"
"365627"	"Chen, ChihHung"	"Stochastic noise characterization and modeling of sub-100nm MOSFETS for low-noise, low-power RF IC applications"	"The research program presented in this proposal is to develop a first-rate research and training environment for the investigation of high-frequency (HF) noise in semiconductor transistors, especially for sub-100nm Metal-Oxide-Semiconductor Field-Effect Transistors (MOSFETs), from device characterization to low-noise, radio-frequency (RF) integrated circuit (IC) designs. This program consists of four major research areas - high-frequency noise measurement, device characterization, simulation-oriented noise modeling and low-noise, low-power RF IC design. The emphasis of this research program is primarily on developing (1) advanced measurement techniques for a novel four-port high-frequency noise measurement system, (2) innovative noise characterization methodologies using interval arithmetic, (3) accurate and physics-based four-port device models using stochastic techniques to take care of the impact on the high-frequency noise behavior of a sub-100nm MOSFET from the gate leakage current, the substrate bias and the induced substrate noise, and (4) novel high-frequency circuits for low-voltage, low-noise wireless applications. All efforts will be taken to fully exploit the research results commercially, generate economic benefits and therefore fuel the Canadian economy. Potential users of these research results will include manufacturers associated with the semiconductor manufacturing, microelectronics and telecommunication industries. In addition, the state-of-the-art infrastructure developed in this research program will be used extensively in collaboration with industrial partners, with the objective of empowering them with the competitive edge that is vital to excelling in the present economic scenario. Finally, the proposed research program will create an essential research and training environment that is appropriate to impart requisite skills to highly qualified personnel (HQP) that are in line with the needs of academia or Canadian microelectronics and telecommunication industries in the area of high-frequency device characterization and circuit-level testing. The program and innovative infrastructure will attract talented and experienced researchers and HQP in the world and retain them in Canada.""366216,""Chen, ChoJieh"
"364032"	"Chen, Li"	"Study of radiation-hard microelectronics"	"Radiation affects the operation of electronics and can lead to total failure or destruction of the exposed components. The consequences to systems controlled or supported by irradiated electronics could be disastrous. The proper operation of electronics in radiation environments is of paramount importance to the space industry, the military, nuclear reactors, high energy physics experiments, and more recently, to the application of large commercial memories in everyday terrestrial environments. The high cost of maintaining dedicated foundries to create radiation tolerant electronics has motivated the exploration of alternatives for next-generation systems. Recent research using advanced commercial electronics manufacturing processes have shown them to be resistance to radiation provided some design precautions are taken.A technology will be developed with a commercial process that can provide enough radiation tolerance for space microelectronics. This project can result in the development of a radiation-hardened, low-power 32-bit stack-based microprocessor which is a collaborative research project with Microelectronics and Signal Processing Brach, NASA Goddard Space Flight Center. The fault-tolerant approaches for programmable devices will also be investigated.  This research has significantly increased the availability and reduced the cost of radiation-tolerant electronics. This proposed technology enables commercial foundries instead of dedicated foundries to manufacture radiation-tolerant circuits broadly used in scientific spaceflight instrumentation thereby greatly reducing costs. Additionally, this approach will likely improve the performance of space electronics. The proposed technology is key to space explorations as well as other applications where radiation effects are a concern.""363710,""Chen, Liang"
"362303"	"Chen, Stephen"	"Exploiting commonality in heuristic search"	"Heuristic search techniques are systematic methods to explore a large set of possible solutions for the one solution that best achieves some objective.  For example, there are many ways to arrange guests at a wedding reception, but it can still take newlyweds a long time to find one that they can both agree on.  For this problem, most couples would use a form of iterative improvement - start with one solution, make changes to it to create a new solution, make additional changes, etc, until a final acceptable solution is found.   Heuristic search techniques such as genetic algorithms, simulated annealing, tabu search, etc are all methods that perform iterative improvement.  Specifically, these search techniques all employ ""change operators"" (methods to create new solutions) and ""control strategies"" (methods to control which new solutions are accepted).  For example, you might suggest swapping two tables, and your partner might say ""no, that's worse"" or ""yes, but this still needs to be addressed"".    Heuristic search techniques tend to be defined by their control strategies, and their change operators tend to be more or less the same - take a candidate solution and make changes to it.  Ideally, the solution components that are changed should be things that need improvement, and the solution components that are kept should be the things that are pretty good.  However, most change operators only examine the things that they change.    The recently developed Commonality Hypothesis (from genetic algorithms) suggests that solution components that are common to ""good"" solutions are the things that should be kept.  For example, all table arrangements that place your partner's parents at the front have been accepted, so this is something that might be unproductive to change.  Unfortunately, many heuristic search techniques (e.g. simulated annealing and tabu search) only work with a single solution, so their change operators do not normally have access to this ""historic"" information.  Thus, exploring the role of commonality preservation in heuristic search techniques may lead to new technique-independent ideas on how to improve them.""372127,""Chen, SzuHan(Antony)"
"366495"	"Cheng, Howard"	"Computer algebra algorithms and image set compression and encryption"	"We propose to study problems from two distinct areas of computer science: computer algebra and image processing.Computer algebra is the study of algorithms which perform mathematical computations symbolically rather than numerically.  Because of its ability to handle symbolic quantities and its lack of numerical errors, computer algebra has many applications in mathematical research, engineering, and mathematical education.  Despite advances in this area, many problems remain impractical to solve.  A common issue encountered in computer algebra is the so-called ""intermediate expression swell.""  In many problems, the size of the input is small but the size of the intermediate results during the calculations can grow significantly.  If this growth is not controlled, an algorithm cannot finish its computation due to time and memory constraints.  The goal in the proposed research in computer algebra is to design and analyze algorithms to control this growth for a variety of problems in computer algebra.  This study will improve our understanding of such problems and provide practical improvements to the current algorithms.In a separate direction, we propose to study issues related to the storage of large image sets.  We plan to investigate compression methods to reduce storage requirements, as well as encryption algorithms for security.  We are also interested in the interaction among compression, encryption, and efficient image retrieval.  The results from this study  have many applications as the use of large image databases have become more common (e.g. medical and satellite images).""370949,""Cheng, JieJoy"
"361402"	"Cherkaoui, Omar"	"Addressing the complexity and scalability of autonomic"	"As the Internet grows in scale, it faces an increasingly adverse environment that the original design did not envision. Faults have become the norm rather than the exception, due to component failures, human operational errors, spreads of software viruses, and malicious attacks. In this environment, automated network management techniques, including autonomic, self-configuring and self-optimizing devices and systems as well as self-protecting and self-healing architectures will become increasingly important.The research will focus on the developing the architecture of the configuration module for autonomic networks. The configuration module, - the core capability of an autonomic network -, will be designed to translate the commands and the files or records into abstract structures, does the configuration, adds or activates services according to policy-based goals using the rules of composition provided by the abstract structures and reconvert the configuration result into commands and configuration files. These features are not available today and are merely implemented in form of text-only commands and files. The configuration information including services, policies, functional and version features will be represented by structured tree-like topologies called ""constructs"" that can be manipulated using a specific algebra with formal composition operations.Our methodology consists first in extending the mathematical model based on configuration logic and formal orchestration constraints enabling the verification of the configuration information through the manipulation of the algebra. We will also use performance simulation to analyze different self-configuration scenarios to assess the self-optimizing, self-healing and self-protecting capabilities. The performance analysis will determine the impacts for various autonomic configuration scenarios. Finally, we plan to experiment with autonomic network implementations and demonstrate its applicability in future systems.""378139,""Cherkaoui, Soumaya"
"364957"	"Chodavarapu, Vamsy"	"Integrated CMOS based sensors microsystems for biochemical monitoring"	"The integration of complimentary metal-oxide semiconductor (CMOS) integrated circuits (ICs), micro/nano technologies, and photonics continues to increase and shows a great promise for the development of a variety of devices and technologies that can pervasively impact all aspects of our society. The long term objective of the proposed research is to investigate, design, and develop sensor microsystems through the integration of CMOS based devices and systems with a variety of existing and emerging biochemical recognition elements including enzymes, DNA, organic/inorganic dyes, and functional nanoparticles. These microsystems will be useful in sensing, imaging, diagnostic or therapeutic applications for in-hospital/out-of-hospital monitoring, disease/epidemics prevention, environmental monitoring, food/water safety, and national security. To this end, during the five year period of this proposed research one can expect a variety of novel sensor microsystems that: [i] are miniaturized and battery-operated for ubiquitous applicability, [ii] have integrated stimulation, detection, and signal processing/transmission components, [iii] are self-contained and can be easily deployed, [iv] can simultaneously detect/quantify multiple analytes in a sample, [v] are simple and inexpensive to construct and operate, [vi] are accurate, precise, and reliable, and [vii] provide adequate detection limits and selectivity. The activities in the proposed research are divided into three themes:  (A) CMOS based phase fluorometric platform for biochemical monitoring, (B) CMOS integrated porous nanostructured sensors, and (C) Lab-on-a-chip sensor microsystems.""363536,""Choh, Vivian"
"365155"	"Cleve, Richard"	"Quantum algorithms and complexity theory"	"The objective is to investigate the power of information processing devices that utilize the behavior of quantum mechanical systems. For example, to discover new algorithms in this framework, establish limits on the computational power of these devices, discover novel ways of communicating, or exploiting the power of quantum systems that are correlated in a quantum mechanical sense (""entangled""). The overall scientific approach is to use the mathematical framework of ""quantum information"" and investigate a number of research directions that extend current approaches in novel directions. The significance of the work is in its potential to lead to actual quantum information processing devices that outperform existing devices in various respects, or possibly to discover fundamental reasons why they cannot be realized.""380459,""Cleven, Nathan"
"364421"	"Coady, Yvonne"	"Managing system infrastructure extensibility with aspect-oriented software development"	"System infrastructure software, such as OS kernels and virtual machines, must be able to service everything from embedded devices to super computers.  Developers of such systems often start with a mainline system and then manually extend it to meet their performance and functionality requirements.  Many of these extensions represent crosscutting concerns in that they do not fit within a single program module and are scattered throughout the system sources---easily affecting a significant number of files.  But maintaining and applying such extensions with commonly used system infrastructure tools is time consuming and error prone.   For example, tools such as ""diff"" and ""patch"" use line numbers such that even a minor textual modification in the mainline system can cause the extension application to fail.   Similarly, low level preprocessor directives saturate code in a way that is inconsistent with higher level reasoning and ultimately undermine the overall design and structure of the system.  It is a significant challenge to ensure a system's correctness when composing multiple extensions supported through these traditional means.Based on previous work that has established that crosscutting concerns are common in systems code, the proposed work seeks to make them an integral part of system architectures by leveraging aspect-oriented software development (AOSD) techniques.   More specifically, the proposed work seeks to capture system extensions as aspects, which provide a language-supported methodology for expressing crosscutting concerns.  The benefits of using aspects in this capacity are twofold. First, they provide a well-defined specification of domain-specific extensions that is separate from baseline functionality.  In this form, they can be easily added to and removed from a system.  Second, they enable more tool support to aid automatic reasoning about the implications of composing several extensions and the identification of semantic conflicts.""366592,""Coates, Mark"
"366655"	"Consens, Mariano"	"Retrieving and managing semi-structured and business process data"	"XML is the dominant format for web data, and is particularly suitable to represent the many forms of semi-structured data present in web pages, blogs, wikis, RSS feeds, etc.XML is also the wire format for data exchanged via web services, and as such is at the foundation of modern Business Process Management approaches that are built on top of service-oriented architectures.The objectives of our research are to develop an array of novel techniques for XML data management that can significantly contribute to the retrieval and management of both semi-structured and business process data.We propose to start by addressing the problem of describing and visualizing the structure of large collections of XML documents using summaries.We are also interested in techniques for processing retrieval queries that combine structural constraints with keyword search. Supporting these queries in a way that can be answered as efficiently and effectively as in traditional keyword searching is an ambitious goal for XML data management systems. To begin, we will address the challenge of how to measure the effectiveness of retrieval results obtained from XML collections.We intend to apply the preceding techniques to facilitate the retrieval and the description of data exchanged among business processes.""362128,""Constabel, Peter"
"364362"	"Cordy, James"	"Source transformation systems"	"Structural source transformation is a recent and increasingly popular computer programming paradigm with a wide range of applications in academia and industry.  We have designed the TXL programming language to explicitly and conveniently express problem solutions using this new paradigm.  In this work we continue to explore and extend techniques for expressing and implementing source transformation systems such as TXL in the context of several application areas, including computer software testing and verification, document recognition and analysis, autonomic computing and the world wide web.""378131,""Corinthios, Michael"
"361854"	"Czarnecki, Krzysztof"	"Methods and tools for effective software modeling"	"Despite important advances in software methods and technology, such as object-orientation, the progress in software reusability, evolvability, and reliability has fallen short of expectations. Evolving software to conform to a changed set of requirements is notoriously hard because developers have to work with low-level code in order to perform changes. Modeling has the potential to address the evolution problem by providing just the right view in order to express the desired change precisely. Unfortunately, in the current software engineering practice, analysis and design models are just documentation artifacts, meaning that the actual change to a software system still has to be performed manually in the program code. The long-term goal is to realize the potential of modeling to address the software evolution challenge by simplifying the task of providing adequate models and keeping the models and the code consistent. The resulting methods and tools will be tested in industrial context, including banking software at Scotiabank and e-commerce software at IBM. The anticipated long-term impact of this research includes increased adoption of modeling in industry, a more direct involvement of stakeholders who are not software engineers (e.g., business analysts, scientists, and control engineers being enabled to create models that can be translated into software with less help from software developers and fewer bugs that might result from miscommunication than today), and significant productivity and quality improvements in software development through more effective evolution. Effective modeling has the benefit of adding clarity to the communication of software requirements and software architecture in a global sourcing context, where software analysts, architects, and integrators can better communicate with globally distributed developers. Such improved communication capabilities have the potential to make Canadian companies more competitive while creating opportunities for these highly skilled jobs in Canada. The total of 1 PhD, 2 MASc, and 20 undergraduate students and staff at collaborating companies will be trained in cutting edge model-driven software technology, which is highly relevant to Canadian industry.""376194,""Czarnota, Gregory"
"365697"	"Dagenais, Michel"	"High performance high availability distributed system performance analysis"	"The society increasingly relies on sophisticated computer systems for numerous applications from search engines (e.g. google.com, yahoo.com) to geographical information systems (e.g. maps.google.com), eCommerce systems and handheld music and video players. Such systems have stringent performance requirements either because of the volume of data, the number of simultaneous users or real-time  constraints. Yet, achieving the desired level of performance assumes  proper interaction between several software programs, the operating system and often several networked computers.The objective of the proposed research program is to develop tools and techniques for highly accurate and low disturbance system level performance analysis of uniprocessor, multiprocessor and distributed systems. This is particularly challenging with newer computer systems using multiple processors, with dynamically changing clock frequency, multi-threaded preemptive operating systems (e.g. Linux with preemption) and multiple virtual machines running over an hypervisor (e.g. Xen).The proposed system uses source code instrumentation at the operating system and user level in order to gather a detailed and precise trace of significant events. The instrumentation needs to have a minimal impact on both overall performance and on events ordering (scheduling). Special purpose analysis and viewing modules will be developed to study the performance of different subsystems (memory, disk I/O, networking...). Collaboration is already under way with industrial partners faced with very demanding applications. These industrial partners will provide both real life sample applications needing sophisticated performance analysis tools, and feedback on the efficiency, accuracy and applicability of the proposed algorithms, techiques and tools.""383235,""DagenaisBellefeuille, Steve"
"366255"	"Daley, Mark"	"Computation in biological processes"	"Three centuries ago, the development of calculus laid the foundation for the formalization of physics. Information and computational science is now well-positioned to assist in the creation of an analogous formal framework for reasoning about biological processes in a rigorous way. The long-term goals of the research proposed here fall into the emerging discipline of Natural Computing and will help to lay these formal foundations for biological processes. Moreover, by studying theoretical models based on natural rules, rather than artificial human-generated rules, we open new insights into the very nature of computation itself. In the short-term, we propose two projects:The genome of the microscopic worm C. elegans has roughly the same number of genes as the human genome; yet, we observe apparently different levels of complexity in these two organisms. One explanation for this dichotomy is that genes can undergo alternative processing events that modify them, allowing a single gene to encode multiple proteins. One such processing event is that of frameshifting during translation. We have modelled frameshift events as a computation and propose to study this model both to uncover the formal properties of frameshifting as well as develop efficient algorithms for detecting, simulating and experimenting with frameshifting in real genomes.The minimization of free energy in a biochemical system is the primary force driving chemical reactions including complex macromolecular behaviours. Most existing models in Natural Computing have thus far abstracted away from such thermodynamic considerations. We propose to develop a general framework for introducing thermodynamic considerations to formal models, allowing them to reflect more closely the underlying physical system under study and providing a novel model of computation.""372852,""Daley, Tracy"
"362879"	"David, JeanPierre"	"Description et synthèse automatique de réseaux de machines algorithmiques évoluées dynamiquement reconfigurables"	"Le génie logiciel est arrivé à un niveau de développement tel que certaines applications se modifient elles-mêmes pour être exécutées plus rapidement par le processeur dans un contexte donné. On appelle cela la compilation JIT (Just In Time). Le génie électrique développe actuellement des circuits qui ont aussi, théoriquement, la possibilité de se modifier en temps réel : les FPGA (Field Programmable Gate Array). Mais, le plus souvent, ceux-ci sont configurés au démarrage et ne sont plus jamais modifiés par la suite. Outre le défi technique que cela suppose et le fait que les circuits actuels sont encore très immatures pour faire de la configuration dynamique, un problème encore plus fondamental est celui du langage à utiliser pour décrire des circuits capables de s'auto-modifier à un niveau d'abstraction suffisamment élevé pour qu'il soit fiable et efficace.Le programme de recherche que nous proposons vise à définir un langage de haut niveau pour décrire un réseau de machines algorithmiques qui travaillent ensemble, s'échangent des données et sont capables de se modifier dynamiquement. En corollaire, il vise également la réalisation d'un outil de compilation capable de générer une description synthétisable d'une telle architecture, par exemple sur un FPGA. Le langage et son compilateur sont destinés à être utilisés autant par des personnes issues du génie électrique que du génie informatique, comme c'était le contexte de notre programme de recherche précédent.À plus long terme, ce programme de recherche pourra également déboucher sur de nouveaux circuits davantage polyvalents et optimisés pour la configuration dynamique. Ils pourront ainsi être utilisés par les entreprises en lieu et place des circuits dédiés (ASICs) qu'elles sont obligées de réaliser à grands frais aujourd'hui et dont les coûts de développement ne cessent de croître.""369827,""David, Robert"
"363350"	"Dean, Thomas"	"Grammer based evolution and teasing of software"	"My long term goal is the discovery of techniques that increase the correctness and reliability of software systems through the use of grammar programming, software transformation and software architecture understanding. While progress in these techniques will have the effect of reducing the cost of maintenance of software systems, the main objective is to reduce the errors, and therefore the risk, introduced by human effort. One of the ways we can reduce errors and risk is by reducing repetitive tasks and concentrating the human effort on those tasks that requires human ingenuity and creativity.     The first part of my research addresses the evolution of web applications. Companies and organizations have invested heavily in on-line systems and as the technology advances, they are faced with the daunting task of migrating their existing systems. This research will take the migration to AJAX based web applications as the exemplar migration task and investigate the techniques needed to assist the migration and can be applied to other migrations. One of the open questions is the extent to which the task is automatable and how much intervention is required.     The second part of my research addresses the security and robustness of network applications.  This research investigates techniques that can be used as part of quality assurance by a software developer or by a customer as part of acceptance testing.  Our approach is unique in that we consider the network protocol as a programming language and can draw on twenty years of existing program comprehension research which we can apply to the security testing problem.     Software plays an increasingly important role in our lives. My grandfather's salary was paid in cash, my father was paid by cheque and I am paid by direct deposit to my bank account. Software applications cannot be safe or secure if the technology on which it rests is not secure or reliable. Both threads of my research address these issues.""383608,""Dear, Andrew"
"362443"	"DeFreitas, Joao(Nando)"	"Probabilistic machine learning and decision making with Monte Carlo particle methods"	"In 2001, my NSERC proposal began by stating that the modelling of vast amounts of data, such as text, video, multimedia, gene data and environmental signals, was one of the most demanding challenges of artificial intelligence and statistics in the new century. At the time, I pointed out that to construct appropriate models for the data, we needed to contend with uncertainty as data is typically noisy, imprecise and only partially observed. This uncertainty, coupled with the sheer magnitude of the problem, precluded the construction of models using knowledge acquired solely from experts. To overcome these problems, I proposed the use of probabilistic learning techniques and Monte Carlo algorithms to infer the models automatically from data.Despite great progress in the fields of machine learning and statistics over the last five years, the data modelling challenge has grown. Many researchers and companies, including Google, Microsoft, Amazon and Yahoo, have an increasing need for more expressive models in their quest to capture more complex structures and relations in the data. With this need comes a demand for faster scalable learning algorithms. Expressive models and efficient algorithms have not only given some of these companies a vital competitive edge, but have also led to advances in many areas of scientific endeavor. For example, in biology, they have enabled us to model increasingly more complex interactions between genes and proteins.Decision theory has also begun to play a more prominent role in modelling. For example, with vast quantities of data spread over networks, such as the world-wide-web and environmental sensor networks, we require techniques to decide when, how and where to look for information in these networks. To attack this and many other fundamental problems in science and information technology, my research focuses on expanding the frontier of learning algorithms, decision theory and large-scale, structured probabilistic representations.""372123,""Deg, Nathan"
"363874"	"DeLara, Eyal"	"Radio fingerprinting support for pervasive environments"	"Pervasive computing embeds communications and computation into the environment to enable people to interact with information anywhere and at any time. We will research the use of radio fingerprinting of ambient WiFi and mobile phone networks to address two key challenges in pervasive computing: (i) enabling secure spontaneous communication between devices in close proximity; and (ii) enabling accurate indoor localization.   Radio fingerprinting is a technique that records characteristics of the radio environment at a given location, such as the signal strengths of a group of radio sources that are heard at a specific location. This proposal takes advantage of two key observations about the nature of radio fingerprints. First, due to environmental factors, the radio fingerprint perceived by a mobile device at a given location varies randomly over time. Second, while the fingerprint at a given location fluctuates over time, the difference between fingerprints taken at locations that are a few meters apart is much larger. This proposal takes advantage of the first observation to enable secure spontaneous communication between devices in close proximity by deriving a shared secret from time and location specific fluctuations of the  radio channel,  and exploits the second observation to enable accurate indoor localization.   Secure spontaneous communication will make possible a wide range of applications, such as enabling consumers to use their cell phones to pay for groceries at a store or tickets at a train station. It will also make it easier for a user to take advantage of pervasive technology available in the environment by, for example, allowing a user to pair her mobile phone to a public full-sized display and keyboard. Accurate indoor localization will make possible applications such as in-building navigation and coordination between peers, and will be a catalyst for location-based services, such as recommendation systems and location-based search. Finally, it will assist in locating 911 calls where accurate location information can make the difference between life and death.""379503,""DeLasa, Hugo"
"362491"	"Delgrande, James"	"Belief change in rational agents"	"In Knowledge Representation (KR) in Artificial Intelligence, an agent deals with a problem domain which could be a specific problem area (such as diagnosis), a physical domain (such as encountered by a robot) or a ""virtual"" domain (such as provided by the web). In all such cases, the agent's knowledge will generally be incomplete, inaccurate to some extent, and constantly changing. An important problem in KR is to manage this change of information, whether the agent has been told a new fact, or has carried out some actions, or has been given a set of observations from different sources.The main focus of this research is on belief change, which deals with how an agent should modify its beliefs on the basis of new information. While much work has been carried out, there are crucial issues that have yet to be fully addressed. For example, it is not clear that the formal models that we have fully characterize all aspects of belief change; as well, there is no agreement as to how an agent should incorporate a sequence of observations about a fixed environment. This research will address the problem of belief change, and the wider problem of reasoning in a dynamic world of agents that can carry out actions. First, mathemetical models characterizing aspects of belief change will be developed, and second, complementary to this, computational models will be developed and studied in the context of their applicability in different areas.A secondary, but strongly related, topic of research concerns reasoning with preferences. Typically an agent will have not just (strict) goals to attain, but also things that are desirable, but not necessary, to establish. The use of preferences in planning, as well as the role of preferences in belief change, will also be addressed.""382709,""DElia, Francesco"
"365349"	"DemkeBrown, Angela"	"Integrated application and operating system optimization"	"The primary objective of this research project is to allow applications to realize the performance potential of emerging computer hardware systems, by dynamically optimizing execution paths across multiple software layers. The main challenge in achieving this objective arises from the growing complexity of computer systems and the many layers of abstraction that have been developed to help manage this complexity. Twotechnology trends highlight the problem.  First, emerging micro-processors (such as Sun's multi-core, multithreaded Niagara) have subtle performance characteristics that are extremely difficult to understand and exploit, even for expert programmers. At the same time, programmers are using languages such as Java, where the code is interpreted by an abstract machine.  The productivity gains that these languages provide are based, in part, on a higher-level abstraction that frees the programmer from the unpleasant details of real hardware. Between the code written by programmers and the instructions executed by the real hardware lie many other layers, including a language-level virtual machine, the operating system, a low-level virtual machine, and a virtual machine monitor. This scenario is quite likely to arise in current web hosting centres.     Our research explores a variety of ways to break down at run-time the barriers imposed by static layers of abstraction. As one specific project, we will explore integrating compiler-based memory management algorithms into a Java JIT compiler, so that the generated code can interact with the JVM's heap management system, which will in turn inform the operating system of candidates for page prefetch and replacement. As a second specific project, we will investigate the use of dynamically-generated execution traces within the operating system code itself (past projects have applied these techniques to user-level code only). Our initial goal will be to support enhanced profiling and performance instrumentation of the OS, followed by dynamically optimizing execution paths within the OS, with the ultimate goal of integrating user-level and system-level execution traces.""379081,""DeMont, Edwin"
"363801"	"Desai, Bipin"	"Discovery and information management"	"Over the last few years our team has been developing the CINDI System. The main thrust of the Cindi system is cataloguing, in a systematic manner using a Semantic Header, the information resources accessible via the Internet or via a traditional dissemination channel. The digital library component of Cindi allows the provider of a resource to catalogue her document using an expert system to guide her in choosing correct subject terms. Subsequently, a searcher can locate the index entry and thence access the source document if it is on-line.  CINDI also implements a ""pull mechanism"" via a CindiRobot, which is guided by a filtering system to judge the relevance of sites for extending the digital library collection. Another component of CINDI is a Virtual Question Answering system which provides precise passages of relevant information from the collection. ConfSys is another pull component which has been used in the last four years to manage the administrative tasks of a number of conferences and add the documents (papers) to the CINDI collection. Challenges that needs to be resolved in this project is the matching of papers to reviewers while avoiding all possible onflict of interests. The improvement of the algorithms used in these components, their integration and tuning are tasks ahead of us.The image component of CINDI is concentrating on medical image databases and retrieval systems. Here we are investigating a content -based image retrieval framework of medical images. The framework under investigation consists of machine learning methods for image pre-filtering, similarity matching using statistical distance measures, and a relevance feedback scheme. We will investigate both supervised and unsupervised learning techniques to associate low-level global image features with their high level visual and semantic categories. We intend to incorporate better user perception subjectivity via a relevance feedback mechanism to dynamically update the query parameters and adjust the proposed matching functions.""376711,""Desai, Rashmi"
"363872"	"Desmarais, Michel"	"Bayesian models for student and user modeling"	"Personalized and adaptive interfaces rely on models of the user's needs, interest, goals, and skills.  Personnalized study guides are successful examples of some innovative applications that have recently emerged.  They guide the user through a study process that can lead to an adequate preparation for an exam, a diploma, a new job, or simply a self-paced, self-motivated learning agenda.  They take full advantage of the one-on-one, personalized tutoring that a computer can offer by focusing the user's effort solely on the topics that are most relevant and require further study. One-on-one training was shown to improve student performance by a factor of two standard deviations over a classroom setting.User models for cognitive skills assessment are critical components for such personalized, adaptive applications. Besides study guides and learning environments, they are also fundamental components for adapting explanations in intelligent help and assistants, or adapting the level of details and vocabulary in adaptive hypertext and adaptive applications in general.The current research program's primary objective is to develop probabilistic models for skills assessment that are automatically built from test data.  We aim to construct skills assessment models from small data sets, yet with fine grained representations of skills.  The approach we advocate was already shown to outperform current approaches in terms of predictive accuracy.  In addition to being  computationally efficient and entirely automated, the approach can lead to generic, easy to use, and cost effective tools for a number of advanced and intelligent interfaces.""379859,""Desmarais, Suzie"
"364646"	"Deutsch, Clayton"	"Mineral resource uncertainty modelling"	"The complexity of mineral deposits have defied simple characterization by geoscientists and engineers.  Engineering decisions are taken with sparse data and significant uncertainty.  The key aim of this research is to quantify the geologic uncertainty in a mining project and to provide means of decision making that are robust with respect to the unavoidable uncertainty.  The specific aspects of uncertainty that will be addressed by this research include (1) the boundaries between geologic domains and rock types, and (2) continuous trends within the geologic domains.  Common practice consists of subdividing the mineral deposit into geologic domains; however, there are special features at the boundaries such as enhanced grades and trends within the geologic domains.  The proposed research will address these special features.  The primary aim will be to develop enhanced numerical geological modelling techniques that address different geological controls on mineralization, the relationship of rock properties across the boundary between domains of different control and transfer these features into subsequent decision making.  The numerical modeling techniques will be developed in the context of the best available mathematical techniques.  Results will be verified with relevant mine data.""364645,""Deutsch, Clayton"
"361221"	"DeVolder, Kris"	"IDE support for incremental modularization from monolith to component assembly"	"After more than 30 years of software and programming language research, good modularity remains hard to achieve in practice for a number of reasons. Some of the reasons are of a technological nature, some are due to intrinsic complexities of software, and some are due to complexities of a realistic development process. Our research makes contributions to address these problems. Our long term goals are to provide technical and conceptual tools that support a realistic, incremental development process for developing modular, reusable components from existing non-modular ones. In the short term, we will develop and evaluate a specific integrated set of composition, analysis and refactoring tools that can be applied today, to existing software implemented in a mainstream programming language (Java) in an existing IDE (Eclipse).""380294,""Devos, Alexandre"
"361819"	"Diamond, James"	"Lossless compression of audio data"	"Data compression is the process of representing data using less space than that required by the data's normalrepresentation. Data compression can either be ""lossless"", in which case the original data can be recovered exactly, or ""lossy"", whereby only an approximation of the original data can be recovered. Lossless data compression is appropriate for textual or other kinds of data where any modifications would have significantdetrimental effects. Lossy data compression is frequently used for image or audio data, where the degree of degradation is adjusted according to some notion of ""acceptability"". Lossy compression of music is currently in the public eye because of intellectual property issues surrounding the sharing of MP3 files.While lossy compression is suitable for applications such as the casual enjoyment of music, in other situations it is not appropriate.  For example, audio data such as that obtained from passive sonar systems typically has a very low signal to noise ratio (SNR); that is, the desired sound is embedded in a relatively loud environmental noise field. Further degradation of the SNR by using a lossy compression scheme may be unacceptable. Also, lossy compression techniques are often based on psycho-acoustic principles which dictate that humans will not hear certain sounds (such as, loosely speaking, a quiet sound occurring shortly before or after a louder sound); these principles are inappropriate if the audio data is to be processed by computer software.This research program will study the lossless compression of audio data, with particular attention to the following topics: (1) defining new audio data models to optimize compressability; (2) differentiating thesemodels, if beneficial, towards sonar data, music and speech; and (3) examining the tradeoffs involved with compression effort vs. decompression effort for particular compression schemes.""373062,""Diamond, Jill"
"366094"	"Dick, Scott"	"Novel developments in computational intelligence with applications to software quality"	"This proposal will support new investigations into the theory of computational intelligence and machine learning, and their application to modeling software reliability. Our principal theoretical contributions will be in the new area of ""complex fuzzy logic,"" a recently proposed generalization of fuzzy logic. Complex fuzzy logic is a novel multi-valued logic, whose truth values are complex numbers. Little is currently known about the properties of this logic, or how it might be applied. Our program of research will address both of these questions, by developing both the mathematical-logic foundations of complex fuzzy logic, and inductive learning algorithms that apply complex fuzzy logic to practical problems. We believe that complex fuzzy logic will be particularly effective for time series forecasting. Our research into machine learning will focus on stratification, a method for reducing skewness in machine learning datasets. Stratification algorithms delete samples from the majority class and/or add examples to the minority class, removing or reducing skewness. We will investigate generalizations of various stratification algorithms, and their implementation in the high-performance computing environments commonly used in business intelligence.  Our theoretical contributions will be applied to modeling certain aspects of software reliability. We will investigate the use of stratification algorithms for software defect prediction. There is virtually no existing work on the use of stratification in software defect prediction. We will provide new evidence in this area through extensive experimental investigation. Another aspect of software reliability is software reliability growth modeling (SRGM). In previous work we have investigated the use of chaos theory for SRGM. We will continue that investigation, and we will also apply our new inductive learning algorithms for complex fuzzy logic. These approaches are highly novel, and we expect that our methods will significantly improve upon current SRGM techniques. Our contributions will benefit the Canadian intelligent systems industry by providing new theoretical guidance and a cohort of HQP trained in intelligent systems.""369720,""Dick, Terry"
"363873"	"Ding, Chen(Cherie)"	"Implementation of personalized search in active web intermediaries"	"Personalized information service is an active area in many research communities. In the web search area, the personalization focuses on how to adapt the search result according to user's content-oriented preferences. In the content delivery area, the personalization focuses on how to adapt the web page according to users' performance-oriented preferences. If the personalization could be implemented considering both content and performance, it would be really beneficial to the end user. It is the purpose of this work to leverage the advances in both areas to implement the search personalization. Our objective is to implement the personalized search service in web intermediaries based on users' requirements on content relevancy as well as on quality of service. Currently, the intermediary mainly refers to the proxy server or edge server. In the long term, the concept of the intermediary could be generalized to any information service agent sitting between clients and servers, and the search target is not limited to the web page, it could also be a web service. There are several major steps involved: 1) to define quality of service factors which are closely related to the web search; 2) to design and implement a content adaptation framework for the personalized search; 3) to design and implement a re-ranking algorithm which could integrate the performance-related quality of service factors (e.g. latency of fetching a page, user's connection speed) into the traditional information retrieval model. The key features of this proposal would be: 1) search personalization is implemented as a content adaptation service in a web intermediary, which could fully utilize the rich user data saved in its logs and its information concerning the page delivery performance and the client capabilities; 2) the proposed re-ranking algorithm could augment the existing ranking models, which is especially useful in a ubiquitous computing environment; 3) we will train the highly qualified personnel to gain a deep understanding on web search, adaptive content delivery, and personalization. They will have concrete experiences on system building and systematic experimental methods, which would benefit their future career.""375397,""Ding, Keyue"
"363357"	"Doedel, Eusebius"	"Numerical analysis of bifurcation problems"	"My main research interest is the development, analysis, implementation, and application of numerical methods for analyzing the solution behavior of nonlinear equations. I have worked on dynamical systems, especially those modeled by ordinary differential equations, and I am also interested in nonlinear elliptic and parabolic partial differential equations.My work is motivated by the fascinating changes of behavior exhibited by solutions to nonlinear equations as parameters are varied: There is tremendous satisfaction in devising new numerical algorithms that help increase our understanding of these phenomena. Recent applications range from a classification of elemental periodic orbits of the Circular Restricted 3-Body Problem, which is relevant to space mission design, to the computation of invariant manifolds, which explains an apparent discontinuous response in a cardiac pacemaker cell model.I try to make the numerical methods available for use in widely varying research areas. A notable success has been the development of the research software AUTO, which continues to evolve in cooperation with researchers in a world-wide network.I will further develop high-order accurate, adaptive discretizations of elliptic PDEs, resulting in software to track solutions, and determine their bifurcations, even in near-singular applications. Another focus is continuation techniques for periodic orbits (including infinite period orbits) and invariant manifolds in dissipative as well as conservative systems. Important applications will be studied.""368260,""Doepel, Lorraine"
"366208"	"Dong, Xiaodai"	"Ultra-wideband communications for wireless personal area networks with low rate applications"	"This research proposal investigates enabling ultra-wideband (UWB) technologies for low rate wireless personal area networks (WPAN). Recently, UWB has been proposed in the IEEE 802.15.4a Working Group as a physical layer candidate for low rate, short range wireless networks.  In the low rate application domain, UWB can easily achieve a transmission speed much higher than that of Bluetooth due to its large bandwidth. The primary features of UWB include low cost, long battery life, multipath immunity and the ability to have communications and simultaneous precision ranging capabilities. Overlaying with the frequency bands of existing radio systems, UWB allows more efficient use of the scarce spectrum resource. The intended applications include smart home and personalized services, consumer electronics, family communications/supervision of children, industrial and building automation, tracking and logistics, asset tagging and management, search and rescue, security, medical applications, and military applications, etc.This proposal focuses on key design issues for UWB low rate systems, such as modulation and channel coding, synchronization, ranging and location, co-existence with current wireless systems, multiple access and cross-layer design. The Principal Investigator has recently proposed a novel transmitted reference pulse cluster technique for low rate communications. It has many advantages over existing methods and will be further developed into a complete system in this proposal.   Moreover, an experimental UWB transceiver testbed will be built to validate the theoretical system design in realistic indoor residential and office environments. The anticipated outcome of the research will have considerable impact on the low rate UWB device design and will accelerate commercial development of these UWB devices.       ""384917,""Dong, Xiaodai"
"362519"	"Drew, Mark"	"Physics-based colour models in computer vision, colour science, graphics and multimedia"	"In my research, I incorporate detailed models based on the physics of interaction between light and surfaces to (a) understand images and video and hence the underlying lights and surfaces that created them; and (b) use these same effects in reverse to create realistic scenes in graphics.  I have developed a method  for generating (Computer Graphics) and understanding (Computer Vision) images using {it whole spectra}, rather than just the usual Red, Green, and Blue of ordinary images. At the end of the day, in fact our visual equipment uses R, G, and B, but on the way, light is transmitted as a whole spectrum, including some red, green, etc., as well as every colour in between. So far, I have created a medical imaging application, for seeing inside volumes, that transforms magnetic MRI data into computer graphics. Since whole spectra are now available, some of the organs, bone, skin etc. can take on a unified colour (or no colour) while other organs are highlighted, and this can happen in real-time, while we use a slider.  I propose using the method to tackle difficult physics-based imagery such as the beautiful effects involved in insect iridescence, diamond facets, and the like.In Computer Vision, I have been looking at ways to {it remove shadows} from images. This is very useful for such tasks as having a robot grasp an object without trying to grasp its shadow, trackingpeople in a security situation, etc. But it can also be used as simply a touch-up tool for images -- removing dark shadows on a face in a digital photo, say.  Shadows are useful in themselves, not just to be removed: they provide a profound cue about shape and lighting. I set out approaches in this proposal that help in the difficult issue of how to ""re-light"" images, by changing the shadows.""363537,""Drew, Murray"
"361472"	"Drummond, Christopher"	"Robust learning and evaluation with impoverished training data"	"Being able to predict what will happen in the near future, with reasonable confidence, is an essential skill of humans and computers alike. Knowing what will come next allows people to plan ahead and use their resources efficiently and effectively. For computers to be useful tools, they must emulate this practice. Computers must plan ahead, predicting a user's needs and finding any items that may satisfy them.As with humans, computers can learn from past experience. But most research so far has assumed that the experience directly carries to the new problem.  In recent times this assumption has become under increasing scrutiny. This project explores the impact of removing this assumption.If the change from past experience to a new problem is too large prior experience will not be useful. So some idea of the range and type of changes that a learning system will experience has be obtained. How well existing systems cope with these changes needs analyzing. This will lead to new variants of such algorithms or totally new algorithms that are robust to these changes.This proposal requests funds solely to support graduate students. It represents a good research topic for new researchers. Those trained in this area will have strong potential as researchers in academia or industry.""372702,""Drummond, Emily"
"365212"	"Du, Weichang"	"High-level programming paradigm for context-aware computing"	"The objective of this proposed research is to investigate a high-level programming paradigm for context-awarecomputing. This paradigm will consist of theoretical and practical foundations for developing programminglanguages that enable and support programming context-aware software applications, and software models,abstractions and infrastructure as the foundations to build context-aware software systems. Here by ""high-level"" we mean that the paradigm will be at a high level of abstraction with support of formal theories. The programming languages and software models, abstractions and infrastructure designed based on theparadigm will have formal abstract semantics that will be independent of concrete context-aware applicationsystems and low-level specific operational environments and platforms.Context-aware computing means that computations of a software application that is aware of, varies in, anddepends on the underlying, runtime contexts and context changes. Here contexts can be generally defined asexternal varying environments that affect or determine the computations of the application. Examples of suchexternal environments or contexts include time, location, situation, and user profile. Context-awareprogramming means writing computer programs that specify how computations or behaviors of context-awaresoftware applications depend on or vary in underlying contexts. The current (low-level) solutions tocontext-aware programming are mainly focused on extending conventional programming languages andsystems with context-aware supports in forms of additional software constructs, APIs, and packages.This proposed research will take an abstract-to-concrete approach to investigating the paradigm. We willinvestigate an abstract computing model for context-aware computing. We will investigate high-levelcontext-aware programming languages for writing context-aware application programs based on the abstract context-aware computing model. We will also investigate implementation of the abstract context-awarecomputing model on various concrete context-aware software and hardware platforms/environments.""380877,""Duan, Annie"
"365049"	"Dubé, Danny"	"Adaptive static analysis for high-level languages applied to abstract profiling of programs"	"The main axis of the proposed research consists in continuing research on adaptive static analyses.While the exact set of abstract entities used by a conventional static analysis is selected at implementation time by the developer of the compiler, that of an adaptive static analysis can be changed at compile time.More than that, an adaptive static analysis is able to determine by itself relevant changes that ought to be applied to the abstract entities.An adaptive static analysis has the potential to be very precise thanks to its ability to select a set of abstract entities that fit the program at hand while keeping the costs under control by avoiding superfluous increases in precision.The main project consists in performing static profiling of functional programs.Static profiling has strong similarities with static analysis in that the particular choice of abstract entities has a great influence on the cost and the precision of the analysis.We intend first to realize a working version of static profiling and then to apply the idea of adaptiveness to improve the accuracy of the results produced using static profiling.The research that we have conducted in the past and the research that we intend to conduct contribute to improve the implementation of high-level programming languages, and that of functional languages in particular.""362780,""Dubé, Dominique"
"364921"	"Dugas, Charles"	"Financial data-mining"	"The high-level long-term objective of this proposal is to develop a data-mining approach to analytical tasks faced by an insurance company, starting with automobile insurers. In the last decade, some insurers, but not all, have successfully implemented data-warehousing solutions that enable them to conduct larger data-mining projects. Traditionnally, actuaries were left to work on aggregate statistics and interactions between explanatory factors had to be discovered one way or another. The increased availability of large and relatively clean databases makes it now possible to analyze individual consumers (insureds). For that purpose, discriminative technologies such as neural networks and support vector machines can be used, given the level of computationnal resources now available.""378546,""Duggal, Krishan"
"364487"	"Dumitrescu, Sorina"	"Algorithms for optimal multiple description codes design"	"The advent of modern communications raises new challenges for multimedia transmission. One of these challenges, encountered in both wired and wireless networks, is the necessity to alleviate the impact of packet loss. A paradigm to effectively solve this problem is multiple description coding (MDC). The basic idea in MDC is to create several descriptions of the signal such that the source can be reconstructed to a certain fidelity from each individual description, and when more descriptions are available, they can refine each other, leading to a higher fidelity reconstruction.The main objective of the proposed research is the development of efficient algorithms for the design of optimal multiple description codes. The emphasis will be on two approaches for MDC: multiple description scalar quantization and uneven erasure protected packetization of scalable codestreams. Beside developing newsolutions (faster and/or of higher accuracy) to previously formulated optimization problems, the applicant will investigate new MDC frameworks and will propose solutions to new problems raised from practical concerns.The proposed research will lead to new techniques for robust multimedia transmission over lossy networks, which more efficiently use the available resources in transmission rate. These techniques will be directly applicable in practice. Additionally, this research will bring new insights in the area of MDC design, thus contributing to the advancement of knowledge and understanding in the area and laying background for further research.""379547,""Dumitrescu, Sorina"
"361412"	"Dutchyn, Christopher"	"Modularizing Control"	"Aspect-oriented programming (AOP), an active research area, is interested in modularizing program features, including synchronization, distribution, and instrumentation, all of which crosscut and entangle with more fundamental application concerns.  By encapsulating and abstracting these features, programs can become easier to write, read, and maintain.  One fundamental form of AOP modularizes these features by a novel mechanism comprised of three parts: dynamic join points, pointcuts, and advice.  Existing semantic accounts of this mechanism are disconnected from AOP's goal of modular separation of concerns because the account imposes an ad-hoc extension to a base language.Based on doctoral research showing that dynamic join points arise naturally from the continuation semantics of a programming language, this research proposes a bridge between the modularity goals of AOP and the semantic structures it works with.  That bridge identifies dynamic join point aspects as modularizing control structure of programs.  In the short term, a number of projects are proposed to refine and extend this model of AOP, especially focusing on types and effects descriptions of the modular units made available by AOP.  By identifying and annotating these ""abstract control types"" in programs, we enable a fuller account of the programmers intentions and provide ways for those intentions to be checked against the actual program code.  This clarity and checking should result in fewer software defects, and possibly greater code reuse.In the long term, this research programme proposes to develop a ""standard model of programming languages"" which characterizes the elements and interactions at work in programming languages.  This modularity over control is the first part of that model.""375896,""Dutilleul, Pierre"
"363335"	"Eck, Douglas"	"Learning musical structure with machines"	"Music sequences are difficult or impossible to learn using standard time-series analysis techniques. This is due to the fact that musical structure is elaborated over long time spans and is hierarchical, making it difficult or impossible to learn from local information. By example compare the phrase ""John ate the <?>"" to the musical sequence ""B C E <?>"" In the linguistic case, the words ""John ate the"" constrain the possible values of next word <?>.  In the musical case, the nearby notes ""B"" ""C"" and ""E"" do little to constrain what can come next.  This is not to say that music lacks constraints.  In music the constraints come from elaboration and repetition within the context of musical structure.  Unfortunately (for standard sequence learning tools, at least) this structure is elaborated over long time spans and at different timescales in parallel. I propose an investigation of how music is structured in time and how music structure can be used to constrain machine learning algorithms applied in the domain of music. The two-fold goal of the research is to (a) further our understanding of how music is structured and how music structure relates to music perception and production and to (b) improve the state-of-the-art for machine learning algorithms for two important music-related tasks: human-realistic music performance generation and music sequence learning. I will also address related tasks such as music classification, music similarity measurement and online processing methods for collaboration with real musicians. The key idea behind all of this research is that a better understanding of how music is structured in time will lead directly to better machine learning algorithms for music-related tasks. This work should have impact in machine learning, audio signal processing and music cognition. Portions of this research should be relevant in domains dealing with speech, video and related time series.""363525,""Eckert, Christopher"
"362453"	"Eckford, Andrew"	"Practical methods for maximizing information transmission in wireless sensor networks"	"In a wireless sensor network, many distributed nodes observe a physical phenomenon, and communicate their findings to some central station, known as the data sink. Sensor networks represent an important emerging field of engineering research, with numerous potential applications in diverse fields, from security to wildlifemonitoring.Wireless sensor networks present a challenging problem for communication. The sensor nodes themselves have stringent energy and computational constraints. Meanwhile, the wireless communication environment is affected by many sources of distortion, such as interference or fading, any of which could cause a loss of signal. It is known that the nodes in a sensor network can achieve higher information rates through co-operation, and especially through acting as relays for each others' signal. Through relaying, each node has access to several independent paths from itself to the sink, and with each additional relay, there is an increasing probability that at least one path to the sink will be usable. However, this solution imposes a burden on the relaying nodes, as they cannot send their own information while helping their neighbor.  Furthermore, it requires the relaying nodes to consume their precious energy and computational resources.The proposed work will investigate methods for maximizing the information transfer rate through a wireless sensor network, particularly addressing the burden incurred in relaying through methods such as distributing it over as large a set of nodes as possible, or using simplified algorithms locally at each node.  With a focus on feasibility, the proposed work will, if successful, lead to the deployment of sensor networking hardware which exploits advanced concepts in wireless communication technology.""381144,""Eckstein, Janine"
"365390"	"ELDarieby, Mohamed"	"A scalable wireless computing platform for wireless mesh networks"	"The proliferation of wireless mesh networks (WMN) in metro-areas is expected to have a large impact on our communities and municipalities. WMN is poised to be the infrastructure that enables new types of value-added applications in public safety, business, and entertainment. Enabling novel applications on top of a WMN infrastructure is expected to have a large effect on how people communicate, interact and do business. This research investigates a systems-based approach for building a WMN-based wireless computing platform. The platform will use technologies such as wireless autonomic and service-oriented architectures. With the platform, mesh nodes- with no prior knowledge of each other- can share their resources to facilitate the execution of a given application task. Resources are autonomic individual elements that incorporate specifications of functional and non-functional behaviors. The platform will be equipped with self-* capabilities in order to handle the dynamic characteristics of operating metro-environments such as highways, consumer radio and news services, firefighting and social networking. This will improve the serviceability and performance of municipal wireless network of Canadian cities.""374834,""Elder, James"
"365614"	"Elio, Renée"	"Computational models for intention handling"	"The research is concerned with expanding the scope of computational cognitive theories to understand the human executive control function, or what might be viewed as the 'human operating system.' One basic question is how the human brain sets a future-directed intention: a decision to execute some action at a future time (such as remembering to buy milk at the end of the day) and the subsequent execution that activity, if and when environmental cues signal that conditions are appropriate to do so. The role of intentions in computational theories of intelligence is to direct attention and reasoning resources. This research develops cognitive theories, based on computer simulations and laboratory experiments, about how intentions are represented and manipulated by the human cognitive architecture. This has practical implications for understanding how humans can do several tasks concurrently. Understanding the cognitive microstructure that supports future-directed intentions is relevant to various cognitive engineering efforts, e.g., for the design of systems that support human performance in dynamic environments in which tasks are interrupted and then must be resumed after some appropriate time or when the environment changes. Interrupted tasks can be viewed as a future directed intention to resume an activity,and it is important to understand what kinds of factors impact whether or not such an intention is likely to be remembered and executed successfully.""374124,""ElJabi, Nassir"
"366341"	"ElKhatib, Khalil"	"protecing user's privacy in ubiquitous computing environments"	"Ubiquitous computing environments will definitely change the way we go about our daily activities, as they will, more than ever before, facilitate our actions to be sensed, communicated, stored and searched electronically. The scale of the change should make every one of us care about the security and privacy problems created by these environments. Ignoring these problems would otherwise turn ubiquitous computing environment into an Orwellian society, where the norm is what Prof. Ron Rivest from MIT called the ""reversal of default"", where ""What was once private is now public"", and ""what was once easily forgotten, is now stored forever"".    The research program proposed here would focus on developing a comprehensive framework of protocols and algorithms for protecting the security and privacy of the occupants of ubiquitous computing environments. The framework would address the problem of data protection at two levels:   (1) at the network level, to ensure the availability of the sensing network, and to protect the integrity and        confidentiality of the sensed data, and   (2) at the user level, by developing dynamic control mechanisms that would allow the occupants of        ubiquitous computing environments to control what information is collected about them, how it is        processed, and under what circumstances it can be shared.""374794,""Ellard, Colin"
"362412"	"Ellis, Randy"	"Technologies for image guided therapy"	"Image-guided therapy uses medical images, 3D position sensing devices, and computer-based navigation systems to improve the ways in which physicians treat patients. Images, such as MRI scans, can be used before an interventional procedure (such as surgery or catheterization) to carefully plan the procedure in a 3D computer simulation. When the plan is aligned with the patient in the treatment room, the navigation system can sense where the patient and the instruments are, so that the physician can see an ""overlay"" of the instruments on the images, allowing the physician to access deep anatomical structures through very small incisions. Additional medical images, such as ultrasound, can also be obtained and integrated into the navigation screens. This gives physicians powerful information that helps them to treat patients more effectively.This research is aimed at improving two aspects of image-guided therapy: providing physicians with stereoscopic views of the medical images, and aligning the planning images with the real-time images to fuse the different images into an easy-to-use whole. These techniques will allow physicians to significantly improve treatments and may usher in the next revolution in laparoscopic surgery: ""scar-less"" surgery that uses natural openings into the body and thus needs no skin incisions.""361910,""Ellison, Sara"
"364914"	"ElMarakby, Randa"	"Context-aware quality of service provisioning for real-time multimedia communications"	"The growing interest in continuous media (audio/video) services, namely media streaming and multimedia conferencing over IP, coupled with technological advances in wireless and mobile networking, present new challenges and opportunities for Quality of Service (QoS) provisioning. For example, some sports events are watched by millions of online users connected to the Internet in a variety of ways including high speed connections (Cable or ADSL), modems, or wireless connections. Accommodating such large number of continuous media users, network connection heterogeneity, and the different QoS requirements for multimedia applications represents significant challenges.       These challenges call for a new generation of QoS provisioning that takes into account the network and the application context to provide acceptable QoS. By incorporating contextual information in the QoS control process, it is possible to determine the reason for poor quality and take the most appropriate control action. Context information considered in this proposal include a variety of aspects including the location of a mobile device, the number of session users, the QoS requirements of the application, and the nature of the connection linking the user to the Internet.       The Real-time Transport Protocol (RTP) is commonly used to deliver real-time multimedia over the Internet. The associated Real Time Control Protocol (RTCP) provides control and feedback information necessary for QoS monitoring in RTP sessions. The proposed research aims at analyzing further enhancements to RTCP to integrate it in a context-aware framework in a scalable and effective way.       The impact of this work is to provide a comprehensive quantitative understanding of the fundamental issues involved in providing context-aware QoS. Another important outcome is the development of a prototype implementation of a scalable context-aware QoS control architecture for the global Internet of the 21st century.""361315,""ElMasry, Ezz"
"365480"	"Eramian, Mark"	"Computer assisted diagnosis using ultrasonography"	"The proposed research is to investigate ways of enhancing ultrasound images so as to aid their interpretation by a medical professional.  Image enhancement algorithms will be designed to make differences between different kinds of tissues more visible, and to increase the difference in appearance between normal tissues and diseased tissues.  The enhanced images are called ""virtual histology"" images after the classical histology methods of staining and viewing tissues under a microscope which are used by pathologists to make diagnoses.  ""Virtual histology"" images will offer a new kind of information to medical professionals which will improve the accuracy and rapidity of diagnoses.  It may be possible to use ""virtual histology"" images to reduce the need for surgical biopsies to determine malignancy.  Invasive biopsy procedures could be replaced in some cases by a harmless ultrasound scan which is enhanced by ""virtual histology"" algorithms.  In the future, ""virtual histology"" images could be generated automatically by ultrasound machines at the time of examination leading to faster time to diagnosis for patients and allowing for low-cost early detection of pathologies.  ""367410,""Erb, Blake"
"365498"	"Falconer, David"	"Multiuser digital wireless communications systems"	"The proposed research program focuses on novel and practical adaptive signal processing and multiple access techniques for efficient wireless access to multimedia communications services. It applies to the ""next generation"" of wireless systems (beyond the currently emerging third generation systems). These techniques applied to next generation wireless systems will provide ubiquitous untethered access at data rates far higher than today's.The proposed research is part of a continuing research program whose main objective is to devise, improve and evaluate means to accommodate large and variable bit rate subscriber densities with realistic bandwidth, power and cost constraints. The envisioned traffic includes broadband multimedia services, voice over IP, and communications to and from myriad small sensor devices. It will be facilitated by adaptive space, time and frequency diversity processing at transmitters and receivers, and by techniques like intelligent relaying, cooperative diversity and cognitive intelligent radio spectrum sharing among user terminals.""368853,""Falk, Kevin"
"365991"	"Fathallah, Habib"	"optical coding technology in fibre optic networking and applications"	"This proposal addresses the use of optical coding technology in three different fibre optic network applications. The first concerns optical encryption of data for communications security, the second focuses on the network management issues in emerging access passive optical networks and the last considers employing optical codes in fibre-optic sensing networks.For optical communications security, coding is used as an information encryption technique. We propose to exploit two-dimensional coding and advanced superstructure Bragg gratings to protect mission critical data from jamming and eavesdropping. Besides multi path immunity, spread-spectrum could be used to provide network protection and fast service restoration for special mission critical network links.For passive network management applications, the code carries the management and network status information instead of data. We propose the use of modified optical code division multiplexing techniques to manage standard passive optical networks including EPON and GPON and more advanced architectures like wavelength division multiplexed WDM/PONs. Our technique promises to be scalable for even more complex access and metropolitan networks including various architectures.For optical sensing network applications, the encoder, i.e. superstructure Bragg grating, serves as a sensor (or transducer). Interaction between the light and the measurand (stress, temperature, pressure, etc.) will affect the code and the correlation function instead of the wavelength (as is the case of Bragg gratings based sensors). Our technique promises to dramatically expand the capacity of sensing networks.""367777,""Fatila, Elisabeth"
"366454"	"Fels, Sidney"	"Anthropomorphic robotic face"	"We are studying theories of speech, facial gesture and affect perception using computational and robotic models. Currently, we are conducting experiments using the UBC anthropomorphic robotic jaw and a graphical, computational jaw model to determine how much information is required for intelligible speech as well as affect. Ultimately, we require a complete anthropomorphic face model that attaches to our existing robotic jaw.  With this equipment, we intend to have  a robot which is capable of interacting with people in an engagingly close human-like way, so that we may investigate how people interact with robotic agents and delve deeper into what it means to be human by trying to synthesize facial expression and gesture beyond 2D graphical representations of the face.The robotic face we are requesting funds for is a 21 degree-of-freedom (DoF) head with an additional 3 DoF for the neck. The equipment will accommodate our existing 6 DoF anatomically correct robotic jaw.  The DoFs of the requested equipment have mobility ranges that are beyond human capabilities, in order to allow for increased facial expressiveness.""361134,""Fels, Sidney"
"366610"	"Fernando, Xavier"	"Signal processing for radio over fiber systems"	"With telecommunication deregulations and technological advancements, traditional communication and media broadcasting services merge together. Cable television (CATV) providers geared to provide voice and Internet services over their hybrid-fiber coaxial network (HFC), entering the so called 'triple play' while, Telco's provide Internet and phone over their telephone networks. The need for bandwidth is higher than ever before, especially at the wake of high definition televisions (HDTV). Fast emerging fiber to the home (FTTH) promises broadband access delivered to our doorsteps. Fiber-wireless systems gain popularity as well, bringing the RF signal closer to the broadband wireless users.In all these access networks (HFC, FTTH and fiber-wireless) fiber is in the back end. Hundreds of video and audio signals are modulated on RF carriers and transmitted over fiber (radio-over-fiber). The fiber medium is often enhanced with wavelength division multiplexing (WDM) and configured as passive optical networks (PON). Although fiber has enough bandwidth, other issues like nonlinear distortion; limited power handling capability; limited dynamic range; chromatic dispersion and large losses challenge the quality of service. The RF signal is essentially analog and signal reconstruction won't be possible to overcome the impairments in an end-to-end path. The research program targets on investigating signal distortions over optical access networks with an electrical last mile. The last mile could be (coaxial) cable or air. Electrical and optical domain signal processing techniques will be investigated to improve the quality of the multimedia signals delivered to the user. Results will be verified experimentally using existing lab facilities.""363980,""Fernlund, Goran"
"361835"	"Finlay, Warren"	"Aerosol mechanics in replicas of extrathoracic airways of infants and children"	"Deposition of inhaled aerosols in children is of increasing interest, partly due to advances in aerosol inhalation therapy for treating disease, but also due to increased awareness of the health effects of exposure to environmental aerosols in an increasingly urban population. In order to predict the efficacy of an inhaled pharmaceutical aerosol or to assess inhalation dosimetry of an environmental aerosol, it is necessary to first predict the amount of aerosol depositing in the extrathoracic region (consisting of the nose, or mouth, and throat). Unfortunately, little data is available in children to develop validated mathematical models for this purpose, largely because of a lack of data on which to base such models. To address this lack of knowledge, in the present application we propose to build physical replicas of pediatric extrathoracic airways and then measure monodisperse aerosol deposition in these bench-top replicas. Numerical simulation (computational fluid dynamics) will be used to supplement the experimental work. Similar experimental and computational work done by us in adults allowed development of dimensionless correlations that predict extrathoracic deposition in individual subjects with good accuracy. We propose to develop similar subject-specific correlations for children. The result of this work will be algebraic dimensionless equations that can be used to predict, a priori, extrathoracic doses to individual children. In addition to the above, we propose to develop idealized physical models of pediatric airways that can be used in bench-top testing for the development of pharmaceutical aerosols. Similar work by us in adults has resulted in an idealized mouth-throat geometry that is widely used by companies in device and formulation development.The proposed work will allow an in-depth understanding of aerosol mechanics in replicas of pediatric extrathoracic airways, thereby allowing improved assessment of ambient aerosol exposure in children, as well as the development of improved therapeutic aerosol drug delivery in pediatric patients.""379391,""Finlay, Warren"
"366623"	"Fleming, Michael"	"Designing decision-making agents for electronic markets"	"With the increased role of electronic commerce in today's society, intelligent computer systems (agents) are being called upon to make decisions on a user's behalf in electronic markets where goods, services and information are purchased, sold and traded. Such decision making requires detailed knowledge of the user's preferences; therefore, efficient and accurate methods for learning about the user's preferences and for reasoning with incomplete information are critical components of this work.  In the case of settings where an agent is negotiating with or competing against other agents (acting on the behalf of other users), additional concerns include the design of effective negotiation strategies and the ability to model the opponent's preferences and goals.The overall goal of this research program is to develop improved techniques for intelligent agents to use in making decisions on behalf of users in electronic markets.  Particular emphasis is placed on the need for such agents to adapt their behaviour to the preferences and goals of each individual user.  It is anticipated that at least 3 graduate students per year will be involved in this research program and the work is expected to make contributions to research in the areas of electronic commerce, preference elicitation, decision-theoretic reasoning, user modeling and automated negotiation.""382609,""Fleming, Michelle"
"364288"	"Fong, Philip"	"Programming abstractions for access control: capability, delegation, obligation"	"Software systems in the Internet era carry out computation on behalf of multiple principals.  Mobile code, scriptable applications and plug-in architectures all involve the execution of untrusted program fragments alongside trusted ones.  More than ever, we find within a single process multiple code units that are mutually suspicious of one another.  In layman's language, the left hand does not trust the right.  There is thus an increasing demand for programming environments to assume the security posture of a multiprogramming operating system, providing protection mechanisms that arbitrate the interaction of code units within a single application.  The advent of the Java platform and the .Net framework is concrete evidence of thistrend.This proposal describes my plan to develop novel and practical programming language abstractions for exercising access control at the programming language level, with the objective of addressing the unique security challenges presented by dynamically extensible systems.""371707,""FonsecaTumilasci, Vanessa"
"364849"	"Freeman, George"	"context-aware speech recognition"	"The goal is to make an automatic speech recognizer that behaves more like the way people hear speech.  Partly, this is done by forming groups of words that share a common feature, such as one sound in the word, so that whole groups can be tested at once based on this common feature.  The way the sound is measured is also related more directly to movements of the mouth when people speak.  Different measuring units are used for different types of sound, for example, vowels versus consonants.  Since sounds occur in a different order in different words, this means that there may be different measuring units applied in a different order.  Most speech recognizers use a common measuring unit for all sounds in all words so that interpretting the measurements is simpler but some distinctions between sounds might not show up as well.  The grouping of words can also be based on other things, such as the way syllables are stressed or the part of grammar that describes the word, for example, a noun versus a verb.  The desired outcome is to make it much easier to adapt the recognizer to different people or to different words in a vocabulary and possibly to reduce the number of mistakes made in recognition.  Normally, the user must train a speech recognizer by reading out loud from a long story that contains examples of many different words.  The proposed system should require much less of this training.""376657,""Freeman, Mark"
"366554"	"Friedman, Joel"	"Expanders and related topics"	"When there is a lot of traffic on a communication network, the network may develop ""hot spots,"" i.e. links (edges) where there is a ""large"" number of packets (messages, phone calls, etc.) which want to use that link.  A type of network or graph where this does not happen (given a good routing strategy) is a type of graph known as an ""expander.""Expanders have a large number of applications to computer science theory, including to randomness, routing, and other areas.  The problem of building them (i.e. giving explicit constructions) has drawn a lot of attention in the computer science theory and mathematics communities.  There has been significant progress towards explicitly constructing expanders, although a number of compelling problems remain open; much of this progress uses an important connection between expanders and the linear algebra of the graph's adjacency matrix, namely its eigenvalues.This project aims to further study expanders and their relationships to the linear (and non-linear) algebra of the graph's adjacency matrix.""366368,""Friedman, Ori"
"366384"	"Ganjali, Yashar"	"Experimental study of router buffer sizing"	"Until recently, Internet routers were widely believed to need large buffers. Commercial routers today have huge packet buffers, often storing millions of packets, under the assumption that large buffers lead to good statistical multiplexing and hence efficient use of expensive long-haul links. Recently, we (while at Stanford) showed that one can build a network with much smaller buffers still---with only a few dozen packet buffers in each router, at the expense of 100% link utilization. There are interesting practical consequences if this is correct. It could remove one major roadblock to building optical packet-switched routers. Recent advances in technology (at UCSB) also make possible optical FCFS packet buffers that can hold a few dozen packets in an integrated opto-electronic chip. Larger all-optical buffers remain infeasible, except with unwieldy spools of optical fiber (that can only implement delay lines, not true FCFS packet buffers).We are interested in exploring the feasibility of an operational all-optical network with just a few dozen optical packet buffers in each router, and studying the boundary conditions where our theoretical results hold. Previous attempts in this direction include experiments performed in limited laboratory settings. Limited number of routers and switches (usually one or two), fixed traffic patterns, trivial dumbbell-shaped topologies, ... are a few of the shortcomings of such experiments. In this project, we propose a broader set of experiments to explore the boundaries of the validity of our buffer sizing results. To this end, we are going to use 4-port Gigabit Ethernet routers, called Controllable and Observable Buffer (COB) routers, recently designed in Stanford University. COB routers are built on the NetFPGA platform, have configurable buffer sizes, and are able to monitor and report the buffer occupancy with a time resolution of 16ns (less than the arrival time of one packet). This will allow us more control and observability than any commercial router, in addition to the possibility of building large topologies of COB routers in our lab.""366385,""Ganjali, Yashar"
"366322"	"Gaudet, Vincent"	"Evaluating energy tradeoffs in coded systems"	"Turbo codes and low-density parity-check (LDPC) codes are two categories of forward error control codes that operate very close to the Shannon capacity bound for communication channels. During the past decade, various aspects of code performance and decoder hardware implementation have been investigated.  Hardware implementation research has focused on either low power or high throughput implementations, whereas code design research has focused on the development of codes that operate at very low signal-to-noise ratios (SNRs).For many reasons, the design of error control systems has traditionally separated the design of a code from the design of its associated decoder hardware.  This two-step process prevents code designers from accounting for decoder performance specifications, such as power consumption, in their design models and methodology.  However, some of my recent research results have demonstrated that density evolution, one of the techniques used in LDPC code design, can provide insight into the toggling statistics in a decoder, and hence into a decoder's dynamic power consumption.   Thus, when evaluating several codes for their SNR performance, one could additionally evaluate decoder power tradeoffs using similar information theoretic tools.This proposal, at the intersection of coding theory and microelectronics, focuses on an extension of these recent research results.  The long-term objectives of the proposal are to develop design and analysis techniques for coded systems that account for decoder computational energy in the code design process.  This will require research into tools that are capable of analyzing decoder power consumption, possibly based on but not limited to density evolution techniques, and will require the verification of analytical results using physical measurements from fabricated integrated circuits or field-programmable gate array-based designs.""372530,""Gaudette, Lisa"
"362490"	"Gavrilova, Marina"	"Adaptive hierarchical approach to surface modeling and data management for biometric applications"	"In information technology, biometric refers to a study of physical and behavioral characteristics with the purpose of personal authentication. In recent years, the area of biometrics has witnessed tremendous growth, partly as a result of a pressing need for heightened security, and partly as a response to the new technological advances that are literally changing the way we live. In academia, government, industry and everyday life, biometric systems are being utilized to ensure secure building access, for passport control, to secure bank transactions, for authenticity of credit card users, admitted as evidence in courts and serve as protection against crime.The goal of this research is to develop a new group of methods based on the innovative paradigm of an Adaptive Hierarchical data representation. The model would allow the storage of biometric data in a compact form, have highly efficient access to it when needed, have the ability to retrieve and visualize the data preserving even the slightest details, have a set of interactive tools for biometric data analysis that  wouldutilize topological properties of the data, and allow the visualization of dynamic changes to the model in a smooth and continuous manner.  Particular emphasis would be given to the application of the model for 3D face reconstruction, fingerprint surface modeling and iris synthesis.  The outcomes of the research will impact not only the way in which biometric information is represented, processed and visualized, but also the state of the art of related disciplines, such as image processing, pattern matching, surface reconstruction, terrain modeling, and information systems research.""382662,""Gavryushova, Anna"
"361222"	"Genov, Roman"	"Smart sensory microsystems"	"After the revolutionary technologies of the microprocessor, cheap lasers, and World Wide Web, the new era promises to be the era of automated interaction. The next decade will be the decade of smart sensors. The global sensors market alone is expected to reach $50 billion US dollars in 2008. Integration of sensors with embedded intelligence and convenient user interface will further significantly expand this market. This proposal outlines the research program carried out at the Intelligent Sensory Microsystems Laboratory at the University of Toronto. We will continue to conduct innovative research on integrated devices, circuits, architectures and algorithms for smart sensory microsystems design. Two specific research projects within the scope of Intelligent Sensory Microsystems are described in this application: 1) Brain interface microsystem for treatment of neurological disorders, and 2) Tip-of-the-catheter microsystem for real-time multi-analyte intravenous imaging. The projects offer opportunities to develop much needed cutting edge smart sensors technologies in Canada. The society will benefit not only economically. All of the applications listed in this proposal will improve the health and increase the quality of life of many people in Canada and in the world.""367911,""Genovesi, Linda"
"364933"	"Gerhard, David"	"Voice modeling for music information retrieval and human-computer interaction"	"The human voice is an expressive and eloquent musical instrument.  Song and other human-instrument music is ubiquitous in many cultures and is one of the primary forms of modern entertainment.  With the rise of internet music sharing and sales, interacting with music is becoming a large research topic, but current techniques often concentrate on timbre, rhythm, and other characteristics that music listeners would consider the ""background"" of the music. Much of the ""content"" of current popular music is contained in the singer's lyrics, expression and style. When pitch is addressed, it is usually instrumental pitch, and when lyrics are considered, they are usually available as text.One of the primary goals of the music information retrieval community is to develop systems whereby a user can hum a tune and the system will identify that tune from a database of possible music.  This difficult task is compounded when users want to add more information by singing lyrics.  One of the aims of this work will be to develop feature models, signal processing techniques, and classification systems that allow for the transcription of human singing.People don't only listen to music, they create it as well. Many people claim that they cannot carry a tune, and yet are capable of humming or whistling along with the radio. This research has the potential to provide amateurs, untrained singers and even non-singers with the tools to interact with music in new and less threatening ways, and gain confidence with music. Professional singers will be provided with tools that make use of their training to facilitate writing of music, to hone their skill, and to interact with music in challenging and intriguing ways.  In the long term, deep and accurate models of human song may lead to a computer that sings to us.""366648,""Gerhard, Jason"
"366467"	"Ghrayeb, Ali"	"Synchronization techniques for wideband multiple antenna systems"	"We have witnessed over the last decade or so a great demand for providing reliable high-speed wireless links to support applications such as voice, video, e-mail, web browsing, to name a few. The combination of multiple-input multiple-output (MIMO) signal processing and orthogonal frequency division multiplexing (OFDM) seems to be an ideal solution for this challenge where bandwidth efficiencies on the order of 10 b/s/Hz are feasible for many of these applications. However, despite the great advantages MIMO-OFDM systems offer, such systems present challenges of their own. One of the most important challenges is synchronization, which is crucial in building reliable wireless communication systems. Synchronization includes time and frequency synchronization, and synchronization tracking. Some work has been done in this area, but most of this work has focused on single antenna wireless systems and little attention has been devoted to MIMO systems. Actually, most of the synchronization techniques that have been applied to MIMO systems were extensions of techniques developed for single antenna systems, and thus were not really optimized for MIMO systems. Even those schemes developed for MIMO systems were developed for generalized MIMO systems and not for specific MIMO applications. The implication of this is that a scheme that is optimal for one application may not necessarily be optimal for other applications. In light of this, we believe synchronization techniques optimized for specific MIMO applications are still lacking in the literature. Motivated by this, the PI plans to 1) develop techniques for time synchronization. An important element to this direction of research is to first develop design techniques for stable preamble (training) sequences; 2) develop robust carrier frequency offset estimation techniques with emphasis on reduced complexity schemes; 3) tailor the developed techniques to accommodate both centralized and distributed MIMO systems, as well as consider both the uplink and downlink channels; and 4) incorporate spatial and transmit data correlation into the techniques developed.""366122,""Ghrib, Faouzi"
"364701"	"Glitho, Roch"	"Service engineering in post 3G telecommunications networks"	"The deployment of 3G networks (UMTS, CDMA 2000,     PackCable) along with the related service architectures has just started. Research has now shifted to post 3G networks. However, very little is being done on the related service architectures. This proposed research program is devoted to the service architectures for 4G. In the proposal we use the world wireless research forum (WWRF) definition of 4G (i.e. co-existing/co-operating legacy and new networks. On the one hand we use 3G networks as use cases for legacy networks. On the other hand we use mobile ad hoc networks (MANETs) and wireless sensor networks (WSNs) as use cases for new networks. Architectures for creating, deploying, using and withdrawing end-user applications/services in stand alone MANETs, stand alone WSNs, and integrated 3G/MANETs/WSNs/ are investigated. Functional entities, algorithms and interfaces are proposed and validated. Mathematical analysis, prototyping, and simulation are used for validation and performance evaluation. The architectures rely on a panoply of technologies such as context awareness, clustering, overlay networks, peer to peer and mobile agents. These technologies are exploited to the fullest possible extent. Furthermore, whenever necessary, we investigate how to extend them.""362831,""Gloaguen, Erwan"
"363779"	"Godin, Robert"	"Formal concept analysis and text mining"	"Our research program addresses two main directions. Formal Concept Analysis (FCA) and text mining. Our first research direction is to pursue ongoing work in the field of FCA and applications, in particular data mining applications. FCA can be viewed as a hierarchical conceptual clustering approach that reveals the significant patterns of commonalties and relationships within tabular data based on the formal framework of Galois (or concept) lattices. Pioneered by Wille and myself in the early eighties, this field has largely expanded and it has become over the last ten years a well established discipline with an elaborated theoretical basis, a large array of applications (data mining, bioinformatics, software engineering, information retrieval, linguistics, psychology, ...) and software tools (commercial and open source), and a growing international community. In the next grant period, we plan to focus on data mining applications within the following main themes: multilevel association rule data mining condensed representation, complex data mining, approximate condensed representations and incremental frequent pattern mining for performance optimization in data warehouses.Our second main research direction is text mining and information retrieval applications. Much of the available electronic information is in the form of text. The Web is an obvious case. Text mining is concerned with extracting useful representations from textual data. These representations are used in information retrieval and text classification applications in particular. Typical single term indexing and weighting schemes suffer from their inability to systematically exploit co-occurrence patterns that are often more close approximations of the underlying concepts. We are working with several approaches that extract significant co-occurrence patterns. More specifically, our work exploits genetic algorithms, artificial neural networks and frequent set mining algorithms.""381781,""Godin, Robert"
"366563"	"Goel, Ashvin"	"Towards designing dependable software systems"	"While modern computer systems have become deeply intertwined with our daily lives, they are insecure, failure-prone and hard to configure and thus hardly dependable. The goal of this research is to improve dependability in software systems. We will focus on designing and building systems that allow analysis, detection and recovery from computer intrusions, software misconfiguration and software bugs. Traditional solutions have focused preventing the occurrence of these dependability problems, but unfortunately, these problems have no decisive solutions but are ongoing facts of life that must be resolved regularly. For example, a software system can be hardened endlessly but that does not eliminate the possibility of an intrusion. Instead, systems that allow detailed post-facto analysis and recovery can be made more resilient to software errors.Our research approach consists of capturing, storing and examining the past events in a system, which allows building powerful dependability tools. For example, the past information allows analysis of problems that may have occurred in the past, and it also allows building system profiles that help detect problems as they occur. This information also allows recovery. For instance, events in the past can be replayed to reconstruct past states of a system, and by selectively replaying events, it is possible to rollback, isolate or recover from intrusions and other failures. It is expected that research in this area will help reduce the enormous costs incurred today as a result of buggy and vulnerable software.""374835,""Goel, Vinod"
"361853"	"Goldberg, Ian"	"Useful security and privacy for internet services"	"Despite decades of research in security systems, the state of computer security and privacy protection today is dismal. Millions of people continue to fall victim to phishing attacks, have their computers hijacked for use in botnets, and leave wide trails of personal information, ripe for the picking by identity thieves.This research aims to provide secure and private access to Internet services, following four important principles; technologies that incorporate these principles are referred to as Useful Security and Privacy technologies. One of the most important principles is usability. In addition to ""ease-of-use"", we must consider the user's experience: if a technology improves security, but severely degrades performance, users will avoid it. Along with usability is deployability: systems must be easy to use, but also reasonable to use. For example, requiring end users to dramatically change how they do things makes the results of research meant to help them less relevant. Effectiveness and robustness are two other important principles of Useful Security and Privacy. Many security systems contain major flaws that greatly compromise their efficacy; it is important that these flaws are discovered and publicized in order to minimize their damage. Additionally, many systems are designed to work as advertised, as long as things go ""according to plan"". Reality is more brittle: users forget their passwords, their computers are vulnerable to malware, they misunderstand security-relevant messages, they fall victim to phishing attacks, etc. Useful systems must maintain as much security as possible in the face of problems like these. This robustness principle is one that most security technologies today overlook; we hope to make significant advances in the design of systems that can withstand various security failures.In this project, we will look at the design of security and privacy systems and make them more useful through the application of these four principles, drawing on modern computer system design techniques.""378842,""Goldberg, Jeffrey"
"362417"	"Gong, Minglun"	"Real-time dynamic scene modeling and rendering"	"Imagine that you are sitting in your living room watching a live NHL game. You have a giant screen and a surround sound system, which make you feel as if you are rink-side. In some aspects, it may be better than watching rink-side -- you have the freedom to pause the game when you go for a drink and resume when you return, which is possible because newer television systems can locally store the signals and play them when you want. However, don't you also want to have the freedom to control your viewpoint so that you can watch the game from your favorite spot, or even freeze the action when someone scores and replay the scene from different perspectives? The proposed research program aims at making this a reality for the next generation of television systems.The objective of this research is to develop efficient methods that can capture the appearances of dynamic real scenes and present them interactively in 3D to local or remote viewers. The problem of dynamic scene modeling and rendering will be investigated from both image analysis and image synthesis points of view. On the analysis side, we will study how to efficiently and accurately estimate 3D locations and motions of objects in the scene. On the synthesis side, we will explore an efficient way to synthesize physically correct novel views based on sparsely sampled videos and the 3D information estimated. To achieve real-time performance, both image analysis and image synthesis algorithms will be designed and optimized for parallel execution on programmable graphics hardware. Our preliminary research in this area has yielded very promising results.""362416,""Gong, Minglun"
"362414"	"Gooch, Amy"	"Preserving salience by maintaining perceptual differences for image creation and manipulation"	"Images on computer screens or on paper are a large part of our everyday lives.  Although the number of images we have access to is growing exponentially, the efficiency, beauty, and distillation of the information we see is not improving at the same rate.  In my research, I address how  to create or extend the communication power of images by mapping visible changes from one domain to another.  There is a precarious mapping between how a scene would be perceived and how an image is perceived. For example, although the human visual system does not have a strong absolute sense of color or intensity,  images are stored as a discrete set of color or intensity values at each pixel.  The goal of this research is to create perceptually salient images by more directly expressing perceived differences in scenes as displayed differences in images.  Perceptual difference preservation is useful for may image generation tasks including automatically creating technical illustrations from 3D models, creating a color to grayscale conversion algorithm that preserves all visible color changes through high-dimensional distance metrics, and for hidden feature finding techniques that identify correspondences and mismatches between X-ray and visible light images of historically significant paintings through segmentation and local gradient statistics.""361871,""Gooch, Bruce"
"366169"	"Gopalakrishnan, Sathish"	"Resource management for large real-time systems"	"This research program focuses on the theoretical foundations and engineering principles for the design of high-confidence real-time systems.Real-time systems execute many tasks concurrently and need to respond to events within expected deadlines. This requirement calls for careful resource allocation and task sequencing. The increased use of embedded computing and greater interaction with the physical world has led to more systems becoming ``real-time.'' Moreover, even services offered over networks are expected to deliver results on time because of service-level agreements between service providers and clients. There is, however, a lot of dynamic behavior -- because of complexities in system architecture and because of varying workload -- that limits the use of traditional deterministic analysis. This research program will develop a mathematical basis for reasoning about predictable behavior (mainly temporal behavior) when workload can fluctuate and when allocations along multiple resource dimensions -- not just CPU share -- affect response times. Theoretical analysis will be backed up with the implementation of mechanisms for ensuring that resource allocations can be made with fine granularity and accuracy. These mechanisms will be developed at the operating system level and below to provide system architects and developers with the primitives for building robust real-time systems. The combination of theoretical tools and engineering principles can drive down the cost of system building, which is currently extremely high because of resource over-provisioning, extensive debugging and re-engineering and certification costs. The lowering of costs will lead to greater market penetration for pervasive computing applications that integrate computation, communication and control and provides a significant opportunity for Canada to take a lead role in the design and development of embedded systems and ubiquitous services.""381429,""Gopalapillai, Salini"
"365698"	"Graham, Peter"	"Dynamic application composition for distributed systems"	"The software that most people use is designed and packaged to run on a single device (e.g. a spreadsheet running on a personal computer). Exceptions to this approach are normally restricted to high-end scientific and business applications that do not run on conventional desktop machines or personal computing devices (e.g. cell phones, PDAs, etc.). This ""standard"" programming approach implicitly assumes an underlying model of computing where everything is done on a single, general purpose computer. This model, however, is quietly fading as new mobile and embedded devices become prevalent. Increasingly, such interconnected devices are collaboratively involved in computations. Among the multitude of examples of this phenomenon are Peer-To-Peer file sharing applications and multi-user gaming (at the large scale) and multimedia tools running on iPods, cell phones, etc. (at the small scale).   In this proposal, I argue that continuing to assume that programs should be monolithic entities running on a single machine and licensed to specific individuals is overly restrictive and should be replaced. I suggest an alternative model based on executable components (i.e. pieces of runnable programs performing self-contained functions) that may be dynamically ""composed"" to create applications on the fly that are tailored to specific user and environmental characteristics. Such dynamically constructed applications can be run more efficiently both on and across a wide range of interconnected devices. For example, consider a scenario where a user needs to edit a document while travelling using a mobile device with limited capabilities. Normally the user would have to pay for and pre-install a large, complex and costly application to support editing the necessary document on the mobile device. In my proposed approach, only the necessary parts of the software would be dynamically composed on demand to run on the mobile device (and, if necessary, on one or more nearby machines offering compute services). Further, thecomposed software could be tailored to suit the small screen size and limited reliability of the device's wireless connection to the Internet.""368117,""Graham, Ryan"
"364647"	"Gras, Robin"	"Advanced probabilistic model building genetic algorithms and application to bioinformatics"	"The complexity of real applications and/or the size of the data to be analyzed makes it difficult for exact solutions to problems to be found in reasonable computational time. This is especially true in bioinformatics. Biological models are extremely complex and the data generated by genome sequencing, proteomics, gene regulation, protein/protein interaction and now macro-biology (cell models, organism or ecosystem simulation, etc.) projects often grow exponentially. Providing efficient heuristics to analyze such problems is therefore very relevant.Dr. Gras propose a research program based on optimization strategies using probabilistically guided heuristics. It is a new domain of heuristic strategies that has emerged recently. It is based on powerful techniques from statistical analysis and combinatorial optimization. This is an innovative and promising approach as a major part of the bioinformatics problems can be represented as optimization problems. Modern metaheuristics, based on the building of a probabilistic model, have proven their potential efficiency to solve such optimization problems. Moreover, due to the construction of the probabilistic model, these methods can reveal very informative knowledge about the intrinsic structure of the data. Dr. Gras proposes to apply such approaches and conceive new ones, associated with the use of expert (biologist) knowledge to solve bioinformatics problems.As this program aims to develop tools to analyze biological data that are correlated to Alzheimer disease, cardiovascular disease, bovine spongiform encephalopathy and cancer, it can lead to a better understanding of these phenomena and to the discovery of new diagnostic techniques and even new therapies. This program also involves the analysis and then the establishment of a model and the simulation of ecological systems. That would be essential for the understanding of evolution of biological populations and of pollution effects on ecosystems.""385789,""Gras, Robin"
"361742"	"Greer, Jim"	"Privacy-enhanced personalization for online learning"	"Computer systems that personalize their actions in reaction to personal preferences or requirements of human users do so by building profiles of the users. These profiles are based on knowledge gained either from analysis of user interactions with the system or from outside data sources. In either circumstance, there is a potential threat to user privacy. Research by me and others has been devoted to providing personalization support while preserving user privacy. One key approach has been to separate the system's knowledge of the personal identity of the user from the behaviour that is being monitored. This can be achieved by providing a pseudonym for the user and associating behaviour information with the pseudonymous identity. But pseudonymity introduces both social and technical challenges for personalization systems.  In computer-based learning environments, preserving learner privacy is important in situations where the release of personal information may result in inappropriate stereotypes or discrimination, potential harassment by other learners, or academic dishonesty through identity theft. There is a trade off between protecting learnerprivacy and gaining necessary and sufficient knowledge for a learning support system to work effectively. In larger learning communities where personal trust relationships may be less pervasive (andsometimes ill-advised), a mechanism for protecting user privacy and gauging trust in others is needed. Methods of pseudo-identity can be successfully applied here. For many learners, particularly children, greaterautomated protection is appropriate. Methods to identify and curb risky behaviour on the part of learners are needed.  Techniques for protecting privacy and modelling trust (appropriate for the learning domain) will be investigated in my research. This includes developing secure and reliable models for privacy-enhanced personalization through identity management, models for private reputation transfer from one pseudonymous identity to another, and models for managing multiple identities associated with a single learner.""383250,""Greffou, Selma"
"364180"	"Greif, Chen"	"Preconditioners for saddle point linear systems"	"Saddle point linear systems are of great importance in many areas of applications, and research on solution methods has surged in the last decade. Problems in constrained optimization and other areas often require the solution of such systems. Examples of relevant applications are fluid dynamics, surface fitting and image reconstruction, and there are many more.The efficient numerical solution of saddle point systems stands at the centre of this proposal. I propose to develop and study methods based on recent novel techniques that I have derived with my collaborators, and extend my work into new directions.Other research projects that are described in this proposal are the development of new generalized Krylov solvers, the study of the problem of condition number minimization and eigenvalue sensitivity, and the numerical solution of Markov chains, with focus on Google's PageRank algorithm.""376618,""Grein, Friedrich"
"363109"	"Greiner, Russell"	"Machine learning for bio- and medical-informatics"	"Many fields have massive amounts of data -- ranging from log files from web browsers, to telemetry data from equipment, to hospital records. The patterns within this data can be very valuable; eg these patterns may identify which machine needs preventative maintanence, which sites a user may want to visit and which products s/he is likely to purchase, or even identify which patients will respond well to a given treatment. Unfortunately, these patterns may be difficult to extract, due to both the quantity, and complexity, of the data. The field of machine learning (ML) provides many technologies, and tools, for this task. Much of this work involves understanding, developing and using tools that take as input a ""labeled dataset"" of instances (eg, a set of patients, coupled with their respective clinical outcomes), and returns a classifier, which can map novel instances to labels -- eg, to determine whether a patient, with a given microarray, will respond well to some treatment.I will explore ways to apply ML to topics in bio- and medical-informatics: designing and building automated tools that [1] learn patterns in Magnetic Resonance images of brains that correspond to brain tumors; [2] correlate patterns of a patients genome (including Single Nucleotide Polyomorphisms) with breast cancer and other patient states; [3] learn properties of proteins (eg, general function, subcellular location and appearance within specific metabolic pathways); and [4] relate diseases to metabolomic profiles and microarray data, based respectively on urine/blood samples and biopsies. I will also explore relevant foundational issues, providing analyses that [1] enable us to effectively and efficiently learn and use probabilistic models, like belief nets; [2] help researchers design experiments more cost-effectively, by determining which tests to run on which training subjects, to provide the relevant information; and [3] use bi-clustering techniques to effectively learn patterns in high-dimensional data, such as microarrays and metabolomic profiles.""382752,""Grenier, Annik"
"366636"	"Gu, Qianping"	"Optimization algorythms for WDM optical networks"	"With the tremendous growth of bandwidth-intensive networking applications, the demand for bandwidth over data networks is increasing rapidly. Wavelength division multiplexing (WDM) optical networks provide promising infrastructures to meet the information networking demand and have been widely used as backbone networks in the Internet, metropolitan area networks, and high-capacity local area networks. Methodologies and technologies for WDM networks have been becoming key research areas for both academia and industry. To realize communication applications on WDM networks, algorithms are needed to decide light paths for routing and to multiplex low-rate traffic demands in the applications to share the light paths. A general goal in the study of WDM networks is to determine the resources required to achieve a given connectivity as a function of network size and functionality of network nodes. Fundamental optimization problems for this goal are to minimize the resources for realizing a given communication application and to maximize the performance of a network subject to the given resources. It is usually NP-hard to find optimal solutions for those optimization problems. Most previous algorithms for the problems are based on the integer linear programming or ad hoc heuristics and the quality of solutions is not guaranteed. It is extreme importance but challenging to design algorithms which proive solutions with guaranteed quality for those problems. With this proposed project, we use new approaches to develop efficient algorithms with guaranteed performances for the optimization problems. One novel approach to be explored is the tree/branch-decompositions based algorithms. We expect to develop efficient algorithms which provide solutions with much better guaranteed quality for the targeted optimization problems and establish new general approaches for tackling hard optimization problems in networking. The outcome of the research is expected to significantly improve the performances and provide solid foundation for the future design and operation of WDM networks. The research is also expected to train and produce a number of high qualifiied personnel for Canadian academia and industry.""380404,""Gu, Qing"
"364665"	"Guéhéneuc, YannGaël"	"Understanding the impact of software design patterns and deisgn defects"	"The cost of software maintenance amounts to at least 50% of the overall cost of software development. This percentage is high because programs under maintenance generally have poor quality characteristics---especially low maintainability---resulting from the developers' high turnover rate, time pressure, lack of explicit design choices, and architectural drift. The goal of the proposed research is to evaluate the impact of design patterns and design defects on maintainability: to assess the use of design patterns by maintainers and to develop and apply a methodology to detect and to correct design defects. This assessment and methodology will deepen our understanding of software maintenance and allow the development of tools to improve the quality of programs and thus reduce the cost of maintenance.""363205,""Guenat, Olivier"
"364284"	"Gutwin, Carl"	"software Infrastructure for next-generation groupware"	"Software Infrastructures for Next-Generation GroupwareReal-time distributed groupware - systems that allow people to work together at the same time over distance - is now becoming common: for example, screen-sharing software, meeting support systems, instant messengers, multi-player action games, and VoIP telephones are now readily available. The design and development of these kinds of groupware has been greatly aided by groupware toolkits and libraries that handle distributed-system issues such as network connection, message distribution, session management, shared data, and remote execution. These software infrastructures have allowed developers to shift their focus from low-level details to higher-level design of the applications.     Despite these successes, however, current groupware is still severely limited. Although people can now make do with current technology, groupware does not come close to the richness and simplicity of working in a face-to-face situation. Systems are still complicated to initiate, still do not allow subtle or expressive interaction, and still perform poorly on real-world networks like the Internet. This research project will design and build a new set of software infrastructures for the next generation of groupware systems - systems that allow much richer interaction, that require almost no effort to set up, and that operate in real-world network conditions.     The project will develop four aspects of groupware infrastructure: performance techniques, session management systems, tools for enriched interaction, and mechanisms for protecting privacy. This research will provide tools and techniques that overcome current barriers to effective collaboration, and will allow designers to look at new interaction styles, innovative collaborative processes, and new application domains.""385129,""Gutwin, Carl"
"362387"	"Hamarneh, Ghassan"	"Computational optimization techniques for medical image analysis"	"I propose to develop highly-automated, robust and accurate computational techniques for medical image analysis (MIA). Novel image processing, data analysis, artificial intelligence, and artificial life techniques will be investigated and validated in a medical context.          Medical imaging provides challenging algorithmic problems and has a potential for improving health. The challenges are primarily due to the large anatomical shape variability (e.g. humeral bicipital groove), non-rigid anatomy (e.g. myocardium), noisy medical images (e.g. MRI, CT, or SPECT), data variety (e.g. 2D colour microscopy, 3D MRI, time-varying PET, or diffusion tensor (DT) MRI), and higher demand for robustness and accuracy in medicine compared to other areas.          Optimization problems are often encountered in MIA. My proposed research involves: 1) investigating the limitations and strengths of existing optimization techniques for understanding the visual data in medical images; 2) formulating MIA problems in new ways amenable to intuitive encoding of heuristics and domain knowledge; and 3) extending optimization techniques to obtain improved convergence and optimality. I will focus on the problems of: (i) medical image segmentation (identifying the boundaries of structures and organs in images, e.g. for subsequent quantification); (ii) medical image registration (spatial alignment of images, e.g. for building variational atlases or fusing multi-modal images); and (iii) structural shape correspondence (pairing homologous points on shapes, e.g. for shape classification and statistical analysis). For segmentation, I will extend scalar image segmentation algorithms to DTMRI data and propose alternative knowledge-driven optimization formulations. For image registration, I will optimize registration strategies rather than using low-level metrics as objective functions. I will also develop alternative optimization approaches for shape correspondence and propose a unified framework for shape correspondence and image registration.""378462,""Hambleton, Ian"
"365582"	"Hamouda, Walaa"	"Design of multiple-input multiple-output sytems for future wireless networks"	"The recent development of communication theory and radio technology has intensified the interest of multi-antenna systems as an effective technique to combat fading and reduce the effect of channel interference. Recently, many proposals within the frame work of multiple-input multiple-output (MIMO) systems have been introduced in order to improve the received signal quality and increase the transmission rates over wireless links. From the signal quality point of view, it is known that the reception of multiple copies of the transmitted data improves the system performance relative to single antenna systems. From the channel capacity point of view, it has been demonstrated that multiple antennas have the potential to dramatically increase the achievable data rates. In this proposal, we will investigate the performance of MIMO systems in wireless ad hoc networks. In the proposed research, we plan to focus on providing rigorous and accurate analyses for MIMO ad hoc networks.  This will include study of both physical and medium access control layer design issues.""373696,""Hamoudi, Safia"
"365169"	"HamouLhadj, Abdelwahab"	"Program comprehension through dynamic analysis"	"The objective of the proposed research is the exploration of new approaches to aid developers in comprehending the behaviour of a software system through the exploration of execution traces. Traces have historically been difficult to work with since they can be extremely large. Therefore, we need to investigate ways to allow software engineers to understand and analyse their content. In this proposal, we intend to investigate the following techniques:Firstly, we will study how various portions of a trace can be treated as the same pattern. The objective is to develop advanced trace simplification algorithms.Secondly, we will study how high-level behavioural design models can be automatically extracted from large traces. These models will record the essence of traces making it easier for an engineer to comprehend the information.Thirdly, we will develop an exchange format for representing traces in a standardised way. This work will be based on an existing format called the Trace Compact Format (CTF). We will investigate how to best extend the power of expression of CTF in order to support various types of traces generated from mutli-threaded systems.Finally, we will study how to best represent traces in a visualization environment. The resulting trace analysis tool will incorporate the outcome of the above-mentioned studies.""377006,""Hampson, David"
"363387"	"Higham, Lisa"	"Correct, robust and efficient concurrent computation"	"My research aims for correct, robust and efficient solutions to problems that arise from systems that use many processing components simultaneously.       Asynchronous shared memory architectures use elaborate hardware and software to speed computation.  Message-passing architectures are similarly complicated by a variety of message delivery services, which provide differing constraints on message ordering.  These mechanisms cause subtle, unexpected behaviour that can differ dramatically from what algorithm designers expect.  Thus, without using synchronization primitives, ""natural"" solutions to ""simple"" problems are not correct.  The catch is: the elaborate speed-enhancing mechanisms complicate the programming task to such a degree that programmers add excessive synchronization, which, in turn, slows down computation.  Thus, the speed-up advantages intended by special components can be entirely defeated!  A major focus of my research is to construct algorithms that are correct given only the weak guarantees that come with these systems, and that still exploit their speed.       An exciting application of my research is to the new and emerging multi- and many-core computers.  Even commodity desktop and laptop computers these days have ""dual cores"".  The near horizon promises ""many-core"" architectures that place possibly hundreds of different processing units on one chip.  Scientists' knowledge of how to program these machines correctly while exploiting their available concurrency is far behind their knowledge of how to build them.  My research aims to define the behaviour of these machines so that they are easier to program.     Even when individual components are very reliable, a failure somewhere in a large system is likely because there are so many components.  A related goal of my research is to design programs that continue to operate well in spite of some dead components, or that can automatically return our systems to correct behaviour after a burst of temporary faults.  ""361355,""Hijri, Mohamed"
"366596"	"Ho, Paul"	"Power and bandwidth efficient MIMO communication techniques"	"Wireless/mobile communications has experienced exponential growh since the introduction of the cellular phone a quarter of a century ago. Not only does cellular phone become a necessity for many people, other wireless technologies such as wireless local area networks and Bluetooth are gradually  integrated into our daily life. This  increasing popularity of wireless technologies, together with advances in multi-media applications, pose a number of technical challenges for the industry: given the physical limitations on the transmit power level, spectrum, and device size, how can we continue to provide affordable devices and services to the ever-demanding consumers? A key word here is MIMO, which refers to a wireless communication technology whereby information is beamed to the user 's device via multiple transmit and  one or more receive antennas. This system architecture is a departure from the traditional practice of employing only one transmit antenna. Essentially, MIMO introduces an added system resource, namely spatial redundancy, at no extra cost! Wireless systems based on MIMO technologies are expected to offer a dramatic increase in capacity when compared to conventional systems. The primary goal of this research is to devise coding and signal processing means that simplify MIMO system implementation while at the same time make them more reliable and more power/bandwidth efficient. A secondary goal is to use the research program as a platform to train highly qualified personnel who eventually will assist the Canadian industry to compete in this multi-billion dollar worldwide market of wireless technologies.""364085,""Ho, PinHan"
"364968"	"Ho, PinHan"	"Availability-aware design for generalized MPLS networks"	"The Internet service subscribers, including both enterprises and general end users, expect for high availability of Internet connections from their service providers. Guarantee of end-to-end (E2E) availability for each connection in dynamic Generalized Multi-Protocol Label Switched (GMPLS) networks has been an attractive design objective. The previous research work in the related areas has focused on development of survivable routing schemes for single or dual simultaneous failures under a given type of protection. However, as the state-of-the-art communication carrier networks evolve to a multi-service bandwidth provisioning environment, the E2E availability of each LSP has become a major performance index for achieving Quality-of-Service (QoS) guarantee percieved by the end users. This fact has imposed a fundamental impact on the development of survivable routing and spare capacity allocation schemes since the end users may not care about how many simultaneous failures the carrier can handle, and what type of protection scheme the carrier adopts. Driven by the observations, this project is committed to work on a general availability-aware service provisioning architecture in GMPLS networks for both real-time and non real-time services, where the long-term and short-term E2E availability of each connection will be defined and manipulated. Based on the availability models, the corresponding enhancement on the reported survivable routing and spare capacity allocation schemes will be investigated. The proposed research is envisioned to provide a complete solution to survivable routing problems by leaving some of the previous design issues, such as ""how many simultaneous failures the carrier can handle"", and ""what type of protection scheme we had better adopt"", no longer as problems. Such survivable routing architecture is expected to better fit into the design requirements of modern communication carrier networks where service availability is the ultimate goal of redundancy investment and survivability guarantee. The project will also lead to HQP training in availability analysis and optimization techniques, which will greatly contribute to the Canadian communication industry.""367399,""Ho, Sunny"
"363389"	"Holte, Robert"	"Machine learning and heuristic search"	"My research aims to develop scientific understanding and improved algorithms in the areas of machine learning, and heuristic search, and to explore practical applications of techniques in these areas.A machine learning algorithm uses training data, or past experience, to improve its performance at a specific task.  In my research the task is playing poker against a single opponent, and machine learning will be used to identify the opponent's strengths and weaknesses and adapt its play accordingly in order to maximize its winnings against this specific opponent. This is very challenging because learning must be very rapid (only 100-200 hands will be played against this opponent in total), and the information available to the learning algorithm is noisy and incomplete.A heuristic search algorithm is a fast method for finding optimal paths between two nodes in a graph.  For example, if the graph is a road map, a heuristic search algorithm could be used to quickly find the shortest route from your current location to a particular destination.  Heuristic search is faster than standard methods because it uses an estimate of the distance to the goal, called a heuristic function.  My research investigates various ways of speeding up heuristic search.  One of the methods I am investigating is to decompose the heuristic function into the sum of a set of smaller heuristic functions. My aim is to understand when this is possible, and when it is superior to other methods.  I am also investigating techniques for predicting how much speedup will be produced by a given heuristic function.""372109,""Holtham, Elliot"
"362577"	"Hooton, Douglas"	"Durability of concrete"	"In spite of developments in concrete technology, concrete in extreme environments continues to prematurely deteriorate. In part this results from inadequate specifications or construction methods, but it also results from a lack of understanding of all the factors which influence durability. The pore structure and resultant fluid penetration resistance of the cement paste matrix and paste-aggregate interfacial regions govern the durability of concrete to most causes of distress. In this proposal, these properties will continue to be studied with applications to resistance to: chloride ingress, reinforcement corrosion, freezing and thawing in the presence of de-icer salts, alkali-aggregate reactions, and sulphate resistance. Other aspects of durability such as crack resistance (to thermal and moisture related shrinkage) and improved durability- based performance specifications are also being studied.Specifically there are several areas of research focus: 1) characterizing pore structure and fluid transport through concrete, including development and standardization of test methods. 2) prediction of performance of fresh and early-age properties of cementitious materials-chemical admixture combinations. 3) effects of different de-icing chemicals on concrete integrity, alkali-silica reaction resistance, and salt-scaling. 4) minimizing volume change and inproving crack resistance of concrete. 5) improvements to evaluation of residual service life of concretes exposed to chloride environments. 6) implementation of test methods and practices in codes and standards.The impact on HQP will be training of 2 PhD's and 7-8 MASc students over the next five years and will encourage graduate studies for the 5 undergraduate summer research assistants.  While these figures will be augmented with student travel from other sources of funding, the NSERC funds are critically important.""362576,""Hooton, Douglas"
"363254"	"Horspool, Nigel"	"Profile-guided code optimization"	"Modern object-oriented programming languages, Java and C# in particular, rely on a technology called 'just-in-time' (JIT) compilation to achieve good performance. JIT compilers generate machine code while the program executes. However a JIT compiler cannot afford to spend much time on extensive analysis of the program to optimize it fully, and thus to improve performance further.The proposed research involves collecting a minimal amount of information while the program runs to identify 'hot' parts of the program, where a large proportion of the execution time is spent. In the light of this information, selected portions of the program can be recompiled. In the recompiled portions, computations have been moved out of hot parts of the program and moved to cold parts, thus improving the program's overall performance.""378632,""Horst, Ulrich"
"362080"	"Inkpen, Diana"	"Natural language processing for speech information retrieval and computer-assisted language learning"	"Existing search engines allow a user to retrieve useful information from texts on the Internet, but not from audio documents such as broadcast news or other recordings. The user has to browse through long audio recordings to find relevant parts. Despite much progress both in commercial applications and in research prototypes that deal with natural language, their performance of information retrieval systems still needs to be improved. We propose new ways of combining symbolic and statistical methods for dealing with the meaning of words and sentences. We plan to apply them in several tasks where they have not yet been tried. In particular, we propose to apply semantic similarity methods to tasks in Information Retrieval, Automatic Speech Recognition, and Computer Assisted Language Learning. We focus on multiple languages and on cross-language techniques.       Information Retrieval from spoken documents can take advantage of a standard text-based IR system if a speech recognition system converts speech into written text. Speech recognizers, however, introduce a high error rate. We propose to correct automatic transcripts using semantic similarity methods. We also investigate techniques of improving the overall retrieval efficiency from spoken documents in a multilingual setting.       Existing Computer-Assisted Language Learning tools do not give a student enough feedback to allow them to increase the learning rate. We propose extensions of such tools that include automatically-produced exercises with false friends and cognate words. Cognates are words that have similar spellings and meanings in different languages. They can help a second-language learner in the tasks of vocabulary expansion and reading comprehension. The learner also needs to pay attention to false friends: pairs of words that appear similar but have in fact different meanings in some or all contexts. Another automated tool that we propose is a semantic corrector that assists a second-language writer by detecting wrong usage of words in context.""361385,""Inkpen, Kori"
"364527"	"Jaekel, Arunita"	"Practical design strategies for optical mesh networks"	"The applicant and her research group will investigate novel approaches and techniques for design of fault tolerant (wavelength division multiplexing) WDM optical networks as well as efficient traffic routing schemes in such networks. We will develop optimal network design techniques for the recently proposed scheduled traffic model, and introduce new design strategies that do not use the traditional protection based schemes, but can still guarantee survivability. We will also use the concept of waveband switching to design cost-effective, robust networks and investigate the problem of traffic grooming in WDM mesh networks.""363439,""Jagersand, Martin"
"364983"	"Janssen, Jeannette"	"Graph models for self-organizing networks"	"The last decade has seen increased interest in the structure of self-organizing networks such as the World Wide Web, social networks,  and biological networks: for example those formed by the interactions of proteins in a cell. This interest has opened up a new direction of study in graph theory, which combines elements of the theory of random graphs, structural graph theory, graph clustering, and infinite graphs. In addition, some of the techniques that need to be employed to model these real-life networks are akin to those used in the natural sciences, but are not part of traditional graph theory: exploration of the data, development and analysis of stochastic models, and validation of the models.My current research interest lies in the analysis of those stochastic graph models that I consider the most promising. In particular, I intend to continue my study of the ""generalized copy model"". This model is based on the evolutionary principle of duplication and error. New nodes joining the graph ""copy"" each of the links of an existing node with a certain probability, and then add a pre-defined number of randomly chosen extra ""error"" links. A promising new tool to analyze this and other models is the study of the infinite limit of the model.A second class of models that hold my interest are models that have a ""geometric"" component. This means that each node corresponds to a point in some Euclidean space, and its attachment to the rest of the graph is influenced by its geometric position with respect to other nodes. Geometric models seem to have many features that correspond to those observed in real- life self-organizing networks. In particular, I would like to study the possibility of ""reverse engineering"" a graph generated according to geometric principles: given a graph that is assumed to be generated according to a certain geometric model, is it possible to estimate the position of the corresponding points? Solving this problem would give rise to new tools to extract community structure in social networks, better Web search methods, and generally give us a better understanding of networked data.""367863,""Janssen, Michael"
"366589"	"Jaseemuddin, Muhammad"	"Routing, mobility management and network middleware for heterogeneous mobile wireless networks"	"Wireless Networks evolve over the years, driven by the development in radio access technology and the diversity of application demands, to 3G cellular networks, ad hoc networks, mesh networks, and more upcoming sensor networks. Diverse and evolving application requirements and usage scenarios indicate that a future wireless network would be a heterogeneous network formed by integrating the above access networks through a common platform of IP network. The objective of the proposed research is to investigate three problem areas in heterogeneous wireless networks: Routing, Mobility Management, and Network Middleware. Each of these problems is critical for future wireless networks. Also, we have opportunity of leveraging our current research and expertise in dealing with pressing issues related to these problems. Routing is a fundamental network service, which becomes complex for wireless networks due to a host of challenges related to mobility, power constraints, wireless link instability, and path interference. Smart antenna and sophisticated power control approach bring opportunities in improving link throughput and reducing the blockage of paths due to interference caused by ongoing communication in multi-hop wireless networks. However, it also brings complexity in the route control and selection process. Mobility management is critical for the performance of maintaining communication of mobile nodes. It becomes more complicated for high-speed and heterogeneous wireless networks. Conventional approach of applications accessing network services is through communication stack and socket API. Heterogeneity, mobility, ubiquity and unique characteristics of above access networks pose new challenges to the access and management of network services, which can no longer be provided by simple conventional mechanisms.  Network middleware is important in providing a platform for the implementation of sophisticated algorithms and the facilitation of application development and deployment.""377034,""Jass, Jana"
"365723"	"Jedwab, Jonathan"	"Aperiodic autocorrelation properties of sequences and arrays."	"The study of the aperiodic autocorrelation properties of binary sequences began in the 1950s and continues to be of central importance in diverse digital transmission systems, including radar information processing, power control for multicarrier wireless transmission, and optical time domain reflectometry. Sequences and arrays whose aperiodic autocorrelations are collectively small in relation to the sequence or array size correspond to signals whose transmission wastes very little energy and which can be recovered efficiently at the receiver.Traditionally, the analysis of autocorrelation properties has concentrated on the periodic case, where the presence of rich mathematical structure admits many theoretical techniques. But the aperiodic case is much more significant in practice, because it arises naturally in so many application contexts. Although conventional wisdom has long been that aperiodic autocorrelations display little structure, and that often the only realistic means of analysis is computer search, there have been several recent indications to the contrary.The short-term objectives of the program are to study particular classical problems in this area for which recent results have opened up new research directions. The long-term objective is to identify emerging methods, and develop new ones, for studying aperiodic autocorrelations of large sequences and arrays. This is becoming increasingly important as modern digital communications systems are less likely to be restricted to the use of smaller sequences and arrays. The general scientific approach will be to combine tools and insights from discrete mathematics, computing science, and digital communications engineering. Where appropriate, experimental computation will be used to reveal patterns, identify key examples, improve intuition, and so point the way to new theoretical results.""381061,""Jeerh, Nisha"
"363430"	"Jullien, Graham"	"Real-time signal processing systems for advanced and emerging technologies"	"My research is based on exploring computational properties of processors built with advanced and emerging electronic technologies, and their application to information, communication technology, and the life sciences. The microprocessors that are contained in the computers we use, and that are embedded in our cell phones, automobiles, appliances, entertainment systems, medical devices etc., all perform computations as fundamental functions in their design. For real-time systems, where streams of data have to be continuously processed, ""embedded"" microprocessors often need to perform many billions of computations every second. The computations mirror the sorts of calculations we might do longhand with paper and a pencil. The design procedures required for fabrication in modern integrated circuit technologies are so complex, that many designers will simply use the available functions that are provided within the design tools. However, in order to use the technologies in the most efficient way possible, it is necessary to examine alternative ways to perform basic arithmetic operations, even down to looking at alternative ways to represent the data. As integrated circuit technology has exponentially improved the packing density of transistors (known as ""Moore's Law""), these basic computational blocks have dramatically increased in speed; however problem features, that used to be neglected, have now become critical and so we need to continually search for solutions to these new problems. One example is the excessive power dissipation of modern high density integrated circuits, that has been responsible for limiting the maximum clock frequency of the microprocessors used in our desktop and laptop computers. In my research we explore alternative ways of building processors on integrated circuits and examine these alternatives in selected application areas. Recent applications of my work can be found in the life sciences, for example: hearing-aid processors and implantable glucose sensors, where extremely low power dissipation is a major constraint, and in very high bandwidth wireless base stations, where very reliable computation involving a trillion basic arithmetic operations per second is required.""363082,""Jumarie, Catherine"
"363624"	"Jurisica, Igor"	"Integrative computational biology"	"Objective: Integrative computational biology is a necessary platform to fathom biology of complex diseases and impact individualized medicine. However, advancing computational tools in isolation is insufficient to impact computational biology. Many theoretically excellent approaches are inadequate for the high-throughput biological domains, due to the scale or complexity of the problem, or due to using unrealistic assumptions. Forming hypotheses and developing computational models of biological systems without the possibility to test them limits the potential to derive realistic models. We propose to improve scalability robustness, sensitivity and specificity of pattern discovery algorithms, and integrate them to support a methodical approach to the systems biology analysis and visualization of high-throughput data in cancer research.   Approach: To address challenges in cancer informatics, we propose to: 1) integrate multiple data resources into biological data marts; 2) design tools for systematic, network-based analysis of heterogeneous biological data; 3) apply the tools to modeling epithelial cancers and initiate validation studies via collaborations. We will augment the tools with statistical and machine learning algorithms to enable probabilistic reasoning, deterministic and sound pattern identification, stability, scalability and robustness. There are three main areas of algorithm development: 1) clustering and data mining algorithms for cancer profiles; 2) graph theory algorithms for protein interaction networks; and 3) integration and visualization of results in the context of networks, to aid in hypotheses generation, interpretation of results, and model creation.      Significance: We aim to develop integrated and scalable algorithms, and apply them to realize intelligent molecular medicine. Results from these analyses will be integrated into the computational models, which will improve their accuracy and thus applicability, and in turn will impact in the respective fields. An important function of this proposal is the training of bioinformatics professionals for which there is still a severe deficit. We will release tools and resources for free academic use.""373650,""Jutan, Arthur"
"364041"	"Kabanets, Valentine"	"Pseudorandomness and complexity"	"Randomness has become an indispensable tool in many areas of computer science: algorithm design, networking, error-correcting codes, cryptography, etc. While the use of randomness is unavoidable for certain applications (e.g., cryptographic constructions), other applications can be made completely deterministic. One of the main challenges of theoretical computer science is to understand the exact power of randomness in computation. In particular, it is a big open question whether every randomized polynomial-time algorithm has an equivalent deterministic polynomial-time algorithm (i.e., can be efficiently derandomized). Answering this question is important for algorithm design, as fast deterministic algorithms are usually preferable to randomized ones. On the other hand, there is a deep connection between trying to derandomize probabilistic algorithms and trying to prove lower bounds on the computational power of Boolean circuits. The latter problem has been actively researched for several decades with the goal of resolving the ""P vs. NP"" question. However, no strong circuit lower bo unds are known for any explicit Boolean function.The goal of the proposed research is to explore various aspects of the basic question: to what extent is randomness useful in computation?""361873,""Kabanza, Froduald"
"362127"	"Kabir, MohammadZahangir"	"Modeling and characterization of imaging detectors for diagnostic medical digital imaging"	"Flat-panel digital X-ray image sensors based on an X-ray detecting medium (either a photoconductor or a phosphor) with a self-scanned active matrix arrays provide excellent X-ray images. These image sensors capture an X-ray image electronically, and hence enable a clinical transition to digital radiography by replacing the traditional film based system. Digital imaging has tremendous implications from a clinical standpoint.  Not only do they allow doctors to retrieve and inspect images quicker, they are highly portable and can be analyzed by specialists who are off-site. X-ray detectors are now commercially available for general radiography and mammography, and are under consideration for use in fluoroscopy. However, there still remain significant scientific challenges associated with dark current, image lag and ghosting, electronic noises, X-ray induced effects and long term stability of the detector. Therefore, there is much fundamental work to be done to enhance detector performance. The objectives of the proposed research are to determine the physical mechanisms causing dark current, transient photoresponse, avalanche gain and noise, X-ray induced change in detector sensitivity (ghosting) and resolution in image detectors, via detector modeling and characterization tasks. A secondary objective of this research is to investigate the material properties of X-ray photoconductors.  The proposed work is vital to understand the physics of detector operation and material properties, to enhance detector performance, and also to identify the important factors that limit the detector performances, which can ultimately lead to the reduction of patient exposure/dose consistent with a better diagnosis. This research work will help X-ray detector community to overcome the present problems in X-ray image detectors, which will influence the rapid commercialization of the X-ray image sensors for cheap medical digital X-ray imaging. This research has direct benefit to Canada's health system as well. Personnel trained by this research will constitute highly skilled workforce for our electronic industries and hospitals.""374415,""Kachanoski, Gary"
"364889"	"Kahl, Wolfram"	"Tool support for relational formalisms in programming and specification"	"The goal of this research is to improve software developers' productivity and confidence in their products by expanding the use of relational abstractions in different stages of software development.  It is well-known that in many application areas, in particular where networks of any kind are involved, concise relation-algebraic specifications are available for many tasks.  This research will open up new ways to make many of these specifications executable, with efficient, automatically tuned implementations.  In addition, similar formulations can be used in a wide range of applications, including ""fuzzy"" decision finding, configuration changes via graph transformations, and communication protocols.  This research will build a unified framework of tool support for specifying, programming, and reasoning in this family of languages.  It will study both stand-alone relation-based specification and programming, and embedding in functional programming, as well as new foundations for functional-logic programming.The unified approach to programming and specification taken by this research will open up more efficient ways of software development, in particular in the increasingly many application areas where software is used in safety-critical environments, and therefore must be certified as correct.Several strands of the more foundational aspects of this research will be used together to enable, as a practical application, generation of high-performance code to be used in medical imaging.  In this project, the theoretical foundations of this research will aid in constructing also the proofs that the generated medical imaging software satisfies its mathematical specifications, stated in the language of signal processing.""362620,""Kahrizi, Mojtaba"
"366430"	"Kaplan, Craig"	"Computer graphics and computational calligraphy"	"We are all familiar with the ways that the computer can be used to store, manipulate, and render text.  We take good type for granted: today, in a few minutes, we can format and print a document that surpasses in quality what would have required years of work at a printing press.  In the process we lose something of the artistry of hand printing, but we accept the trade-off because of the great benefits of efficient communication.Typography is concerned with letters that are highly legible and consistent.  Good type is unobtrusive, almost invisible unless attended to; we should be able to ""see through"" the letters directly to the meaning.However, not all text needs to be type.  There are many historical and modern examples of text that has been deformed, distorted, and downright deconstructed for the purposes of art.  Calligraphers turn fragments of text into inscriptions as beautiful as any painting.  With this kind of lettering, artistic goals such as balance and composition prevail; consistency is much less important, and even legibility may be forsaken.In this project, I will use techniques from computer graphics to explore the calligraphic capabilities of the computer. I am interested in studying three specific styles of calligraphy:  ""representational calligraphy"", in which an inscription has been deformed into a recognizable shape; graffiti tags and murals; and Ambigrams, short inscriptions that look the same when turned upside-down.I will create tools that give computer users, graphic design professionals, and artists the same expressive power with calligraphy that they currently enjoy with type.""367609,""Kaplow, Robert"
"363543"	"Karakostas, George"	"Algorithmic and game-theoretic analysis and implementation of network design"	"Network problems are among the most basic optimization problems. Whether the goods transfered through them are data or commodities they must be designed so as to optimize metrics such as time, costs, or revenue. It has been long known that although these networks may be very different, their modelling can capture their underlying common characteristics. A very recent manifestation of this trend (alongside the older examples of network flow, routing, fault tolerance etc.) is the introduction of selfish behaviour in the study of large data and communication networks such as the Internet; while this is a new paradigm for most computer scientists, it has been studied for more than 50 years by economists and the transportation community. The design of network protocols based on auction mechanism design is another such example. The purpose of this research is to explore the application of these new paradigms in network design from an algorithmic point of view, alongside the study of more `traditional' aspects such as the development, analysis and implementation of better approximation algorithms for network flow and routing problems. In a world that is more inter-connected than ever, the expectation of this research proposal is to have a significant impact on the more efficient distribution of information, goods, services etc.""376068,""Karakul, Mustafa"
"362608"	"Kari, Lila"	"Theoretical aspects of biocomputing"	"In the same way we use letters of the alphabet to write text, and bits 0 and 1 to write computer machine code, the four basic DNA units(adenine,guanine,cytosine,thymine) are used by Nature to write genetic information as DNA strands. The possibility of encoding symbolic information on DNA, and the fact that biochemical processes such as cut-and-paste of DNA strands can be used to perform arithmetic and logic operations,led to the development of the field of DNA computing. The primary goal of my research is to analyze this new way of storing and manipulating data on DNA strands. Indeed, bioinformation and biocomputation are different from their electronic  counterparts in several aspects. First, biodata is not associated to a memory location but consists of infinitesimal DNA strands free-floating in solution. Second, in contrast to the electronic computer, data-encoding DNA strands can interact with each other in un-programmed ways. Third, each data encoding DNA strand is usually present in  millions of identical copies and the bio-operations operate according to statistical laws.This proposal addresses a) the issue of data encoding on DNA by studying  properties that ensure that biocomputations proceed error-free, and  b) lies the foundation of a ""theory of bioinformation"" by studying the algebraic properties of the new concepts defined in a). Another of the proposed research topics is  an investigation into the mathematical properties of genomic bioinformation by  i)  exploring the use of Chaos Game Representation as a  genomic signature, and ii) searching for the optimal tool to measure the complexity of genomes, both of which could lead to methods of quantifying and comparing genomes of various organisms. The last aspect of the proposed research is an exploration in the theory of self-assembly of small components into complex structures as a computational tool, which  could provide insights in the way nature stores and processes information, as well as have implications for   nanoscience and nanotechnology.""385764,""Kari, Lila"
"366191"	"Keast, Patrick"	"Numerical cubature for fully symmetric regions and adaptive methods for PDEs"	"This research programme has two principal thrusts. One direction continues work carried out in thelast five years on the numerical solution of time dependent partial differential equations in onespace variable. Differential equations occur in many physical models, and often the solutionexhibits narrow layers with peaks or sharp variations. In many cases these peaks or areas of rapidchange move in time. For an approximate method of solution to be able to catch these features, thediscrete model must adapt to the solution. Software to approximate to the solutions with reliable errorestimates will be developed. We will investigate the use of high order interpolants in a method oflines context, in order to obtain inexpensive and reliable error estimates. Using anequi-distribution algorithm we will ensure that the error globally satisfies a user-defined errortolerance.The second thrust, which also is a continuation of a long standing research programme, isthe construction of methods for the numerical evaluation of integrals over multidimensionalregions having specified symmetries.  Examples of such regions are the n dimensional cube,sphere, simplex, octahedron, spherical surface, and the whole of n-space with a symmetricweight function. As shown in an earlier paper there is a strong connection between formulasfor the surface of the sphere and for the simplex. It is intended to investigate connectionsbetween formulas for the octahedron and the simplex. A long term plan is to set up softwareto accept as input the symmetries of the region, and a requested polynomial degree, and thento compute consistent structures for the formulas, set up the defining equations, and solvethem.""374343,""Keating, Pierre"
"363119"	"Kennings, Andrew"	"Methods for improving the physical optimization of asics structured asics and fpgas"	"Many issues confront modern IC designers: (1) circuits are large and require scalable algorithms; (2) different design styles are available, including ASICs, structured ASICs, and FPGAs; (3) designs have multiple objectives including wire length, routability and timing. Placement remains a pivotal step in physical design; it consumes significant run-time and largely determines the overall interconnect. Placement has enjoyed a renaissance in research activity in recent years.  The intent of this proposal to improve upon the state-of-the-art in placement for ASIC, structured ASIC, and FPGA design styles.ASIC and structured ASIC placers make approximations to achieve efficiency.  Inaccurate models are used to approximate interconnect and placers may not reserve enough resources for routing. A circuit may be unroutable and design iterations may be required to converge to a feasible  solution of good quality.  The objective of the first part of this proposal is to improve ASIC and structured ASIC placement by: (1) developing fast and scalable techniques for global routing which are capable of handling congestion and blockages; and (2) integrating global routing into our previously-developed placement technology that will result in higher quality placements which are routable.FPGAs have predefined routing structures that are designed to be routable. The emphasis in FPGA placement is on achieving better timing. The second part of our proposal will improve FPGA placement by:(1) investigating smart move generation during placement which will yield higher quality placements with less run-time; and (2) improving packing for cluster-based devices that will also improve results.This research will yield new and novel results that will improve upon the current state-of-the-art in placement for different design styles.""378932,""Kenny, Glen"
"366333"	"Kenny, Patrick"	"Speaker and session variability in speech processing"	"Two of the principal problems in speech technologyare speaker recognition (who is speaking?) and speechrecognition (what is being said?). These problemshave resisted solution because of the numerous typesof variability in speech. For example, the performance ofspeaker recognition systems degrades as a resultof channel and microphone effects and careful modelingis needed to distinguish these nuisance effects from aspectsof the speech signal which are useful in discriminatingbetween speakers. In speech recognition on the other hand,variation betweenspeakers is considered to be a nuisance and the only type of variabilitywhich is of real interest is phonetic variability.This proposal is concerned with developing newstatistical methods of modeling speaker and channel variation using`hidden variables' which I call speaker and channel factors.The techniques that I am developing are purely statistical soit is hard to say exactly what the hidden variables represent butit is reasonable to surmise that they capture features such asa speaker's age and sex, electrical noise in transmission channels andordinary background acoustic noise.""366312,""Kenny, Shawn"
"363701"	"Kent, Kenneth"	"Software tools for reconfigurable platforms"	"One route to achieving better computer performance is to attach peripheral boards with reconfigurable hardware units. By configuring the chip appropriately, one can provide a complete System-on-Chip solution that is optimal for a given set of factors. (i.e. cost and time to market). To improve performance, the most CPU-intensive parts of the program can be compiled into a custom hardware solution for a programmable device. If communication overhead is kept under control, the result should be a speed improvement as well as a reduction in development time.   However, the challenges for anyone attempting to implement such a mixed software-hardware solution are enormous. The programming languages, the compilers, the simulation tools and the run-time monitoring tools all provide woefully inadequate assistance. In general, the tools rely heavily on an expert system designer to guide and direct the process. The payoffs would be enormous if development tools could guide the process and make effective use of a hybrid microporcessor - configurable unit platform.   I propose to work on the following aspects of the problem:1) Decomposing a program into those components which are best suited to be sped up by a configurable hardware implementation, and those remaining components which are best handled by traditional software.2) Developing programming language constructs and techniques which facilitate #1.3) Enhancing existing compilers to support the new programming language constructs.4) Developing simulation tools which wil enable program decompositions to be tested and evaluated.   Assorted applications available from my previous work relating to computational biology, co-designed virtual machines, and logic synthesis will be used to validate the proposed research.""379835,""Kenth, Sukhdeep"
"364457"	"Keshav, Srinivasan"	"Foundations and applications of tetherless computing"	"Tetherless computing is a style of computing where smart mobile devices opportunistically communicate withcentralized servers over heterogeneously administered wireless networks. It enables numerous applications thatcheaply connect people to information sources and to each other. For example, it allows villagers in remotecommunities to gain Internet access by opportunistically using cars, buses, and trains that pass through. Asanother example, it allows users of 'smart phones' to receive email through cellular modems, but bulky emailattachments opportunistically from nearby wireless access points.The component elements of tetherless computing--broadband wireless networks, server clusters, andincreasingly powerful mobile computing devices--are already in place or are rapidly becoming available. Yetmany critical problems remain. Broadly speaking, computing systems built using these elements lackproperties such as seamless connectivity, disconnection-tolerance, ability to use multiple network interfaces,ease of programming, and security. Indeed, what is missing is an infrastructure that is cost-effective, robust andefficient; an infrastructure that people can afford, yet trust with their data, their computing needs, and in somecases, their lives.My research addresses the practical and theoretical problems involved in establishing thefoundations of an affordable, efficient, secure and robust infrastructure for tetherless computing. I am alsointerested in applying my research to create novel applications, especially those that benefit rural communities in developing countries.""364456,""Keshav, Srinivasan"
"362005"	"Khazaka, Roni"	"Design automation for managing the complexity of mixed-domain microsystems"	"Electronic design automation (EDA) and computer aided design (CAD) tools were the key enablers that allowed the microelectronics industry to flourish into a multi-billion dollar industry world wide, with Canada at the forefront of this revolution. CAD tools have allowed designers to manage the complexity of very large scale designs containing millions of transistors, and to significantly reduce the design cost and time to market. As a result, microelectronic components are currently an increasingly significant part of most consumer products ranging from computers to cars to toaster-ovens, with significant growth expected particularly in personal wireless communications products and digital entertainment products in addition to the traditional desktop/notebook computers. This is due to important technological advances which have resulted in unprecedented opportunities as well as significant challenges. Advances in lithography have lead to smaller device sizes well into the nanometer range with commercial 45nm devices on the horizon. Packaging technology is advancing at a very fast pace in order to address integration, signal integrity and power distribution issues. Finally novel technologies and devices based on nanotechnology, micro-electro-mechanical systems (MEMS), opto-electronics technologies and bio/chemical sensors are being integrated with microelectronics in order to form Microsystems with wide ranging potential applications most notably in the biomedical technology area. While these technological advances represent an important opportunity for economic growth, and for improving the lives of Canadians, they also result in a significant increase in the complexity of design. This research program consists of coordinated research activities leading to design automation tools and methodologies for addressing this increasing design complexity with the aim of enabling the Canadian industry to take full advantage of the potential of advanced Microsystems.""377419,""Khedri, Ridha"
"361810"	"Khouas, Abdelhakim"	"MultiSPICE:  un simulateur de circuits analogiques optimisé pour les applications à simulations répétitives"	"Dans le but de répondre aux contraintes économiques de coût, de délais de réalisation et de qualité, les concepteurs de circuits intégrés utilisent des outils d'aide à la conception (CAO). Le simulateur de type SPICE est l'outil par référence pour un concepteur de circuits analogiques, il permet de simuler le comportement d'un circuit analogique et il est utilisé par plusieurs autres outils de CAO. En plus du domaine analogique, SPICE est aussi de plus en plus utilisé dans le domaine numérique en raison des effets parasites qui sont de plus en plus importants dans les nouvelles technologies submicroniques (90 et 65nm). L'inconvénient majeur de SPICE est le temps de simulation. Le but de ce projet de recherche est de développer des techniques d'accélération des simulateurs SPICE utilisées dans d'autres applications qui nécessitent des simulations répétitives d'un même circuit avec de petites modifications, c'est le cas par exemple, des outils de synthèse analogique, des simulateurs de pannes et des outils d'analyses Monte-Carlo. Les méthodes d'accélération seront développées en fonction des caractéristiques spécifiques à chacune des application visées et en exploitant le fait qu'on effectue des simulations répétitives d'un même circuit. Les différentes approches que nous envisageons d'exploiter sont : 1) réutilisation de la structure du circuit nominal, 2) utilisation de la communication inter processus, 3) développement d'algorithmes pour un choix efficace de solutions initiales, et 4) réduction du nombre d'itérations par des algorithmes d'arrêt de la simulation avant convergence basées sur des métriques d'approximation de la solution finale. Ces techniques d'accélération seront implémentées dans un simulateur existant pour réaliser un nouveau simulateur MultiSPICE optimisé pour les simulations multiples. Pour valider le tout, nous utiliserons ce simulateur comme noyau de simulation pour implémenter les applications citées ci-dessus. Grâce à l'amélioration des outils à la disposition des concepteurs industriels, MultiSPICE aura un impact significatif sur le développement de nouveaux produits électroniques fiables et à faible coût. Un domaine qui revêt une importance croissante pour le secteur industriel canadien.""380367,""Khouchane, SofianeZacharie"
"363130"	"Kiringa, Iluju"	"Managing peer database transactions"	"Many of the data sources upon which today's education, science, business, and government depend have been generating increasingly heterogeneous, networked, and vast amounts of data stored in a variety of formats. Our past work has contributed to building networks of relational databases (called peer databases, or simply peers) that make such a gigantic amount of data available for sharing. Peer databases share data using a peer-to-peer (P2P) network. A P2P network is point-to-point  communication infrastructure which is used by peers to establish acquaintances prior to sharing data. Ensuring data sharing across heterogeneous boundaries in a semantically meaningful way has been at the core of our research.  In collaroration with others, we have produced a prototype peer database management system (PDBMS) that uses the so-called mapping tables to solve the heterogeneity problem. Mapping tables are correspondences between data values as well as between schema elements at different peers. Using mapping tables, a PDBMS typically establishes acquaintances with remote peers, answers queries using data originating in acquainted peers, and propagate local updates to remote peers. Our research program has two main objectives which are to design advanced transaction models for the PDBMS setting, and to implement these transaction models on top our existing PDBMS infrastructure. Since PDBMSs are similar to traditional multidatabase systems in which the local data sources are not integrated, but rather coordinated, we closely examine the transaction models proposed for traditional multidatabase systems and relax many of their assumptions, as well as we add new ones that are more suitable for the PDBMS setting. The main issues we deal with are the relationship among local peer transactions, the consistency of the latter, concurrency control and recovery across a large amount of peers.  We aim at making the vision of  end-user oriented Internet data sharing possible. Users will only need to focus on their local data while the intricacies of the interoperability with remote peers will be handled as automatically as possible at the system level.""366613,""Kirk, Andrew"
"364405"	"Kirk, Andrew"	"Nanoplasmonic optical biosensors"	"Surface plasmon resonance sensing allows the detection of very low concentrations of a wide range of different molecular species of interest. It has important application in many fields. One example is medical diagnostics where it can be used to sense the presence of proteins that indicate that the patient has a medical condition well in advance of other symptoms being visible and thus enabling early treatment. In drug screening it can be used to rapidly identify candidate compounds (and so is of great value to the Canadian pharmaceutical industry) and in environmental sensing it may be used to detect pathogens in the public water supply. Here we propose a research program which will improve the sensitivity, specificity and practicality of surface plasmon resonance sensing. To do this we will make use of periodic optical nanostructures in order to increase the intensity of the electromagnetic fields at the surface of the sensor and hence increase sensitivity. We will combine recent advances in the analysis of isolated nanostructures with our own research into periodic nanostructures in order to obtain the maximum benefits of both. We will also incorporate quantum dots in order to increase specificity (i.e. fewer false-positive detections) and will investigate techniques to make the sensing systems better integrated and lower cost. This program will provide training for 4 PhD students and 3 Masters students in a multidisciplinary environment in which they will gain expertise in electromagnetic modeling, micro- and nanofabrication, instrumentation, biochemistry and biomedical engineering.""364392,""Kirk, Donald"
"362521"	"Kondrak, Grzegorz"	"Word form similarity computation and application in natural language processing"	"In natural language processing, words are often treated as abstract entities divorced from their actual form. However, the actual orthographic or phonetic form of words is an important feature that can be utilized in various tasks. The algorithms that I previously developed for computing word similarity have proven to be effective in a variety of applications, including identification of confusable drug names, and matching person names across different scripts. In the current proposal, I describe how I plan to achieve my two principal research goals: continue developing new measures of similarity, and improve the state-of-the-art in several areas involving sequential processing. With respect to the first goal, my specific objectives are: to propose data-driven learning schemes that integrate many-to-many correspondences and sensitivity to context and to design methods that incorporate linguistic knowledge and require no training data. With respect to the second goal, the areas in which I plan to make an impact are: letter-to-phoneme conversion, machine transliteration, statistical machine translation, and cognate identification. The development and application of word similarity methods leads to a number of challenging research problems, and promises to bring a unifying perspective on several distinct tasks that have so far been perceived as unrelated. Based on the experience of the last few years, I am convinced that this line of research will continue to have an impact beyond the areas of its original application.""363294,""Kondratieva, Margrita"
"364984"	"Konemann, Jochen"	"Algorithmic game theory and approximate network design"	"The last decade has seen renewed interest in classical game theory -- a field that studies the behavior of rational self-interested agents in competitive environments. The current focus is on algorithmic aspects of games: Can we find Nash equilibria efficiently in certain games? What is the loss in social welfare (the price of anarchy) if players are allowed to act independently and greedily? Are there incentive mechanisms that force players to behave in a truthful and fair manner?  For many of these questions, ideas from combinatorial optimization and theoretical computer science provide key new insights.  Conversely, Gupta et al. [STOC '03, FOCS '03] showed that game-theoretic cost-sharing ideas can be fruitfully applied to the design and analysis of approximation algorithms for network design problems.  The long-term goal of my research is to further explore the relationship between game theory, theoretical computer science and combinatorial optimization.I have contributed to this area in joint work with S. Leonardi and G. Schaefer. We studied a game-theoretic variant of the Steiner forest problem, where each player j, out of a set of k players, strives to connect the vertices of her terminal pair (s_j, t_j) in a given undirected, edge-weighted graph G.  We show that a natural adaptation of the primal-dual Steiner forest algorithm of Agrawal, Klein and Ravi [Siam J. of Computing, 1995] and Goemans and Williamson [Siam J. of Computing, 1995] yields a 2-budget balanced and group-strategyproof mechanism for this game.  We also showed that the mechanism-design ideas used in our work give rise to a new linear programming relaxation for the Steiner forest problem which is strictly stronger than the well-known undirected cut relaxation for this problem. As short-term goals, I plan to address the following questions: can we use the new Steiner forest LP relaxation to obtain better Steiner forest approximation algorithms? Can the mechanism design techniques used for the Steiner forest problem be applied to a larger class of network design problems? Can we obtain tighter LP relaxations for these problems?""364396,""Konermann, Lars"
"365485"	"Konstantinidis, Stavros"	"Theory and applications of automata and codes"	"The proposed research program addresses fundamental questions regarding code properties and automata (including transducers), and their applications in areas like reliable coding for data communication,  error modeling, spelling correction, and DNA computing. Code properties include synchronization, error detection and correction, as well as properties motivated by reliability issues in DNA computing. Error models are abstract representations of physical channels and can, in many cases, conveniently be described by automata and transducers. This specific application of automata raises new questions about the structure of automata, some of which belong to the core of the theory of these objects. Unlike the classical theory of error correcting codes, which is mainly concerned with block codes, this research focuses on variable-length codes for which little is known with respect to their behaviour in the transmission/storage over noisy channels. My long term goal is to classify and identify meaningful code properties, and to find methods for the construction and quality evaluation of codes with respect to a given set of constraints. My proposed research consists of the following parts.1. Provide explicit constructions of maximal solid codes. These are codes with remarkably strong synchronization properties.  2. Study certain abstract theoretical concepts of automata and formal languages that can be used as tools in attempting to answer questions with a specific meaning in more applied areas.3. Design efficient algorithms for testing code properties of regular languages and computing maximal error-detecting capabilities of such languages.4. Apply the theoretical tools and algorithms in the areas of  DNA computing, error correction, and code design. Moreover, implement the algorithms and make public web interfaces.""381436,""Konsztowicz, Sue"
"364757"	"Koutsakis, Polychronis"	"Traffic control and resource allocation for wireless cellular networks"	"The estimated number of mobile phone users worldwide by the end of 2006 is two billion; this means that wireless cellular networks are and will remain the dominant type of wireless networks. The proposed research program will focus on wireless cellular networks. Our research will provide a key element for next generation wireless networks: a framework which will bridge the gap between the conflicting needs of the multimedia users for Quality of Service identical to that of wired networks, and of the wireless provider companies' for maximization of their revenues.To achieve this goal, next generation wireless cellular technologies will need to incorporate new sets of traffic control procedures, which will not carry the main inadequacy of existing approaches for 2nd and 3rd generation wireless networks, i.e, traffic control procedures should interact instead of attempting to solve problems independently. This is even more important as new multimedia services (e.g., videoconference, telemedicine) are often very sensitive to delays and information loss, problems inherent in the wireless environment because user mobility causes traffic conditions in the cells to change very quickly and because of transmission errors in wireless channels.Differentiated service contracts, i.e., contracts based on the Quality of Service the user is willing to pay for, have already started to be adopted by many wireless carriers. The proposed research will use new traffic modeling, mobility modeling and user satisfaction modeling techniques, combined with new traffic control and bandwidth allocation algorithms, to achieve an optimal tradeoff between user satisfaction and wireless providers' revenues.For all of the above reasons, the proposed work has a strong potential for generating results which will be of significant interest to wireless carriers, and consequently will be of equal benefit to mobile multimedia users.""373978,""Kouwen, Nicholas"
"365431"	"Kremer, Stefan"	"Sequential pattern induction for bioinformatics"	"The proposed research applies a number of machine learning methods to problems of bioinformatics.Emphasis is on learning algorithms that learn to classify, generate and transmute sequences of symbols basedon example sequences. Over the past 10 years a number of new approaches to inducing sequences have beenproposed by myself and other researchers in the field. These have been sparsely applied to selectedbioinformatics problems and analysed.The objectives of this research are to discover which techniques work best on which problems, and how thesemethods can be adapted to the unique constraints arrising from practical problems on real biological datasets.This will allow scientists working in bioinformatics to judiciously apply these techniques to problems ofinterest as well as provide some insight into alternative approaches that might not otherwise be considered. Itwill effect an easier transfer of knowledge from research in machine learning to practical bioinformaticsproblems.The research will focus on (but not be limited to) the following techniques: artificial neural networks,automata models, formal grammars and hidden Markov models.The research is a follow-up to-- and extension of-- previous work in this area at a much enhanced scale. Thisincrease is due in part to the recent accredidation of my department to grant PhD degrees and three PhDstudents under my supervision.""365519,""Krems, Roman"
"366036"	"Krivodonova, Lilia"	"High-order adaptive methods for hyperbolic conservation laws"	"This proposal is aimed at the development of new high-order adaptive algorithms for the numerical solution of hyperbolic conservation laws using discontinuous Galerkin methods (DGM). These methods are becoming important  for the computation of  solutions of large scale problems modeled by partial differential equations. Their attractive features are an element-wise conservation property, high-order accuracy, easy adaptivity, and simple parallel solution procedures. They are used in such diverse applications as fluid dynamics, aeroacoustics, electromagnetics, and optics. This work will lead to more accurate computer simulations and shorter design times.By representing the solution via a high-order basis, the DGM is capable of computing a highly accurate numerical solution. For the method to be competitive, it must do so efficiently and rapidly. This proposal will develop several tools in that direction. Adaptive algorithms concentrate computing resources in regions where they are most needed. Thus, fewer cells are needed to achieve the prescribed accuracy. New adaptive algorithms capable of combining two types of adaptivity: h-refinement (mesh enrichment) and p-refinement (order variation) will be developed.  New stabilizing (limiting) techniques will be investigated for such fully adaptive schemes. We will further seek efficiency by developing a time-adaptive scheme for the method of lines with explicit Runge-Kutta time integrators. By using a local time step on each cell, the scheme will need an overall fewer number of function evaluations. Thus, the simulation will be completed in a shorter time. We then will develop a discontinuous Galerkin method on Cartesian grids with embedded boundaries. Such grids are easier to construct and faster to compute on than unstructured triangular/ tetrahedral meshes. Training of graduate and undergraduate students will be an important component of the proposed program.""371048,""Krizan, SylvaJana"
"366236"	"Kruchten, Philippe"	"Capture and visualization of software architecture design decisions"	"This software engineering research program is about bringing design decisions, and in particular architectural decisions for large and long-lived software-intensive systems, to the front stage of the engineering process. The ultimate objectives are  to support: 1) Reasoning about the decisions, 2) Supporting the decision process, 3) Providing a design rationale, 4) Supporting design reviews, in particular incremental design reviews, 5) Maintaining cohesion of large distributed development during its maintenance and evolution, 6) Impact analysis of proposed changes, and 7) Harvesting architectural experience from a project: reusing knowledge rather than just code. For many years software designers and architects have focused on the consequences of the design decisions they made, capturing them in the ""design"" or the ""architecture"" of the system under consideration, using often graphics such as UML diagrams. Representations of software architecture were centered on views, as captured by the IEEE 1471 standard, or the usage of an architecture description language. But doing so, we lose much of the knowledge that is attached to the decision itself, and to the decision process: rationale, eliminated alternatives, etc. By just looking at the design of a system, or in the code itself, we're likely to miss the dependencies between decisions, the scope of certain decisions, how the decisions are related to the requirements and needs, or to organizational constraints. As the system evolves and its original designers move on, the system is harder and harder to maintain, precisely because it is difficult to evaluate the impact of certain decisions. The solution often suggested is to diligently keep track of the Design Rationale: an explanation of the reasons of the choices, but this is hard to do in practice, as it requires additional work that has no immediate visible benefit or reward to the designer at the time the decision is made, and therefore it is almost never done in practice. Moreover, when designing a new system, it would be more useful often to be able to see sets of interconnected design decisions together with the actual design itself: Architectural Knowledge = Architectural Design + Architectural Design Decisions.""373544,""Kruczek, Boguslaw"
"366375"	"Ktari, Béchir"	"Sécurité et fiabilité des logiciels"	"Une des grandes préoccupations actuelle demeure la sécurité et la fiabilité des logiciels : Les utilisateurs désirent des logiciels sûrs qui effectuent efficacement les tâches qu'on leur demande sans faire d'opérations malveillantes comme transmettre des données critiques à des personnes non-concernées. Face à la multiplication des menaces informatiques, toutes les instances fédérales et provinciales du Canada s'affairent à sécuriser leurs systèmes d'information. Ces dernières années ont vu apparaître, tant au niveau fédéral qu'auniveau provincial, des centres d'étude et de prévention de la sécurité informatique. Les risques sont réels et la moindre erreur peut engendrer des conséquences indésirables.Ma recherche s'inscrit dans ce contexte de sécurité et de fiabilité des logiciels et vise à proposer de nouvelles solutions pour les systèmes actuels (ordinateurs classiques) et les systèmes futurs (comme les ordinateursquantiques). Plus précisément, ma recherche se concentre à la réalisation de deux objectifs:1- Le premier vise à créer un pont entre le domaine de l'informatique quantique et la certification de logiciels via deux piliers principaux : le développement d'un assembleur quantique qui permet le support naturel de lacertification de logiciels et l'adaptation et la découverte de méthodes de certification de logiciels appropriées pour le domaine quantique.2- Le deuxième vise à concevoir et à implanter une architecture, basée sur une analyse statique de code, de certification de logiciels à travers la génération par un compilateur-certificateur, outre du code assembleur, d'un modèle abstrait et certifié du programme à analyser (à compiler).""365160,""Ku, Hyejin"
"366586"	"Kunz, Thomas"	"Wireless mesh network performance: throughput and fairness"	"Wireless mesh networks are a new, disruptive technology to provide high-speed last-mile access to rural and urban users. Toronto Hydro, for example, announced that they will build Canada's largest WiFi mesh to provide low-cost Internet access to Toronto residents. While the core building blocks of such networks (radio transmitters and basic networking protocols) exist, many problems remain. In the past few years, my group has worked on a range of multi-hop wireless network topics. In conjunction with a small Ottawa startup company, we built one of the few wireless mesh testbeds in Canada. In the proposed research, we will build on this basis to address one of the core challenges: providing end-users with fair and high-bandwidth (i.e., high throughput) access to the wireless mesh and through it to the Internet.1) Throughput: wireless links are typically bandwidth-constrained and subject to interference. Most MAC and routing protocols fail to completely utilize the network resources, resulting in lower than possible aggregate end-to-end throughput. A few analytical models exist to determine the maximally achievable throughput rates, but cannot be applied directly due to their centralized nature and high computational complexity. We will develop approximations to such models and use them to derive distributed protocols that implement the key ideas to maximize end-to-end throughput.2) Fairness: users should achieve their ""fair"" share of network resources when accessing the mesh. Yet using current protocols, bandwidth sharing is very uneven and location-dependent, with users closer to the fixed infrastructure and with connections with lower RTT getting better service from the network.The two topics are intrinsically linked: ensuring fairness will come at an aggregate throughput penalty. We will explore these topics using analytical model, simulation studies, and through implementation in our testbed. The goal is to develop better network protocols from the MAC to the transport layer and to evaluate them through simulation and in our testbed.""379184,""Kunze, Eric"
"365024"	"Kutulakos, Kiriakos"	"Frontiers of three dimensional photography"	"The long-term goal of this research is the automatic construction of geometric representations that accurately capture the three-dimensional shape and appearance of real, highly-complex scenes in the physical world---objects, indoor/outdoor environments, and people. Toward this end, the proposed work aims to push forward the state of the art in the field of ""3D photography"" by tackling, among others, two classes of everyday scenes for which no general (or practical) 3D photography methods exist: (1) objects made of transparent, refractive, or highly-reflective media, such as glass or metal, that cause complex light propagation; and (2) natural scenes with high geometric complexity, such as hair, fur and dense foliage.The scenes under investigation require new approaches to 3D photography and pose challenges both at a fundamental theoretical level (modeling image formation, solving inverse problems, establishing reconstructibility conditions) and at a practical level (efficient acquisition techniques, robust algorithms). To attack them, the PI will pursue two general directions recently developed within his research group: (1) design and analysis of Light-Path Triangulation algorithms for reconstructing specular scenes, and (2) design of high-resolution 3D photography algorithms that rely on control of a digital camera's focus and/or aperture.This research will establish a theoretical foundation and a computational framework for designing 3D computer vision algorithms able to operate in a wider range of scenes than is currently possible. Moreover, tools and insights gained through this work are likely to lead to applications that impact photography enthusiasts at large (e.g., producing high-quality photos by controlling the aperture of a single-lens reflex camera).""379379,""Kutz, Susan"
"361046"	"Langlais, Philippe"	"practical statistical machine translation"	"Currently, the dominant Statistical Machine Translation (SMT) paradigm capitalizes on pairs of sequences of words, namely phrases. Despite its success, phrase-based SMT has several well-understood shortcomings among which: no (or little) syntactical or semantic sensitivity, bad generalization capabilities, and a strong dependency on large parallel data. This severely limits in practice the deployment of SMT as a standalone technology. In this NSERC Research Program, I want to tackle these limitations by investigating three avenues.In the first theme, I will investigate ways of building translation engines based on dependency n-grams: a head word (called governor) and the set of its dependents in a sentence, as identified by a dependency parser (DP) or a comparable device. DP-ns have several advantages over standard phrases including a better compromise between lexical expressiveness and generalization power.The second theme of this research proposal is intended to solve part of a key challenge in the MT community: demonstrating that semantic information benefits SMT. As intuitive as this might appear, we have yet to demonstrate this empirically.The third and last avenue pursued in this proposal is the one of alleviating the strong dependency current SMT technology has on the availability of parallel training material. Parallel corpus enrichment as well as improved parallel data mining will be investigated.""380638,""Langley, HughGregory"
"363008"	"Langlois, Pierre"	"Méthodologie  de conception pour processeurs spécialisés"	"L'implémentation d'algorithmes de traitement d'image et de traitement du signal en temps réel passe souvent par la conception d'un processeur spécialisé. Les besoins en calculs, les contraintes de synchronisation, la réduction des coûts et les limites en consommation de puissance pour ces applications écartent habituellement les solutions purement logicielles implémentées sur un processeur à usage général.Pour implémenter le système, différents choix architecturaux se présentent au concepteur. On compte entre autres les coprocesseurs, les processeurs configurables et les multiprocesseurs. Étant donnée une description dans un langage de haut niveau d'un algorithme à implémenter, l'évaluation d'options architecturales nécessite des connaissances poussées en conception matérielle. À cause de la prévalence écrasante des architectures de type Von Neuman et Harvard, une proportion relativement faible de concepteurs a les compétences nécessaires pour développer des processeurs sur mesure et exploiter leur potentiel à fond. Il y a donc un besoin pressant à la fois pour du personnel ayant des compétences avancées en conception matérielle, et pour des outils de conception au niveau système (Electronic System Level design - ESL) facilitant la création automatique d'architectures spécialisées à partir de descriptions de haut niveau.Le programme de recherche proposé a pour but principal la découverte et le développement de nouvelles méthodologies de conception de processeurs spécialisés. Il a comme but connexe la formation de chercheurs et d'ingénieurs capables de rencontrer les besoins de l'industrie canadienne en conception de processeurs spécialisés.""365548,""Langlois, Robert"
"362655"	"Lawford, Mark"	"Formal methods for safety critical real-time control systems"	"Increasingly public safety is dependent upon the safe operation of computer control systems in anti-lock brakeson cars, fly-by-wire controls on airplanes, medical devices and other systems. These systems rely upon thecorrect operation of their software to provide previously unimaginable benefits to humanity. Therefore wewould like to have some sort of ""proof"" or ""formal verification"" that the software will operate correctly and notendanger the public. Unfortunately, construction and formal verification of safety critical real-time software isvery costly, requiring significant time of specially trained personnel. As a result, mathematical proofs ofsoftware correctness are typically only used by industry in the most safety critical control software when it ismandated by a regulatory body to insure public safety. To make the construction of provably correct real-timesoftware less labour intensive and more cost effective, the proposed research will create a library ofpre-verified software components that meet commonly occurring real-time requirements. The library willconsist of software components and portable theorems about their properties. By leveraging the proofs of theproperties of the individual sub-components, engineers will be able to easily prove properties of largercomponents as they are constructed from the sub-components. The portable theorems will appear in an XMLbased standard to allow the use of multiple verification environments so that the most appropriate environmentcan be used to establish a property of a design. Theorem provers can be used for proofs of the software's logical correctness while computer algebra systems can be used to check how the software will interact with acontrolled system's continuous dynamics. By allowing the designers to reason at a higher level, using theappropriate tools, the pre-verified real-time software library will open up the possiblity of creating formallyverified software systems to more companies at a more reasonable cost.""365243,""Lawless, Jerald"
"363729"	"Lawrence, Ramon"	"Dynamic database integration for knowledge sharing"	"Organizations are drowning in data. The basic question is how can we manage all this data and turn it into valuable information?  Data is scattered around different systems, in different formats, and controlled by various organizations. It is like a giant jigsaw puzzle.  In order to answer questions, you need pieces from various places, and only when the pieces are put together can meaningful results be produced.  The key to turning data into information are the tools that allow you to construct the whole picture from the individual pieces.   This proposal aims to develop the algorithms and associated tools to help all users find, query, and integrate data from many data sources.  The approaches are applicable to the integration of medical information, business data, and the dissemination and integration of the vast amounts of scientific data.   Database integration is the process of combining data from multiple data sources into a single unified information system.  Although advances have been made including integration architectures and schema matching algorithms, database integration remains a challenging and human-intensive problem.  One of the major challenges is the construction of an integrated view combining the information in all data sources.    The proposed research is to develop techniques for supporting rapid, dynamic integration of database systems.  The research is in two areas: query processing and optimization for integration systems and ontology-based schema matching and merging.  Evaluating and ordering joins in distributed query processing is critical to achieving high performance.  Research will be performed on how to improve the hash-based early join algorithms and utilize them with optimizers specific for the integration environment. The second area of study is using ontologies (domain representations suitable for computer processing) to improve the accuracy of matching schemas.  The goal is to support iterative schema merging and refinement to produce an integrated view.  The research will use the previously constructed Unity integration architecture for experimentation and evaluation of join algorithms, query optimization heuristics, and schema merging approaches.  ""361730,""Lawryshyn, Yuri"
"364149"	"Lemire, Daniel"	"Data mining and OLAP over sequential data"	"Researchers work over increasingly large distributed data sets. Literary researchers have access to most classical works in digital format. There are terabytes of biological data freely available on the Web. Sequential data such as text or time series is common and has been identified as an area of critical importance  by the database research community.Mining patterns in terabytes of time series or  text with user-driven techniques, is an important open problem. We want to provide tools to users so that they can interactively aggregate sequential data into views, as in On-Line Analytical Processing (OLAP). To do so, we need well adapted hierarchies over the data so that drill-down and roll-up queries can be supported. We need robust time series indexing. Finally, we need a good model for sequential databases: should we complement existing information retrieval tools or extend current databases? In either case, the amount of data is so large that we need good strategies to preaggregate the data.In  OLAP, attribute values  are aggregated by star schema hierarchies (store/city/province, client ID/age group). Words can be aggregated in phrases, sentences and paragraphs or by hyponymy (car/vehicle) though there are issues with word sense disambiguation. An equivalent for time series is segmentation. Segmentation divides the data into intervals where it behaves approximately according to a simple model (e.g. constant, linear, quadratic, unimodal, monotonic, convex). Segmentation is  a hierarchical process since intervals can be further subdivided.Recently, some progress has been made in time warping indexing.  When comparing two time series, time warping locally accelerates or slows time in one time series to compute robust similarity measures.""378545,""Lemire, Francis"
"361255"	"Leung, Albert"	"Cost-effective assembly of three-dimensional microstructures using a wirebonder"	"Almost all integrated circuits are fabricated using planar technology. As a result, integrated circuits components occupy only the top 5-20 microns of the silicon substrate.Microelectromechanical Systems (MEMS) research often uses well-established integrated circuit processes. Unfortunately, the planar structure limits MEMS devices performance, and expanding to the third dimension is necessary. One approach is to elevate planar structures out-of-plane to form three-dimensional (3-D) microstructures. Generally a mechanical force is required to carry out the lifting. Researchers have used thin film stress, magnetic force, electrostatic force, centrifugal force, and surface tension to lift 3-D microstructures out of the silicon surface plane. While these approaches have different levels of success, their applications are limited to the research laboratories in a small scale. Applying them in the production environment remains a major challenge. In this research, we will develop a cost-effective means of elevating 3-D microstructures. Specifically we will develop techniques and device structures which can exploit the mechanical motion and force of an automatic wire bonder in the assembly of 3-D microstructures. Automatic wire bonding is a standard integrated circuits packaging operation which provides electrical connection to the silicon chip. It is a mechanical process in which gold or aluminum wires are welded onto the bond pads of the silicon chip and the package. The wire bonding machine moves the package and silicon chip in the X and Y directions to align the bond wire to the bond pad, and provides a vertical bond head motion and force to complete the welding process. Utilizing this motion and force capabilities to lift the 3-D microstructures has the advantage that only standard equipment (automatic wire bonder) compatible with volume production is required. Applications of this cost-effective 3-D microstructure include accelerometer, optical components, anemometer, magnetometer, low-cost radio frequency coil structure etc.""381074,""Leung, Amy"
"366611"	"Leung, CarsonKaiSang"	"Efficient mining of constrained patterns"	"With the advance in technology, a flood of data can be produced in many applications such as wireless sensor networks. Consequently, we are drowning in data but starving for knowledge. To be able to ''drink from a fire hose'' (i.e., to make sense of the flood of data), methods for extracting useful information from the flood of data are in demand. This calls for data mining, which refers to the search for implicit, previously unknown, and potentially useful knowledge (such as frequent patterns and exceptional patterns) that might be embedded in the data. Over the past few years, I have developed interactive algorithms for finding frequent patterns satisfying a certain class of constraints. The algorithms are enhanced with some optimizations such as a light-weight structure that provides sharper bounds on the frequency of frequent patterns. Furthermore, I have also developed a novel tree structure for effectively capturing and updating the contents of the database in an incremental environment. Along this direction, I propose to build a more efficient, user-friendly, and powerful mining system such that it (i) incorporates users' preferences, (ii) allows users to visualize the data, (iii) permits users to change the mining parameter and/or constraints during the mining process, (iv) provides users with comprehensible feedback in a ''real-time'' fashion, (v) discovers and exploits any unknown properties of constraints to avoid unnecessary computation and to further speed up performance, and (vi) keeps a good fusion of theory and practice via the exploration of real-life applications (e.g., mining from market basket data, Web click stream, agricultural/meteorological data, and medical/biomedical data). The discovered frequent patterns reveal the common trends; the discovered exceptional patterns trigger alarm bells for prevention of outbreaks or disasters. In the long term, this proposal can also be extended to handle various types of data, ranging from traditional alphanumeric data to non-traditional multimedia data, from structured data to semi-structured XML data, and from traditional ''static'' transactional data to ''dynamic'' streams of continuous data.""380579,""Leung, Clara"
"361961"	"Leung, Cyril"	"Multimedia wireless communication systems"	"During the past decade, a major development in the telecommunications industry has been the explosive growth of the cellular telephone market. There are currently over two billion subscribers world-wide. In the US alone, the cellular service industry generates an annual revenue in excess of one hundred billion dollars. Even though the demand for cellular phone service shows no sign of slowing down, the industry is preparing for the next wave of innovation which includes wireless services such as high-speed data, audio/video streaming for web conferences, news, lectures, music, movies, etc. These different applications require inherently different Quality of Service (QoS) levels. The challenge is to design systems capable of supporting applications which are both appealing and affordable.Beyond the cellular network, there is a whole range of emerging wireless communication technologies. Ultrawide bandwidth technology is being proposed for very high speed data transfers over short distances. New generation wireless local area networks are being designed to support higher bit rates while offering improved security. Standards are being adopted for wireless metropolitan area networks which will establish wireless local loops to the home. Mobile ad-hoc and sensor networks are emerging as potentially important for specific applications.This research program will investigate a number of issues related to the design and performance analysis of multimedia wireless communication systems. It will train highly qualified personnel in the telecommunications field, one in which Canada has historically made significant research contributions. The results from this research will assist in the development of innovative communication services to the consumer at an affordable cost.""377715,""Leung, Debbie"
"361717"	"LeytonBrown, Kevin"	"Competitive Multiagent Systems: Bridging the Gap between Theory and Practice"	"Competitive multiagent systems are environments in which two or more self-interested agents interact. It is this ingredient of self-interest that makes a multiagent system ``competitive"": agents' desires cannot be relied upon to coincide.  Game theory is the dominant approach used to study such environments.  Unfortunately, a significant gap still exists between theory and practice.  For example, game-theoretic models are typically very small and are built on the assumptions that agents are omniscient (hold no false beliefs) and utility-maximizing (computationally-unbounded).  Even if such an agent did exist, existing theories would not tell him how to reason about more limited competitors.  Approaches to resource allocation usually assume that goods for sale cannot be resold and are unique (i.e., they are available only in the one-shot market being designed), that agents face no transaction costs or budgets, that markets can be redesigned without any constraints, and that agents know their valuations for the goods being allocated.  In practical settings of interest, some or even all of these assumptions can fail to hold.I aim to join the researchers who are taking steps towards bridging this gap.  This task will not be accomplished easily or all at once: it is not for a lack of effort or insight that current theories have been unable to move beyond the restrictive assumptions mentioned above. However, past work has shown that it is often possible to make game-theoretic models better reflect practical domains when doing so is the explicit focus of the work. Furthermore, the computational approach offers promise: techniques like simulation, approximation and adaptation offer hope in settings where the analytical methods usually favoured by game theorists have fallen short.  Because of the fundamental importance of the research problem---both to computer science and to the economy---a research push in this direction is needed.""365893,""Leznoff, Daniel"
"363243"	"Li, Cheng"	"Multicasting in mobile ad hoc and wireless sensor networks"	"Mobile ad hoc and wireless sensor networks have become a very active research area in the past few years due to their promising potential applications and involved research challenges. A mobile ad hoc network is a wireless network with no fixed infrastructure. Each node in the network operates in a self-organized and decentralized manner. Communication within the network can be established by the collaboration of a number of neighboring network nodes. Wireless sensor networks are a special category of ad hoc networks which are used to provide communication among densely deployed sensor nodes in a specific application region. Sensor nodes are tiny, autonomous devices which possess sensing, data processing, and communicating capabilities. As more and more small, cheap and power-efficient sensor devices become available, wireless sensor networks become increasingly attractive for applications such as environmental monitoring, health care, home security, industrial sensing and diagnostics, and data collection in battlefields or in harsh environments which are not accessible to human beings.      Multicasting, an efficient method of delivering information to a group of destinations, plays an important role in many applications which require group communication capability. A scalable and reliable multicast support is an essential feature for many applications in ad hoc and sensor networks, such as emergency search and rescue, military communications, real-time data sensing, and emerging applications such as mobile TV and peer-to-peer communication for music and video sharing among personal digital assistant devices.      The proposed research includes the development of novel and scalable algorithms and architectures to enable efficient group communication in mobile ad hoc and sensor networks. It is expected that the proposed research will play an important role in future unified multicast architecture as the trend of wire-line and wireless network convergence continues. This research will help solidify Canada's place in the forefront of communications technology.""378441,""Li, Chenkuan"
"364098"	"Li, Hua"	"Reconfigurable computing techniques and VLSI implementations for IT security and image processing"	"Reconfigurable computing is intended to fill in the gap between hardware and software, potentially achieving a much higher performance than software, while maintaining a higher level of flexibility than hardware. By placing the computational-intense portions of an application onto the reconfigurable hardware, it will accelerate the system performance greatly. The implementations of cryptography services for IT (Information Technology) security on wireless devices and embedded systems present a great challenge, particularly to the memory, power, computation resources, and mobility constraints. Reconfigurable computing can serve as an excellent platform for hybrid cryptosystems on restricted computing environments and wireless devices. The reconfigurable architectures increase the flexibility of system schemes and the efficiency of computation resources, and reduce the total cost.  Moreover, hardware piracy issues cause  huge  profit loss in the  IT industry.  In this research project, we also examine how to use the reconfigurable computing/architecture to protect the hardware IP (Intellectual Property).  Image processing plays a crucial role in areas of medicine, crime prevention and investigation, publishing, entertainment, education, and scientific research.  It has become urgent to find the desired images from an image database quickly and automatically. This technique is known as content-based image retrieval (CBIR). However, little research has been done to use the reconfigurable computing technique to speed-up the image retrieval. We will explore new approaches of image retrieval and propose efficient algorithms and fast implementation to improve the retrieval rate of current CBIR systems. My research goal is to investigate and design reconfigurable architectures with low costs, high speeds, and high flexible structures to support real-time online secure data communications, hardware IP (Intellectual Property) protection, and image retrieval. The proposed reconfigurable architectures have regular structures and are very suitable for high speed VLSI implementation.  The academic and technical achievements of this research program can be applied to many areas including but not limited to cable network services, electronic commerce, wireless network and devices, remote system access, smart cards, medical records, hardware privacy prevention, crime investigations, while providing high flexible scheme within the applications.""367571,""Li, Huawei(Colin)"
"361737"	"Lie, David"	"Low-level system software support for information security"	"Computer security is of paramount importance in our highly technology-reliant society.  Many people rely on computer systems to conduct business, communicate with friends and family, and manage their personal information.  Unfortunately, the safety and security of our current computing systems is not adequate for amount of trust people place in them.  The proposed research aims to improve the security of our computer systems by securing the foundation upon which they run, which is composed of low-level systems software such as the operating system and the software layers below it.  Securing this low-level software will not only make our computing infrastructure secure, but will reduce the cost of adoption by leaving higher-level software such as applications and servers intact.This proposed research will achieve these goals by exploring mechanisms and techniques that can be incorporated into two low-level systems layers: operating system kernels and virtual machine monitors (VMM).  The VMM is software layer that runs underneath the operating system kernel.  The proposal will aim to improve computer system security through three main thrusts of research.  One thrust will explore the use of VMMs to create secure containers that will be able to protect critical software and data from attack.  Another thrust will explore the use of VMMs to detect and eradicate malicious software that has infected critical software on a system, including the operating system.  The final thrust will leverage current commodity multi-core processors to allow the operating system kernel to better fend off attacks.  This research will produce a more secure foundation for our current computing infrastructure, thus allowing businesses, individuals and governments to have more confidence in the safety and security of their computing systems.""378030,""Liebeherr, Jorg"
"366637"	"Liu, Jiangchuan"	"Pervasive media distribution over heterogeneous wired and wireless networks"	"With the widespread penetration of broadband access, audio/visual multimedia services are becoming increasingly popular among network users. Recent advances in low-rate low-power media coding and embedded systems have also opened up new opportunities for media over wireless channels. Yet given the limited network and server resources, and the best-effort nature of the current networks, the performance of the existing media distribution systems is far from satisfactory, particularly during peak hours. More importantly, the scale of the existing systems remains limited and there is still no a unified platform to deliver media content pervasively and seamlessly to the users.We thus ask the following critical question: how can we distribute quality media content to a large population of highly heterogeneous clients seamlessly through wired and wireless networks?To achieve the ultimate goals of ""anytime-anywhere"" media distribution, technology advances in various areas need to be re-examined and jointly utilized under a coherent framework. New models and architectures in both media encoding and distributed and collaborative network communications are needed to intelligently adapt the media content to suit user preferences, meet device and network constraints, and achieve better communication resource utilization.In this proposed long-term research, we will address the above challenges, particularly media over novel delivering architectures (e.g., peer-to-peer) and network infrastructures (e.g., wireless mesh networks). We will focus not only on the optimization of the individual components, but also on their interactions that pose many additional challenges to designers. A thorough understanding of the modules in this complex system as well as their interactions will facilitate the development of such popular applications as movie-on-demand, IPTV, distance learning, and video conferencing - all are essentials in the emerging digital entertainment/business world.""370952,""Liu, Jie"
"361210"	"Liu, Yanni"	"Resource management in computer networks"	"Advances in computer, communication, and networking technologies have enabled many network applications. An important category of applications requires timely delivery of real-time data. Traditional IP-based networks provide a best-effort service; their delay performance is good when the traffic is light. However, they do not scale well when the level of traffic increases; for example, when congestion occurs along the data path. Thus to support real-time applications, network quality-of-service (QoS) strategies are needed. Example strategies are channel scheduling, admission control, and pricing. Over the past two decades, two approaches to network QoS provision have been developed: flow-based and deadline-based. In this research, we will design and evaluate a unified approach to network QoS provision, based on solutions and findings obtained from both flow-based and deadline-based approaches. Our goal is to develop cost-effective end-to-end QoS solutions that support a wide variety of real-time applications. A key player in QoS provision is the network (or Internet) service providers (ISPs). In this research, we also target the design of ISP QoS strategies. Differ from the end-to-end QoS study, the emphasis here is the resource management within an ISP network. Results from these studies would aid in the design of QoS infrastructure for next generation computer networks. A well-designed and well-dimensioned network should accommodate most of its traffic. Mathematical models may be used in network design and dimensioning. Good mathematical models have been developed for similar networks and may be applied to IP-based networks, provided that certain traffic characteristics are presented. Such traffic characteristics may be obtained by the proper design of those entities that affect traffic characteristics. Example entities include higher-layer protocols, network resource management, and traffic multiplexing. We study the effect of such entities on traffic; our long-term objective is to design such entities and to come up with sound performance models for end-to-end data communication. Results from this research will provide invaluable insight into the performance modeling of next generation computer communication networks.""374149,""Liu, Yi"
"364281"	"Lounis, Hakim"	"Supporting software engineering with knowledge acquisition, formalization, and application"	"Software development is a design process where every involved person has to make a number of decisions, each of them with several possible choices. These decisions concern different aspects e.g., designing, implementing, testing, maintaining, etc. Software practitioners need support for deciding which alternative is better for their specific context. Improving the ability to develop ""good"" software implies activities that analyze results from several previous projects in order to identify similarities and differences between them. The results of these analyses can be formulated as knowledge. Examples of such knowledge are patterns, heuristics, best practices, assessment models, etc.We consider that software engineering performances could be improved if knowledge generated during software developments and projects is captured, structured, and formalized. However, a lot of knowledge is implicit, residing in the brains of the software practitioners or engineers. This fact makes knowledge elicitation, acquisition, and formalization a challenge.In this context, we believe that:1) Different sources and types of knowledge can be handled by various approaches, and can support software practitioners at different stages of the software life cycle; for instance, for identifying relevant patterns and pattern combinations, promoting reusability, making design decisions, fixing software defects found during testing, analyzing change impact during maintenance, etc.2) We need to be supported by appropriate technology and tools. Artificial intelligence techniques are being used for knowledge discovery, acquisition, and formalization; for instance for defining/using various ontologies, for knowledge representation and utilization, machine-learning, formal concept analysis, etc.""379047,""Lovasik, John"
"362310"	"Loyka, Sergey"	"Performance analysis and optimization of MIMO systems for wireless communications"	"High-speed wireless communication systems and networks, which offer various services and multimedia contents, constitute an important part of the Canadian ""high-tech"" industry. Multi-antenna technology (MIMO), which is widely considered to be a major break-through in wireless communications, can be used to significantly improve the performance of such systems and networks. This project will develop new techniques to evaluate performance and to design such systems and networks. Both analytical and numerical methods, which are based on powerful principles of the convex optimization theory, will be used. The available results for single-user systems will be extended to multi-user (network) context, including resource allocation, quality-of-service and other issues at the physical and MAC layers. Practical implementation issues will also be considered.""381077,""Lozinski, Michael"
"365035"	"Ludwig, Simone"	"Automation of service computing"	"The long-term objective of this research program is the vision of services that are autonomous platform-independent computational elements that can be described, published, discovered, orchestrated and programmed using standard protocols for the purpose of building agile networks of collaborating service applications distributed within and across organizational boundaries. However, before this service-oriented computing paradigm becomes reality, there are a number of challenging issues that need to be addressed including among other things service modeling and design methodologies, architectural approaches, service development, deployment and composition, programming and evolution of services and their supporting technologies and infrastructure. The key to this is an infrastructure where all services, are adequately described in a form that is machine-processable, leading to semantic interoperability and automation.This research program addresses fundamental computer science issues by identifying the nature of provenance, by defining the means of generating it, and by offering reasoning mechanisms based on provenance. However, a number of fundamental research issues still remain. First of all, the principle design of provenance data, and the understanding of their role in reasoning are crucial to deliver a provenance model that is relevant to workflow enactment. Then provenance data generation is a cooperative process that requires the agreement of multiple parties involved in workflow enactment; such parties need to adhere to a common protocol. Furthermore, the specific properties of such a protocol will determine the quality of the provenance data generated, and therefore the level of trust that scientists can have in such data. Finally, the engineering of a provenance architecture suitable for a Grid context needs to be addressed, and must take into account the dynamic and open nature of such an environment, but also some of the domain performance requirements.""369097,""Lueke, Jonathan"
"361713"	"Lutkenhaus, Norbert"	"Optical quantum communication theory"	"Communication is one of the key technologies of modern society. As we turn more and more to electronic communication, questions such as confidentiality of the information exchange and trust in origin of messages come  to the foreground. Today our communication networks are protected by cryptographic methods that seem to be very powerful. However, they depend on assumptions that have to be checked again and again over time. Our research aims at changing this picture.Quantum mechanics offers some features that are unknown in the classical world: in general, it is not possible to copy quantum signals. Moreover, any attempt to access the content of a quantum message will in general change the message, thereby leaving traces in the system. Thanks to this property, quantum mechanics offers solutions to some problems of our communication society where the asserted properties can be rigorously proven. Unconditional secure communication is one such example.Our work aims at transforming more of the known abstract quantum communication solutions to solutions that can be practically implemented, thus bringing it closer to commercial applications.""380685,""Lutley, Jamie"
"364455"	"MacDonald, Steven"	"Testing and debugging concurrent software by deterministic execution"	"Testing any software is difficult because of the large number of paths through the code.  To test different paths, the program must be run with a variety of inputs.  Concurrent software makes this already-difficult problem harder by adding non-determinism.  In concurrent software, a program is decomposed into units called threads, which run independently but share data in memory.  Each thread reads and writes shared data, but the order of these operations changes with each run of the program depending on the timing of thread execution.  This non- deterministic order can occur even if the program is run with the same input, and can impact the final results.     Not long ago, concurrent software was rare.  It has become more mainstream recently and will only increase in importance in the future.  Hyperthreaded and multicore CPUs create environments in which concurrent programs can achieve performance benefits by using different parts of a CPU simultaneously.  Getting full performance from these new environments will require programmers to write concurrent code.     My approach to the problem of testing concurrent software is to execute a concurrent program deterministically - that is, run the program where the order of events is determined in advance.  My approach, which forms the basis of this research proposal, uses advanced compiler analysis techniques to identify places in a concurrent program where thread timing can impact values for read events.  This analysis is then used to control program execution by supplying values to read events that simulate the desired event order.     Testing and debugging are crucial parts of the software development process.  Quite simply, software errors cost real money.  As concurrent code becomes increasingly important, it is essential that testing and debugging tools keep pace.  Otherwise, software with new classes of errors that are difficult to detect and correct will reach consumers.  For example, after Intel announced hyperthreading, several commercial programs emerged to exploit the potential benefits, including Adobe Photoshop and Symantec Anti-Virus.  My goal is to improve software quality by helping developers produce correct applications in the important concurrent domain.""362264,""MacDougall, Andrew"
"365458"	"MacGillivray, Gary"	"Combinatorial problems with a view towards algorithms and complexity"	"The proposed research involves the design of efficient algorithms, or a demonstration that no such algorithm is likely to exist, for certain combinatorial problems.  As much of the research program as possible is to be carried out in such a way that opportunities are provided for undergraduate students, Master's students, Ph.D. students, post-doctoral fellows, and young colleagues.  There are three main topics involved: homomorphisms of graphs, discrete-time graph processes, and domination in graphs.  The first of these has applications in computer science, scheduling, and logic.  The second topic involves, among other things, a very simple yet computational difficult model of the spread of a virus or disease through a network or population.  The final topic is arguably among the most well-studied in discrete mathematics and theoretical computer science.  Besides being interesting for its own sake, it has proved to be an excellent vehicle for involving students in research.""383507,""MacGillivray, Stuart"
"365957"	"Maheswaran, Muthucumaru"	"Trust management in autonomous social networks"	"This project focuses on trust management in autonomous social networks (ASNs). The ASNs are a new class of social networks where people connect with each other in peer-to-peer manner rather than going through a centralized website. This has many benefits including: support for mobile and ad-hoc networks, lack of central site of failure or vulnerability, and lack of a central policy making entity. The ASNs are inspired by the success of many peer-to-peer applications in areas such as file sharing and voice-over-IP services. An ASN peer runs on each device (e.g., Microsoft Zune, Nokia Internet Tablet) that is part of the network.This project will focus on three aspects of trust management within ASNs: trust modeling, benchmarks for trust management systems, and applications of trust in the context of ASNs.""381573,""Maheu, Tasha"
"364640"	"Majedi, AmirHamed"	"Characterization of superconducting microwave photonic devices & systems"	"The combination of photonics and high-speed electronics has been established as a   successful technology to generate, manipulate, transmit and detect information either in optical or electrical forms.  The increasing quest for achieving ultra-sensitive, ultra-low noise and high-speed/high-frequency devices with applications in sensing, processing and metrology is constantly pushing researchers to find novel optoelectronic devices and systems. The continuous exploration of such systems has recently led to the possibility of using superconducting materials in optoelectronic and microwave-to-THz photonic devices to take advantage of their macroscopic quantum behavior. In order for superconducting optoelectronics to attain its full potential, devices and systems must be developed and characterized. This proposal requests the purchase of two research tool infrastructures; Wedge ultrasonic wire bonder to make the raw superconducting devices to connect to the rest of photonic and electronic devices and measurement systems and a PNA network analyzer to perform high-frequency optoelectronic characterization. The integration of these two units to our previously established infrastructure laboratory, Integrated Quantum Optoelectronics Lab (IQOL), provides a full operation of our cryogenic four-probe microwave station to test our existing and future superconducting optoelectronic devices. Superconducting nano-wire single-photon optical detector, photomixers, and optoelectronic mixers and switches are paramount examples of our devices with applications ranging from fiber-optic quantum cryptography to live-cell imaging and spectroscopy.           ""369773,""Majedi, AmirHamed"
"361042"	"Marchand, Yannick"	"From speech synthesis by analogy to simulations of human reading"	"Text-to-speech synthesis is concerned with automatically determining the pronunciation of words from their spellings. This field has important applications including telecommunications-based information services and aids for blind and non-speaking people. There is an apparent commonality between the problem of speech synthesis and the process of human reading. Given this situation, it is surprising that there has been minimal interaction between the two fields. On one hand, studies on human reading can bring some valuable insights into the very challenging problem of mapping letters to sounds in speech synthesis. On the other hand, the study of human reading can benefit from the precision of mathematical models.          In the last few years, the applicant and colleagues have developed a method for converting letters to sounds for text-to-speech systems. The research program is concerned with using this computer method as a way of increasing our understanding of how people read aloud.          This proposal can be divided into three main objectives: 1) Improving our existing model for text-to-speech applications; 2) Demonstrating the capability of our model to account for the key human reading phenomena; 3) Extending the model to languages other than English, as a way of furthering the understanding of the relation between spelling and speech sounds.""373040,""Marchbank, Jim"
"362504"	"Marcus, Brian"	"Entropy rate of hidden markov chains and capacity and coding for input-constrained channels"	"In many practical noisy channels, input sequences are constrained in order to enable the channel to operate effectively. For instance, in magnetic and optical recording channels input sequences are constrained in a variety of ways to ensure accurate timing and reduce the likelihood of certain kinds of error events. Input constraints also model memory that is typically present in data recording channels and band-limited communication channels.For many of the most important input-constrained channels, there is no explicit formula known for capacity, which is defined as the maximal rate at which information can be transmitted without error. We plan to focus on two main themes for input-constrained channels: 1) estimates and asymptotics for capacity of input-constrained channels and 2) construction of good codes for such channels.   For 1), we will investigate estimates and asymptotics for entropy rate of Hidden Markov chains, and apply this to capacity of input-constrained channels.For 2), we will focus on coding schemes that limit error propagation and coding schemes that combine input constraints with error correction.""378430,""Mare, AugustinLiviu"
"361932"	"Martel, Sylvain"	"Magnetotactic bacteria-based microrobots"	"Due to technological constraints in microfabrication coupled with limits in electrical energy sources and conversions that can be embedded in future fully autonomous untethered microrobots only a few hundreds micrometers in overall length, the energy requirement collected through embedded photovoltaic cells is minimized by exploiting the motility of flagellated MC-1 Magnetotactic Bacteria (MTB). The encapsulation of such bacteria in special micro-reservoirs embedded in the microrobot provides the mean of propulsion. Directional control of the microrobot is achieved from an onboard microcircuit inducing a torque on a chain of single domain magnetic nanoparticles called magnetosomes embedded in each bacterium. Integrated 3D coil solenoids based on standard CMOS technology are used to generate local magnetic fields to orient the swimming directions of the bacteria inside the micro-reservoirs.Although the possibilities and advantages of this method have been shown by our group through simple experiments where the controlled manipulation of microbeads was performed by MTB, additional fundamental issues and techniques related to its use for the conception of Magnetotactic Bacteria-based Systems (MBS) must be investigated. This is the main fundamental motivation for this proposal.The longer term objective is to implement instrumentation based on a new paradigm where swarms of instrumented microrobots would be mixed with the sample itself. Through collective behaviour and swarm intelligence, some types of relatively complex tasks could be performed without any infrastructures while maintaining electrical energy within each microrobot to a minimum and within constraints dictated by technology and the rules of physics.This research initiative is significant with great potential impact for researchers involved in this field since it proposes to push further the development of smaller microrobots through a new controlled micro-actuation method based on MTB with potential integration since it is low powered (low voltage) and independent to dielectric properties as opposed to other known methods used in modern engineered microsystems.""379480,""Martel, Sylvain"
"362492"	"Marzi, Hosein"	"Interprocess communication for real-time embedded systems"	"In large-scale distributed real-time embedded systems, there are periodic and aperiodic processes driven by messages that affect the system's performance. Due to limitations of the resources in embedded systems, message-based designs impose significant overheads and degrade system performance, because of high execution time and memory requirements.The mechanisms for defining and addressing these issues in order to achieve quality of service are still maturing. Current architectures are not perfect and are still in their infancy in addressing these issues in large distributed systems. This research program will provide a benchmark for the current demands of industry and the research community. In the proposed approach, overheads are removed and communication delays are minimized, resulting in significant improvement in performance of real-time embedded and distributed systems.""376151,""Marziali, Andre"
"362432"	"Matrawy, Ashraf"	"Improving internet security through traffic analysis and examination of the internet`s basic properties"	"Improving Internet Security through Traffic Analysis and Examination of the Internet's Basic PropertiesThe Internet is arguably the biggest man-made entity. It is a very complex network that is not centrally controlled and that is continuously growing. This fashion of growth complicates network operation and management. However, if the end-goal is to have a network that facilitates communication the way the Internet does, it is inevitable that it will be that complex. This complexity led to questions on how the Internet actually works in terms of traffic patterns, logical and physical topologies, and geographical location of nodes.The objective of this research program is to improve Internet security through exploiting knowledge of Internet traffic and of its basic properties. The basic properties of the Internet refer to its topology and to the relation of  geographic location to the way Internet addresses are distributed and managed.  This is a twofold objective: the first is to better understand and gain knowledge of Internet traffic and basic properties. The second is to apply this knowledge in building new network security architectures, applications, and protocols, as well as in securing existing protocols and applications. The proposed research program will focus on the following lines of research:1- Traffic analysis of the patterns and structures that exist in Internet traffic.2- Characterization of malicious activities. (e.g. SPAM, Denial-of-Service)3- Relating network elements (hosts and routers) to geographical location.4- Enabling a new class of security applications that are based on actual knowledge of how the Internet works rather than relying solely on modeling and simulation.  ""364318,""Matsakis, Pascal"
"365278"	"Maurer, Frank"	"Integrating interaction design techniques into agile software development"	"The overall goal of the proposed project is to combine agile methods with approaches from interaction design to help agile teams to cost-effectively create usable and maintainable software systems.Agile software development methods (like Extreme Programming, Scrum and Lean Software Development) are quickly moving into the mainstream of software development organizations. Agile methods are human-centered approaches that encourage continual realignment of development goals with the needs and expectations of all the stakeholders. They concentrate on significantly improving communications and interactions (among team members and with the stakeholder representatives), promote continuous feedback, focus on transparency & ""clean code that works"", and merciless testing to achieve high software quality.Interaction design and usability engineering  approaches promote focusing on users, their goals and needs in software development. They highlight that usability is key to success in software development projects.In the proposed research program, we will investigate processes, methods and tools to help agile teams to increase the usability of their software products. We will empirically investigate how interaction design approaches can be integrated with agile methods. Based on the results of these studies, we will identify integrated techniques as well as opportunities for tools to support the integrated process. We will then conduct experiments with student subjects to fine-tune the techniques and tools. Case studies in industry will be conducted to evaluate benefits and limitations of the proposed approaches. Initially, we will focus on agile project planning and executable acceptance testing. We will extend our current approaches by the ability to do user interface level acceptance testing. In addition, we will use low fidelity prototyping to support agile release planning and project charting.""383329,""Maurice, Alexandre"
"366352"	"Mayers, André"	"Modélisation d'un agent expert générique pour un système tutoriel intellignet"	"Un problème important pour les systèmes informatisés de formation est la modélisation de l'expertise pour les domaines scientifiques ou techniques. Il est nécessaire que l'expertise soit opérationnelle, c'est-à-dire que le système soit en mesure de résoudre les problèmes qu'il pose à l'apprenant et d'expliquer comment il a résolu ces problèmes. Des recherches montrent qu'il est aussi important que les structures de représentation des connaissances s'inspirent de celles proposées par la psychologie cognitive pour faciliter l'acquisition des connaisances par l'apprenant, pour suivre les actions de ce dernier dans l'environnement d'apprentissage et pour construire le dialogue tutoriel.Dans nos recherches, nous avons conçu un agent expert capable de comprendre la solution d'un apprenant et de l'aider à poursuivre sa solution. Cet agent expert a été construit pour faciliter l'acquisition d'habiletés cognitives procédurales, ces dernières permettant d'accomplir des tâches où à chaque étape le nombre d'actions permises est petit. Un exemple simple est la programmation d'un magnétoscope (VCR). Cependant, pour plusieurs tâches, par exemple l'analyse d'un circuit électronique, le nombre de procédures à apprendre pour couvrir chaque scénario est trop grand. Dans ce cas, il est préférable d'enseigner une stratégie, si elle existe, qui permette à l'apprenant d'inférer la prochaine étape.L'objectif de la recherche est de construire un agent expert générique qui soit en mesure de faciliter l'acquisition d'habiletés cognitives procédurales ou non. Un outil, qui permet à un expert d'un domaine scientifique ou technique ou un enseignant d'encoder son expertise, est aussi proposé. Ce projet se distingue par une caractéristique originale importante. En effet, s'il y a beaucoup de système tutoriel intelligent avec un agent Expert possédant l'expertise d'un domaine, il n'y en a pas de générique offrant la possibilité d'encoder, de simuler et d'enseigner explicitement plusieurs stratégies associées aux domaines scientifiques ou techniques.""372962,""MayerSmith, Jolie"
"362677"	"McGrenere, Joanna"	"Design and evaluation of adaptive and adaptable information technology"	"Information technology is complex and the people who use the technology are themselves complex. Almost by definition, therefore, the fit between the technology and its users is complex. Technology today includes a plethora of applications, each includes a multitude of features and each interacts with other applications. There are also many computational devices/platforms which interact together. Those who use this technology have diverse tasks, skills, knowledge, expectations, and motivation. Although there is always room for general improvements to the design of any given technology, most individuals would benefit from having the technology designed specifically to fit their own particular needs. Designing technology to accommodate such individual differences, however, is very difficult to do.     The objectives of this research are to explore boundaries between adaptive (system-initiated) and adaptable (user-initiated) approaches to personalized technologies and to derive generalizable design principles. In adaptive designs, the system modifies the interface based on what it knows about the user; whereas in adaptable designs, it is the user who makes the adaptations to the interface. Both approaches have benefits and drawbacks. A third, less well known approach, called mixed-initiative design, is a hybrid in that it attempts to balance the benefits of the other two designs by having the system and the user work together to achieve a personalized interface.     Research questions include: (1) What aspects of the personalization process are users willing to let the system to do on their behalf and which do they want to control themselves? and (2) What factors impact users' willingness to personalize? For example, is it the type of application or device, or their level of expertise?     The proposed research is in the field of human-computer interaction, with a strong linkage to software engineering. The research questions will be explored in the context of graphical user interfaces and various computational devices, including the desktop PC, electronic whiteboards, and handheld devices.""367518,""McGuigan, Benjamin"
"361818"	"Mcheick, Hamid"	"Évolution et distribution des logiciels selon les thechniques orientées aspect"	"Dans un système d'information d'entreprise, une même entité peut jouer plusieurs rôles ou supporter plusieurs fonctionnalités à différents moments de sa durée de vie. De plus, différents usagers et applications peuvent utiliser et accéder à différents sous-ensembles de ces fonctionnalités. Nous nous intéressons à l'évolution des fonctionnalités et du comportement des objets/logiciels dans le temps et à leur distribution selon les techniques dites ""orientées aspect"", en particulier la programmation par vues, où une vue est une construction logicielle réalisant un rôle fonctionnel. Ces techniques permettent de modulariser les exigences qui ne peuvent pas être traitées par les classes et les fonctions. Ce travail vise à développer une méthode et des outils supportant le développement d'applications orientées objet et permettant aux objets i) de changer de comportement durant leur vie en activant ou désactivant des aspects fonctionnels sur demande, et ii) d'offrir différents sous-ensembles de fonctionnalités dans un environnement distribué.Nous proposons une solution intégrée pour répondre à ces objectifs d'une manière unifiée sans accumuler les coûts de chacun de ces objectifs. Dans cette solution, nous utilisons la programmation par vues et les plates-formes distribuées. Nous allons traiter particulièrement les problèmes suivants : i) distribution d'objets avec vues, ii) autoadaptation d'objets selon les appels de fonctions par les programmes clients et la préservation des droits d'accès de chacun de ces programmes à différentes parties de l'objet en question d'une façon transparente, iii) partitionnement d'objets selon le treillis de Galois, et iv) gestion de cycle de vie d'objets en utilisant les techniques orientées aspect.""366013,""McHugh, John"
"363456"	"McIlraith, Sheila"	"Modeling, analysis, control and customization of component-based systems"	"My research program focuses on the development of mathematical modeling and automated reasoning techniques that support the modeling, analysis and control of complex real-world systems.   My work is largely driven by application domains that require breakthroughs in these areas.  The domains that drive my work are typically component-based and sometimes distributed, often the result of networked software and/or connected component devices.  Specific domains that I have addressed include distributed component software systems such as web services or distributed heterogeneous information sources, and component-based engineered hardware and software systems such as those found within NASA space systems.  These application domains are instances of a broad array of seemingly disparate problems that share core technical challenges including how to specify the behaviour of these systems, and how to develop automated reasoning techniques to compose, monitor, diagnose, reconfigure and repair these systems.  By addressing mathematical foundations my work often transcends the specifics of the application domain to develop principled approaches of broad applicability.A focus of this proposal is on issues related to modeling and analysis of component software and component electromechanical systems.  We will model the functional and nonfunctional properties of such component systems so that other computer programs can understand what the components do and how to use them.  Using these descriptions, we will develop theories and technology to enable disparate components to be automatically queried, composed and sometimes customized according to a user's preferences.""363455,""McIlraith, Sheila"
"364851"	"McKenzie, Pierre"	"Computational complexity of polynomial time problems"	"The goal of complexity theory is to rigorously classify problems solvable by computers on the basis of the amounts of resources needed to solve them. Computing time is considered as a resource, as well as memory, processors, random bits, communication bits, etc. The methodology of complexity theory rests on the definition of abstract computer models, on the development of algorithms solving a problem on such models, and (ideally) on the mathematical proof that the algorithms found are the best possible. In some cases, such a proof would have great practical value ; for example, the perceived -but as yet unproven- difficulty of problems such as factoring a large number into two smaller numbers that multiply out to this number underlies much of today's cryptography.The main long-term goal of my research is to make progress in elucidating the structure of the complexity class P of problems solvable in polynomial time. In particular, I will continue to devote effort to the question of whether an arbitrary polynomial time computation can be performed using only a logarithmic amount of memory. This is a 35 year-old open question, and it hints at the main shortcoming of complexity theory : despite intensive efforts by a generation of researchers, very few significant lower bounds on the complexity of specific problems within the complexity class NP are known.The short-term goal of my work is to use the tools of complexity theory to better understand the complexity of specific problems by (A) comparing this complexity with those of other well studied problems, (B) proving lower bounds on restricted models of computation, and (C) further investigating the connections between algebra, logic and complexity.In particular, I will further develop and investigate models of computation that are intermediate between the monotone Boolean circuit, for which significant lower bounds are known, and general Boolean circuits, for which lower bounds are mostly inexistant. The new models include the semantic incremental branching program, recently introduced, and a proposed generalisation of monotone span programs which replaces equality constraints by inequalities. Studying such new models offers hope to extend current proof technology and thus move closer to being able to determine the complexity of problems of immediate practical interest.""362550,""McKeown, Martin"
"361894"	"McNeill, Dean"	"Neural computation and embedded signal processing"	"Digital computing systems have become a key component in the design of a vast range of consumer products from cellular telephones to washing machines, from portable music players to anti-lock breaking systems in our cars. These systems all rely on sensors of various types to gather information from the environment and from the users of these devices. They then use microprocessors, embedded within the devices, to process the sensed information and to convert it into a new and more meaningful form.In some cases this processing is straightforward, such as the decoding of pre-recorded music into sound that we can hear. In other applications, the precise processing method needed to extract meaning from the sensed information may not be well understood, or there may be no existing step-by-step (deterministic) procedure one can identify in advance that is able to process the available information correctly in all cases. Nonetheless, by examining the information it may be possible to extract some meaning from underlying regularities within the collected data.This proposal is intended to explore methods of dealing with these difficult signal processing tasks in an efficient and cost-effective manner that is suitable for use in embedded computing applications. In achieving this, various methods will be explored which fall into a class of information processing algorithms known collectively as neural computing. However, unlike the majority of previous investigations into the use of these techniques, the additional constraint of operating in an embedded environment will be considered. In addition, the proposed investigations will explore unique methods of encoding the information being processed in these systems, in order to permit the use of novel computing approaches which may be well suited to specialized embedded signal processing applications.""385159,""McNeill, Dean"
"364173"	"Meyer, Irmtraud"	"Comparative RNA structure prediction beyond stochastic context free grammars"	"RNA molecules are at the core of numerous fundamental processes in any cell. They encode information on the set of proteins to be synthesized in a cell; they help synthesize proteins; they control protein-production rates (see Nobel Prize in Medicine 2006); and they also act as genes in their own right (so-called RNA genes). Many of these functions rely on the structure of the RNA molecule.RNA molecules are linear chains of variable lengths made of four elementary building blocks, the nucleotides denoted A, C, G and U. An RNA structure is defined by weak hydrogen bonds between pairs of non-consecutive nucleotides in the RNA sequence; the three possible pairs are {A, U}, {G, C} and {G, U}. The number of possible RNA structures that an RNA sequence can assume grows exponentially with the length of the sequence. The challenge to the theoretical scientist is to single out the RNA structure that confers the functional property to the RNA molecule.To address this challenge, most existing methods rely on the assumption that RNA molecules assume the most stable structure. Recent results show that this assumption does not hold in general. We propose to develop novel,significantly improved, theoretical methods which do not rely on this assumption and which overcome the major conceptual limitations of the existing methods in order to improve RNA structure prediction, RNA gene prediction and to address many of the most fundamental and exciting questions in molecular biology today.""365430,""Meyer, Rudolf"
"364282"	"Mili, Hafedh"	"Understanding key transformations in model-driven development"	"Software development is a very complex process that requires vast amounts of knowledge and skills both in software development itself and in the application domain, and researchers have long been trying to manage the complexity of the process. One of the earlier strategies - recently rehabilitated by OMG's Model-Driven Architecture- viewed the development process as a possibly long sequence of labour intensive, but automatable transformations that start with a specification of the software to produce a working version. The long-term goal of this research program is to develop a better understanding of the software development process within the framework of a transformational, model-driven approach. More specifically, we are interested in exploring two transformations that embody a non-trivial shift in focus: 1) the transition from process models to information system models, and 2) the transition from analysis to design. By automating the transition from process models to analysis models, we improve communication between the business users of software, and its developers, and we ensure that developers develop what the business users need. By automating the transition from analysis to design, we relieve designers and developers from the tedious and error-prone tasks involved in designs, and let them focus on the creative and intellectually challenging tasks where they add the most value. By automating both transitions, we empower business users to develop their own applications by specifying or ""drawing"" the business process they wish to support with the information system.""369123,""Milicevic, Sinisa"
"361316"	"Mirabbasi, Shahriar"	"Architectures and circuits for high-performance highly integrated wireless/wireline communication systems"	"Analog, mixed-signal (designs that include both analog and digital subcircuits), and radio-frequency (RF) building blocks are often the performance bottleneck of communication systems and microsystems. These blocks usually dictate the speed and/or power consumption of the system. The goal of this research is to develop circuit design techniques and transceiver architectures that are suited to high-performance highly integrated wireless/wireline communication systems as well as microsystems. The emphasis of the research is on the systems implemented in deep submicron bulk CMOS technologies and operating in the frequency ranges up to a few tens of GHz. Bulk CMOS technologies have emerged as promising low-cost alternative (as opposed to compound semiconductor technologies such as GaAs) for high-performance integrated solutions. In, addition CMOS technologies provide the possibility of integrating analog and digital circuitry on the same chip and are suitable candidates for mixed-signal microsystems. However, advances in bulk CMOS technologies are mainly driven by the digital world and therefore digital performance metrics of CMOS technologies have grown significantly faster than corresponding measures in analog, mixed-signal, and RF circuits. Although both analog and digital components take advantage of the higher speed of operation due to technology scaling, unlike their digital counterparts, analog circuits are more constrained by noise and accuracy requirements. Hence, the performance of analog, mixed-signal, and RF circuits often only conditionally benefits from technology scaling and can even deteriorate with the trend in decreasing supply voltages. In general, the net benefit of CMOS implementation of systems due to technology scaling, that is, the overall performance improvement due to (re-)designing systems in a more advanced CMOS technology, is a strong function of the system architecture and circuit design techniques. Therefore, it is the goal of this research to develop new CMOS-friendly circuit design techniques for analog, mixed-signal, and RF building blocks in advanced CMOS technologies.""369511,""Miraftab, SeyedVahid"
"361534"	"Misic, Vojislav"	"Design of service-oriented applications"	"Businesses both small and large are more and more dependent on accurate, dependable, and timely functioning of various software systems, the complexity of which has been steadily increasing. A number of techniqueswere introduced over the years to manage this complexity, the most recent one being the approach known as service orientation or service oriented computing, in which software entities are identified with the service(s) they provide (with the desired functionality and performance characteristics) through standardizedinterfaces. However, the success of service-oriented approach necessitates a sound set of methods and techniques for design, development, and testing such applications. To that end, we propose to derive, elaborate, and validate a comprehensive approach that would(a) build upon the existing foundation of the extensive body of knowledge about developing software applications, including structured, object-oriented, component-based, and (last but not least)architecture-centric approaches; but at the same time(b) take into account the characteristic traits of service-oriented systems in a systematic and rigorous fashion.Different analysis and design tasks will be undertaken, and the feasibility of using various formal techniques, including Petri Nets and others, will be assessed. Particular attention will be given to the need to accurately model and specify non-functional requirements such as performance, security, and availability, which are not well (or not at all) addressed through current design techniques.Furthermore, we will examine the role of standards, and define a minimum set of necessary standards, as well as the appropriate extensions, to be used in the design and development of service-oriented applications.In this manner, the proposed research will contribute to the development of the full potential of the service-oriented approach and facilitate the design of information technology that caters to the needs of itsusers.""372399,""Miskelly, Andrea"
"362382"	"Mitchell, Ian"	"Algorithms and software for the hamilton-jacobi equation and applications in control, robotics and verification"	"I propose development of a Hamilton-Jacobi Workshop (HJW), which is a collection of algorithms and implementations for approximating the solution of both the time dependent and stationary versions of the Hamilton-Jacobi (HJ) partial differential equation (PDE).  This equation shows up in such diverse fields as fluid and combustion simulation, graphics and animation, image processing and computer vision, financial mathematics, robotics and optimal control, and verification; for this reason it has received considerable research attention.  My research agenda is distinct from and complementary to the study of this equation undertaken by numerical analysts.  While that community has developed powerful, efficient and accurate numerical methods, researchers and practitioners from other fields find it difficult to choose among and implement the various schemes, and students new to HJ PDEs must replicate an enormous software infrastructure to demonstrate their own contributions.  Therefore, my research will focus on developing easy to use and extensible implementations at the cutting edge of accuracy and power, yet factorized in such a manner as to provide graceful flexibility for trying various alternative schemes.  Where necessary the code may not achieve optimal execution efficiency, but with access to all of the source code users can tune or reimplement once they have determined the best schemes for their particular application.  Such a collection of techniques cannot be developed in isolation, and this research will be informed by feedback from HJW users and by personal study in application fields, such as provably safe control synthesis and filtering for vehicles, path planning under dynamic motion constraints and multiple objectives, and automated tools for reachability in continuous and hybrid systems.""377003,""Mitchell, Jane"
"361370"	"Morrison, Jason"	"Applied computational geometry in fundamentals of bio-imaging and analysis"	"As a computer scientist working in the discipline of biosystems engineering, my long term research program focuses on the application of computational geometry to the fundamental physical sciences of imaging and analyzing food and biological materials.  In the analysis of materials, traditional spectroscopic techniques provide the spectral signature of a chemical sample, modern spectroscopic imaging provides entire spectra at each pixel of a sample.  While each technology and manufacturer has equipment with differing spatial resolutions many devices offer hundreds of thousand of pixels per sample.  Similarly spectral resolution varies from the multispectral (tens of spectra per pixel) to hyperspectral (hundreds to tens of thousands per pixel).Computational geometry typically analyzes structure within a geometric abstraction of a data set.  As an example of geometric structure, a particular substance's spectra can be considered geometric data by considering individual peaks in the spectra and their magnitude as separate mathematical values (features) of that data point.  Sets of substances (i.e., data points) can be analyzed to find clusters/structures of chemicals with similar chemical bonds (e.g. chemical sub-groups).   The long term goal of the proposed research is to establish computational geometry as a key component in the analysis and acquisition of spectral signals and their use in the determination of quality of food and biological materials.  My short term goals focus on two areas:  the use of nearest neighbour methods to perform cluster and classification analysis of spectral data and the use of polygonal simplification and estimation techniques to improve data acquisition in nuclear magnetic resonance spectroscopy.""380645,""Morrison, Jennifer"
"361421"	"Moshovos, Andreas"	"AENAO: high-performance, power-aware and reliable processors and computer systems"	"At the core of virtually all modern electronic devices such as computers, cell phones, and network routers there is at least one processor that is the ""brains"" of the device. Processors manipulate information represented as numbers. Processors are extremely useful because they can interact with the environment - via sensors (e.g., a microphone) and actuators (e.g., a speaker) - following a prespecified, yet complex, set of actions. Processors are also extremely useful in scientific exploration as they can evaluate complex models of physical phenomena such as chemical, biological and inter-planetary interactions that are too expensive or impractical to study otherwise. Today many everyday activities and much of scientific exploration rely on the availability of optimized, inexpensive processors. Historically, it was possible to continuously improve the performance and other characteristics of processors (e.g., energy consumption) while reducing cost. A typical computer in the early 80's cost several tens of thousands, while today just a few hundred dollars while being 1000 times as powerful. Much of the communications and computing industry relies on this trend. A key mechanism for improving processors is computer architecture, which studies how to build processors given the available manufacturing technologies while taking into consideration the applications that these processors will be used for. Computer architecture faces continuous challenges for two reasons: (1) the properties of the underlying technology change significantly over time, and (2) so do the applications. This research is targeted at improving processors by addressing new challenges that stem from the continuous shrinking of the underlying manufacturing technology and the new applications that have emerged as processor-based devices have become ubiquitous. The challenges addressed include performance, design complexity, and energy consumption. Such techniques must be developed if the historical trends of advance in processor performance and cost are to be maintained. A variety of processor-based devices ranging from smart-phones to supercomputers will benefit from this research.""367220,""Moshurchak, Lee"
"365796"	"Mould, David"	"Stylization of virtual scenery"	"The virtual worlds found in computer-generated films and computer games are populated by characters, objects, and landscapes that are created by skilled digital artists. Although computational tools are available to create simple virtual scenery in the form of hills, trees, and mountains, the creators of virtual worlds want fantastical effects. In the movie Cars, for example, the world was designed with a car-centred aesthetic: flowers resembled tailfins, and mountains resembled hubcaps. Virtual worlds are populated by strange, alien scenery and by the remnants of antiquity: gnarled trees bristle with spirits, and ruined temples sprawl hidden behind every hill.This project will extend the range of computer-generated scenery to include stylized natural scenery and historical human-made artifacts. Computers will create trees which resemble human faces when seen from precisely the right angle, and mountains which reveal ancient writing when the sun shines from exactly the right direction. They will produce sculptures, architectural adornments, mosaics, and all manner of other artworks. The computational tools created by this project will allow artists to create ever more vivid, complex, and engaging virtual worlds for the digital entertainments of the future.""376488,""Moule, David"
"365453"	"Mousavi, Parvin"	"Computational methods in biomedicine using image and large-scale molecular data"	"The proposed research in biomedical computing is aimed at developing innovative methods to support prediction and characterization of complex biological phenomena. The new era of large-scale biology is characterized by insurmountable amounts of data including high throughput genomic data, large public databases, medical images and clinical signals. This abundance of data stands in contrast to the limited knowledge about the biological events underlying disease. This research includes the development of feature extraction methods to examine multi-modal biomedical data (high-throughput gene expression, Magnetic Resonance Imaging [MRI], ultrasound signals), and computational models to integrate identified features and explain disease processes. The developed methods are discussed in the framework of two major disorders, prostate cancer and Multiple Sclerosis [MS], to serve as examples; however, they can be extended to apply to other biological processes.The unparalleled integrative approach proposed in this research, that uses features from multiple-modalities (MRI and gene expression), and clinical information, enables formulate accurate models of MS progression. Such models, can help with early diagnosis of disease, and facilitate the discovery of new drugs and the realization of personalized medicine. In addition, the novel approaches to acquisition and analysis of ultrasound signals has the ultimate potential to accelerate the rate of early detection of cancer, specifically the diagnostic grade without the need for biopsy. This advance will result in improved clinical management of cancer and can reduce mortality rates drastically.Training of highly qualified personnel is a primary focus of this work; three PhD and four Master's students, as well as five summer undergraduates will be involved in the proposed research.""361440,""Moussa, Medhat"
"365006"	"Mudur, Sudhir"	"Computational techniques for processing and visualising very large datasets in 3D"	"The long term objective of my research programme is to be able to create computing environments in which humans can intuitively interact, comprehend complex behaviour and relationships more easily through suitable visualisations, and productively collaborate to accomplish real worlds tasks efficiently. Rapid advances in sensor based data acquisition, real time data logging and voluminous data from complex simulations have led to the situation that data sizes grow much faster than advances in graphics hardware acceleration or in algorithmic techniques. Most 3D processing, rendering and visualization techniques do not scale to these data sizes, certainly not with standard commodity hardware. New computational techniques that can efficiently process and enable 3D visualization of very large and complex data sets are essential. The proposed research directly builds on my previous work during my tenure in India (until 2002), and more specifically on the research programme and collaborations initiated after joining Concordia in 2002. With funding support from NSERC, CFI and other agencies, over the last 4 years I have set up the ""Advanced 3D Graphics and Visual Computing Laboratory"" at Concordia. This lab is currently equipped with a two-head 3D scanner, virtual and augmented reality devices, a powerful 3D graphics cluster and a suite of high end graphics workstations. Specifically, we shall investigate: 1)Computational techniques for closely coupling visualisation with macro-level feature/pattern discovery techniques applicable to large data sets; this forms the core of this research, 2)New methods of processing raw 3D scan data into a format  readily useful in applications such as gaming, cinema, engineering, security, etc. 3)Functionality and data distribution on CPUs and GPUs (looked upon as co-processors) for very large datasets using the graphics cluster in the lab. and 4)Development of interactive and immersive systems incorporating these techniques for multidisciplinary applications in web usage, bioinformatics, patient education and clinical data. The results would immensely benefit the graphics research community at large as well as Canadian industries of gaming, cinema, healthcare and engineering.""374200,""Muehlenbachs, Karlis"
"362782"	"Mullins, John"	"Analyse probabiliste des protocoles de sécurité à flux d'information admissible"	"La diffusion croissante des cyber-services pose sous une forme beaucoup plus complexe qu'auparavant le problème de la protection de l'information. L'analyse formelle des systèmes conçus  pour assurer  cette sécurité i.e. les protocoles de sécurité, est devenu par conséquent un champ de recherche très actif. Il recouvre aussi bien le développement d'une théorie de la sécurité de l'information que les protocoles doivent assurer que celui de méthodes et d'outils aptes à assurer la correction de ces protocoles. L'approche de la théorie du flux de l'information, comme celle de presque toutes les théories existantes de la sécurité de l'information  développées à ce jour, est une approche possibiliste selon laquelle le non-déterminisme sert à modéliser les mécanismes aléatoires de génération de tous les comportement possibles des systèmes. Toutefois, l'approche  possibiliste est trop grossière pour décrire le flux d'information probabiliste et prévenir ainsi les attaques basées sur l'analyse statistique capable d'inférer un secret à partir de ses observations. Or de plus en plus de protocoles de sécurité utilisent le bourrage aléatoire pour atteindre certains objectifs de sécurité. Par exemple, pour assurer l'anonymat des transactions sur le Web, le ""Crowds protocol"",  masque les communications de chaque usager en les faisant acheminer par un usager choisi de manière aléatoire. Pour cette raison, nous croyons que le fondement théorique du flux d'information étendu pour exprimer ce type de propriétés de sécurité à flux d'information admissible doit être de nature probabiliste. Ce projet vise donc, à court, moyen et long terme respectivement, à: 1. développer un langage de modélisation des protocoles de sécurité où le flux d'information considéré est probabiliste, 2. établir un contexte formel uniforme pour exprimer les propriétés de sécurité à flux d'information probabiliste admissible et 3. développer des méthodes et des outils d'analyse de ces protocoles. La méthodologie proposée repose pour: 1. sur une algèbre de processus probabiliste étendue de primitives cryptographiques, 2. sur un schéma général et une classification de ce type de propriétés 3. sur une équivalence observationnelle qui reflète la puissance de l'attaquant et des méthodes de réduction à un modèle fini.""382163,""Multani, Kanwarpal"
"365693"	"Munro, Ian"	"Efficiency of data structures"	"The size and complexity of today's information systems dwarf those of five, let alone twenty, years ago. The demands of modern industrial applications for faster systems with greater memories are moving from ""large"" databases comprising a few billion bytes, or gigabytes, of information to specialized systems in the range of many terabytes (trillions of bytes). As demand escalates, the manner in which data is organized and the algorithms (or problem-solving methods) used to manipulate information become increasingly critical. More effective techniques, that address time and space constraints, must be created to allow large systems to better perform search operations in response to sophisticated queries and updates. In particular, this work will focus on succinct representations of data structures and their application to text indexing, as well as structures working efficiently in a complex memory heirarchy. In tandem with this application driven work is the fundamental notion of actually proving that the methods developed are indeed the best possible for the tasks at hand.""387440,""Munro, Ian"
"362658"	"Mylopoulos, John"	"Semantic Models for Information Systems"	"Individuals and organizations alike now have access to more data than ever, from a variety of sources ranging from databases to the web and digital media. Unfortunately, data is not the same thing as information. To make meaningful use of the data, users must sift through it, classify and structure it, also maintain it current. Such information management tasks have always been important for managers and knowledge workers. As the amount and variety of available data continues to grow, this form of management will become ever more demanding and ever more critical.The basic hypothesis of this research is that advances in information modeling and knowledge engineering have made it possible to develop integrated technologies along with methodologies which can greatly facilitatethe management of data and turn it into useful information.Our research over the next five years will focus on two topics, one related to software engineering, the other to information modeling and knowledge management. On the first topic, we propose to continue work on asoftware development methodology for agent-oriented software (the Tropos project). Our approach adopts concepts such as ""agent"", ""goal"" and (inter-agent) ""dependency"" to specify requirements and designs for agent-oriented software. This work will result in a systematic method for building software thast can communicate, negotiate, plan and adapt to address stakeholder requirements in a dynamic, open operating environment. On the second topic, we propose to conduct research that extends existing methodologies for designing databases. We also propose to work on novel theories of conceptual models and modelling that combine Logical and Category-theoretic approaches to address syntactic and semantic issues for models.""377259,""Mymryk, Joseph"
"365221"	"Nairn, David"	"High performance analog circuits for large scale integrated systems"	"The objective of this research is to greatly improve the way that digital systems such as cellular phones and patient monitoring devices interact with the real world. In this project, new circuits will be designed to exploit nanoscale electronic devices. Since the nanoscale devices are very small, it will be possible to incorporate some very sophisticated circuits into very small, low-cost packages. This will enable the overall cost of the systems to be reduced. The new circuits will also increase the portability of such systems by greatly extending their battery life. In addition, highly qualified personnel will be trained in this area to further enhance Canada's international competitiveness in both of the communications and medical instrumentation fields.""378292,""Naish, Michael"
"364688"	"Nakhla, Michel"	"Advanced computer-based techniques for simulation and design of high-speed microelectronic systems"	"The current trend towards higher operating speeds and more complex designs demands essential changes in simulation and optimization of high-speed microelectronic systems and modules.  High-speed interconnects introduce numerous signal degradations which are currently recognized to be among the dominant factors limiting the overall performance of microelectronic systems.  In addition, the trend towards low power consumption and increased integration of analog circuits with digital blocks has further complicated the design and analysis process. Also, traditional and artificial boundaries between different design disciplines such as electrical, thermal and mechanical are fast vanishing.  Furthermore, the system performance both in nominal and in extreme operating conditions must be simulated at an early stage of the design, in order to choose design strategies and solutions that lead to cost-effective and reliable products. To cope with these new requirements, coordinated research activities spanning various design domains are proposed with the goal of developing a new generation of advanced computer-aided design methodologies and algorithms to manage the fast growing complexity of high-speed microelectronic systems. The research objectives are focused on generic strategic issues that are common to the majority of high-speed applications. Specifically, the following three interrelated subprojects will be the main focus of the proposed research activities:   a) Modeling and simulation of high-speed circuits and interconnects.   b) Parallel algorithms and high-performance computing.   c) Mixed-domain applications. The proposed multidisciplinary tools and methodologies are targeted towards a wide range of physical implementation levels. In addition, particular emphasis will be given to development of efficient strategies for transferring the new design techniques and tools to the industrial user community.""382565,""Namavarian, Ali"
"365807"	"Nguyen, Ha"	"Bandwidth-effecient cooperative diversity techniques for wireless systems"	"The wireless radio channel poses a severe challenge as a transmission medium for reliable high-speed communications. As a signal propagates through a wireless channel, it experiences random fluctuations in time due to changing reflections and attenuation. This random variation of the channel quality is generally referred to as fading and it is the main reason for poor performance of wireless transmission. Diversity offers an effective solution to combat fading by providing the receiver with multiple copies of the same information that are effectively transmitted with independent channel gains. Spatial diversity with the use of multiple transmit and/or receive antennas is the most effective and attractive diversity method. Achieving the full diversity gain, however, requires multiple antennas be placed sufficiently far apart, which is problematic for size-limited terminals. An alternative approach to provide spatial diversity without using multiple antennas is cooperative communications. This approach is motivated by the fact that the wireless channels from different users to the destination are likely to fade independently and users can assist each other with the transmission of their messages. Because information is transmitted multiple times, once from the user itself and then from the partnering users (or relays), spatial diversity can be achieved. Cooperative diversity has been recognized as a very promising solution for future wireless ad-hoc and wireless sensor networks. Developing practical transmission schemes to realize cooperative diversity is the main subject of this proposed research, which includes both user-cooperation diversity and cooperative diversity with the help of relays and network coding. Different from the previous works, our emphasis is placed on the high bandwidth efficiency of the cooperative schemes in order to support high data-rate transmission. The research work includes both analysis and computer simulation to develop and evaluate the performance of the new techniques. The success of the proposed research program should enhance Canada's competitive position in providing advanced broadband wireless communications systems.""382675,""Nguyen, Hang"
"362765"	"Nkambou, Roger"	"Knowledge Discovery and modeling in Intelligent Tutoring Systems"	"Knowledge acquisition has always been the major bottleneck for knowledge-based systems (KBS). This is especially true for Intelligent Tutoring Systems (ITS) in which knowledge base includes domain ontology and problem solving knowledge.  In order for ITSs to thrive in fields where knowledge evolves rapidly, such as business and industrial settings, new, easy-to-use tools must be developed for knowledge acquisition and maintenance. The first knowledge creation problem finds its source in the fact that the knowledge base usually has to be created manually, from scratch. However, crucial information already exists, hidden in legions of existing documents and databases within communities and organizations. Domain expert are required to make it explicit and provide relevant knowledge to the artificial tutor so that it will be able to guide the learner through competence and problem solving skills development. However, in several domains, problem solving knowledge is not easy to predetermine and it evolves.To find more efficient solutions to these problems, I propose to investigate Knowledge Discovery (KD) techniques to build domain intelligence within an ITS.  First, this project will produce some KD techniques that can support new domain knowledge discovery and ontology evolution in dynamic domains by mining documents flowing in the context where learning will take place. Second, I am interested in the automatic learning of task models from users' action traces to extract new procedures (correct or incorrect), new problem spaces and new problem solving strategies. We know how time-consuming are both the cognitive task analysis and rules authoring process aimed at producing effective problem spaces or task models to support valuable tutoring services such as model and knowledge tracing, coaching, error detection and plan recognition. Current authoring systems fail to simplify these processes. This project offers a great opportunity to overcome those two main stumbling blocks of knowledge acquisition in ITS.""368344,""Nkongolo, Kabwe"
"366016"	"Okouneva, Galina"	"Computer vision algorithms for aerospace applications"	"This proposal describes continuing research sponsored by NSERC held in the Computer Vision Management Laboratory (CVML) at the Department of Aerospace Engineering of Ryerson University. Dr. Okouneva's research interests lie in the area of applications of computer vision to the aerospace industry. In the space domain, applications include planetary exploration, on-orbit rendezvous and spacecraft inspection, automatic docking and satellite servicing operations. Dr. Okouneva's proposed research program research program includes the following projects: 1) Feature selection for 3D registration of range images of spacecraft objects. This project includes the research and development of an intelligent procedure of feature selection on spacecraft objects to achieve the best registration accuracy; 2) Research on properties of the initial guess for iterative pose estimation algorithms, in particular, what parameters of a pose estimation problem affect the initial guess and how to optimally choose it; 3) Optimal Strategy of Target Placement on Spacecraft Surfaces. The goal of this project is to estimate a possible accuracy improvement when targets form an optimized set on the surface of a spacecraft module.The research described in this proposal has substantial significance both for Canada and for the computer vision community as a whole. Future planetary exploration missions, development of satellite technology and satellite servicing, increasing safety requirements for spacecraft vehicles, and on-orbit astronaut's work require reliable vision systems. Dr. Okouneva research programs will highlight perspective avenues for innovation in space vision applicable to industry and academia. The proposed research contributes significantly to the training of highly qualified personnel. The program will support a PhD student and two MASc students each year. Students contributing to the proposed research will gain valuable skills and knowledge. Direct exposure to modern Computer Vision techniques will aid them in developing technical expertise, special engineering knowledge and expertise in computer science which are required for engineering leadership.""365418,""Okouneva, Galina"
"364249"	"OLeary, Stephen"	"Semiconductors for large area electron device applications: materials issues and devices implications"	"Since the dawn of the electronics era, progress in electronics has primarily been achieved through a reduction in device dimensions. There is, however, a certain class of electronic device that requires size in order to be useful. Displays, scanners, solar cells, and x-ray image detectors are all examples of large area electronic devices. While in conventional electronics the emphasis may be presently on sub-micron device features, in large area electronics the focus instead is on electronic devices fabricated over substrates with dimensions of the order of a square meter. As crystalline silicon cannot be uniformly and inexpensively deposited over large areas, alternate electronic materials must be employed instead for large area electronic device applications. Amorphous and polycrystalline (disordered) semiconductors, such as hydrogenated amorphous silicon, have emerged as the active electronic materials of choice for many such applications.The proposed research aims to study some of the fundamental materials issues related to this class of semiconductors. In particular, we aim to develop a series of quantitative relationships between the distributions of electronic states and the various optical properties associated with disordered semiconductors. We will then use these relationships for the interpretation of optical response data corresponding to a broad range of disordered semiconductors of interest. The transport processes that occur within these materials will also be explored. We will then use the insights gleaned from these studies in order to explore the resultant device implications. In particular, the photoconductive response of a variety of x-ray photoconductors (disordered semiconductors) will be modeled and the sensitivity and resolution of proposed digital flat panel x-ray image detectors, using these materials, will be examined. The modeling of solar cells will also be pursued.This research will help nurture the growth and development of the large area electronics industry here in Canada, an industry that has been estimated by some to have the potential to reach $ 100 Billion in sales worldwide by the year 2010.""362927,""Olekhnovitch, Andrei"
"366561"	"Oore, Sageev"	"Adaptive high degree-of-freedom interaction techniques"	"While it is true that computing power and information access has increased by orders of magnitude over the last few decades, it is also true that raw human abilities such as our perceptual and motor systems- astonishing as they are- remain effectively similar from one generation to the next.Yet, the common keyboard/ mouse/ monitor trio is hardly playing to the full potential of human motor and perceptual abilities.The significance of this is that for tasks where today's massive amounts of data and computation are involved,the critical bottleneck in many scientific/creative workflows is now the interface itself-- the bandwidth between the human and machine.As a scientist with extensive professional music experience,I am fascinated by instruments that allow people to perform at an extremely sophisticated level:instruments that require and reward complex cognitive& motor skills and that can be fluidly used to realize sophisticated ideas.I am interested in discovering building blocks of these powerful interaction techniques,and qualities that allow some instruments to be playable in this way.As a computer science researcher I want to create instruments that provide such fluid interaction within the digital realm: tools that let humans naturally drive the immense processing power of today's digital engines to reach their goals and play with their visions more directly.These observations lead to a key question driving my long-term research programme:How do we design interactions in high degree-of-freedom(DOF) audio/ visual/ haptic spaces that allow a user to view and interact effectively with complex data? I explore two different testbeds for data interaction:creating multimedia content and visualizing scientific (aerospace&seismic) data. In both cases,there are shared principles,ideas and techniques for effective and fluid high-DOF interaction,and discovering and identifying these is at the heart of my research.Inasmuch as these are my goals, machine learning algorithms will be a key means of achieving this.Using probabilistic adaptive methods to give the user a wieldy interface over the large complex data sets will be a powerful and practical element of my approach.""375067,""Oosthuizen, Patrick"
"365280"	"Osborn, Sylvia"	"Role-based access control: database systems research"	"The overall goal of this research is to provide theory and tools to help users manage access control in complex computer systems.  Access control is the ability provided in a computing system to say who can do what operations on what data.  This research does not involve cryptography, nor does it involve users identifying themselves to a computer system (this is called user authentication).  Access control takes over once the user has been validated to decide what that user is allowed to do, read and write.  In a complex computer environment, this can be a complex management task.Our research in access control primarily concerns the paradigm of role-based access control (RBAC).  This access control model can be applied to any large, complex system, and can simulate the traditional access control models:  both mandatory access control which is typical of military systems, and discretionary access control which is typically found in commercial database system and operating systems.  As well, complex combinations of access rights can be expressed as roles, which can then be assigned to users or groups of users.Our more recent research has involved how to administrate complex access control systems, specifically RBAC systems.  The original RBAC models assumed a single ""super-user"" administrator controlling the whole system.  It is more realistic to have a model which allows multiple administrators to manage different aspects of a company's resources.  Our decentralized administrative model is designed to allow for such multiple points of control.  It is even possible for a human resources person to assign users to roles, while a security-knowledgeable person can design the roles and how they are related.The scientific approach in this type of research is to develop a model, prove properties of the model, and develop prototypes and tools to demonstrate the model.  The main contribution of the research will be a better understanding of what access control mechanisms can express, and how to better use them.  Any technology which helps the user community to better secure its information systems is a valuable contribution.""376804,""Osborn, Thomas"
"362773"	"Ouarda, Taha"	"Conception et opération de structures hydrauliques dans un cadre de variabilité et de changement"	"La conception et l'opération des ouvrages hydrauliques nécessitent une bonne connaissance des caractéristiques des débits extrêmes. En effet, une surestimation des crues de conception entraîne un surdimensionnement des ouvrages hydrauliques et par conséquent des coûts supplémentaires. D'autre part une sous-estimation peut se traduire par des dégâts matériels et des pertes de vies humaines. L'analyse fréquentielle locale permet d'obtenir de bonnes estimations des événements extrêmes dans le cas où une information suffisante est disponible au site d'intérêt. Lorsque l'information hydrologique au site est absente ou insuffisante, l'estimation peut être effectuée par un modèle régional qui consiste à transposer au site cible l'information provenant d'autres bassins dont le régime hydrologique est similaire. Pour ajouter à la complexité du problème, les changements climatiques (CC) affectent l'occurrence et la récurrence des événements hydrométéorologiques extrêmes et peuvent donc avoir des répercussions majeures sur les infrastructures hydrauliques. Le principal objectif de ce projet de recherche consiste à développer des méthodes permettant l'adaptation à la variabilité et aux CC à l'échelle locale et régionale en termes de construction et d'opération des structures. Ces méthodes représentent des outils d'aide à la décision aux concepteurs et aux gestionnaires pour assurer la conception intelligente et durable et l'opération sécuritaire et rationnelle des structures hydrotechniques dans le contexte du changement anticipé. Le programme de recherche comprend deux volets : A) ESTIMATION LOCALE DES DÉBITS DE CONCEPTION DES OUVRAGES. Les travaux consistent à développer des modèles robustes d'analyse fréquentielle locale non-stationnaire. B) ESTIMATION RÉGIONALE DES DÉBITS DE CONCEPTION. Les recherches concernent le développement d'un cadre Bayésien automatisé pour la combinaison de toutes les sources d'information historique, régionale et climatique, ainsi que de modèles régionaux non-stationnaires de crues dans lesquels les paramètres dépendent des covariables temporels. Les travaux visent également la quantification évolutive de la précision des estimations des quantiles de crues et du risque associé.""362772,""Ouarda, Taha"
"364869"	"Pai, Dinesh"	"Human movement and contact in computer animation"	"The goal of the proposed research is physically based computer animation, particularly the challenging problems involving contact. Computer animation is a topic of scientific and economic importance to Canada, which pioneered the early research in the field and has a large industrial receptor capacity for our students. In the longer term, the proposed research will hopefully also contribute to a deeper algorithmic understanding of human movement and its applications.Specifically, the proposed research will address the following major problems:(1) Interactive simulation of contact.  We will develop efficient algorithms for detecting, representing, and simulating complex contacts at interactive rates.(2) Multiscale neuro-musculo-skeletal models.  Computer animation has traditionally ignored the details of the human musculoskeletal system and neural control that are responsible for movement. We will develop efficient new algorithms for the simulation of human movement at multiple resolutions.(3) Interaction Capture and Activation Capture. We will measure both motion and forces during contact, and estimate muscle activations. This could greatly extend the traditional technique of motion capture.(4) Simulation and control of human hands and manipulation.  We will build a detailed functional model of the human hand. In addition to its inherent importance, this model will integrate our work in musculoskeletal modeling, contact simulation, and interaction capture.""364868,""Pai, Dinesh"
"365361"	"Parsons, Jeffrey"	"Using classification principles to support semantic integration in information engineering"	"The problem of semantics is one of the pressing issues in effective, large-scale sharing of information across sources, such as those available over the Internet. The prospect of a ""Semantic Web"" has received considerable attention in recent years.  Addressing the problem of semantics effectively requires overcoming two challenges: (1) specifying semantics for a single information source; and (2) providing mechanisms for reconciling the semantic specifications provided by independent and heterogeneous sources.  The proposed research will develop new, theory-based mechanisms for providing semantic specifications to information sources and for reconciling independent sources, and will evaluate the effectiveness of these approaches. The research program will focus on two areas. First, it will develop methods to enhance the semantics of structured data sources, such as traditional relational databases, by exploiting the inferential capabilities possible as a result of classifying the instances. That is, when we classify something, we can infer information about the thing beyond what was used to classify it. The effectiveness of these methods will be evaluated using available heterogeneous data sources. Second, the research will use the inferential capabilities resulting from classification to provide a semantic layer to collaborative tagging mechanisms that have recently emerged as popular tools to annotate resources in social bookmarking contexts (e.g., del.icio.us and Flickr.com). We will evaluate these methods by assessing their effectiveness in facilitating the retrieval of information about resources on popular social bookmarking sites, and in Knowledge Management applications.As the availability and scope of networked resources grow, the need for effective methods and tools to manage information is becoming more critical. This research aims to contribute to information engineering for the Semantic Web by supporting the effective search, filtering, and use of online resources.""367026,""Parsons, Matthew"
"362926"	"Pawlak, Miroslaw"	"Semiparametric learning in signal processing, communication systems and pattern recognition"	"Learning is the process of moving from concrete examples (training data) to models that can explain and predict the underlying process.  Formally learning is the problem of recovering (from the training data) a mapping from input signals to output ones. The accuracy of the mapping to be learned from the data depends on the a priori knowledge of the process. Fully nonparametric learning methods do not need any a priori information and therefore are robust and do not suffer from risk of misspecification. On the other hand they exhibit slow learning rate, which deteriorates considerably with the dimensionality of the underlying objects, e.g., images. In contrast, classical parametric learning algorithms carries a great risk of misspecification, but if they are correctly specified they will enjoy fast learning rates with no deterioration caused by multivariate data. These two basic learning schemes have found numerous applications in such diverse areas as: medical diagnostics, data mining, qualitative economics, communication engineering, speech and pattern recognition. In practice, the dimensionality and sparseness of data force us to accept an intermediate model (semiparametric model) which lies between parametric and fully nonparametric cases. The parametric part of the model defines parameters of finite-dimensional projections of multivariate nonlinearities, whereas nonlinear characteristics run through a nonparametric class of univariate functions. This semiparametric model allows one to design practical learning algorithms which share the efficiency of parametric modeling while preserving the high flexibility of the nonparametric case, i.e., we wish to take the best of both worlds. In fact, in semiparametric models the curse of dimensionality can be entirely eliminated. The purpose of this research is twofold. First we propose to examine theoretical advancements, numerical implementations, and testing the accuracy of specific learning schemes within the aforementioned semiparametric framework. Second, we intend to apply this methodology to concrete cases beyond the traditional AI field such as: signal processing, communication systems, pattern recognition , and image analysis.""381529,""Pawlick, Darcie"
"365011"	"Pelletier, Francis"	"Inference: Theories and computation"	"The long-range objective of my research is the development of an English conversational system that handles the problem of inference in a natural, human-like manner.  The issues involved in my more immediate goals can be divided into three separate arenas, together with the overarching aim of fitting them all together.  The arenas are: (a) knowledge representation, (b) natural language semantics and semantic information processing, and (c) automated theorem proving.  A main goal is the continued development of a logically and conceptually nice interface among these three areas in some limited domains (e.g., areas where the information contained in natural language sentences that involve vagueness or quantifier underspecification). Each of these domains has its own independent interest, and the thread that ties them together (and which is the main thrust of my research) is the notion of inference.  In each of these realms, we are to envisage how some agent can take information of the sort relevant to that field and legitimately draw relevant conclusions.  The general methodology I continue to pursue involves a study of how one or another of these phenomena manifests itself in language or in human performance, and then moves to a formal characterization (usually in terms of a logical system, focusing especially on the notion of inference within that system), followed by an implementation phase.  This last phase is then followed by an evaluation of how well it stacks up to human performance.   All in all there is a symboisis among human performance, formal characterization, and machine performance -- all in the realm of inference (in its many manifestations).""377086,""Pelletier, Joelle"
"364697"	"Periyalwar, Shalini"	"Resource management and medium access control in multi-hop cognitive wireless networks"	"The demand for wireless communications is growing, requiring intelligent and economical use of wireless resources (spectrum, infrastructure, etc.). To improve system capacity, research has been focused towards two new areas: multi-hop communications and cognitive radios. This project's objective is to address the resource management and medium access control functions for the combination of multi-hop networks and cognitive radios to deliver the next-generation of high-capacity wireless technology.This project addresses the resource management (channel/power/air_interface selection and scheduling) problem pertaining to multiple wireless hops between source and destination and the dynamic choice of one of multiple air interfaces (WWAN, WLAN, PAN) per hop enabled by cognitive radios. The project will focus on maximizing coverage and capacity in such a wireless environment by using resource management algorithms that exploit the cognitive knowledge (interference conditions, spectrum and air interface usage) in the networks, when available. A complementary part of this project addresses Medium Access Control (MAC)in multi-air-interface cognitive radio networks with multi hop communications. Typically, MAC is specific to the air interface. In a cognitive radio system, the optimal design of the most efficient means of delivering medium access control across diverse physical layers by exploiting the available cognitive knowledge is another area which has not received much attention, particularly from the perspective of supporting multi-hop communications. It is the intent of this research to progress this area to enable the cognitive intelligence to be exploited across all layers of the radio interface in a multi-air-interface, multi-hop system. The above research will be conducted with a combination of analytical tools and simulation tools. The outcomes of the project will provide an evolutionary vision for next generation standards in the area of multi-hop cognitive radio networks.""378377,""Perkins, Edwin"
"366155"	"Pesant, Gilles"	"Design of generic and robust search heuristic in constraint programming to solve practical combinatorial problems"	"This research program aims to develop new algorithms for improved computerized methods to solve complexplanning problems that occur in a number of areas of human activity, with important economic and socialimpacts. Examples of such problems are employee scheduling, sports scheduling, telecommunications design,and transportation logistics. The concrete combinatorial problems encountered in these areas are reputedly verydifficult to solve in a satisfactory manner.Constraint Programming is a powerful technique to solve combinatorial problems. It applies sophisticatedinference to reduce the search space but, to date, lacks a generic and robust search heuristic. Thisputs the latter technology at a disadvantage in terms of ease of use even though its modeling flexibility is anasset to address realistic industrial problems.Global constraints have played a central role in Constraint Programming because they capture keysubstructures of a problem and efficiently exploit them to boost inference. This research project proposes doingthe same thing for search. Whereas most generic dynamic search heuristics in constraint programming rely oninformation at the fine-grained level of individual variables, we investigate dynamicsearch heuristics based on coarser, but more global, information. The search heuristics proposed revolvearound the knowledge of the number of solutions for individual constraints, the intuition being that a constraintwith few solutions corresponds to a critical part of the problem with respect to satisfiability.""376613,""Peslherbe, Gilles"
"363247"	"Petriu, Dorina"	"Software performance engineering in the context of model-driven development"	"Model-Driven Development (MDD) is an evolutionary step in the software field that changes the focus of software development from code to models. MDD is based on abstraction and automation: abstraction separates the model of the application under construction from underlying platform models, and automation is used to generate the code from models. The platform models (PM) are reusable and layered, each providing services to the layers above, and requiring services of the layers below. The Object Management Group (OMG) uses the copyrighted term Model-Driven Architecture (MDA) to describe the initiative of producing standards (such as UML, MOF, XMI and CWM) for MDD.The long-term objective of the proposed research is to integrate software performance engineering techniques in the MDD process. Performance (defined here as the degree of providing timeliness) is a pervasive quality of any software system that is determined by all its layers: application, middleware, operating system, hardware (which may be described as layered ""platforms"" in MDD). A performance model would allow us to reason about the performance properties of the system at the level of assemblies of components/platforms early in the life cycle, when the correction of performance problems is easier and more effective. While previous research of the applicant was concerned with the automatic transformation of a monolithic UML model to a performance model, the proposed research will extend the transformation techniques to a set of inter-related UML models that have to be transformed and composed: a platform-independent model (PIM) of the application and reusable models of the underlying platforms (PMs).There are three short term goals. The first addresses model transformation techniques for generating an overall performance model from a set of input PIM + PMs annotated with performance information. The second goal deals with separating the performance contribution of each platform through performance annotations, which can be composed later. The third goal applies the proposed methods and techniques to two case-studies.""363253,""Petriu, Emil"
"363728"	"Peyton, Liam"	"Model-based engineering of trusted information processes"	"Organizations (and businesses) increasingly operate in an ""on-line"" world in which computer systems are able to access, integrate, and apply data (and digitized content) from sources, both inside and outside an enterprise, around the globe.  The challenge for organizations is to be able to build computer systems that support business processes in a systematic manner that:a)Ensures the requirements and objectives of the organization are being met.b)Leverages relevant data inside and outside an enterprise effectively.c)Secures and authorizes all processing of data according to ownership, privacy and access rights of participants.d)Ensures and documents compliance with legal, financial, and institutional regulations particularly in the areas of privacy and fraud.We define a trusted on-line business process to be a modeled set of coordinated information processing tasks and activities that can be defined and managed dynamically on-line in terms of a), b), c), and d).  We are particularly interested in trusted on-line business processes that share sensitive data between organizations in a business to business (B2B) network.  The focus of our research is on model-based approaches to engineering trusted on-line business processes within a service oriented architecture (SOA).  Health care is an application area for our research that provides a rich, real-world test bed.""376577,""Pézolet, Michel"
"366294"	"Pientka, Brigitte"	"SAVE: towards a foundation for safe and verified software"	"Today, software is an integral part of our infrastructure, and our society increasingly depends on its proper functioning. While we have made substantial progress in creating software tools for proving shallow properties of programs, one important, often neglected aspect of achieving safe and verified software is the language in which the software is written. We advocate a comprehensive approach which supports modelling the operational behavior of programming languages and allows us to specify and verify general safety properties about programs.The goal of this project is two-fold: First, we plan to continue our work towards a practical logical framework for modeling formal systems, in particular programming languages, and mechanically checking some of their meta-theoretic properties. While we have made substantial progress towards this goal, there are significant challenges to permit the rapid prototyping and large-scale experiments with safety policies and realistic languages.  Second, the goal is to bring logical framework technology to mainstream programming. This will be an important step towards narrowing the gap between specifications and practical implementations, and facilitate the reasoning about the actual implementation. The objective is to develop a theoretical foundation for the approach based on type theory and logic, build efficient verification and programming tools, anddemonstrate their effectiveness using realistic applications. Our research ultimately contributes towards a safer information technology infrastructure.""375000,""Pieper, Jeff"
"363029"	"Pierre, Samuel"	"Architectures et plates-formes de services mobiles géolocalisés"	"L'objectif principal de ce programme de recherche est de concevoir une architecture de services Web géolocalisés permettant, entre autres, de découvrir les services Web les plus proches possibles du contexte de localisation des clients dans un réseau mobile. Pour développer les services géolocalisés, la technologie des services Web est généralement suggérée car ces services sont faiblement couplés et doivent être omniprésents, interopérables et disponibles pour d'autres applications. D'où le concept de service Web géolocalisé. À plus long terme, l'objectif visé est de réaliser une plate-forme de création et de découverte de services assujettie aux contraintes d'expansion et de flexibilité/adaptabilité.Pour y parvenir, nous préconisons une approche par décomposition. En effet, la conception de cette architecture doit reposer sur un ensemble de spécifications et d'exigences qu'il faudra définir dans un premier temps. Cet ensemble de spécifications et d'exigences se doit d'être cohérent afin d'éviter des anomalies de fonctionnement qui résulteraient en une architecture non robuste ou peu fonctionnelle. Nous chercherons à définir les contraintes du problème qui doivent être transformées en des requis fonctionnels et non fonctionnels. Les requis fonctionnels nous permettront de développer les fonctionnalités du système. Les requis non fonctionnels, quant à eux, nous permettront d'imposer des contraintes de performance et de communication inter-systèmes telles que l'expansion des noeuds de la  topologie, l'interopérabilité du système avec les entités externes, la réduction du temps global de transit entre un serveur de découverte des services et son MAP associé,  l'autonomie de prise de décision dans la sélection d'un serveur d'applications, la consistance des données de qualité de service et la contrainte de délai de migration. Ces requis fonctionnels et non fonctionnels nous guideront dans l'élaboration du modèle et la conception de l'architecture de services.""362613,""Piers, Warren"
"362858"	"Pigot, Hélène"	"L'assistance cognitive: remédiation des déficits cognitifs par l'environnement"	"L'assistance cognitive promeut le maintien à domicile des personnes avec troubles cognitifs (personnes âgées souffrant de démence et les adultes souffrant de traumatisme crânien, de schizophrénie ou de déficience intellectuelle) grâce à un environnement intelligent qui s'adapte aux besoins de la personne et l'aide à compléter les activités de la vie quotidienne. Les personnes avec troubles cognitifs expérimentent de grandes difficultés pour gérer leur quotidien : (1) rappel des rendez-vous, (2) rappel des activités essentielles à leur bien-être (exemple : toilette, repas, activités sociales), (3) rappel des procédures à suivre pour réaliser des activités même si elles sont de base (exemple : sortir de chez soi sans oublier son portefeuille), (4) aide à la planification de la journée. De par sa conception, ce projet est interdisciplinaire et s'appuie d'un côté sur les besoins des personnes avec troubles cognitifs et de l'autre sur les avancements de l'informatique diffuse et de la modélisation cognitive.Ce projet de recherche sur l'assistance cognitive se décline en trois objectifs. Le premier est tourné vers la modélisation de l'assistance cognitive. Il s'agit alors de modéliser trois aspects de l'assistance cognitive : les séquences des activités de la vie quotidienne qui se déroulent dans le logement, les mécanismes cognitifs et leurs déficits et le dialogue instauré entre la personne et son environnement. Le deuxième objectif  concerne le développement des artefacts environnementaux pour aider la personne, soit un agenda portable et l'assistance cognitive dans une maison intelligente. Dans le troisième objectif, l'utilisation des artefacts environnementaux est évalué auprès de la clientèle.""373131,""Pike, Bruce"
"361153"	"Pitassi, Toniann"	"Topics in proof complexity, circuit complexity, and communication complexity"	"A promising approach aimed at ultimately resolving the P versus NP question is proof complexity. There is a natural family of algorithms corresponding to a given proof system: those algorithms that can be verified to be correct in the proof system. Thus proof complexity gives a natural way of classifying algorithms for solving SAT, and lower bounds for a given proof system implies lower bounds for the corresponding class of algorithms for SAT. For example, lower bounds for Cutting Planes proofs implies lower bounds for a broad class of linear-programming algorithms for SAT and other NP-hard problems.My proposed research is to develop new methods for proving lower bounds for various proof systems, and to use these methods to obtain new insight, new lower bounds and inapproximability results for natural, concrete models of computation. I am also interested in applications of proof complexity to other areas, including learning theory, and algorithms for solving SAT, quantified SAT, and Bayesian inference.""361152,""Pitassi, Toniann"
"366591"	"Psaromiligkos, Ioannis"	"Efficient transmission and reception methods for future wireless communications systems"	"Wireless personal communication has emerged as a key technology that dramatically impacts our society. Wireless cellular and local area networks are currently experiencing significant growth driven by a strong market interest for highly mobile, widely accessible, multimedia communications. To address the ever-increasing consumer needs and demands, current research on wireless communications strives to: (i) Increase Capacity (i.e., increase number of mobile clients in a given geographical area); (ii) Reduce complexity (both hardware and software); (iii) Improve performance through intelligent processing at the receiver and/or transmitter. The short and long-term growth of consumer-based mobile and portable communication systems is tied closely to the achievement of the above four objectives.This research focuses on the investigation of new algorithms that achieve the previously mentioned goals. Special emphasis  is given in the development of algorithms that exhibit improved performance in realistic wireless communications environments. More specifically, this research focuses on the design and analysis of low-complexity reception and highly accurate parameter estimation algorithms, specifically for realistic non-stationary wireless environments where, due to rapid changes in the user population and the physical characteristics of the environment, the size of the data record available for receiver adaptation and parameter estimation is limited.""365593,""Ptacek, Carol"
"362561"	"Pu, KenQian"	"Managing and querying heterogenous and evolving data"	"In recent years, modern information systems have evolved in fundamental ways. As opposed to the traditional client-server architecture of data warehouses, modern informations systems consists of large volumes of distributed data sources, all connected by the Internet.  Aside from being distributed, the data sources are also evolving, with their schemas updated frequently and asynchronously.  Today's information systems are also heterogeneous.  Beyond traditional databases, there is an emergence of novel sources of data.  They include real-time streamed data, data services, sensor and radio frequency identification (RFID) readings.  The proposed research program is on developing techniques for resilient user applications in the presence of asynchronous schema updates.  We will develop techniques to allow user applications and queries to automatically adapt to evolving schema of heterogeneous data sets.  These techniques will also enable mission critical applications to limit the evolution of remote databases with minimal restriction to ensure correct and uninterrupted data access.  The proposed techniques will take into account the unique characteristics of novel data sources such as web services, data streams, and sensor/RFID data.  The proposed program will also study the problem of autonomic storage and exchange of heterogeneous data in distributed information systems.  Our objective is to design efficient layouts for storage and exchangefor unstructured or semi-structured data sets or data streams.  By analyzing the data to be stored or exchanged, we will develop algorithms to automatically design the storage and exchange layout that efficiently make use of available database systems and network resources.  Aside from static data sets, we are particularly interested at data storage and exchange problem for evolving data sets and data streams.""376355,""Puddephatt, Richard"
"362847"	"Quintero, JoseAlejandro"	"Gestion de l'itinérance des réseaux mobiles hétérogènes de prochaine génération"	"L'apparition des réseaux sans fil et l'évolution de la portabilité des terminaux ont encouragé la mobilité des utilisateurs. Les utilisateurs mobiles ont de plus en plus besoin d'avoir accès à un ensemble de services multimédias avancés en utilisant n'importe quel terminal disponible, avec une qualité de service acceptable à travers n'importe quel réseau d'accès. Les réseaux mobiles hétérogènes de prochaine génération essaient de répondre à ce besoin en permettant à l'usager d'accéder à un ensemble riche de services réseau, à partir de n'importe quel réseau d'accès, en particulier celui de recevoir et d'émettre des appels. Les systèmes à terminaux mobiles sont caractérisés par leur habilité à localiser et à identifier un terminal mobile lors de son déplacement et à lui donner accès aux services de télécommunications. Cette capacité à se déplacer est appelée itinérance. De ce fait, le point d'accès réseau pour un usager change lorsqu'il se déplace et l'identité d'un mobile ne fournit pas, implicitement, sa localisation. La gestion de la localisation est un des processus les plus  importants et sûrement le plus complexe dans la gestion de la mobilité, qui permet au système de connaître en tout temps la position courante de l'unité mobile. Il faut donc un mécanisme permettant de localiser l'abonné sous la couverture du réseau. Ceci est un problème majeur pour les réseaux hétérogènes de prochaine génération.  Les méthodes actuelles de gestion de l'itinérance dans les systèmes mobiles engendrent un  trafic de signalisation important dans les éléments du réseau.L'objectif principal de ce programme de recherche est de développer des modèles et des algorithmes de gestion de l'itinérance pour la planification, le dimensionnement et l'exploitation des réseaux mobiles hétérogènes de prochaine génération. Ainsi, nous nous intéresserons aux  algorithmes pour la mise à jour des données de localisation des unités mobiles, aux modèles de gestion de l'itinérance, aux modèles et algorithmes pour la gestion efficace des ressources du réseau dans les systèmes mobiles de prochaine génération.""369061,""Quinton, Bruce"
"362766"	"Raahemi, Bijan"	"Stream Data mining in telecommunication industry with privacy preserving consideration"	"Knowledge Discovery and Data mining is the nontrivial process of extracting implicit, novel, and useful information from large volume of data. It has emerged as a unique combination of several fields of science and technology including statistics, data management, computer programming, machine learning, and artificial intelligence.Stream data mining represents an important class of data-intensive applications where packets of information flow dynamically in large volumes, often demanding fast and real-time processing of the most current information. Applications of stream data mining include web data flow analysis, stock market analysis, and network security.Many of the currently established data mining algorithms perform well on static data yet fall short of addressing the challenges of analyzing data streams including high volumes of data, and temporal characteristics of streaming data. Unlike data processing methods for stored datasets, solutions for analyzing streaming data require fast and memory efficient techniques.The focus of this research is to explore novel methodologies for mining streams of data.  The research is aimed at discovering and understanding the various challenges in stream data mining, and to perform a systematic investigation of the principles, algorithms, and applications of stream data mining techniques. Additionally, since data mining techniques facilitate discovery of hidden patterns which might be confidential and legally protected, we will consider privacy preserving issues in mining streams of data, and develop a framework to compare various privacy preserving techniques.The results of the research will be used to develop efficient and scalable methods for mining streams of data, and subsequently applying them to specific applications in telecommunications industry.""378081,""Raahemifar, Kaamran"
"366602"	"Radecka, Katarzyna"	"Tools for scalable microsystems with emerging technologies"	"With increased requirements on usable systems that were traditionally created on electronics circuit substrates, the demands on the computer-aided design tools have increased. Not only will future systems need to include disparate incoming technologies, with significantly higher aggregate complexity, but the demands for the manufacturability, handling design complexity, and resolving fundamental energy limitations will increase. The objective of this project is to advance and unify the techniques for designing, testing and verification of integrated microsystems. First, we will explore the  synthesis and test of reversible logic that address the energy dissipation bottleneck in a fundamentally better way.  We will further extend the use of the word-level circuit representations, and the means to facilitate expandable and scalable specifications, verification, test and optimization capabilities. We will develop practical tools capable of composing the specifications, while dealing with higher levels of abstractions and reasoning about, and optimizing, the precision of the implementation, all within the same framework. In particular, as a continuation of the recent work, we plan to extend the use of word-level representations to the microsystems that will employ incoming nanotechnology substrates, including the coherent quantum computing and reversible systems. Based on our original quantum circuit modeling platform, and the inherent properties of quantum computing systems, where word-level quantities naturally get employed in quantum bits (qubits), the extension of the word-level representations will be developed as well. Finally, the design case studies of practical distributed microsystems, such as wireless sensor networks will be used to derive reliability, testability and security solutions tailored for low-power and low-overhead implementations.""363683,""Rader, Stephen"
"362411"	"Ray, Nilanjan"	"Hybrid computational strategies for image segmentation and object tracking"	"Image segmentation and object tracking are two fundamental tasks upon which several automated image analysis and computer vision applications rely. Ideally, image segmentation and object tracking algorithms should make use of available information as much as possible. The information about the objects of interest or segments typically can be geometric or topological (e.g., shape, size, solidity or hollowness etc), as well as some image-based characteristics (e.g., intensity/color/texture). Spatial point process techniques, a tool-set from probability and statistics, utilize these pieces of information in a theoretically sound Bayesian framework. However, the drawback of point process techniques is non-efficient computation. On the deterministic side, level set methods, which were originally introduced as numerical techniques for front propagation, are computationally efficient in image segmentation and object tracking. However, level set methods are not as versatile as the point process methods in terms of utilizing the information about the objects/segments. In this research I aim to merge these two fascinating image analysis tools together to complement each other. This marriage of the two methods is a novel concept and is proposed in a theoretically sound stochastic sampling framework. The anticipated outcome of this research is a computationally efficient yet generic image segmentation and object tracking tool.""364371,""Ray, Saibal"
"366614"	"Robert, JeanMarc"	"Secure Telecommunication  Infrastructures"	"With the convergence of the telecommunication networks towards a common IP technology and wider deployments of wireless access technologies (e.g. Wi-Fi 802.11 and WiMax 802.16), telecommunication infrastructures are exposed to greater security risks than ever before. Paradoxically, the private and public sectors, as well as society as a whole, increasingly rely on these technologies to operate.The central objective of this research program is to propose new methods that will improve the capability of large telecommunication infrastructures to mitigate the impact of malicious attacks. In the framework of the proposed research program, we will investigate more closely how to integrate security monitoring functionality into the telecommunication equipment (i.e. the IP routers and Ethernet switches), the application servers and, ultimately, the personal end hosts in order to detect large distributed outbreaks of malicious malware. This aim will be achieved by developing ubiquitous sensors distributed across the telecommunication infrastructures which cooperate with each other and are aware of the network topologies.In the long term, we expect these distributed self-monitoring infrastructures to better detect and react to self-reproducing worms or to large networks of compromised hosts (a.k.a. botnet) flooding targeted services to disrupt them (a.k.a. distributed denial-of service). This novel approach is an alternative to the traditional one, which uses dedicated specialized equipment, such as intrusion detection and prevention systems (a.k.a. IDS and IPS). This equipment generally has only a partial view of the environment, which has the effect of reducing its efficiency, and requiring independent training, deployment and management and to consequently increase their costs.""362051,""Robert, JeanMarc"
"365139"	"Rochette, Martin"	"All-optical signal processing devices based on optical nonlinearities"	"The next generation of optical data transmission infrastructure will be based on optical networks that are transparent. Transparent optical networks are beneficial in terms of cost and complexity by avoiding the conversion between the optical and electrical domains at network nodes and the accompanying wideband electrical processing. In this regard, nonlinear optics is a new topic in optical engineering that has a tremendous potential for all-optical signal processing. Nonlinear optics enables light to directly control light and this represents a key feature for transparent systems. The goal of the research program presented in this application is to develop devices based on optically-induced dynamic filter devices. These include optically reconfigurable filters and laser sources based on all-optical dynamic filters. Transparent optical networks require optical filters with properties such as wavelength tunability, passband adjustability, the ability to be switched on and off, with a short time response, all-optical command recognition, small scale and low power consumption. All-optical dynamic filter could also be used in laser sources to enable ultrashort pulse generation in a Q-switch architecture and stable laser operation in a continuous wave emission architecture. This research program involves extensive numerical modeling, device fabrication and testing, and system experiments. The expected contributions in terms of optical filtering would lead to advancement of the state-of-the-art in unique filtering devices for use in broadband optical communications and optical instrumentation. Additionally, the research in terms of lasers sources based on dynamic filtering would enable unique short pulse sources and narrow linewidth sources. Graduate student training is an integral part of these research activities and students will be involved in all aspects of the work. The results that arise from fundamental studies and initial technology demonstrations associated with this proposal will be important for maintaining the international reputation and leadership of Canada in photonics.""363908,""Rochette, Rémy"
"366581"	"Rogers, John"	"Radio frequency integrated circuits for biomedical applications"	"Wireless communications devices now allow us to get information anywhere anytime.  This technology, often used in consumer applications, can improve our lives by keeping us in touch with others.  In the future, wireless devices can also be used to improve the quality of our health care.  Medical sensors can be embedded in the body, ingested, or simply placed on the surface of the skin from where they relay vital information over a wireless link.  Researching this technology will provide the advantage of medical care with less invasive, lower risk procedures.  Since it is often impractical for an embedded or ingestible sensor to operate from a conventional battery power source, these applications will require innovative low power radios that are self-powered.  New biomedical applications are going to pose challenges both at the circuit and the system level for the radio design.  New radio architectures will need to be developed to handle progressively challenging requirements.  As well, innovation at the circuit level will be needed to implement architectures with extremely low size, and power requirements.  Thus, a broad understanding of issues at all levels of radio design, from device to system architecture will need to be applied.  It will be the goal of this research project to first fully understand all these issues at the circuit and system levels, and then use that understanding to realize new approaches to allow wireless sensors to be used to improve the quality of health care for all Canadians.""372665,""Rogers, Leslie"
"365441"	"Rokne, Jon"	"Interval analysis and computer graphics algorithms"	"The proposed research encompasses a number of areas of activity under the unifying themes of interval analysis, computer graphics and discrete computations. Interval analysis algorithms relating to the fundamental question of computing good inclusions for the range of values of a function over a closed set. These algorithms are applied in global optimization algorithms, computational geometry computations, graphics algorithms and geographic information systems computations. The discrete computations research provides the connection from interval based research to practical algorithms. Algorithms for computing the exact sign of a sum of floating point numbers are used to develop algorithms that compute guaranteed results for computational geometry algorithms.Research is conducted in the area of physically based computer graphics. Particular attention will be given to the applications that are of interest to the current investigation of solar phenomena planned for the International Heliophysical Year in 2007. These applications include computer graphics simulations of the plasma processes in and around the Sun such as the visual display of magnetic structures and the coupled solar-planetary system. This is a continuation of previous research on the visual display of the northern lights and ball lightning.""375675,""Roland, Jens"
"363058"	"Roweis, Sam"	"Practical and scalable algorithms for machine learning"	"Machine learning and statistical pattern recognition have been extremely influential in many scientific and engineering fields which require the analysis and interpretation of large amounts of data. However, many of the state of the art methods in statistical learning involve extremely complex algorithms which require large amounts of computation, both while building the systems (""training"") and at implementation or runtime (""testing""). This significantly limits the scope of problem to which these techniques can be applied, because most algorithms simply cannot be run on large problems in reasonable time or with reasonable storage requirements even on state of the art computing platforms. While much has been gained from sophisticated treatments of machine learning problems, including Bayesian statistics, convex optimization, and Markov chain Monte Carlo computations, these paradigms have overshadowed the strengths of older, simpler but still extremely powerful approaches. In fact, in many large-scale real world applications such as web search, spam detection and computer vision the methods of choice are extremely simple but often customized to fit the particular domain.  The objective of this research proposal is to focus on *extremely simple*, *highly scalable* and yet competitive and effective methods for machine learning problems including classification, clustering, alignment, record linkage, denoising and outlier detection. The approach will be to start with classic algorithms, such as nearest neighbour classification, logistic regression, Parzen density estimation, multi-layer backprop networks and to carefully examine under what circumstances their performance is significantly inferior to more sophisticated ""state of the art"" methods. Having identified their shortcomings, we will focus on understanding the fundamental origin of these deficiencies and correcting them by making modifications (potentially small but important) to the classic algorithms or by deriving new algorithms inspired by the simplicity and scalability of the old ones but designed to avoid their weaknesses.""363057,""Roweis, Sam"
"363898"	"Ruhe, Guenther"	"Planning and re-planning of software product releases"	"Planning and re-planning for software product releases assigns features to releases such that most important technical, resource, risk, budget and stakeholder constraints and objectives are met. Release planning is a computationally and cognitively complex problem: The problem is hard to solve, but even harder to be formulated.Five extensions of existing planning methods are considered:(i) Planning with flexible release dates,(ii) Planning across products (system and product line release planning),(iii) Resource-centric planning of releases,(iv) Re-planning of product releases, and(v) Non-linear release planning and re-planning.Evolutionary problem solving combining the strengths of specialized optimization techniques based on formalized description of the problem with the guided involvement of human experts is the fundamental principle applied to all directions of research. The research is part of a larger effort to provide support for solving computationally and cognitively difficult software engineering decision problems.""380954,""Ruhl, Mark"
"364926"	"Ryan, James"	"Binaural signal processing"	"The human auditory system is able to use binaural cues to determine sound-source locations and to increase speech intelligibility in noise. Due to technical limitations, however, hearing-aid technology has traditionally been applied independently to the two ears. Such independent operation  can lead to the disruption of the binaural cues that are important to hearing. Within the next few years, however, it is anticipated that advances in low-power digital signal processors and wireless technology will enable the linking of hearing aids and other ear-worn devices. A hearing aid with this capability will be much more than today's acoustic amplifier: it will become a true binaural hearing system. Unfortunately, little is known about how to adapt the features common in today's hearing aids in a manner that will achieve an effective binaural hearing system.This research program will develop signal processing algorithms for future binaural hearing systems. The systems targeted by this work are bilaterally fit ear-worn devices that are designed to operate in unison through a data link. The long-term goal is to offer perceptually relevant improvements in sound quality and speech intelligibility compared to similar devices that have no binaural capability.""366228,""Ryan, Jennifer"
"364339"	"Sachdev, Manoj"	"Architectures, circuits for ultra low voltage, low power, robust SRAMS"	"Static Random Access Memory (SRAM) content in integrated circuits is increasing irrespective of its application. In microprocessors up to 90% of transistors are in embedded SRAMs. Therefore, embedded SRAMs influence various aspects of Systems on Chip (SoC) such as power, energy, yield, quality, and reliability. In this proposal, several aspects of SRAMs will be investigated.Power consumption of embedded SRAMs is growing rapidly with integration. The aim of this research component is to realize SRAMs capable of hibernating data at 0.4V without compromising the operational speed. Investigations include devising architectures such that all but the access word during read will have the reduced supply voltage. The research will also investigate theoretical as well as implementation issues of realizing low power, low voltage SRAM circuits.The signal charge at digital storage elements (SRAMs, flip-flops) has become highly susceptible to upsets by energetic particle (alpha particle) strikes owing to reduced supply voltage and shrinking transistor dimensions. By imparting unwanted charge in silicon, these particles can flip the logic state of a node, thus disrupting the circuit functionality. This phenomenon is referred to as a single event upset (SEU) or a soft error. The soft errors related failures in time (FIT) can exceed the FITs caused by all other failures combined by a few orders of magnitude. In particular, SRAM is more prone to soft errors due to its lower node capacitance. In this research we will investigate techniques for soft error mitigation.Technology scaling makes transistors increasingly susceptible to process variations. Transistors with smaller dimensions such as in SRAMs exhibit a higher susceptibility. Therefore realization of robust SRAMs is becoming increasingly difficult. Such issues also give rise to asymmetric cell behavior, offset voltages in sense amplifier, etc. In this research segment we will investigate circuit techniques to mitigate these issues, and enhance SRAM robustness.""364338,""Sachdev, Manoj"
"362040"	"Salem, Kenneth"	"Database management in utility data centers"	"Enterprises are changing the way that they deploy software applications.   Silo-based deployments are giving way to deployments based on shared, centralized pools of computing resources managed in so-called utility data centers.  In silo deployments, a stack of related applications, such as a web server together with an underlying application server and database management system, is deployed on dedicated computing resources,  with dedicated administrative and management functions.  In contrast, the utility data center consolidates computing resources and allows them to be shared among multiple application stacks and managed for the benefit of the enterprise as a whole. One advantage of the utility data center model is cost savings resulting from increased utilization of resources.  The technologies surrounding resource consolidation are becoming more mature, making flexible management of these data centers a reality.    Database systems are a key component in  many application stacks. From the perspective of database management systems, the move from silos to utility data centers is significant because database systems can no longer count on known, dedicated computing resources, as they can in a silo. The goal of the work proposed here is to adapt database systems, and their related management utilities and practices, to the utility data center. This will enable flexible and effective deployment and management of database systems under the new model.""374046,""Salenikovich, Alexander"
"361168"	"Samarabandu, Jagath"	"Goal driven visual object identification and tracking system for a heterogeneous sensor network"	"A smart space is a physical environment that is saturated with sensory, computing and communication capabilities together with the software that integrates them. It will anticipate the needs of its occupants and will provide services as and when they are needed. Such spaces have applications in diverse areas such as healthcare, manufacturing, education, environment monitoring, command centers and security. A key building block that is needed to make such a system a reality is the capability to identify, locate and track objects (both animate and inanimate) in this space. Such a capability also paves way for determining patterns of activity among the occupants of a smart space. In this proposal, we discuss the development of a software framework together with a set of tools for sensory data collection and analysis that will implement this building block for use in a sensor network.Each tool in this system will encapsulate an algorithm that provides a particular functionality such as face detection, speech recognition and object location etc. More importantly, each tool will also incorporate the knowledge required to use the algorithm within itself. Such knowledge includes information about input data, algorithm parameters, output formats as well as information about the task it can accomplish and how well it can accomplish this task.Our primary contribution in this proposal is the development of the software infrastructure that allows a society of expert tools to accomplish a given task, development of  templates for individual tools, development of knowledge representation mechanisms and vocabularies for individual tools as well as example based learning methods that allow certain tools to expand their capabilities.A critical area that we plan to investigate is the aspects of security and privacy in a smart space, which are now gaining considerable attention. We propose to develop a set of security and privacy requirements for a smart space as well as develop the algorithms and mechanisms to support these requirements.""367125,""Samarah, Amer"
"365069"	"Samavati, Faramarz"	"Intuitive interfaced for geometric modelling"	"The main objective of the proposed research is to develop intuitive methods for the interactive design of 3D models, suitable for graphical applications.Geometric modelling has a solid mathematical base. Models are built from several well-studied classes of parametric and/or implicit functions; for example, the use of NURBS in current modelling techniques leads to very efficient algorithms for manipulation and rendering. However, even with sophisticated software packages such as MAYA or 3DS MAX, modelling is still a challenging and time-consuming task that requires significant expertise. The main reason for this is not the lack of flexibility of the modelling software but the way in which users interact with it.  The user interface for geometric modelling using such representations as NURBS and subdivision surfaces is typically based on the idea of manipulating discrete values such as control points or weights associated with control points. The problem is that these manipulations do not correspond to the processes a designer or artist might intuitively use on any natural medium. Humans usually manipulate objects directly by pushing, pulling, bending, squeezing and twisting in 3D. In addition, traditionally other metaphors such as drawing, sketching, painting, and gesturing are added to the repertoire when 2D media are used. In particular, artists (i.e. traditional illustrators) are able to depict 3D forms and shapes with just a few 2D strokes. Furthermore, they use a multi-scale design process that progressively enhances the details of the shapes. Over thousands of years, artists have learned optimum ways of communicating with the human visual system. Although there is rich related work in graphical modelling to utilise such metaphors, there are still many unexplored techniques that provide a fascinating area of further research, with significant prospective applications. To achieve this main objective, I propose several research projects such as sketch-based surface modeller, intuitive interfaces for deformation of volumetric data sets, and sketch-based interfaces for complex models with branching structures.""379944,""Samborska, Bozena"
"366590"	"Sampalli, Srinivas"	"Security and resource management in heterogeneous wireless networks"	"Wireless networks have seen phenomenal growth in the last few years and are being increasingly targeted in a wide range of application areas such as health care, the environment, distance education, pervasive computing, enterprise communications, personal communications and emergency services. Future mobile applications will run on a heterogeneous wireless network platform that will integrate different access technologies such as WiFi, WiMAX, 3G+ Cellular, as well as variable topology ad hoc, sensor and mesh networks. The objective is to give wireless users a secure, seamless access and ubiquitous ABC (""Always Best Connected"") coverage. However, the success of such a network will largely depend on the effective unification of techniques for security and resource management, which stand out as two critical design issues.The challenge stems from the fact that disparate access networks pose their own resource constraints and have varying vulnerabilities.The primary objective of this research is to develop an integrated security and resource management framework for heterogeneous wireless networks. Toward the realization of this objective, it will investigate effective mechanisms to support end-to-end Quality of Service (QoS) for wireless users accessing multimedia services over heterogeneous networks. Vulnerability analysis of each component network will be carried out to determine the intrusions that can be launched. The analyses will be used to design intrusion prevention techniques, with emphasis on lightweight mechanisms. Next, resource requirements of each component network and techniques for maintaining QoS for a given level of security will be evaluated. Finally, an end-to-end architecture that integrates the optimal set of security and resources for a given heterogeneous network configuration will be proposed. The research will provide training for several HQP, including three Ph.D. and twenty masters students. The outcome of the research will be a significant step in the design of ubiquitous and heterogeneous wireless networks, that are not only secure but also provide guaranteed QoS.""383106,""Sampasivam, Lavanya"
"364210"	"Sargent, Edward(Ted)"	"Solution-processed optoelectronic devices: investigating and controlling materials parameters for enhanced performance"	"Semiconductors are the foundation of the information age. Computers run on silicon. The Internet's fiber-optic backbone uses beams of light generated in InGaAsP lasers. Digital cameras register images in silicon sensors for visible light, or InGaAs photodetector arrays when infrared night vision is needed.    Pure, perfect crystals are the basis of much of today's semiconductor technology. However, the equipment to grow single-crystal semiconductors is costly, as is its operation, especially in view of the dangerous reagents that are often required.    We propose herein to advance our work in solution-processed optoelectronics. We have recently shown that spin-cast infrared optical sensors can outperform their perfectly single-crystal counterparts. Now we seek, through the present proposal, to deepen understanding of the operation of solution-processed optoelectronic devices; and to use this understanding to improve device performance through rational control.    We will put forth and experimentally validate a model of the origins of responsivity and noise inside our solution-processed photoconductive photodetectors; demonstrate experimental control, through materials processing, over the structural and electronic properties of materials; and use this control to demonstrate rational engineering of photodetector performance towards to specific application-oriented goals.    In sum, we will pursue the materials chemistry, processing, and device physics to make it practical to achieve high-yielding, consistent, optoelectronic devices tailored to particular applications. At a scientific level, the research will yield insights into the controlled manipulation of colloidal quantum dot bulk materials and surfaces. At an applied level, the research will lead to record-performance devices (one for visible-wavelength video-frame-rate operation, the other tailored to night-vision applications) for imaging. The project will, through the insights it generates, also provide foundational scientific support to our parallel activity in the commercialization of our solution-processed photodetector technology.""365571,""Sargent, Risa"
"363323"	"Sarshar, Nima"	"Multimedia coding and communication for network applications"	"Multimedia streaming spans a wide range of network applications, from video conferencing and voice over IP(VoIP) to TV broadcasting over the Internet (IP-TV). Streaming, as current estimates suggest, accounts for more than a quarter of the overall Internet traffic. As such, efficient solutions to media streaming problem have the potential to directly impact the Internet's bandwidth economy. Current streaming solutions essentially separate network media communication into two layers: media compression layer and network communication layer. In contrast, the proposed research aims at developing new solutions that go beyond the current, layered, approaches. The development of these new solutions is based on the realization that there is an inherent loss of performance when media processing and network delivery methods are designed separately. By a joint consideration of source compression and network communication, our developed solutions will be able to utilize available network infrastructures and bandwidth resources more efficiently. To realize this potential, an optimization framework will be developed that captures the complex entanglement of source coding and network communication. The objective of such optimization will be to maximize a collective measure of quality of service to all network clients, given the bandwidth constraints of the underlying network. Well studied network communication strategies (e.g., routing or network coding) and source coding elements (e.g., progressive coding or more general multiple description coding) will be jointly optimized within this framework. Simple examples (such as the one reported in te proposal) suggest that the impact of using these new approaches to digital multimedia distribution can be significant.Finally, The proposed research will be more than a pursue in efficient algorithms and network protocols. It will also provide new insights into the innate correlation of source coding and network communication and the fundamental tradeoffs between the quality of service possible to different network clients, and advances the scientific knowledge on network communication of compressible sources.""366107,""Sartipi, Kamran"
"364398"	"Sawada, Joe"	"Fast combinatorial and graph algorithms"	"Suppose you are given a bag containing blue and yellow beads and are asked the following  questions: How many different necklaces can you make with exactly 5 blue beads and 5 yellow beads?  For this problem observe that the necklace ""bbbbbyyyyy""  is the same as ""bbyyyyybbb"" when the beads are rotated by 3 positions.  Thus, when you attempt to produce a list of such necklaces, it is not a trivial task to ensure that there are no duplicates.This is an example of a problem encounted in the field of ""Combinatorial Generation"" which is a branch of theoretical computer science.  In this area, the primary research goal is to develop fast computer algorithms to automatically produce exhaustive lists of fundamental mathematical objects - with no duplication.  These objects, such as permutations, combinations, necklaces, and vertex orderings of graphs are often used to model important scientific problems.   As an example, one of the most famous of such algorithms is the Binary Reflected Gray Code.  This algorithm lists all binary numbers of length N so that each successive bit-string differs by exactly one bit from the previous bit-string.   This algorithm was patented and used in pulse code communications.The primary goal of my research is to continue to develop efficient algorithms to produce exhaustive lists for specific objects.  When developing such algorithms, intense study of each object is required, and this often leads to new insights about the object itself.  The algorithms also enhance a toolbox of results that will be crucial for future researchers.""361848,""Sawan, Mohamad"
"366293"	"Sawan, Mohamad"	"Medical microsystems dedicated for wireless sensing"	"Today, numerous severe and spread afflictions that remain not thoroughly understood are related to the central nervous system. Indeed, the complexity of the phenomena involved and the small scale of the afflicted areas, combined with a lack of dedicated tools, hinder investigations. Getting biological insight in situ of tissues and studying neurodynamics require to monitor and control physical and chemical activity at the cellular level. Our research program consists in building dense implantable electrochemical sensing microsystems intended for distributed monitoring and regulation of cortical neurodynamics. The envisioned devices will enable performing simultaneous electrochemical measurements from several sites in numerous areas of the cortex. These neuroengineering capabilities promise to enable multidisciplinary studies and should prompt investigations for new advanced rehabilitation approaches and diagnostic tools. We expect that these capabilities will enable gaining more knowledge and will bring new avenues to tackle neural diseases such as blindness, epilepsy and Parkinson. To do so, we plan to build wireless devices capable of interactions (sensing/actuating) with neural structures and fluids, and aim to build high data rate biotelemetry systems allowing for operation of several devices in ad hoc networks. Increasing implants reliability for long-term and safe operation is also among our goals. If successful, such challenging initiative should undoubtedly advance the state of the art in biomedical apparatus. Counting on this remarkable capacity to innovate, our program is devoted to generate social wealth and contribute to strengthen Canadian hi-tech industry.""387965,""Sawan, Mohamad"
"366575"	"Schlegel, Christian"	"Wireless packet and ad hoc networking"	"Widespread wireless packet and ad hoc networking applications are seen as the next revolution in data communications. Such applications require advanced wireless enabling technologies at the device, system, and computer engineering levels, in particular for ad hoc networks that are to operate without a preestablished infrastructure.It is well known that the information carrying capacity of such highly flexible and largely uncoordinated wireless networks is strictly limited and grows with a rate less than the number of nodes. In order to make such networks efficient, the most appropriate and advanced signal processing methods need to be applied to the physical layer of these networks.Additionally, experiments with known wireless packet protocols such as 802.11 and 802.16 have shown that current accessing methods are inadequate for large uncoordinated ad-hoc networks. New protocol strategies need to be developed which make maxium use of the physical layer capabilities of the individual nodes.This project addresses these issues from two angles: i) designing and verifying novel signal processing methods, in particular joint detection and interference cancellation methods. These are designed to approach the theoretical maximum information flow at any given node. ii) The design and implementation of novel protocols which are based on traffic and signal statistics to maintain high link throughputs and reliability with a minimum of control messages, thus allowing the network configuration to change dynamically without compromising network performance. An important component is the efficient integration of these two research objectives to demonstrate overall network efficiency.""362368,""Schleich, Kristin"
"365222"	"Schneider, Kevin"	"Collaborative software evolution"	"The objective of this research is to increase the effectiveness of software teams in developing next generation interactive systems. A key aspect is the ability to effectively change software in all stages of the software life cycle in response to changing business and technological requirements, and to adapt to user interaction.The research will explore how software changes, how software can be designed to accommodate change, and how team-based development can be coordinated to efficiently affect change. To this end, I will investigate automated software engineering and computer science techniques to facilitate software change, either when the software is being designed or when the system is running.The research addresses software evolution issues of large-scale software systems, but will focus on next generation interactive systems in particular. The interactive systems to be investigated are characterized by their use of aspects such as: novel interaction techniques; 3D graphics; interactive simulation; end user languages; and the utilization of large high resolution display spaces.Models, notations, tools and techniques will be developed using case studies. Formal language processing and software analysis techniques will be used to support automated change of large-scale software systems by software development teams.""373776,""Schneider, Marc"
"361160"	"Schober, Robert"	"High data rate communication for future wireles systems"	"A recent survey by the Wireless Innovation Network in British Columbia (WinBC) found that the wireless communication sector sales in British Columbia (BC) total $1B and over 50 % of the companies started after 1999. Moreover, 75 % of the employees had a university degree, including 24 % with graduate degrees. This clearly shows the importance of the wireless communications industry for Canada and for BC in particular. However, the Canadian communications industry can only stay internationally competitive if high quality research supplies it with new ideas for its products and with highly qualified personnel. The proposed research program was carefully designed to address these needs. Furthermore, although in the last few years we have witnessed several major advances in the wireless communications area, many more scientific and technological challenges have to be overcome to satisfy the still exponentially increasing demand for high speed wireless communication and to improve the quality of wireless services.Of particular interest are novel and practical techniques that enable future wireless communication systems to make better use of the available resources (space, time, frequency, power, etc.) than existing ones. The proposed research program targets some of the most pressing problems in multiple antenna system design and analysis and consists of three parts. In particular, we will investigate (a) new techniques that exploit partial channel knowledge at the transmitter, (b) novel signaling schemes that enable different users of a network to cooperate efficiently, and (c) the analysis and design of communication techniques in a non-Gaussian noise and interference environment.The goal of the proposed research is to provide fundamental theories and technologies that will have a lasting impact on the field of wireless communications, lead to new or improved products, and will be used by other researchers and engineers. While we do not focus on a specific system or standard, many of the anticipated results will be directly or indirectly applicable to existing or future wireless standards.""384119,""Schober, Robert"
"361900"	"Schötzau, Dominik"	"Discontinuous galerkin methods for flow and electro-magnetic problems"	"Discontinuous Galerkin methods are specialized numerical methods for the computer simulation of complex phenomena in the sciences and engineering. Although the first discontinuous Galerkin method was devised back in 1973, these methods experienced a significant development during the nineties which brought them to the mainstream of computational fluid dynamics. In recent years, these methods are being applied to many other problems of engineering interest including electro-magnetics and structural mechanics. The recent success of discontinuous Galerkin methods can be attributed to their flexibility and versatility. These methods can deal robustly with partial differential equations of almost any kind, as well as with equations whose type changes within the computational domain. They are ideally suited for multi-physics applications and for problems with highly varying material properties in complex geometries. Moreover, discontinuous Galerkin methods can easily handle irregularly refined meshes and variable approximation degrees; a property referred to as hp-adaptivity.The long-term objectives of this proposal are the development, analysis and implementation of discontinuous Galerkin finite element methods for the improved simulation of problems in computational fluid mechanics and electro-magnetics. In particular, we plan to address some of the most relevant issues of the current development of discontinuous Galerkin methods: the design of new methods that have fewer globally coupled degrees of freedom, the development of exactly divergence-free methods, the analysis of new and robustly convergent adaptive algorithms, and the devising of new solvers and implementation techniques. We further plan to develop a flexible simulation code for problems in incompressible fluid flow and electro-magnetics, which incorporates modern developments in computational mathematics, such as hp-adaptivity, efficient iterative solvers, and high-order time-stepping.""384762,""Schötzau, Dominik"
"362418"	"Scott, Stacey"	"Advanced interface technologies for interactive multi-user displays"	"Imagine trying to share electronic documents on a communal digital work surface just as you would share paper documents at a meeting table. Fundamental limitations of conventional personal computer technology make this scenario difficult. To address this issue, researchers have begun to explore the use of interactive, multi-user displays, such as interactive wall and tabletop display systems to support collocated collaboration. These systems enable collaborators to interact with digital information, such as maps, images, documents, and sensor data, on large, digital work surfaces. However, designing effective interfaces for these systems is challenging due to their large size, shared input space, and, in the case of a digital tabletop, horizontal display orientation. This research program addresses the lack of suitable interfaces and associated software applications for these systems. It will contribute improved software user interfaces for interactive, multi-user display systems. It will also improve our understanding of how interactive wall and tabletop systems can be effectively utilized in the context of real world collaborative activities. Through this research, students will be trained in human-computer interaction, collaborative technology design, and qualitative and quantitative research methods, which are critical skills for industries developing information technologies to support teamwork, including the defence, healthcare, education, and corporate industries.""379563,""Scott, Stacey"
"361223"	"Sedaghat, Reza"	"Dynamic delay fault testing of high-performance digital circuit in deep submicron technology"	"Aggressive technology scaling has been the mainstay of digital CMOS circuit design for the past 30 years. It has resulted in the design of multi-gigahertz integrated circuit (IC) and unprecedented levels of integration. Modern integrated circuits operate at clock frequencies of more than 3 GHz, and their dies contain close to 300 million transistors. Digital IC performance has followed Moore's law, improving annually by 30%. However, ATE performance has improved by only 12% annually. The discrepancy between ATE edge placement accuracy and circuit under test (CUT) performance1 will make at-speed logic testing increasingly difficult for future deep-submicron technologies (DSM).Most of today's advanced delay faults algorithms are able to propagate those delay faults which are creating logic or glitch faults, but we propose an approaches for gate-delay fault diagnosis in deep sub-micron by a series of injections and evaluations to propagate the actual timing faults as well as those delay faults that eventually creating logic faults to the primary outputs. Unlike the backtrack algorithm that predicts the fault site by tracing the syndrome at a faulty output back into the circuit, this approach propagate the fault from fault site by mapping a nine-valued voltage model on top of a five-valued voltage model. In such a forward approach, the accuracy is much higher because all the composite syndromes at all faulty outputs are considered simultaneously. As a result, the proposed approach is robust and applicable even when the delay size is relatively small. Experimental results show that the number of fault candidates produced by this approach is considerable.""361934,""Sedaghati, Ramin"
"362865"	"Selouani, SidAhmed"	"Reconnaissance distribuée et robuste de la parole optimisée par une approche évolutionniste et intégrant un dialogue coopératif"	"Le but du projet est de proposer une nouvelle approche pour la reconnaissance de la parole distribuée (DSR: Distributed Speech Recognition) dans la perspective du développement des réseaux sans fil supportant les standards de troisième et éventuellement de quatrième génération (3G et 4G). Un consortium formé autour du projet de partenariat de 3ème génération (3G Partnership Project : 3GPP) a recommandé comme codeur-décodeur (codec) pour les services de commandes vocales, l'utilisation du système évolué de codage étendu (XAFE : eXtended Audio Front-End). Notre approche constitue une alternative aux codecs DSR-XAFE actuellement utilisés, en proposant d'incorporer une technique d'analyse acoustique multi-variable plus robuste et à plus bas débit, dont l'optimisation est effectuée par une approche évolutionniste afin d'assurer une auto-adaptation et une meilleure robustesse du système de reconnaissance dans des environnements acoustiques changeants du coté client. Par ailleurs, nous visons également une innovation au niveau du système de dialogue, implémenté coté serveur, qui améliore la prise en compte de l'aspect sémantico-pragmatique du dialogue personne-système (DPS). En effet, dans la plupart des systèmes de dialogue Personne-Système, l'un des problèmes majeurs est celui relatif au maintien de l'interaction en dépit des erreurs de compréhension ou d'interprétation par le système : il s'agit de continuer le dialogue de manière constructive malgré les incompréhensions et de lever les malentendus entre le système et l'utilisateur. De ce fait, notre projet s'inscrit dans une perspective mondiale, qui préfigure du monde multimédia mobile de demain en proposant des systèmes fondamentalement avancés en termes d'interactivité avec tous les utilisateurs, avec une attention particulière pour ceux atteints de divers troubles du langage. En effet, il est prévu dans le cadre de ce projet, de mettre à leur disposition, lorsqu'ils communiquent à travers les réseaux mobiles, un système-prothèse générant de la parole intelligible à leur place.""374076,""Selvadurai, AntonyPatrick"
"366008"	"Shahrrava, Behnam"	"Turbo synchronization for group-blind multiuser detection"	"This proposal considers the problem of synchronization and channel estimation for turbo multiuser detection (MUD) in CDMA systems operating over multipath fading channels.The aim of this project is to investigate using the estimated synchronization and channel parameters in a manner motivated by the general turbo principle. Specifically, we consider the synchronization unit to be another decoder, which will be employed in a turbo scheme. Instead of passing a synchronized signal, the synchronizer passes extrinsic information for each of the coded bits to the next stage.Finally, in this project we propose a receiver structure that takes all the necessary signal processing operations into account simultaneously. In other words, the proposed iterative receiver can jointly perform synchronization, channel estimation, channel equalization, blind/group-blind MUD, and channel decoding. The  proposed  iterative (turbo)  processing  approach  allows  earlier  stages  (or submodules) of a receiver (e.g. the synchronizer and the channel estimator) to refine their processing based on soft information obtained from later stages (e.g., the channel decoder) to provide a refine estimate of some signal parameters.The expected results in this work are as follows: the applicant and his research group can develop a mathematical framework for the design of turbo receivers that can perform jointly synchronization and channel estimation with muliuser detection according to the maximum a posteriori (MAP) criterion, using the expectation-maximization (EM) algorithm or the space alternating generalized expectation and maximization (SAGE) algorithm.The performance of the proposed turbo receivers can be expected to be much better than that of the conventional (or non-iterative) receivers but very close to that of the ideal receiver which assumes perfect knowledge of synchronization and channel parameters as well as all the signatures of active users in the system.""367129,""Shaker, George"
"364084"	"Shami, Abdallah"	"Hybrid next generation access networks"	"Recently, access networks have been exposed under substantial challenges with the exponentially increasingper-user bandwidth demand and ever increasing backbone capacity. Although current access technologiesoffer affordable solutions for residential data users, they still pose fundamental distance and bandwidthlimitations. Next generation access networks are expected to be able to support various emerging broad-bandapplications as well as emulating many kinds of legacy services over the same infrastructure, with minimalengineering investment.This proposal focuses on the design and analysis of wireless and optical broadband access network solutions aswell as on hybrid wireless/optical access network architectures. Namely, wireless communications will offerubiquity and immediate access to the medium. Alternatively, fiber optics will provide ample bandwidth, longreach, and low interference. Specifically, this research proposal intends to devise and validate innovativenetwork architectures and algorithms to address the ""access bottleneck"" problem. Here the end goal is a suiteof innovative wireless/optical network architectures and protocols that provide ratepayers with fair access,lower costs, higher speeds, and greater bandwidth choices.""366780,""Shamim, Atif"
"362566"	"Shayan, Yousef"	"Towards reliable and high data-rate wireless communication systems"	"In recent years, residential, business and mobile users are combining high speed internet access with real-time applications such as voice and video which results in high data rate wireless communication systems. In these systems, achieving high reliability is a very challenging task due to multi-path fading. The recent technology called ""Multiple-Input Multiple-Output"" (MIMO) promises a solution for this problem.A common drawback of MIMO transmission schemes is that they are not flexible in rate-performance tradeoff. Some work has been reported to address this problem but more novel approaches are required for addressing increasing data-rate requirement of mobile users. Another drawback of MIMO systems is complexity of the mobile receiver due to availability of multiple antennas. To resolve this problem, technologies will be developed to achieve small and low cost mobile units.For more efficient use bandwidth in today's multi-rate wireless systems, signaling parameters must adapt to varying channel conditions and required quality of service. Therefore, adaptive coding and modulation which is a standard feature of modern wireless systems including MIMO-OFDM has to be developed. This is another topic to be investigated in the proposed research.Design of a new Space, Time and Frequency spreading technique for MIMO OFDM-CDMA is another important task to be performed in this project. In addition to improved performance via diversity, space-time-frequency spreading offers increased degrees of freedom in multiple access scheduling. Motivated by the fractional frequency reuse in WIMAX standard, multiple access scheduling schemes will be developed to fully taking advantage of the increased degrees of freedom in MIMO OFDM-CDMA.""370048,""Shayesteh, Alireza"
"365053"	"Shepherd, Bruce"	"Algorithms, graphs and polyhedra"	"The main focus of research is  the algorithmic theory of network routing and design. The PI's previousposition in industry provided continual exposure to new fundamental research problems arising from realtechnological constraints (such as those for optically-switched networks or traffic engineering and network routing protocols). The PI is also exploring connections between geometric relaxations for combinatorial optimization problems and efficient algorithms with provable approximation guarrantees. In the latter endeavour, techniques from polyhedral combinatorics are especially exploited.""375578,""Shepherd, Gordon"
"366106"	"ShiriVarnaamkhaasti, Nematollaah"	"Towards advanced data management systems"	"This research aims at development of advanced data management techniques, which uses and/or extends existing database theory and technology for modeling data and processing queries required in numerous emerging applications. Examples of such data includes logical data, uncertainty data, web logs, sequence data, etc. Our focus is on logical data and sequence data. We consider logical data in information integration and study query rewriting and evaluation of conjunctive queries using views, where the queries and views are logical rules possibly extended with constraints which are linear arithmetic comparisons. Literature discusses attribute comparisons and for certain classes of connstraints practical solutions are developed to find maximally contained rewritings, when exists, for queries with all but inequality comparisons. We will develop constraints resolution and optimization modules for the rewriting algoritms in our context above. This extends our results in CoopIS 2005, where we considered constraints which are linear arithmetic expressions with equality.In another research theme in this proposal, we investigate modeling and processing sequence data, which do not have any structure. Emerging applications generate and use enormous amounts of such data. We focus more on biological sequences: DNA and proteins. The main issues are the required uge storage space and lengthy process of index construction. Further, efficient support for exact match and approximate searching is central to such applications. Hence a desired solution should be space efficient, scalable, with resonable index construction time and efficient various search tecniques. While string searching is not new, the enormous size of ever-growing biological sequences, and the variety and volume of queries to be supported pose new research challenges. There is a growing need for more sophisicated indexing structures and techniques. Based on our results in CIKM 2005, the expected contributions of this work include both theoretical and practical advances in the management of biological sequence data.""365061,""Shirmohammadi, Shervin"
"363975"	"Sica, Francesco"	"Elliptic curve cryptography - implementation and security issues"	"Privacy and security are two obvious necessities of modern life. Yet, when it comes to protecting digital data, these are new concepts that are still not well understood. The invention of public-key cryptography in 1978 made such endeavours become a reality, so that now we have a practical way of performing securely many electronic tasks where a proof of identity is essential. Signatures, authentications, key agreements are nowadays part of our daily electronic life, be it in an online transaction or when withdrawing money from an ATM.Still many problems remain, also because hackers have increasingly more sophisticated attacks.In this line of thought, we require from the public-key primitives that perform these tasks that they achieve a standard level of security, with the fastest possible implementation.My research involves speeding up these computational implementations through the use of elliptic curves, which  are now faster than RSA.In another direction I will also investigate the security of elliptic curves and make sure that nobody can produce ""phony curves"" which are easily crackable.""375569,""Sica, Robert"
"364842"	"Smid, Michiel"	"Optimization problems in computational geometry"	"The general theme in this project is the design, analysis, and implementation of optimization algorithms for geometric problems. The research will concentrate on the following three topics:Geometric networks and shortest path problems: How to construct a good network that connects a given set of sites. Given such a network, how to use geometric information to compute (exact or approximate) shortest paths between any two given sites.Geometric problems in computer-aided manufacturing: How can techniques from computational geometry be used to improve the time needed to manufacture three-dimensional objects from their CAD-representations.Geometric data structures: How to organize geometric data, such that a ""summary'' of the subset of the data that is contained in any query region can be computed.""374797,""Smilek, Daniel"
"362370"	"Smyth, William"	"Improved algorithms on strings"	"I conduct research into methods (algorithms) for processing sequences of letters called strings -- objects so simple that there seems to be no need of ""algorithms""!  But, first of all, strings are ubiquitous: a text file, a computer program (millions of letters), a book (in English or Chinese), the human genome (3 billion letters), one day's e-mail traffic (trillions of letters).  Secondly, efficient processing of strings is essential: repeating substrings in a genome may be of great biological/genetic significance, phrases may need to be identified/changed in a text file or computer program, books may need to be searched for significant phrases, e-mail traffic may need to be searched for undesirable messages (spam).For 18 years I have been designing such algorithms.  Current and future research focusses on three main areas:(1) Using recent insights into the nature of ""regularities"" in strings, I seek new methods to efficiently compute repeating substrings -- with applications particularly to computational biology, data mining and data compression.  Essentially I seek a more precise mathematical understanding of periodicity in strings.(2) The ""suffix array"" is a specialized data structure used to improve the efficiency of string algorithms.  I seek new efficient algorithms that use suffix arrays, again with frequent application to computational biology and many other areas.(3) In many problems that arise in practice, it may be necessary to process strings that contain ""indeterminate"" (not fully defined) entries.  I seek faster more effective algorithms for doing pattern-matching and other operations on these indeterminate strings.""373297,""Snedden, Wayne"
"364953"	"Somayaji, Anil"	"Improving security through lightweight learned models"	"The insecurity of modern computer systems threaten the success of the Internet.  Malicious software, in the form of viruses, worms, and spyware are widespread, causing data loss, privacy compromises, and losses of service. Even more disturbing, phishing emails and web pages now automate social engineering attacks on users themselves.  These problems stem from the imperfections endemic in complex software and the limitations humans have when interacting with such systems.  If we are to meaningfully improve the security of computer systems, we need mechanisms and strategies that can succeed in the face of such limitations.My ongoing research program is centered around the idea that lightweight, learned models based upon observed behavior can be used to improve computer security.  Models of program behavior can be used to detect security violations; models of networks can be used to reduce the impact of malicious network use; models of users can detect the presence of unauthorized individuals.  In the future I hope to develop, evaluate, and prototype models that can provide flexible and robust protection against the ever changing threats facing computer users.""366391,""Somé, StéphaneSotèg"
"365960"	"Sousa, Elvino"	"Autonomous infrastructure CDMA wireless networks"	"This project introduces a new concept in cellular communications networks - that of autonomous infrastructure wireless networks. These networks are designed based on an enhanced 3G or future 4G physical layer withenhancements that allow base stations and access points, i.e. cellular telecommunication network infrastructure, to be deployed automatically in an autonomous and organic fashion by the network users. This project introduces a vision for cellular networks that allows these networks to have the characteristics of WiFi networks in terms of ease and cost of deployment but with the benefits of cellular technologies that were developed specifically for the infrastructure mode of operation and consequently display dvantages in terms of interference management - a weak point with wireless LAN networks such as WiFi. With these characteristics the autonomous cellular concept allows the development of 3G compatible cellular networks that have greater coverage, security, and stability, and at the same time the broadband characteristics of WiFi networks.""370882,""Sousa, Rodolfo"
"366185"	"Srinivasan, Venkatesh"	"Randomness and complexity"	"This research program studies the strengths and limitations of randomized algorithms in various computational settings. A randomized algorithm is an algorithm that has access to a source of random bits and uses it as an auxiliary input to guide its behaviour. During the last two decades, understanding the role of randomness in computation has been an fascinating area of research leading to several fundamental results.It is now well known that randomized algorithms are capable of performing some computational tasks much more efficiently than known deterministic algorithms in specific settings, even those that are impossible deterministically. There is also good evidence that randomization does not give additional power in a general setting.However, our understanding of the true power of randomness as a resource is still far from complete. The goal of this proposal is to further explore the power of randomness in various computational models and to find new applications of these results to other areas of computer science.""384920,""Srinivasan, Venkatesh"
"362520"	"Stacho, Ladislav"	"Combinatorial algorithms in bioinformatics and communications networks"	"In my research, I study problems that arise in graph theory and related areas, often motivated by applications incurrent network technologies such as high-speed networks, and bioinformatics.With my students I develop new algorithms for various practical problems. Recently, these algorithms were for faster and better long sequence alignment, for haplotyping via galled tree networks--a problem related to genetic desease susceptibility, and inverse protein forlding--an area playing a major role in modern drug designs.In the area of communications networks I am interested in two fundamental problems, the virtual path finding and topology control. The mathematical structure inherent in these problems has made them central to the theoretical computer science community. My interest in theoretical cost models for mobile (ad-hoc) networks is motivated by the fact that most previous work is experimental with little qualitative analyses.""361214,""Stadnik, Zbigniew"
"364071"	"Stevenson, Suzanne"	"Automatic acquisition and use of lexical semantic information"	"Recent natural language processing (NLP) applications---such as web searching and information extraction---have achieved impressive results by adopting `knowledge-lean' approaches that treat word semantics very superficially.  Nonetheless, we are frequently left with a feeling of frustration that these systems do not truly understand our queries or the documents being processed.  Building systems that can automatically determine the meaning of electronic text at a deeper level requires more sophisticated linguistic knowledge about words, yet hand coding such knowledge is infeasible on a broad scale.The research proposed here has two primary goals.  The first goal is to develop methods for the automatic acquisition of lexical knowledge, in order to better capture the complex information encoded in words, as well as their interrelationships.  We adopt a `knowledge-informed' approach that is distinguished by the incorporation of linguistic and psychological observations into novel techniques for statistical corpus analysis, enabling us to alleviate the knowledge-acquisition bottleneck in NLP.  The second goal of the research is to apply the same knowledge-informed perspective within the processing of lexical resources, to ensure that computational lexicons and word/concept ontologies can be used more effectively and with less manual effort.In the long run, research on automatic acquisition of deeper lexical information from text corpora, and techniques for more effectively using the resulting computational lexicons, will revolutionize the use of computers.  Even in the short term, the research proposed here will lead to improved NLP applications, such as web searching based on actual meaning rather than simple keyword matching, and information extraction that can be applied beyond circumscribed types of extracted information within a narrow domain.""371938,""Stevenson, Tyler"
"365227"	"Storjohann, Arne"	"Exact linear algebra: nearly optimal algorithms for nonuniform problems"	"The objective of this research program is to design and analyse new algorithms for the exact solution of problems in the area of linear algebra.  Our focus is on problems involving matrices filled with integers and polynomials.  The goal is to discover algorithms that make efficient use of computer resources including time (number of required machine word operations) and space (memory requirements).Fast algorithms for exact linear algebra are a central component of the mathematical engines of computer algebra systems.  Computer algebra is very widely used by researchers and practitioners in the mathematical, scientific and engineering communities.  Algebraic computations arise in diverse areas such as graphics and robotics.  The solutions of complex problems in these areas typically requires the solution of exact linear algebra problems, either as key steps during the computation or, in many cases, as the computational bottleneck.  Computational problems involving large integer matrices arise, for example, during the security study of certain types of cryptographic systems.  The development of new algorithms that are both rigorously analysed and practically fast is especially welcome.  We plan to implement prototypes of the algorithms we design and use them to solve large problems occurring in practice.""379505,""Storms, Reginald"
"366290"	"Stuerzlinger, Wolfgang"	"Enhanced software tools for early 3D design process"	"Today, 3D design is important to many application areas, such as architecture & interior design, industrial & mechanical design, simulation, training, entertainment, etc. Most current 3D design software is targeted at the late design process, where refinement happens. Few tools can be applied productively in the early design process, where ideas are generated, alternatives are investigated, etc. The consequence of this is that designers do not explore the design space well, which leads to suboptimal design results. The insight behind the current proposal is that different tools are needed for the early phases of the 3D design process to enable people to be creative, explore the design space better and hence generate better designs, which in turn leads to more competitive products. While tools cannot directly be creative, they can stimulate creativity. For example, software tools for the early design stages need to be simple to use - so that users can concentrate on the design, not the interface - yet powerful and need to foster creative exploration. My previous work has already yielded 3D software that is easy to learn and simple to use.The current proposal will develop enhanced software tools for the early 3D design process. In particular, it proposes novel techniques to create and modify common object shapes such as slanted surfaces and curved objects. Also, we will look at efficient methods for the easy manipulation of large-scale designs. A second area of research will develop software tools to make work with several versions of a design easier, as early design work rarely involves only one design alternative. Another aspect will focus on merging different versions and investigate techniques to support the iterative nature of the design process better. The third area of research will target collaborative design, as most 3D design projects involve several stakeholders. Due to limitations of existing systems, collaboration is usually limited in the design process. Based on a novel interactive table and wall infrastructure, the research will evaluate in collaboration with professional designers how design is affected by systems that allow participants to freely collaborate during a meeting.""362444,""Stull, Roland"
"365196"	"Szczecinski, Leszek"	"Analysis and design of iterative (turbo) digital receivers"	"The objective of this project is to develop methods for analysis of iterative (the so-called ""turbo"") processing employed in digital receivers as a mean to yield close-to-optimal performance, i.e. almost-error-free communication in adverse conditions. The timeliness of this proposal is due to ever-growing importance of digital communications integrating profoundly the modern life. New technological solutions are ""a must"" to yield cost-effective products, so research efforts are continuously deployed to design digital transceivers. This research is related to the invention of the so-called turbo-codes, whose break-through idea of iterative processing was quickly extended on many applications. Using the ""turbo"" principle, the receiver is divided into relatively small blocks (devices) processing the data locally, and exchanging the reliability signals iteratively to approach the globally optimal processing. The reason behind the growing interest in turbo processing is that it yields close-to-optimal solutions with manageable complexity, bringing the performance of practical receivers close to its theoretical limits. It is, therefore, important to analyze theoretical and practical aspects of the iterative algorithms and study their applications.  Currently, various ""turbo"" algorithms are being proposed but the tools to analyze them formally are scarce. This is because the iterative processing is highly non-linear and resists formal analysis in a majority of cases which significantly limits the researchers' capacity to devise new receivers. This provides motivation for our research. Our objective is to devise analytical or semi-analytical tools to obtain probabilistic models of messages exchanged between the constituent devices of the receiver, with benefits such as simple and optimal design, as well as adaptation of the receivers to the time-varying channel conditions.The funding will be used to support graduate students and to disseminate the research results.""364992,""Szczyglowski, Krzysztof"
"364476"	"Szpakowicz, Stan"	"discovering subjectivity in language"	"The abundance of texts on the Internet makes exciting research projects in computational linguistics possible. Texts need to be interpreted intelligently and affordably, but sometimes more costly processing leads to linguistically more satisfying results. Certain properties of texts require deep processing, even if they can to some extent be discovered with simpler methods. A case in point is sentiment analysis, a new and active area of research. The results have been the strongest in recognizing polarity (e.g., positive/negative movie reviews); sometimes a neutral case is included (e.g., buy stock, sell stock, do nothing). The intensity of opinion has been studied too. All these phenomena are instances of subjectivity in language. In recognizing subjective expressions or classifying texts by opinion values, researchers often apply supervised and slightly primed unsupervised machine learning. The former depends on the availability of annotated lexical resources, whose necessarily manual creation is labour-intensive and obviously subjective.My research has dealt, and will continue to deal, with subjectivity. The language of electronic negotiations contains elements that can predict success or failure, such as expressions of the negotiator's attitude. Short story summarization is a challenging new variation of my work on automatic summarization of news items. Fiction, with its subjective nature, requires methods different than news: complex syntactic and semantic information, including indicators of subjectivity. All my work relies on statistical language processing and on symbolic techniques which include full parsing and linguistically motivated heuristics; this dual strategy will be kept in the proposed research. Work on the language of negotiation will continue once new relevant text data become available. Short story summarization will go on, with much more emphasis on subjectivity. Summarization of news articles will evolve in a novel direction: constructing summaries of multiple documents with varying viewpoints and opinions. Work has also begun on recognizing certain types of opinions in blogs.""365688,""Szpunar, Jerzy"
"361551"	"Szymanski, Ted"	"Traffic engineering for emerging telerobotic services"	"Canada is emerging as a world leader in Telemedicine and Telerobotic  services. The world's first telerobotic-assisted operation over the Internet was performed in Canada in 2003, between Dr. Anvari of McMaster University's Center for Minimal Access Surgery (CMAS) in Hamilton, and a patient in North Bay Ontario, over a network using CISCO MPLS and VPN technologies managed by Bell Canada Enterprises (BCE). The Canadian Space Agency and the US NASA have also partnered with McMaster's CMAS in a project entitled 'NASA Extreme Environment Mission Operation' - (NEEMO-9), with the goal to assess the prospect of Telerobotic Surgery in future space missions. Telerobotics is also being exploited in automated mines in northern Canada, and there is speculation of high-pressure under-water Telerobotic Mining, as a means of recovering more resources from harsh environments with less risk to human life.While emerging Telerobotic Services may have a potentially significant impact on our way of life and our future economy, there are several key challenges before such services can be delivered on a wide scale basis. The control of telerobotic systems is highly sensitive to the network delay: a small fixed network delay can usually be tolerated in the control system, while any highly variable network delay will usually render telerobotic control ineffective. Traffic Engineering can be informally defined as the use of computer models to plan, deploy and manage end-to-end connections over the Internet, while meeting upper and lower bounds on the end-to-end bandwidth, delay, delay jitter, reliability and availability. This discovery grant will explore the topic of 'Traffic Engineering' over wired and wireless networks to support important emerging Telemedicine and Telerobotic services. In particular, this application will focus on switch scheduling algorithms to eliminate virtually all avoidable variable delays (delay jitter) in the Internet.""375379,""Szyszkowicz, Barbara"
"361397"	"Tahvildari, Ladan"	"A model-based framework for self-adaptive software"	"As software systems become more interconnected and diverse, architects are less able to anticipate and design interactions among components, leaving such issues to be dealt with at runtime. The objective of this research project is to investigate, design, and develop a novel model-based framework to enable self-adaptive software. The proposed methodology can be divided into three crucial phases: i) developing methodologies and tools to define and describe a rich syntax and semantic common context to allow definition of rules for reflective component models and dynamic composition and interactions between them, ii) designing a feature that accommodates an extensible set of reconfiguration algorithms, and ii) devising a systematic reconfiguration engine that monitors a system and decides whether a change is necessary.We anticipate that such a framework will allow self-adaptive software to be developed with a clean separation of concerns. We also believe that the proposed model-based mechanism will carry out changes which consist of reconfiguration algorithms and change constraint components which can be considered independently from both the application and the adaptation process that drives change. Finally, we anticipate that the adaptive change engine will lead to decisions as to what change is necessary, and what mechanism should be used to carry out that change.This research leverages the value of existing software systems, and provides engineering approaches to increase the return of investment for corporations by prolonging the lifetime of their software systems. The proposed research will be validated through experiments with industrial and open source software. We believe the developed techniques will help industrial organizations to effectively keep their adaptive systems up-to-date during their evolution and maintenance.""372442,""Tai, Kelly"
"365425"	"Takaya, Kunio"	"3D shape recovery from images viewed by multiple video cameras"	"The proposed research ""3D Shape Recovery from Images Viewed by Multiple Cameras"" will attempt to estimate the 3 dimensional coordinates of all pixels in a 2 dimensional image with the help of additional images viewed from multiple cameras.   3D modeling of objects and scenes contained in multiple 2D views is the conceptual objective.  This research goes beyond still images and works on video images viewed by a multi-camera assembly of 4 (or 5) video cameras interfaced to a personal computer.  The real time multi-camera vision system continuously produces and displays a dense stereo disparity map that shows the distance to every point of the observed scene by the multiple cameras.  The system is also capable of displaying a virtual reality image viewed from an arbitrary viewpoint separate from the positions of real cameras.  Established mathematical theories of multiple view geometry are extensively applied to the epipolar (cross-eye stereo) configuration as well as to the parallel view axis configuration.  Potential applications include (1) robotic applications, such that the multi-camera vision system continuously provides distance information to a self-navigating robot for collision avoidance, (2) video surveillance capable of detecting 3D motions to indicate whether a video object is moving closer to or away from the cameras, and (3) synthesis of 3D stereo views for projection on stereoscopic 3D LCD (Liquid Crystal Display) equipped with the parallax barrier,  which will set a new standard for stereoscopic video, video games and TV.""373955,""Take, William"
"364020"	"Tapp, Alain"	"Quantum information"	"Quantum information is a relatively new field that involves physics and computer science. Researchers involved in quantum information study how quantum mechanics can influence our capacity to process information. It turns out that quantum mechanics can help in solving computational tasks, performing distributed computation and performing new cryptographic protocols. It is also a field where quantum mechanics is studied using the tools, ideas and concepts belonging to computer science. Although the field is young, it has been extremely successful. In this proposal, I wish to address both how computer science can help to understand physics and how quantum mechanics can be used to solve useful computational tasks.  First, I plan to study how the quantum computer can compute functions more efficiently than the classical computer. Computer science is also interested in distributed computation problems where the amount of communication is the main concern. We know that entanglement can some times be used to reduce communication. I plan to study the relationship between entanglement and communication. One of the areas where quantum information was extremely successful is cryptography.  I also wish to study how quantum information can be used to enable a group of entities to perform computation on private data without revealing precious confidential information. Finally, I wish to study quantum interactive proofs.""371077,""Tappenden, Andrew"
"364122"	"Tennent, Robert"	"Paramatricity and possible worlds"	"Representation independence is of primary importance in modern programming methodology: it is at the heart of the specification/implementation distinction for such linguistic features as modules, abstract data types, classes, and application-programmer interfaces. However we still lack a completely satisfactory  *theoretical* foundation for this fundamental concept.A notion of relational parametricity introduced by J. C. Reynolds in 1981 is clearly relevant and we will attempt in this project to find a rigorous general formulation of parametricity using concepts of category theory, a branch of abstract mathematics. We expect this to lead to significant advances in our abilities to analyze programs and to verify system correctness.""383051,""Tennessen, Karl"
"366279"	"Toms, Elaine"	"Designing task sensitive retrieval systems"	"To date information retrieval uses a 'bag of words' approach which primarily limits retrieved items to thosewith at least a partial word match. The output from these system rarely provides a fully relevant match. There are many contextual variables that affect the search process and its outcome that have yet to be examined,evaluated and integrated. Building on prior success that examined the relationship between document genre and work task as key factors that affect search, this research will probe work tasks in differing information useenvironments to ascertain how retrieval can be tailored to the specific work tasks undertaken within those domains, and how the interface should be modified to accommodate those differences. This research will entail the study of those environments and the development of context driven search interfaces to existing text retrieval systems.""386303,""Toms, Elaine"
"363478"	"Trajkovic, Ljiljana"	"High-performance networks:  traffic, protocols, and control algorithms"	"My research proposal deals with analysis and simulation of traffic, protocols, and control algorithms in high-performance packet networks.Packet data networks, such as the Internet, are the infrastructure for delivering voice, data, and video applications. Their performance depends on a complex interaction of network traffic, protocols, and scheduling mechanisms employed in network elements (routers). Evaluating this performance is a difficult task that is of importance both to service providers and network users. The objective of my research is to first characterize, model, and analyze traffic that is collected and measured from deployed packet networks. Of particular interest is application of statistical, data mining, and clustering approaches in the search for traffic invariants as well as the understanding of underlying dynamical behavior of the complex system representing a packet network carrying such traffic.The proposed research program covers three subareas: analysis and characterization of traffic data collected from deployed networks, simulation and improvement of network protocols and algorithms and evaluation of network performance, and application of control theory towards the analysis of complex dynamic network behavior. Collected traffic data will enable us to examine various aspects of traffic data collection, user behavior, and network operation and to evaluate performance of deployed routing protocols. Simulation tools and models will be developed to evaluate and improve performance of various wired and wireless network protocols and algorithms. Developed mathematical models of network protocols and mechanisms will be used to better understand complex behavior, such as chaotic and bifurcation phenomena, that has been observed in models of TCP/IP networks.""367281,""Tran, AnhThuy"
"363062"	"Trappenberg, Thomas"	"Interpretation and representation of learned classifiers"	"Machines that learn from examples have long been a dream of computer scientists, and new generations of such machines are becoming increasingly versatile. For example, when properly trained, such machines can sometimes classify medical data much faster and with better accuracy than domain experts. However, many researchers applying these techniques need to know what the machines learned or which facts were taken into account when making the decisions. In my research I try to develop techniques and tools to provide this information in a human-comprehendible way. I also investigate new generations of learning machines based on our increasing knowledge of how the brain processes information.""371924,""Trask, Robert"
"361714"	"Trefler, Richard"	"A visual semantics for communication protocols"	"Communication protocols enable several computers to interact with each other.  Safety critical applications of these protocols, such as those governing the communications of embeded components in aircraft, demand an exceptionally high degree of confidence in their performance under all input conditions.  Automated reasoning tools, such as model checkers, can perform this analysis and, if a protocol under study is found to have a bug, report on the exact nature and cause of the bug in order to help redesign the protocol.  This type of reasoning requires that the protocols be expressed in a mathematically precise formalism that lends itself to automated reasoning.  Further, the anlysis can be carried out only when the protocol description is of a managable size. This project will develop visual, straightforward notations for describing the behavior of communications protocols.  Visual notations that are both precise and intuitive will help developers in modeling and analyzing the protocols under study. Due to the enormous size of the protocols, automated reasoners are needed to help designers manage the complexities inherent in their analysis. Based on these visual notations, automated reasoning tools, such as model checkers, will be developed to analyze next generation communication protocols. Ultimately, these tools will help deliver service guarantees for the essential communications tools such as phone systems and the Internet, that we use every day.While my work is based in the world of communication protocols, the algorithms and concepts that are being developed will, it is hoped, have wider application in software engineering generally, as we come to require more and more stringent reliability from computer systems.""372876,""Trefry, Sarah"
"363334"	"Tzanetakis, George"	"Sound source description for music information retrieval"	"Music Information Retrieval (MIR) is an emerging area of multimedia research dealing with the analysis and retrieval of music in digital form. The proposed research program is focused on improving the state-of-the-art in MIR algorithms for processing audio signals using ideas from sound source separation. In the past few years there has been steady progress in various topics of audio MIR such as genre/style/mood/artist classification, segmentation, summarization and similarity retrieval. In the majority of existing algorithms the audio signal is represented statistically without any effort to identify and characterize individual sound sources.The main objective of the proposed research is to explore how sound source separation and music transcription techniques can be used to aid large scale MIR problems such as classification and similarity retrieval.As an example the characteristics of the singing voice play an important role in how we characterize/perceive a piece of music. Using sound source separation techniques the singing voice part of a music mixture can be enhanced and characterized individually. This information can then be combined with other more ""classic"" audio features for classification or similarity retrieval. We hope that the shift to large data collections from isolated examples will suggest insights and improvements to existing source approaches and inspire new ones.MIR is one of the most important emerging areas of multimedia information retrieval. The success of portable music players and online music stores indicates the large impact that MIR can have. In addition to commercial applications, improvements in MIR systems can lead to advances in music libraries, pedagogy and preservation of culture. The proposed research has the potential to improve existing sound source separation approaches and therefore make human/computer communication more effective.""361115,""Tzerpos, Vassilios"
"361764"	"VanBeek, Peter"	"Constraint programming: models and algorithms"	"Constraint programming is a powerful framework for solving difficult problems. In the constraint programming methodology, one models a problem by stating constraints on acceptable solutions. Once a model has been formulated, various algorithms---usually based on search---are available for solving the model. There are many interesting tasks for which this approach is particularly suited. These tasks range from ones that are simple for humans, such as vision and language comprehension, to ones that are difficult for humans, such as scheduling and planning. What these tasks have in common is that constraints are a natural part of the problem. A good example is scheduling people or machines. What is readily available are constraints such as a worker is only available for certain parts of the week or only able to do certain jobs.I propose constraint programming research projects that fall within the solving stage. A key to improving many applications of constraint programming is to improve the underlying search algorithms. Much progress has been made recently, but more work remains to be done. I propose several projects for improving backtracking search, the most commonly used form of search for solving constraint models, as well as a project investigating an alternative to backtracking search. In addition to this more theoretical and experimental work, I propose to continue an application-driven research project on effectively solving real-world optimization problems. Focusing on applications has had the benefit of identifying shortcomings in existing constraint programming approaches as well as making a contribution to the practice of constraint programming.""371444,""VanBommel, Andrew"
"363338"	"Vanderkooy, John"	"Investigations in acoustics, electroacoustics and signal processing"	"My aim is to support the audio engineering community with basic research in acoustics, electroacoustics, measurement tools, and signal processing.  It is motivated both by personal curiosity and a genuine desire to help the industry.  This area is multidisciplinary, drawing on physics, mathematics and engineering.  Projects chosen for study derive from contact with prospective graduate students, industry, and personal priorities.   Completed research is (a) a study of the benefits of a high magnetic field strength (high Bl) on the power efficiency of normal cone loudspeakers, especially the combined benefits of high Bl in combination with switching or class-D amplifiers, (b) a study of the acoustic centre of normal sealed-box monopolar loudspeakers, particularly relevant to understanding how loudspeakers act in rooms, and (c) measurement of acoustic surface impedance with a new, accurate and rapid method.  Newly-proposed loudspeaker research will be chosen from several areas: (i) further work on acoustic surface impedance measurements, (ii) a study of the acoustic nonlinearities that are responsible for the poor performance of concentric speaker systems, involving the acoustics of moving boundaries, and (iii) study of viscoeleastic loudspeaker damping and the inertia of the moving air near the cone.   For over 20 years a colleague and I have worked on dither, which is the addition of noise to a quantizer to turn its distortion and noise modulation into benign low-level noise.  These studies have influenced the way digital audio has flourished.  Properly dithered systems have infinitely fine signal resolution and no noise modulation, becoming truly analog in their behaviour.  We will apply these same concepts to image processing, extending these studies to different forms of dither in multi-bit image processing as well as partial dithering in low-bit half-toning.   An ongoing peripheral project is a study of the propagation of acoustic waves in concrete and other civil engineering solids, in order to detect flaws and cracks.""382414,""Vanderkooy, Matthew"
"361072"	"Vavasis, Stephen"	"Preconditioning for linear systems arising in modeling and optimization"	"A central problem in scientific computing is the solution of large systems of linear equations.  Linear equations arise as a subproblem in practically every branch of computational science and engineering, including solid mechanics, fluid mechanics, electromagnetics, astrophysics, chemistry, solid and high energy physics, as well as in all manner of optimization and statistical problems.  Problems as disparate as designing automobile or aircraft engines, scheduling work crews at a large company, or developing next-generation imaging devices all require solving linear equations.  Indeed, for many of these problems, solving linear equations is the most computationally intensive step.Since the 1970s, asymptotic considerations have increasingly favored iterative methods for solving very large linear systems.  The key to efficiency of iterative methods is the selection of a good preconditioner.  A preconditioner is defined as a matrix that approximates the system under consideration and yet is easy to solve.  In our previous work with graduate students and colleagues, we have discovered several new families of preconditioner, the most recent and perhaps most exciting being based on underlying graph structure (i.e., information about how the variables underlying the scientific problem are interconnected).  I propose to develop novel methods for preconditioning for many applications in optimization and scientific modeling.  I also plan to apply these preconditioners to practical scientific problems.""371918,""Vavassis, Angela"
"361745"	"Veksler, Olga"	"Discrete optimization methods and their applications in computer vision"	"Computer vision, simply stated, is about teaching computers to see, that is to perceive which objects are present in a scene, where they are located, and where they are moving. The ability to atomatically process visual information leads to a virtually unlimited number of applications, ranging from the traditional ones such as industrial inspection, security and surveillance, robot navigation, to the newer ones such as video conferencing, virtual travel, and special effects for the movie industry. New applications are being developed each year.An optimisation based approach to computer vision is particularly promising and has already proved quite successful in applicability across different vision domains. There has been a tremendous progress in recent years in such fundamental computer vision tasks as stereo correspondence, image segmentation, image restoration, multi-view reconstruction, in large part due to the development of effective optimisation techniques. It should be noted that the recent progress is due to the optimisation algorithms themselves, since most of the objective functions they are being applied to have been used in computer vision for a long time but without good means to optimise them, and, therefore, with mediocre performance. A large portion of my previous research has been devoted to developing efficient optimisation algorithms, designed specifically for computer vision problems. At the same time, I am very interested in applications of optimisation methods to specific computer vision tasks. I find that there is a synergy between the theoretical methods and practical applications. It is from trying to solve a practical application that one finds interesting new optimisation problems. In turn, solving a new optimisation problem leads to an improved  performance on a practical application. I propose two research efforts, one directed at the theoretical issues in optimisation based techniques, and the other direction is towards practical applications of optimisation techniques.  The research in these two directions is closely intertwined.""375048,""Veldhuis, Stephen"
"364777"	"Veldhuizen, Todd"	"Restricted languages for domain-specific programming"	"The field of software reliability seeks methods for making software less prone to failure.  Standard tools include software testing and software verification.  A method growing in popularity is to design restricted programming languages in which it is impossible to write programs that behave badly.  For example, part of the control software for the Airbus A380 aircraft is written in a restricted language called Esterel that ensures prompt action is taken in response to control signals.  This research will extend the ideas of restricted languages to address high-performance applications.One project will address the challenge that as one makes software more abstract, performance tends to diminish.  The result is that one sometimes has to choose between high performance and good software engineering practices.  The proposed solution is to develop programming languages in which a separate sublanguage is used for packaging and composing software components, and to guarantee that this sublanguage incurs no runtime overhead.  This technology has applications in embedded systems, scientific computing, and web services frameworks.A second project will develop programming languages that ensure good average case performance.  Programs in such languages might run slowly on unusual inputs, but are guaranteed to be fast on typical inputs.  This is a departure from traditional restricted languages that ensure worst-case performance bounds.  For many practical applications it is average-case performance that matters.  This project will develop general-purpose methods, and apply them to developing languages in which it is impossible to write programs that run poorly on parallel computers.These projects have the potential to improve software reliability in industries that require high-performance computing, and will contribute to training the next generation of Canadian HQP needed in these fields.""366993,""Veljanovski, Vasko"
"366144"	"Verbrugge, Clark"	"Compiler, runtime and language techniques for the efficient use of multiprocessor systems"	"This research is intended to investigate and develop compiler, runtime and language improvements for modern concurrent programming systems.  Many new programming language modifications have recently been proposed based on ""transactional programming"" as a paradigm.  These designs vary in the kind and in the exact semantics of the features provided, but all aim to improve programming practice with as little impact on performance as possible.  Our research will perform a practical investigation into the implementation aspects, developing appropriate optimizations and exploring which features can be implemented feasibly and which will be suitable for improving programming.  Further optimizations for concurrent programming that go beyond the specifics of transactional programming will also be covered, allowing our results to be useful for a wide variety of new and existing concurrent language designs.""376859,""Verhaegen, Frank"
"364802"	"Vetta, Adrian"	"Algorithms and game theory in computer science"	"The focus of this research is the design and analysis of algorithms. A major goal is the development of efficient algorithms with provably high quality for assorted problems in network design and optimisation. In particular, we are concerned with routing under restrictive internet protocols, and with the design of effective telecommunication and transportation networks. Moreover, I will also investigate the interactions between theoretical computer science and economics, such as the impact of competitive behaviour in computer science.Practical instances of this type include mechanism design, broadcast pricing, fixed investment decisions, routing in networks, and internet auctions. Another example concerns the application of techniques from theoretical computer science to economics. As an illustration, the importance of speed and efficiency in computing equilibria and in auction design is now well accepted. The breadth of application motivates the algorithmic approach used in theoretically analysing these interactions.""366737,""Vette, Albert"
"364109"	"Viktor, Herna"	"Utility-based mining of large-scale relational databases"	"Relational databases, with vast amounts of data and complex schemas, are ubiquitous in our society. For example, health care practitioners, financial establishments, insurance companies and government institutions such as Royal Bank, Revenue Canada or Yahoo, to name but a few, are the owners of very large databases which need to be analyzed for trend analysis, fraud detection, customer profiling, privacy-preservation, law enforcement, epidemiological research and so on. The newly emerging field of relational data mining involves the development of new approaches to directly analyze these vast repositories, without requiring extensive transformations by die end user.This research focus on the development of a new framework to directly mining relational data. We focus our attention on the development of scalable solutions with high utility. We address the intricacy when considering complex database schemas containing many huge relations. The aim is to not only consider the building of models utilizing information in different relations, but also to explore the links in between them, thus leading to the discovery of novel, interesting knowledge nuggets with high utility.""383605,""Vila, Ava"
"361539"	"Vincent, Pascal"	"Learning higher level representations and invariant transformations"	"Can we design an artificial intelligence system that, once plunged in an unknown world, like a newborn baby opening his eyes for the first time, would be able to form high level concepts exclusively from the enormous quantity of raw sensory information that it receives? Such a system should be able to discover deep regularities and stable structures in this ever changing information stream, so that it may eventually form its own subjective notions of space, time, of things (more or less stable entities) moving around. This research program aims at exploring new fundamental principles to guide the building of next generation autonomous learning systems with this kind of abilities. Our approach will be based on computer implementations capable of growing ""layers of understanding"", stacked one upon the other. Each such layer shall be building a slightly higher level ""understanding"" of the representation extracted by the previous one, by exploiting regularities that the previous layer failed to capture. We intend to focus on a particular class of regularities called ""invariant transformations"", that we want to automatically discover and learn. The layer by layer extraction of ever higher level, ever more meaning carrying representations, is expected to be a key element for opening the door to near human-level performance in artificial learning systems (thus advancing the state-of-the-art in the fields of machine learning, pattern recognition and data mining). It is also expected to produce computerized systems capable of far more accurate predictions from past observed data. This would have direct technological applications in a wide range of fields where superior prediction systems are immensely useful: from engineering to finance, from business decision making to medicine.""367139,""Vincent, Robert"
"362591"	"Vorobyov, Sergiy"	"Robust adaptive beamforming for multi-antenna systems"	"In the future-generation cellular mobile radio systems with base stations equipped by multi-antenna transceivers, the use of adaptive beamformers is a key technology for spatially selective signal reception (uplink beamforming) and transmission (downlink beamforming) which has recently emerged and is now an area of intensive research. Using the spatial selectivity of antenna arrays, the receive and transmit beamforming facilitates an increase in system capacity and a decrease in the transmission power. Thus, multi-antenna technology demonstrates the potential to support high data rates while at the same time using less power. However, these can be achieved by possibly increasing the computational complexity of the transmitter and receiver. As a result, multi-antenna technology poses optimization challenges that conventional design techniques cannot resolve directly. Our research within this project aims at resolving such challenges. In designing adaptive beamformers, the robustness issues have to be addressed. Indeed, the conventional adaptive beamforming techniques are developed for idealized signal and channel models. However, various imperfections, which occur in practice, can have deleterious effects on system performance. The main objective of this project is to capture the essential aspects of robust multi-antenna system design. Achieving this objective is expected to contribute, specifically, to defining beyond 3rd generation (B3G) wireless systems by taking into account the robustness issues.""374456,""Voroney, Paul"
"362403"	"Wadge, William"	"Infinitesimal and intensional software"	"I will investigate theoretical and practical aspects of a new type of logic I call ""infinitesimal logic"".Like fuzzy logic, it uses a spectrum of truth values between completely true at one end and completely false at the other. But unlike fuzzy logic, the additional truth values are discrete and separated from each other, not unlike quantum mechanical energy levels. For example, the truth value just below standard truth can be considered as qualitatively less true - in fact, incomparably (even ""infinitely"") less true.One theoretical use is in the semantics (meaning) of rule based systems with negative conditions - such as Prolog with negation-as-failure. On the practical side, the intermediate truth values can help users express preferences and priorities.For example, suppose you want to fly to Ottawa and prefer an aisle seat. Infinitesimal logic allows you to formulate this so that there are three answers: an unqualified Yes, which means you can; a qualified Yes, which means you can get to Ottawa, but not in an aisle seat; and an unqualified No, which means you can't get to Ottawa.You can therefore make clear the fact that having  an aisle seat is incomparably less important than getting to Ottawa; for instance, that you are not interested in aisle seats to Calgary.""381821,""Wadhwani, Aman"
"361294"	"Walsh, Timothy"	"Design and analysis of efficient combinatorial algorithms"	"An algorithm is a step-by-step solution to a problem, explicit enough to be translated into a computer program.  The time taken to solve a problem on a given computer depends upon the size of the problem and the efficiency of the algorithm.  For example, it takes longer to look for a word in a large dictionary than in a small one, but for a given size of dictionary it takes longer by searching through the dictionary from beginning to end than by using the more efficient algorithm anyone would use.Combinatorial algorithms do things like combining and arranging objects in various ways and counting all the objects of a given type and size, either by constructing them all or by finding a formula for calculating the number of objects of that type as a function of its size.  An illustrative example: which is the faster algorithm for counting the number of ways of arranging n objects, constructing all the n!=1x2x...xn arrangements or calculating the product 1x2x...xn?  If the objects are strings of letters like 00, 01, 10, 11, it may be possible to arrange them in such an order that each one differs from the next one by a small number of letters (00,01,11,10).  Such an arrangement is called a Gray code.  The challenge: given a Gray code, design an algorithm that transforms each string into the next one in a small amount of time no matter how long the string is.A graph consists of points and lines, with each line joining a pair of points.  Not every graph can be drawn on a piece of paper so that the lines don't cross each other (try drawing five points and joining every point to every other point with no lines crossing).  The challenge: design an algorithm that calculates the number of graphs with n points and m lines that can be drawn in this way in less time than it takes to construct all the graphs and test whether they can be drawn.""364539,""Walter, Stephen"
"363356"	"Wanderley, Marcelo"	"Design of novel digital musical instruments: gestures sensors and mapping strategies"	"My main research goal is to find ways to musically use computers as part of novel (digital) musical instruments(DMI) that eventually match the expressive possibilities available with today's acoustic instruments. I pursue this direction by studying gestural control of sound synthesis, through a two-pronged approach:-The study of a generic DMI and the suggestion of novel approaches to its design;-The analysis of performer movements in acoustic instrument performances with the aim of finding cues to improve the design of DMIs.In this grant, I focus on the first approach. Of specific interest is the study of the various transduction technologies available commercially and their possible use in computer music, including a thorough review of existing sensors, actuators and associated conditioning circuits (SensorWiki - www.sensorwiki.org). Apart from the use of commercially available sensors, I will also explore ways to develop sensors by systematically studying different materials with electrical properties that vary with distance or with force. Results include the development of various efficient prototypes of force sensors using paper as the main conductive material, at a fraction of the cost of commercial sensors. Of similar importance is the development of methodologies to compare existing DMIs and design novel ones, including the proposition of ways to visualize DMI characteristics and the suitability of sensor technologies to specific musical functions. Finally, I am interested in the intrinsic role of mapping strategies between sensor outputs and sound synthesis inputs and their influence on instrument design. In this research, I focus on the definition of explicit mapping strategies and the analysis of their influence on DMI's expressiveness potential. Expected significance of this research includes he development of technology in various areas related to transducer technologies, electronic instrumentation, human-computer interaction, musical instrument design, computer music performance and perception and cognition.""366237,""Wanderley, Marcelo"
"366319"	"Wang, CaoAn"	"Tetrahedralization and graph property in 2D and 3D"	"Triangulation is to partition an object (a set of points or a simple polygon) into simplices (triangles) and is oneof the fundamental problems in Computational Geometry. In addition to its theoretical significance, it findsapplications in many areas such as Computer Graphics, Scientific Visualization, and Geological InformationSystems. While the problem in 2D is well studied, its 3D counterpart (called `Tetrahedralization') is relatively unexplored. This is because the geometric structure of tetrahedralization is much more complicated.For example, triangulation in 2D is always possible and the number of triangles in different triangulations of apoint set is the same. On contrast, not all the 3D non-convex polyhedra can be tetrahedralized without addingSteiner points, and for different tetrahedralizations may have different number of tetrahedra.In the related graph problems in 2D and 3D, the difference also exists.In this proposal, I shall propose the following three main problems:(1) To study tetrahedralizations with different constraints. For example, tetrahedralization with maximum number of tetrahedra and tetrahedralization with max-min inner angle of tetrahedra.(2) To study related graph problems. For example, the Hamiltonian path in any tetrahedralization of a given point set, the connectivity of any two tetrahedralizations by diagonal flips, and the drawability of graphs as a constrained tetrahedralization.(3) To study the sum of the square-root of integer problem, which is importantfor proving a hard geometric problem to be in NP.Investigating these problems will help us to further understand tetrhedralizations and probably to design algorithms for many applications.""365578,""Wang, ChunyanCW"
"362597"	"Wang, Lingyu"	"Defending Against Multi-Step Network Intrusions in large-scale computer networks"	"In the long term, the proposed research program aims to develop techniques for automatically interpreting and predicting attackers' progress, strategies, and intentions in defending computer networks against intrusions. In the short term, the research will be focused on scalable and efficient knowledge representations and algorithms, and on the design, implementation, and evaluation of practical systems for defending against multi-step intrusions in large-scale, distributed computer systems. More specifically, the proposed research program will identify root causes to the poor scalability of current knowledge representation and remove the limitation by proposing novel ways of representing knowledge about vulnerabilities and network connectivity. The research will devise heuristic algorithms for finding minimum-cost solutions to harden a network and evaluate the devised algorithms through extensive experiments. It will also propose security metrics for measuring risks, costs, resistance to attacks in an integrated framework and design efficient algorithms for optimizing network configurations based on the proposed metrics. Finally, it will study the distributed computation and analysis of attack graphs while taking into consideration potential local security policies that may prohibit the disclosure of vulnerability information to other parties.The proposed research will have positive impact on the theory and practice of defending against multi-step intrusions. First, the research will present interesting new concepts, such as compressed attack graphs, quantitative vulnerability analysis, and distributed vulnerability analysis and intrusion response. These concepts will provide new insights to existing problems and formulate new research problems and directions. Second, the research will lead to practical methods and systems that are applicable to a variety of distributed applications. These methods will enable members of a grid, a database federation, or a peer-to-peer application to jointly analyze global vulnerabilities, to reconfigure the network so as to maximize its resistance to potential intrusions, and to collaborate on the defense of ongoing intrusions in real time.""367067,""Wang, Linlin"
"362745"	"Wang, Yingxu"	"Theories, methodologies, and tools for intelligent software code generation"	"It is recognized that software engineering, as one of the high-technology disciplines, is using the lowest technology - human labor - in contingent software development. Now, it seems to be the time to turn the focus on the fundamental needs for automatic generation of software code for various applications.      Automatic program generation is both an ultimate goal and a tough challenge in software engineering and the software industry. This research program proposes the investigation into theories, methodologies, and intelligent supporting tools for software code generation. The objectives of this project are as follows: a) To investigate the transdisciplinary theories of intelligent software code generation based on the latest development in software engineering, cognitive informatics, and denotational mathematics. b) To explore applicable methodologies for the coherent integration and automatic transformation between conceptual models, formal models, and code of software systems. c) To implement a supporting environment and tool for the intelligent software code generation system. d) To transfer the methodologies and technologies of the intelligent software code generation tools to the software industry.The completion of this work will lead to the development of cutting-edge techniques for the software industry that would replace the intensive labor-dependent programming practice. This work will strengthen the Canadian fundamental research in software science and engineering. The intelligent tools and systems developed in the proposed program will directly benefit the Canadian software industry and the information/knowledge-based new economy.""366797,""Wang, Yongjin"
"364667"	"Ward, Paul"	"Commodity distributed systems"	"The creation of a distributed system is a major engineering exercise, often costing many millions of dollars to design, and several times more to operate.   Indeed, the total cost in North America just to keep existing computing systems functioning exceeds $40 billion annually.  In spite of this expense, computer systems are typically insecure and unreliable.   The single major cause of this problem is that the complexity of these systems exceeds the abilities of all but the very best of human experts.  Even the installation of a networked home computer often requires significant systems knowledge.  The evidence suggests that this complexity is inherent.  Over time this problem will get worse not better, as we build larger systems and inter-connect those systems.  The objective of our research is to address these hard problems of distributed-systems operation that currently require such deep knowledge to keep the system functioning correctly.  We plan to do so by having the systems self-manage.  In addition, we wish our systems to be based purely on commodity components, since we consider cost to be a major issue.  To then make the systems a commodity, we must limit ourselves to commodity hardware.  While computing systems today can use commodity hardware, the networks that interconnect them can be expensive to deploy and operate.  Thus we are adapting commodity wireless hardware to build cheap multi-hop wireless networks.Our end-vision is that a user will simply plug computer equipment into the necessary power outlet and it will self-configure and self-manage as required, in the same manner that other major systems, such as telephones, self-configure.  The significance of this research is that, if realized, it will enable users to focus on their tasks, rather than on the operation of the system.""361419,""Ward, Rabab"
"364080"	"Warfield, Andrew"	"Systems support for virtualization"	"Recent years have seen a decided effort to deploy hardware virtualization as a technology to help manage the scale and  complexity of modern computer systems.  Virtualization allows a single physical computer to be partitioned and made to appear as a collection of isolated virtual computers, each running an independent operating system and application stack.  AMD and Intel have released CPU extensions to aid virtualization, and all major OS vendors have announced virtualization support in the next releases of their systems.This proposal is for a research program that explores the challenges and opportunities for systems software as virtualization is deployed in large-scale, enterprise environments.  The use of virtualization in facilities such as industrial datacenters and rent-a-server-style hosting providers dramatically changes many aspects of these systems, which must suddenly scale to support more virtual hosts on the same physical resources, and may be administered by considerably less experienced users due to lower entry costs.  Additionally, virtualization presents a narrow layer of interposition at which systems software may be deployed independent of overlying operating systems and applications.As short term goals, I describe three virtualization projects that are already underway with masters students at UBC.  ""SecondSite"" is a disaster recovery system that maintains a live, remote copy of a running group of servers and allows this copy to take over operation of the system in the face of failure.  Second, my investigation of packet symmetry enforcement aims to impose limits on the ability of servers to transmit certain types of malicious traffic, protecting hosting facilities from inadvertently being the source of denial-of-service-attacks.  Finally, I consider the use of large caches of network-based memory, as a community resource to avoid the use of mechanical disks.""375297,""Warkentin, Andrew"
"365238"	"White, Anthony"	"Swarm intelligence: tools and applications"	"The objectives of the proposal are to develop theories, tools and techniques that facilitate the construction ofsystems which solve problems using Swarm Intelligence. In order to achieve an understanding of information processing in swarm-based systems, we propose that a new information field view be developed. This, we expect, to be based upon the investigator's own Ph.D. thesis, the work of Situated Cellular Agents of Bandini et al and extensions of our own work in understanding crowd dynamics based upon interacting information fields. As practical examples of the utility of the proposed theoretical formulation, we expect to apply swarm algorithms to problems in the area of robotic soccer and continue work in the understanding of crowds. Our focus in the former problem will be dynamic role assignment, while in the latter problem understanding how  information can be more effectively communicated within a crowd in order to effect, for example, more rapidegress from a building. We expect to use evolutionary algorithms, such as Genetic Programming, in order todevelop agent behaviours for swarm-based robots in a manner similar to the SwarmBot project, which usesneural networks for robot behavior. In understanding sensor networks we expect to develop algorithms thatensure robust information gathering in scenarios where individual sensors fail; again, techniques fromEvolutionary Computation are expected to be provide behaviours. As a further demonstration of the utility ofswarm processing, we expect to solve problems in the area of Autonomic Computing. Here, we anticipate thatautonomic managers -- agents by another name -- will learn to self organize to automate the management ofnext-generation computer systems.""377248,""White, Bradley"
"365250"	"Wiese, Kay"	"Evolutionary computation and applications in bioinformatics"	"The objectives of this research program are:1) To develop improved combinatorial algorithms for problems in Bioinformatics, particularly for structure prediction of biomolecules, building on the algorithms that have been developed in my lab so far2) To extend jViz.Rna, a visualization tool for RNA secondary structureThe algorithm RnaPredict, developed in my lab and based on evolutionary computation, has been demonstrated to predict certain known RNA secondary structures with very high accuracy (up to 90%) and to have a higher prediction accuracy on certain short and medium length sequences than mfold, the current benchmark for ab initio RNA secondary structure prediction. Particularly, RnaPredict is not as prone to predicting false positives as mfold is. Rnapredict uses a number of advanced operators and representations. I plan to further evaluate, improve, and extend RnaPredict. I have developed a visualization tool for RNA secondary structure which has a number of benefits over existing tools, including: wide support of input and output formats, ability to produce high quality graphics for reproduction, interactive output for further manipulation by the end user, ability to display pseudoknots in a classical structure plot, and the ability to overlay two different structures for comparison. jViz.Rna also produces quantitative and qualitative comparisons between two structures. The quantitative comparison is currently taking into account sensitivity, specificity, and F-measure. I plan to extend jViz.RNA to incorporate a comparison of structures based on common substructures using dual graphs.""368198,""Wiesel, Torsten"
"365645"	"Wildes, Richard"	"Early representation and analysis of monocular and binocular temporal image sequences"	"The research that my collaborators and I carry out is in the general area of computer vision, the attempt to endow machines with a sense of sight. From a theoretical point of view this is an important endeavor as it bears on fundamental issues in complex information processing, and may yield results that bear on our understanding of visual information processing in natural systems (e.g., humans), as well. From a practical point of view, machines that can see have the potential to interact with humans in a much more natural and useful fashion than is currently the case. Our particular studies of visual information provide insights into the fundamental properties of temporal sequences of image (e.g., video) and extend our technological capabilities by indicating novel applications. We use mathematical techniques to analyze images, develop processing algorithms based on our analyses and carry out empirical tests to evaluate their performance. This allows us to ask basic questions about temporal sequences of images, a pervasive source of information about our surround world. Questions of interest include: How can complex streams of video be parsed into meaningful components (e.g., persistent structures and moving objects)? How can multiple views of a scene be used to reconstruct a three-dimensional model of the scene? This research contributes to Canada in three important ways. First, it advances the discipline of computer vision as we expose basic properties of image information and develop corresponding algorithms for incorporation into vision machines; this allows Canada to compete successfully in a critical area of science and technology. Second, it provides students with hands on experience and skills in vital areas of investigation, computer vision and, more generally, computer science. Students go on to make careers in science, technology, teaching and elsewhere, taking with them the enrichment of having been involved in research. Third, it paves the path for novel ways to exploit image data. Applications of interest include video processing for everyday concerns ranging from the internet to personal handheld devices, video enhancement so that image content is more readily interpretable and intelligent processing modules for robots.""361790,""Wilds, Christopher"
"364279"	"Williams, Alan"	"Improved automated tools for software testing"	"Software applications, especially web-based applications, are typically constructed from a number of components; common components are web browsers, web application servers, data bases, and business logic entities.  Each of these components may use a different language to specify the functionality. An added element of complexity is that there may be several versions of system components; for example, Internet Explorer and Mozilla Firefox are two of the browsers in use, and for each of those browsers, there are typically several version numbers in use.With a diverse range of technology, ensuring that all these components can be integrated successfully into a working application is a challenge.This work aims to address two aspects of this challenge.  The first is to investigate ways that designers of component-based applications can focus on the functionality instead of the various languages used by the components.  That is, identify opportuntities for automated tools to assist in automatically generating parts of applications from models.The second is to improve the testing and testability of component based applications.  Testing components requires being able to control the component and observe its behaviour, but when combined into a complete system, some components may be difficult to access.  This work will develop methods and tools to improve access to the interfaces of web-based applications. It will also continue previous work on selecting test configurations for interaction testing, in order to achieve measurable coverage of combinations of component versions.""371580,""Williams, Branwen"
"362067"	"Winter, HorstMichael"	"Verified object-oriented programs"	"Every user of computer systems is familiar with errors and/or misbehavior of software products. This is an annoying situation but for most applications not serious. On the other hand, especially in the area of ebusiness correct programs become more and more important. For example, every user of such a program wants to be sure that secret data (credit card number, pin, etc.) are not accessible to unauthorized persons. Another example might be software controlling a nuclear plant. It is desirable that such a program does not fail. One method for developing reliable software products is program verification, i.e. providing a mathematical proof of correctness versus a given specification. For the reasons mentioned above a certification of a software product following the European ITSEC (Information Technology Security Evaluation Criteria) at level E4-E6 - the three highest certification/assurance levels of that standard - requires the application of formal methods including correctness proofs of at least parts of the system.      The existing tools and methods are very limited in providing mechanisms to transfer properties, their proofs and proof methods from one construction to another. One is forced to start on a very basic level, which generates a huge amount of proof obligations. In real world examples proving all obligations is often not feasible or at least too expensive. A system and its underlying methodology, which allows less expensive development of verified programs would be of great benefit for software developers and finally for all users. My objective is to establish such a system and methodology, and, thus, to transfer theoretical results to real world applications.      The overwhelming success of object-oriented programming languages is based on reuse of software components. I will apply the same idea to the corresponding proof system. Properties of programs and their proof will be integrated into the class hierarchy such that a suitable inheritance mechanism transfers operations as well as the related properties and proofs to subclasses.""372367,""Winter, Rebecca"
"365137"	"Wong, Vincent"	"Cross-layer design for wireless mesh networks"	"The next generation fixed wireless broadband access networks are being increasingly deployed as wireless mesh networks (WMNs) in order to provide ubiquitous access to the Internet. Research and development of WMNs is motivated by several applications including broadband home networking, community and neighborhood networking, and metropolitan area networks. The aggregate capacity and performance of WMNs can be increased by the use of multiple channels. A multi-channel wireless mesh network (MC-WMN) consists of a number of stationary wireless routers, forming a wireless backbone. The wireless routers serve as access points for wireless mobile devices. Some of them also act as gateways to the Internet via high-speed wired links. Each router is equipped with multiple network interface cards. Each interface operates on a distinct frequency channel in the IEEE 802.11a/b/g bands. Using multiple radios in each wireless mesh router can improve the network performance significantly. However, there are various challenging research issues to be resolved in order to support quality-of-service (QoS) for multimedia services. Due to the limited number of available channels within the 802.11 frequency bands, interference between different neighboring links can occur if those links use the same channel. Other issues such as interference from other co-existing systems, user mobility, and constraints on control overhead and computational complexity also need to be considered.  The objective of this research is to investigate and develop algorithms for reliable and efficient information transfer over MC-WMNs. In particular, five specific research tasks are identified: (1) Development of adaptive logical topology formation and interface assignment algorithms; (2) Design of load balancing and routing protocols; (3) QoS provisioning in MC-WMNs; (4) Determine the capacity region of MC-WMN; and (5) Support security and user privacy in MC-WMNs. We foresee that MC-WMNs will be widely deployed as one of the wireless access network technologies in future. Results are intended to find practical solutions and applications for the benefit of Canadian and global telecommunication industries.""376192,""Wong, Willy"
"361850"	"Woodbury, Robert"	"Theory and practice of design space explorers"	"Designers work by creating and evaluating alternatives. Current computer aided design software limits designers to a single design at a time. This project will develop new concepts and programs that enable designers to create, record, recall and reuse their design ideas. It will help designers create better designs more suited to their context and design brief. The project combines rigorous scientific inquiry into how alternatives are represented, creative design of new human-computer interfaces and application of both in professional practice and graduate education. Design research and the design professions will benefit through being able to consider and demonstrate to clients a wider range of design possibilities.""371067,""Woodcock, Jennifer"
"364040"	"Wormald, Nicholas"	"Random structures and algorithms"	"Random structures are useful for designing and analysing computer algorithms which solve modern-day problems. They can also be useful for understanding properties of large complex systems such as the link structure of peer-to-peer networks. This proposal aims to continue the study of some relatively new techniques for proving properties of random structures, as well as developing some very new techniques. It will advance the field of combinatorial mathematics in areas of fundamental importance, and aims also to include application-oriented topics arising from computer science, real-world networks, and other contemporary fields of science.Some of the problems to be tackled involve load balancing. These problems arise when requests are sent to a set of processors (or storage disks), and one wants to allocate the requests  such that no processor is assigned many  more than its fair share. The aim is to show that certain very fast algorithms for performing the allocation often succeed in balancing the load quite evenly. Other problems relate to peer-to-peer networks. The aim there is to study the theoretical properties of the networks used in practice, concentrating on those properties that are desirable from performance considerations. Yet other problems involve further properties of random graphs, an abstraction of the above problems whose study has yielded the sort of techniques that are to be used in this work.The overall impact of the proposal should be the solution of a number of problems of high contemporary interest, and a significant improvement of the understanding of the field.""387462,""Wormald, Nicholas"
"366392"	"Wu, Jing"	"Management architecture of peer-to-peer optical networks"	"User-controlled optical networks have flexibility of composing application-specific transport networks and using network resources from different suppliers. Each user independently owns and manages some network resources. A group of users can compose application-specific transport networks. The end-to-end connections between the users in the group are established in a fully distributed, peer-to-peer manner. There is no centralized network management authority to coordinate such end-to-end connection establishment. The management architecture of peer-to-peer optical networks is dramatically different than the existing centralized optical network management architectures.     This research is to explore the potentials of peer-to-peer technologies in the context of optical network management. Our research goal is to explore the technologies that enable to search optical network resources in a peer-to-peer manner, just as searching for music files. By taking into account the special requirements and characteristics of optical networks, we will customerize the existing peer-to-peer technologies for the management of peer-to-peer optical networks. Such technologies include discovery of group members, search of available network resources offered by group members, communications between group members, granting and receiving the resource management privileges, etc.     We plan to compare the economics and availability of peer-to-peer optical networks with those of carrier managed optical networks. By economics analysis of peer-to-peer optical networks, we want to reveal the benefits of user centric network management, when they take advantage of the diversity and competitions between carriers. The availability of peer-to-peer optical networks is different than that in a single carrier network. Although individual network resource may not be highly available, peer-to-peer optical networks enjoy massive redundancy of similar network resources and therefore potentially offer highly available services. We plan to analyze the availability of peer-to-peer optical networks using a new measurement.""378303,""Wu, Jonathan"
"365200"	"Yan, Yuhong"	"Building self-manageable web service processes"	"The objective of this project is to study the techniques for building self-manageable Web service processes. Web services are software components which communicate and interact with each other using standard Internet protocols. Software components developed in different programming languages and deployed on different platforms can easily be invoked if they are wrapped as Web services. This is because Internet protocols, such as HTTP and the information format XML used by Web services, are commonly used by many programming languages and on many platforms. Web services are not for human beings to browse, but for software to operate automatically. Web services are a new generation of middleware that have been successfully used for system integrations problems, e.g. connecting heterogeneous data sources. There is a demand to build fully automatic business processes by connecting distributed Web services. Current techniques allow us to build automatic Web service processes in a predefined manner. A Web service process can be defined in a Web service process description language such as BPEL. In such a process, all the Web services are fixed; how they interact with each other is determined at during their design. This process cannot change at run time. If any faults occur, the process simply halts. It would be ideal if the business process could automatically select the best Web services to achieve its goals and, if a Web service were to break down, the business process could switch to other similar Web services and compensate the failure effects. A self-manageable Web service process is a fully automated Web service process with mechanisms for adapting to changes, such as run time execution monitoring, sophisticated way of fault diagnosis and recovery, run time partners selection and changing. As Web services are widely used in e-business applications, scientific computing and software systems, this project will bring great impact.""380075,""Yan, YunFei"
"365833"	"Yang, Boting"	"Pursuit-evasion problems on terrains"	"There are many situations where a group of mobile agents, such as police officers, soldiers, or even robots, must search a prescribed area for one or more mobile fugitives. For example, consider a search party looking for a missing person in the wilderness.  Coordinating this group of searchers is an intimidating task, and guaranteeing that the search party does not overlook an area seems overwhelming.  Technology such as GPS can help searchers improve coordination, but the search party still relies on search strategies that are developed by humans. Our proposed research focuses on automating the computation of search strategies in such scenarios.  We model the area that needs to be searched by a so-called terrain, a mathematical model that easily captures information about different elevations.  We are concerned with finding algorithms for computing search strategies of terrains that are guaranteed to find the  missing person.  Many interesting questions arise with relation to this problem.  For  example, given a terrain, what is the minimum number of searchers required to search the  terrain? and what is the minimum cost to search the terrain?  These questions are particularly important for applications where the number of searchers is limited.We are very interested in investigating mathematical models of search strategies that are commonly  used by humans.  For example, consider our motivating example where a search party is looking for a missing person. In order to ensure that no area is overlooked, the search party will often employ a chain-search strategy: the searchers form a chain that sweeps over the search area.  Reseachers have considered algorithms for computing chain-search strategies for mathematical models of building floorplans, but these algorithms cannot be used in scenarios where the search area contains hills and valleys. Our research will provide new algorithms and eventually software that will assist in various search-and-rescue applications.""377016,""Yang, Burton"
"363059"	"Yao, JingTao"	"Inductive learning methods for critical decision making applications"	"Machine learning is an important research area in computer science. The key activity of machine learning is to extract rules from data sets that can be used to predict results for unseen data accurately. In other words, we try to develop algorithms and techniques that allow computers to learn from data. In this proposal, I will study the theoretical aspects as well as the techniques of granular computing, Self-Organizing Feature Maps (SOFM) and Support Vector Machines (SVM), and apply them to critical decision making problems, such as intrusion detection, marketing analysis, financial prediction, and cancer diagnosis.     As an emerging technique, granular computing has attracted many researchers. The goal of this research is to enhance the learning ability of conventional algorithms as well as to provide more general and flexible granulation algorithms for classification problems and decision making applications.     In SOFM, data are clustered based on a set of neurons with the ability to distinguish themselves in terms of differences between input vectors. The competitive process of determining which neuron best fits a particular input vector is challenging when there are many similarity measures are taken into consideration. Game theory and other techniques will be explored to enhance the competitive process in SOFM and thus give it the ability to better organize and analyze more data characteristics.     SVM has demonstrated excellent performance in real-world applications such as categorization, recognition, and classification. We have adapted rough set theory to SVM for feature selection problems and successfully applied it to an intrusion detection problem. I will continue to explore possible improvements in SVM models for this problem.     Our ultimate goal is to develop a decision support system that will help users determine which learning method is most suitable for different applications.""362673,""Yao, Yiyu"
"362109"	"Ye, Andy"	"Area and power efficient interconnect architecture for multi-bit processing on FPGAs"	"Field-Programmable Gate Arrays (FPGAs) are devices that are designed to implement digital systems. They are optimized for hardware algorithms and have the added advantages of being able to change their functionalities in a fraction of a second. Being both hardware-oriented and programmable, FPGAs provide a unique blend of performance and functionality that is essential to many applications. With over 80,000 design-starts and $4 billion in sales annually, FPGAs have become one of the most popular computing media.     The programmability of FPGAs has made their design particularly challenging. Typically only 25% of FPGA area is used to perform computation. The remaining 75% is used to connect the computing elements together. Consequently, the design of FPGA interconnects can be as important as the design of the computing elements.     The goal of this research is to increase the area and power efficiency of FPGA interconnects. Virtually all existing investigations in the field have dealt with computing elements that process one bit of information at a time. Modern FPGAs, however, also contain elements that are designed to process multiple-bits of information simultaneously. The increased use of these multi-bit processing elements has introduced new unexplored aspects and variables to the problem.     In this work, we will focus on two unique properties of the multi-bit processing elements --- the correlated behaviours of their signals and their size variation. First, we will take advantage of the correlated behaviours to exploit the shift capabilities of interconnects for area and power efficiency. Then, we will investigate the most efficient method of connecting computing elements of various sizes together. Finally, we will study how the single-bit processing elements can be grouped into multi-bit processing units to improve the efficiency of their interconnects.""378644,""Ye, JuanJuan(Jane)"
"364062"	"Yu, Fei(Richard)"	"Cross-layer design and optimization for next generation wireless networks"	"Recent advances in wireless communications show that the interactions of different layers play an important role in wireless networks, and cross-layer design is necessary due to the intrinsic characteristics of wireless networks. Although some work has been done for cross-layer design in wireless networks, most of previous work concentrates on a single air interface standard such as a cellular network. However, in next generation wireless networks, wireless devices are equipped with multiple wireless transceivers for different air interface standards to satisfy the different needs for wide area, local area and personal area wireless networks. Moreover, there has been an explosion of interest in cognitive radio for next generation wireless networks, in which wireless devices can sense their environment and adapt to many frequency bands with multiple transmission protocols. In this framework, the need to consider the interactions of different layers in different air interface standards to develop efficient and effective algorithms that optimize the performance of the integrated wireless networks presents significant technical challenges. The objectives of this research proposal are to address important research problems based on this framework. Our long-term goal involves the study of wireless networks with cognitive intelligence over the air interface jointly across physical, data-link, network, transport and application layers. The research methodology of the work will employ mathematical modeling and analysis techniques to evaluate the effectiveness of the novel schemes. We will also ascertain the practicality of our schemes by pursuing experimental implementations and evaluations. The proposed work will contribute to the advancement of wireless networking technologies through the next generation and beyond. Some of the technologies and techniques resulting from the work may impact the evolution of emerging technical standards or lead to the development of new technical standards. Some results may be suitable for commercialization. The proposed research will also contribute to the training of highly qualified personnel in the information technology, communications and wireless networks fields.""371008,""Yu, Francois"
"363542"	"Yu, Wei"	"Capacity,  coding and optimization for network information theory"	"Network information theory is a fundamental theory of communication in a multiuser environment. This past decade has witnessed a tremendous surge of research interests in this area, driven, in part, by insatiable demands for high-speed wireless service and Internet access. It has been increasingly recognized that not only are the capacity theorems of network information theory ideally suited in providing a theoretical framework for the physical-layer design of communication systems, they also play an instrumental role in providing practical engineering insights for networking and content-distribution protocols at the network and application layers. The proposed research program aims to gain fundamental insights into long-standing capacity problems in network information theory by focusing on the following areas. First, the proposed research will investigate capacity limits and develop practical coding and modulation strategies for wireless cooperative communications. The proposed research will take advantage of recent advances in error-correcting codes and in statistical physics to develop capacity-approaching strategies for multiuser channels, such as the broadcast channel and the relay channel. Second, the proposed research will address the computation of capacity limits in network information theory using convex optimization techniques. This is especially important in practical wireless multi-antenna communication scenarios where many degrees of freedom in transmitter design are available. Finally, novel multiuser channel models inspired by networking problems will be formulated, and insights gained from the study of capacity and coding for these channels will be used to address the design of practical networking protocols. Together, the overall research program aims to tackle theoretical issues in multiuser communication, and to gain fresh insights into the nature of information flow in a network.""384548,""Yu, Wei"
"365564"	"Yu, Xiaohui"	"Managing risk and uncertainty in query optimization"	"The query optimizer, as one of the most important and most complex database system components, is key to the management of large databases.  Its major task, query optimization, is to choose the best way to execute a query. Often there are several different ways (execution plans) to perform a given query, which might differ in the indices used, the orders of operators executed, etc., but all lead to the same correct result. The query optimizer generates and examines the candidate execution plans, and makes the decision as to which plan to follow for execution. The quality of the optimizer is very important, because costs of plans can vary dramatically and choosing a bad plan might result in extremely poor performance.Most existing approaches to query optimization aim at choosing the execution plan with the least estimated cost. However, in many cases, this is insufficient, as other qualities, such as robustness and predictability, can be equally important. Like many real-life decision making problems, query optimization inherently involves risks and uncertainties, which must be taken into consideration when choosing a plan.  The primary goal of our research is to develop systems, methods, and techniques that can effectively utilize various sources of information available in a database system, in order to reduce the risk and uncertainty of query optimization decisions, and make query processing more robust and predictable.""383095,""Yu, Xingye"
"362920"	"Zaccarin, André"	"Algorithmes de prédiction pour la compression vidéo"	"Le vidéo numérique est de plus en plus distribuée sur l'internet, et le jour n'est pas loin où la distribution de vidéo se fera principalement par réseau.  Les câblodistributeurs offrent des services de vidéo sur demande, iTunes et la plupart des réseaux de télévision américains distribuent des épisodes des séries télévisées populaires sur internet, et un grand nombre de films sont téléchargés à l'aide de service paire-à-paire comme BitTorrent. La distribution par internet de vidéo est rendue possible par de meilleurs réseaux, par une meilleure pénétration de l'internet à haute vitesse, par le faible coût des disques dures, et par de meilleurs algorithmes de compression. La grande majorité du contenu vidéo transféré sur internet est en format MPEG-4 (et non MPEG-2) et le contenu dans le format H.264 est en pleine croissance.  L'augmentation du contenu vidéo, de la demande pour ce contenu ainsi que pour du vidéo de meilleure qualité et à résolution plus élevée ne peut être satisfaite totalement par l'augmentation de la densité des systèmes de stockage ou l'amélioration des réseaux de transmission. L'augmentation des taux de compression vidéo demeure un important élément de l'équation.L'augmentation d'un facteur deux des taux de compression sera réalisée par une combinaison d'améliorations aux techniques de compression. Une de ces améliorations proviendra de meilleurs algorithmes de prédiction des trames vidéo. Nous proposons d'investiguer une approche fondamentalement différente de l'approche d'appariement par blocs. Cette approche est constituée d'une étape de détection suivi d'une étape d'appariement, et permettra de réutiliser efficacement de l'information déjà transmise, même si elle a été précédemment acquise dans des conditions différentes (point de vue, échelle, illumination). Cette approche demeure cependant dans la lignée des techniques de prédiction qui ont fait leur preuve, i.e., qu'elle génèrera des ensembles de pixels similaire aux ensembles de pixels à transmettre sans requérir une analyse sémantique de la scène.""365496,""Zaccour, Georges"
"363112"	"Zach, Richard"	"Computational aspects of the epsilon calculus"	"Many formalisms used in theoretical computer science such as database query languages, formalisms for specification and verification, and type systems for programming language semantics use non-deterministic choice functions.  A choice function picks an element from a specified class non-deterministically.  Such formalisms can be fruitfully investigated using logics incorporating a logical choice operator.  Logical choice operators were investigated by Hilbert in the epsilon calculus: in the epsilon calculus, a term of the form epsilon-x A(x) is some x which satisfies A(x), if A(x) is satisfied, and arbitrary otherwise. The epsilon calculus and proof theoretic methods developed for the epsilon calculus have mainly been applied to the proof theoretic analysis of mathematical systems. In recent years, however, it has also been extensively applied in computer science and computational linguisitics. For instance, epsilon operators have been used in dealing with the witness construct in relational database query languages introduced by Abiteboul and Vianu, the choose construct in Abstract State Machines, and as a type foundation for dynamic linking in extensible software systems.The aim of the project is to expand on previous work in applying the epsilon calculus in computational contexts and to provide a systematic foundation for it. This includes the development and investigation of proof theoretic methods for the epsilon calculus, development of formal systems for various versions of the epsilon calculus, the study of variant semantics for epsilon terms as choice functions in database languages, as well as investigating the behavior of epsilon operators in non-classical logics. In particular, intutionistic epsilon calculi and natural deduction systems for them will be studied: they are a desideratum for applications in programming language semantics, where type inference calculi typically mirror natural deduction systems under the Curry-Howard isomorphism.""369152,""Zadeh, MehrdadHosseini"
"365803"	"Zaïane, Osmar"	"Off-line and integrated data mining for intelligent systems"	"The widespread and ease of use of the World-Wide Web have helped make web-based applications a regular practice in software industry. This practice is even more pronounced when it comes to distributed applications such as electronic commerce and distance learning. With these applications, there is an important need to better understand the user on-line behaviour in order to improve the application (i.e. increase profit in the case of e-commerce, and improve the learning experience in the case of e-learning). Web-based applications track on-line resource accesses in a web access log, which typically becomes very large in size. What distinguish the data that result from web-based activities in general and e-learning in particular are the sheer complexity and the vast size of data, and the fact that simple information extraction is not possible. Relevant information about user behaviour must be deduced by interactive data mining and associated data visualization techniques. This research focuses on devising data mining techniques specific to the context of web-based information systems and in particular with application in e-learning. The research will address issues for (1) off-line data mining (i.e. data mining performed after the collection of data), and (2) integrated data mining (i.e. data mining performed real-time on data streams). The objectives for off-line data mining are to devise new algorithms (such as in clustering, classification, association analysis, sequence analysis, etc.) and new visualization techniques for visualizing website path traversals and resources access and usage to help web designers and educators in the case of e-learning assess analytically the on-line activities and eventually evaluate the users (i.e. learners). The objectives for integrated data mining are to design intelligent software agents that monitor on-line learners activities and dynamically adjust and restructure the online resources accordingly, as well as automatically recommend learning activities to learners based on previous successful learning behaviour of similar learners in the community.""380585,""Zaikova, Elena"
"363412"	"Zeh, Norbert"	"Theory and implementation of algorithms & data structures for  memory hierarchies"	"My research focuses on the theory and implementation of algorithms and data structures for memory hierarchies.  Current technological developments lead to a rapidly widening gap between processor speeds and memory (RAM) transfer rates.  This gap is even more significant if disk access times are taken into account.  Hence, today's processors are useless unless there is a way to bridge the gap between processor and memory speeds.  The approach to this problem taken in state-of-the-art computers is the use of a hierarchy of several levels of fast, but relatively small, cache memory.  This approach is useful only if the programs that are run have sufficiently local memory access patterns that ensure that most memory accesses can be served from cache rather than from main memory or disk.  Achieving this locality is challenging for many computational problems.  This is the focus of my research.  My research programme has a theoretical and a more applied facet.  The theoretical work aims at developing general techniques for designing algorithms and data structures with high access locality.  The applied work will apply these techniques and evaluate their usefulness in particular problem settings through implementation and experimentation.  The particular problems I focus on are:(1)  Graph problems.  This is motivated by applications that have to process massive graphs efficiently, such as web mining and web modelling, geographic information systems, and information retrieval.(2)  Data structures and algorithms for fundamental geometric search problems, most notably range search problems.  The motivation is the need to answer such search problems efficiently in database applications and geographic information systems.""384658,""Zeh, Norbert"
"361388"	"Zhang, Hao"	"Transform-based geometry processing and analysis in computer graphics"	"As frustrations start to set in when one struggles to solve a problem, it is often natural to attempt to look at the problem from another angle. Indeed, by transforming a given problem from the space in which it is defined into an alternative domain, one may be able to gain an insight more easily or reveal simpler patterns or structures that have been obscured by the problem's complexity in the original domain. Many such examples can be found in various computer science and engineering disciplines, e.g., transform coding for image compression, eigenfaces for face recognition, geometric duality in computational geometry, Hough transforms in image processing, and kernel-based methods in machine learning and pattern recognition.With the rapid advances in 3D scanning and acquisition technology, discrete shape representations, such as triangle meshes, are playing a dominant role in computer graphics, allowing for realistic and high-precision modeling of 3D geometry. Mesh models are ubiquitous in applications ranging from computer aided design (e.g., in automobile and aerospace engineering), data visualization (e.g., in archeology and GIS), to medicine and computer games. In the proposed research, we wish to investigate transform-based approaches for solving practical mesh processing and analysis problems in computer graphics. The main focus will be on spectral methods, which utilize selected eigenvectors, eigenvalues, and/or eigenprojections of appropriately defined linear operators to carry out desired tasks,  such as mesh segmentation, mesh layout, shape correspondence, and object recognition. We also propose to study 3D Hough transforms, parallel coordinates, Gaussian images, among others, and to find innovative ways to apply them to classical or new geometric problems in computer graphics, including visibility, shape inference from point cloud data, and manifold Delaunay triangulation. From our past experiences, we believe that these special transforms can bring new insights into our understanding of the problems at hand and contribute substantially to advancing the state of the art.""362656,""Zhang, Hong"
"361466"	"Zhang, Huajie"	"Discriminative learning of Bayesian networks for data mining applications"	"Bayesian networks (BNs) have been widely used in data mining applications, in which learning a BN from data is crucial. Most BN learning algorithms learn a joint distribution over all the variables to best fit the training data, i.e. generative learning. Discriminative learning directly targets a task-specific model, instead of a generative model, and demonstrates superior performance. Discriminatively learning a BN includes two steps: structure learning and parameter learning. Structure learning determines the structure of a BN, and parameter learning sets the local distribution for each variable.The objective of this research is to investigate the discriminative learning of BNs for various data mining tasks: classification, ranking, and probability estimation. In this research, we will investigate the following issues. (1) Effective discriminative BN learning: We are going to study discriminative structure learning that can effectively discover the underlying structure, and discriminative parameter learning that can compute the optimal parameters efficiently. (2) Scalable discriminative BN learning: One objective of this research is to develop efficient discriminative BN learning algorithms scalable to large-scale data mining. We are going to study various scaling up methods for  both structure learning and parameter learning. (3) Discriminative BN learning for ranking and probability estimation: In some data mining applications, accurate rankings and probability estimates are often desired. Is discriminative learning superior in terms of yielding accurate rankings and probabilities? How can BNs be discriminatively trained for this purpose? What kind of scoring functions and search strategies should be used? Our research will answer these questions.Due to the inherent features of discriminative learning, we would expect that this research could result in more effective and efficient data mining algorithms.""369589,""Zhang, Hui"
"361317"	"Zhang, Lihong"	"Manufacturability-aware performance-driven layout-centric design automation of analog and RF integrated circuits"	"Analog/RF circuits have been recognized as the design bottleneck for getting System-on-Chip products to market due to the high sensitivity to the associated analog/RF effects and the limited availability of analog/RF computer-aided design (CAD) tools. In particular, so far most of analog/RF layouts have to be handcrafted by circuit designers. Thus, the layout-related constraints are hard to be satisfied and included in the circuit synthesis. In this proposal, the development of design methodology and CAD tools for robust circuit/layout synthesis of analog/RF integrated circuits in deep-submicron and nano technologies is proposed. This work will focus on setting up complete circuit synthesis flow, investigating performance-driven automatic layout generation, fully taking into account distinct manufacturability issues in the advanced technologies, and exploring layout automation aided by circuit tuning. This will be the first fully automated manufacturability- aware performance-driven layout-centric synthesis methodology for analog/RF integrated circuits. In the circuit synthesis flow, the layout automation is integrated. Circuit tuning takes into account the layout-related effects so that circuit topology and sizing can be refined. The proposed complete layout design flow is composed of automatic template generation and template-based compaction. We will develop a new template-based scheme based on the macro-cell layout approach to facilitate the intellectual property reuse. Thus, the generated layout is reliable in the sense of performance and manufacturability without compromising the design efficiency.        Having attracted strong interest in academia and industry, the circuit/layout design automation of analog/RF integrated circuits has enormous potential for commercialization. This proposed research work addresses the increasingly serious performance and manufacturability problems for analog/RF integrated circuits in deep-submicron and nano technologies. It will benefit analog/RF designers by significantly improving the design efficiency and reliability. This research program would enhance the competitive ability of Canada in this field.""382693,""Zhang, Lijun"
"362602"	"Zhao, Vicky"	"Behavior forensics in multimedia security"	"In the past decades, advance in multimedia, communications and networking technologies has led to the proliferation of multimedia data in government, military, and commercial applications. However, the illegal alteration and unauthorized redistribution poses serious threats to multimedia security and intellectual property rights, and raises a critical issue of protecting multimedia content and enforcing digital rights. The emerging technology of digital fingerprinting embeds unique labels (known as fingerprints) into multimedia content, and provides proactive forensic tools for the content owner to track the usage of multimedia data and identify the source of the illicit copies. Multimedia fingerprinting involves a lot of users with conflicting objectives. They influence each other's decisions and performance, and the complex dynamics among users make the problem of reliable and trustworthy traitor tracing very challenging. To support multimedia forensics, in this proposal, we plan to 1) formulate the dynamics among users and understand how they interact with and respond to each other; 2) investigate how such user dynamics affect the traitor tracing performance of multimedia fingerprinting; and 3) in the long term, build a multimedia fingerprinting framework that provides trustworthy and reliable traitor tracing performance. We also propose to build a multimedia security and forensic center to study the underlying principles and theories, verify the technologies, and advance the young field of digital forensics.The anticipated significance of the proposed research includes: 1) uncovering the importance of behavior forensics and multimedia fingerprinting; 2) opening a new research direction in media security; and 3) addressing timely issues of practical importance. The proposed research is interdisciplinary, and covers a broad range of areas, including security, multimedia, digital signal processing, coding and game theory.""378784,""Zhao, Xiaoqiang"
"365073"	"Zhu, Ying"	"Model-based application-aware overlay networks over heterogeneous substrates"	"The main objective of this proposal is to introduce a next-generation overlay network that lies between the applications and the underlying heterogenous substrate networks.  This overlay understands the performance requirements specific to each application, and optimizes them by managing and allocating various types of network resources. The overlay acts as the glue that holds the different substrate networks together and presents a transparent interface to the applications. It also functions as an intelligent optimizing network managing layer which takes specifications from applications and optimally delivers services to satisfy the differing requirements.To achieve such an objective, first an accurate mathematical model of the overlay network residing on top of heterogeneous substrates (wired, wireless mobile, wireless sensor) will be developed.  Then, the model identification must be obtained, in a decentralized manner.  Finally, methods and algorithms for the application-aware network optimization will be developed.""381850,""Zhu, Yucong"
"361941"	"Zielinski, Adam"	"Underwater acoustic applications"	"Due to their low absorption in water, acoustic waves are frequently used for remote sensing of various oceanographic variables, bottom sediment classification, ocean bathymetry, fish stock assessment, underwater acoustic communication, navigation and positioning. Most of these applications require good understanding of underwater acoustic channels and sophisticated algorithms for signal compression, transmission and reception. Acoustic transmissions in highly reverberant, time-varying underwater channels often require an adaptive receiver combined with adaptive acoustic beam forming and tracking.  This is particularly relevant in shallow water transmissions.  Efficient acoustic ray tracing algorithms are needed for precise acoustic navigation and multi-beam acoustic bathymetry.  The large amounts of data associated with acoustic sensing require efficient compression for transmission or storage.  Any practical design of acoustic sensing systems must address these problems. The proposed research will recognize all these problems but it will concentrate on novel acoustic methods for precise navigation and positioning of underwater platforms such as bottom crawlers and various underwater vehicles. Such platforms are more and more frequently used for ocean research, bottom inspection and in ocean observing system such as planned by Neptune Canada program and several others. New methods for transmission and signal processing will be developed to achieve precisions not attainable by presently available systems. Specific applications to be addressed include also measurements of plate movements where required precision is in order of millimeters over distances in order of kilometers. Achieving such an extraordinary accuracy require new approaches to such measurements. Another area of research will concentrate on new methods for bottom characterization and classification using depth sounders and side-looking sonars. The mapping of bottom type and its topography is important for fisheries, fish stock assessment and other area of ocean exploration.""365611,""Zielinski, Barbara"
"364043"	"Zou, Ying"	"Business oriented software understanding, development and evolution"	"Rapid changes to business requirements are forcing organizations to reengineer their business processes and to evolve their supporting software systems. The agility and evolvability of software are becoming increasingly important at each stage of the software life cycle.  If such characteristics are ignored, software systems will deteriorate and will eventually be abandoned and rewritten with large costs to their owning organization. The objective of our research is to support the understanding, development and evolution of long lived software systems. Specifically, our research aims to maintain the consistency of business processes and source code, and to develop agile software systems that are responsive to changing business processes. Moreover, our research will create tools and techniques to ensure that the quality of software systems remains at satisfactory levels.To ease the evolution of software systems, we recover business processes from the implementation of software systems, and provide a high-level functional view for developers to understand complex software systems. To ensure that business changes can be easily propagated to the code, we will automate the generation of business applications based on business process specifications. We capture implicit design information embedded in business process specifications, and use Model Driven Development techniques to represent the recovered design information in an intermediate design model (i.e., UML model). Once new changes occur in business processes, the UML model is updated automatically and tweaked manually, if needed, to re-generate new code. Finally we provide a systematic approach to preserve specific software quality attributes while changes in functionality are introduced. Our research ensures the successful evolution of large complex software systems by preventing the decline of their quality and ensuring that they are adaptive to their changing environment. We expect that this research will graduate 3 Ph.D. students and 6 M.Sc students in the next five years.""370330,""Zou, Yun"
"364290"	"Zulkernine, Mohammad"	"Methods and tools for intrusion-aware software systems"	"Software systems must be engineered with reliable protection mechanisms, while still delivering the expected value of the software within the budgeted cost and time. The principal obstacle in achieving these two different but interdependent objectives is that current software engineering processes do not provide adequate methods and tools to achieve security goals. As a result, security loopholes often leave the system vulnerable to malicious attacks and abuses. In this research, various methods and tools will be developed with a view to building intrusion-aware software systems.    Available software specification languages are not completely suitable for specifying security related aspects of software, while attack languages are not useful for software development. The use of two different languages for software specification and security specification involves a number of unwanted but complicated issues. This research will focus on the factors related to the limitations of the existing intrusion scenario description mechanisms and the corresponding intrusion detectors, and will design ways to overcome them. A specification method will be proposed which can be used to specify both normal operational behavior and the behavior of the target when it is under known attack. Formal syntax and semantics of the features required for intrusion scenario specifications will be provided. Tools will be built for intrusion scenario description and automatic signature generation from the scenarios. Finally, intrusion detection systems will be developed to detect any intrusions into the software systems by comparing the run-time behavior of the systems with the generated signatures.    This research will play a vital role in bridging the gap between software engineering and security engineering practices. One of the unique aspects of this research proposal is to provide a balanced training facility for graduate students (2 PhD and 2 MSc) including both software engineering and security engineering principles for building secure software products.""380588,""Zulonas, Kevin"
