"id"	"researcher_name"	"application_title"	"application_summary"
"395391"	"Abolmaesumi, Purang"	"Medical image processing and computer-assisted surgery"	"This proposal focuses on the development of advanced computational methods for ultrasound image processing with applications in computer-assisted surgery and cancer diagnosis. Ultrasound imaging, in contrast to conventional imaging technologies, has significantly lower cost. It is also non-ionizing, which is especially important to children and pregnant women, and reduces potential harm to all patients and surgeons.    A computational framework will be created to integrate real-time ultrasound imaging with computer-assisted surgical procedures. By combining the segmentation of the anatomy from medical images with the registration of the patient to the surgical plan, this framework will enable real-time visualization of the anatomy throughout the surgery. This framework will also be used to generate probabilistic, rather than absolute, surface-based statistical anatomical atlases. The resulting atlases will be employed in the development of new atlas-to-patient registration techniques. Although the initial focus of this research will be on orthopaedic surgery, the framework will also be applied to soft tissue anatomy, with applications in radiotherapy and laparoscopic procedures.   As part of this research program, new computing algorithms for processing ultrasound signals, with applications in cancer diagnosis, will be developed. These algorithms will examine a new method in tissue typing: raw ultrasound signals may contain tissue typing information that is not available from conventional ultrasound images. This hypothesis will be verified by extracting spectral and fractal features  from ultrasound signals collected from human tissue samples.  The correlation between the extracted features and the existence and grade of cancer will be investigated using advanced pattern recognition techniques. This research is expected to positively impact the lives of many Canadians suffering from prostate and other forms of cancers.   This research offers HQP training opportunities in computer-assisted surgery and in image processing. Students will apply their technical knowledge to high-impact medical problems with practical implications.""407039,""Abotossaway, MarkAnthony"
"406146"	"Affes, Sofiène"	"Enabling signal processing techniques for cognitive wireless transceivers"	"Fourth-generation (4G) wireless communications are expected to deliver very high transmission rates to or from mobile terminals over different multi-carrier and multi-antenna air-interfaces in a seamless and versatile fashion, no matter what the surrounding environment, link conditions and network topologies are. This stringent requirement calls for developing new ""cognitive"" transceivers that swiftly and optimally adapt to variable operating conditions such as changes in multi-carrier and/or multi-antenna air-interface configuration, mobility, traffic location and/or load, propagation characteristics, and/or topology in mobile ad hoc networks (MANETs) in particular. This research project is directed at developing enhanced signal processing techniques as powerful tools for the efficient design of such new ""cognitive"" transceivers. These transceivers would not only provide the self-adaptation capability required by future wireless technologies to optimally operate in variable transmission conditions, but will also offer transparent migration between different radio interfaces and network topologies, all available form then on at low hardware cost on a unique wireless terminal. They would bring extremely efficient solutions to the tremendous challenges posed by future deployment of 4G such as designing multi-mode terminals and allowing transceiver software radio reconfiguration by software downloads over the air. From this perspective, they could have a major impact on future deployment of 4G by simplifying both the design of the terminals and the discovery/selection of the wireless system among the many that will be offered simultaneously to the user.""399100,""Affes, Sofiène"
"393616"	"Affes, Sofiène"	"Enabling signal processing techniques for cognitive wireless transceivers"	"Fourth-generation (4G) wireless communications are expected to deliver very high transmission rates to or from mobile terminals over different multi-carrier and multi-antenna air-interfaces in a seamless and versatile fashion, no matter what the surrounding environment, link conditions and network topologies are. This stringent requirement calls for developing new ""cognitive"" transceivers that swiftly and optimally adapt to variable operating conditions such as changes in multi-carrier and/or multi-antenna air-interface configuration, mobility, traffic location and/or load, propagation characteristics, and/or topology in mobile ad hoc networks (MANETs) in particular. This research project is directed at developing enhanced signal processing techniques as powerful tools for the efficient design of such new ""cognitive"" transceivers. These transceivers would not only provide the self-adaptation capability required by future wireless technologies to optimally operate in variable transmission conditions, but will also offer transparent migration between different radio interfaces and network topologies, all available form then on at low hardware cost on a unique wireless terminal. They would bring extremely efficient solutions to the tremendous challenges posed by future deployment of 4G such as designing multi-mode terminals and allowing transceiver software radio reconfiguration by software downloads over the air. From this perspective, they could have a major impact on future deployment of 4G by simplifying both the design of the terminals and the discovery/selection of the wireless system among the many that will be offered simultaneously to the user.""413630,""Affes, Sofiène"
"402808"	"Aiello, William"	"Security for enterprises and ISPs"	"It is a truism that the Internet has become integral to nearly all aspects of modern life, from science to entertainment, from government to industry, from the economy to our culture.  But while the open architecture of the Internet has been the chief enabler of it's explosive growth, this same openness, combined with the abundance of insecure commercial software installed in end-hosts, has provided attackers with a nearly inexhaustible supply of vulnerable hosts to exploit, either as primary victims or as intermediate steps in a chain of exploits.  Some of these exploits have resulted in attacks of massive scale such as worms, viruses, and distributed denial of service attacks, creating major disruptions in enterprises and routing subsystems throughout the Internet.  We propose here research into techniques and tools for enterprises to better secure their networks, hosts, and data resources and for large Internet Service Providers to protect their customers.  Two themes recurr throughout this research.  The first is an emphasis on the anomaly detection approach rather than the misuse detection approach.  In the later, models are made of illegitimate traffic and alarms are raised when traffic or behaviour matches a model.  In the former, a model is made of legitimate traffic and an alarm is raised when traffic or behaviour deviates from the model.  The majority of security tools rely on misuse detection as there is a wide-spread belief that the behaviour of computing systems are too complex to model for use in anomaly detection.  Here we investigate several anomaly detection schemes that have significant promise.  The second theme is an emphasis on using real traffic data collected from operational environments to evaluate the efficacy of our methods.""406741,""Aiffa, Mohamed"
"392204"	"Aïmeur, Esma"	"Privacy issues in electronic commerce and e-learning"	"Web personalization is an Internet technique for adapting websites to individuals. In general, it is based on the user profile, which summarizes what the website knows about the individual user. Unfortunately, this information is too often obtained without the user's knowledge or consent. In this research, I focus on a special class of web personalization technique, known as recommender systems, which have been used primarily in the context of electronic commerce. Recommender systems have also been used in the context of e-learning in order to help students choose programs and courses.Recommender systems are very useful, but they typically come at a price: in order to operate, it would seem that they need to collect information on those who use them. It follows that most current recommender systems suffer from various privacy-protection vulnerabilities. In the case of electronic commerce, customers who wish to obtain accurate recommendations are often asked to reveal information that they may consider private. The potential abuses from unscrupulous merchants could result in a serious erosion of the customer's privacy, who should be able to keep private their personal information and not be tracked against their will.In current e-learning systems: privacy concerns are nearly absent. Only primitive forms of privacy are offered in some platforms, such as not allowing the tutor any access to information about auto-evaluations performed by the learners. Nonetheless, the tutor has access to virtually all the remaining information including who are the students, what parts of the course they referred to, how many times and for how long, and all the information about the quizzes and tests the learner took in his course.The main focus of the proposed research is to develop recommender systems capable of offering the electronic services that we have come to take for granted, but in ways that prevent the creation of user profiles that could be abused. The tutoring process itself, not only its recommendation, will also be studied from a privacy point of view. I shall use my dual expertise in electronic commerce and e-learning to cross-fertilize the two fields.""394486,""Aimez, Vincent"
"394815"	"Alhajj, Reda"	"Adaptive data mining techniques for challenging applications"	"VIREX is a system initially developed at the University of Calgary to facilitate the transformation of relational and object-oriented data into XML format. It has a visual user-friendly flexible interface that allows users to specify their queries and requests with minimum keyboard usage. After getting the first web based version of VIREX, our plan is to expand VIREX to suport a variety of web related applications; and hence turn VIREX into a comprehensive service-oriented system that satisfies the needs of a wide range of users.The main applications of VIREX considered by this research program include, fuzzy query interface, information extraction from the web, content-based image retrieval (CBIR), social network analysis, and gene expression data analysis. The last three applications are attractive because of the growing amount of related information available on the web. The aim is to have VIREX work in a human-understandable way by supporting fuzzy queries. For this task, we will develop an approach capable of analyzing the underlying data, decide on the fuzzy sets for more effective query evaluation. Further, the web is a huge exponentially growing repository, and we will be able to take advantage of the information stored if there is an automated process to extract relevant information and store it in structured databases. CBIR is another application investigated by this proposed research program. Similarity search techniques and clustering the images based on their content will improve their retrieval. It has also been recently realized that knowledge discovery and machine learning techniques are effective for analyzing social networks to extract useful information that guides in identifying terrorist networks, criminal networks and for targeted marketing. The same also applies to gene networks. To meet the needs of the considered applications, the main objective of this research program is to  develop novel techniques for clustering, classification, similarity search and feature reduction by integrating fuzziness, multi-objective optimization, and multi-agents in the process. This will turn VIREX into  an attractive system providing a comprehensive integrated solution for a range of problems.""407803,""AlHalabi, Feras"
"395369"	"Amer, Aishy"	"Advanced real-time algorithms for object-based video content extraction"	"Video is becoming increasingly used in applications as diverse as entertainment, security, education, medicine, and even wireless devices. The introduction of video over mobile devices will accelerate this trend. For example, statistics Canada revealed that Canadians spent 21.6 hours per week watching TV in 2002, and security studies show that security concerns of government agencies, enterprises and individuals are driving the video surveillance market to a constant and robust expansion to be sustained for decades. A fundamental task in all these applications is the extraction of dynamic video content such as moving objects and their features. Video content is subject to different interpretation and video  description can vary according to observers and applications. Video usually includes multiple objects, occlusion, and artifacts. The stable extraction of video content in real-world applications is, therefore, a challenging task. In projects from previous funding, we have investigated real-time stable solutions to the problem of extracting multiple moving objects and features from signals of fixed video camera. We also integrated our solutions in a real-time video surveillance system and tested their individual integration into hardware (FPGA). In this proposal, we will focus on extraction of video content using advanced methods to handle video acquired by a moving camera and variable background. These solutions will use advanced tools, such as wavelet transform or MPEG-7, and be based on multi-levels: video  enhancement to estimate and reduce image noise, video stabilization to compensate for camera motion and changing background, and video analysis to extract moving objects and their features. Expected techniques will be integrated into an advanced (next-generation) video surveillance system. We aim also to implement the multi-level processing on one FPGA. Expected solutions would facilitate and advance the use of video in two intended video applications: in video  surveillance (such as to detect events) and in TV systems (such as to convert between video formats).""410132,""Ameri, Golafsoun"
"394833"	"Anand, Christopher"	"Optimization in MRI and NMR"	"The results of this research will lead to the better utilization of Magnetic Resonance Imaging (MRI) and Nuclear Magnetic Resonance (NMR) equipment.  This will result in a reduction in scan-time for MRI procedures and improve the quality of the resulting scans.  It will allow more NMR experiments to be conducted in existing labs.  MRI is used in clinical imaging, medical research and material science.  In dynamic imaging (e.g., imaging a beating heart, or imaging the passage of a contrast agent through the kidney) the longer the experiment, the more moving tissues and blood will be blurred.  By optimizing the collection of MRI data, this research will give physicians the ability to observe processes at higher resolutions, especially in patients with irregular heartbeats and limited ability to remain still, and thereby improve our ability to diagnose disease.  In addition to reducing blurring as a side-effect of faster exam times, the research in this proposal represents the only attempt to incorporate motion-insensitivity into the trajectory used to collect MR raw data.  This is important both when the tissue of interest is in motion (e.g., cardiac imaging) and when static tissue is obscured by errors caused by nearby tissue in motion (so-called motion artifacts).NMR is the best tool available to study proteins dynamics and thereby determine how they bind to other molecules, but the experiments are very time consuming, including both long experiment times, and the considerable skill required to interpret the ambiguous data.  I have developed a mathematical model to predict the information content of such NMR experiments.  This will allow such experiments to be optimized, dramatically shortening the required experiment time.  This will speed up the process of understanding protein dynamics.""411426,""Anand, Karan"
"389205"	"Antoniou, Andreas"	"Digital signal processing for communications and other applications"	"The availability of highly versatile systems-on-a-chip and powerful computer hardware have led to a heavy demand for digital signal processing (DSP) algorithms and methodologies that can be used in a wide variety of applications, for example, to design digital filters, to process images, or to analyze DNA sequences.  This research will deal with a fairly broad selection of projects concerning the development of DSP algorithms and methodologies for communications and other related applications such as the following:  (a)    )Powerful optimization techniques will be used to develop advanced design methodologies for digital filters that would satisfy a variety of specifications.(b)    )Genetic algorithms that operate on the principles of natural evolution will be applied to a diverse range of  specialized design problems to exploit the flexibility of these algorithms and their ability to explore multiple locales of the parameter space simultaneously.(c)    )The so-called set-membership concept will be applied along with modern optimization methods to develop new classes of  adaptive filters and to apply these filters to applications such as echo cancellation, near-end crosstalk cancellation, and the elimination of multipath signals in wireless communications.(d)    )We plan to combine analysis techniques for nonnegative matrices, the singular-value decomposition, and state-of-the art optimization techniques for the purpose of enhancing the quality of the information that can be extracted from DNA sequences and to extend the application of these techniques to microarrays.(e)    ) We will continue our work on the application of state-of-the-art optimization techniques to the image super-resolution problem.   Image super-resolution is a class of techniques that can be used to construct high-resolution images using a collection of low-resolution images.""408707,""Antoun, Ghadi"
"392106"	"Barbeau, Michel"	"Broadband wireless access mesh network management"	"This project is about high speed wireless networks, such as the WiMax/802.16 mesh networks. They are of growing importance for emergency communications, Internet wireless access, and wireless voice over IP. They are growing in scale and complexity to a stage where management automation is required. Because they feature ad hoc mechanisms, network management needs to be distributed and result from the cooperation of the participating nodes, in contrast to an infrastructure type of network management predominated by the manager-managed node relationship. The manager function is a feature present in every participating node.In this research, the focus is on network security management and network topology management. Our work on security is guided by the analysis of threats. We don't aim at absolute security, which is in a utopia and extremely costly to come close. We look instead at the possible attacks. They are prioritized according to an evaluation of their risk. The level of risk is determined by looking at the factors of potential attacker motivation, level of difficulty, and eventual impact. For instance, in practice a good number of critical risk threats tend to be related with rogue devices. To approach this problem, we dig into wireless network protocol and system descriptions to find data sources that can used to detect anomalies. Geometric and statistical techniques are used to create anomaly detection tests. The effectiveness of the tests is measured in simulation and real network settings.Our work on network topology management is rooted in the area of distributed algorithms for the resolution of graph problems. Such algorithms do address aspects such as cluster formation or dominating node selection. Because of their distributed character, these algorithms potentially address practical ad hoc network topology management issues, such as sponsor node selection in WiMax/802.16. We refine distributed algorithm specifications into protocol descriptions and conducting actual evaluations on simulated and real networks. The performance is evaluated using metrics that are meaningful to network managers.""406520,""Barbeau, Myriam"
"396562"	"Barbosa, Denilson"	"Web data management"	"The Web has become a vast, dynamic repository of information about virtually every topic, including people, their opinions, and their connections to one another. Recent developments in Web technology have led to easier ways with which users can produce, annotate, and consume data. Managing all this data, and in such a heterogeneous environment, is a daunting task and a major challenge embraced by many, in several areas of computer science, including databases. In this proposal, we are interested in data management problems that arise in the context of the Web as well as in exploiting the latent knowledge in the Web for solving traditional data management problems. More specifically, we are interested in the efficient management of XML data, which are the preferred format for representing and exchanging data on the Web, as well as the underlying foundation of Web services and the Semantic Web. We propose to continue our work on efficient XML management, as well as to address the need for notions of Information Theory that are suited for XML content. We are also interested in exploiting Semantic Web documents, particularly ontologies, for helping in solving data exchange and integration problems. In particular, we are interested in exploiting domain knowledge to build wrappers for deep Web sites, which contain valuable information not accessible through search engines. Moreover, we intend to exploit domain knowledge to help in the understanding mappings between schemas, and for evaluating their quality. We are interested in developing distributed search algorithms that exploit both the structure and the content of nodes in a network for efficiently routing queries in the system.Our goal is to develop algorithms, tools and systems based on solid theoretical foundations, while providing opportunities for the training of HQP at the University of Calgary.""396561,""Barbosa, Denilson"
"390754"	"Beaudry, Martin"	"Langages formels, groupoïdes et automates"	"Mon projet porte sur les algèbres finies (associatives: groupes, semigroupes et monoïdes; non-associatives: quasigroupes, boucles et groupoïdes), leurs propriétés en ce qui a trait à la reconnaissance de langages, et leurs applications potentielles dans d'autres domaines de l'informatique théorique.En ce qui concerne la reconnaissance de langages, mon effort principal porte actuellement sur les boucles. Ces algèbres sont exactement celles qui ont un élément neutre et dont la table est un carré latin, c'est à dire que chaque ligne et chaque colonne de la table est une permutation des éléments de la boucle. Les boucles finies fournissent une caractérisation originale d'une classe de langages, les ouverts réguliers, qui ont été définis dans un contexte totalement différent. Je travaille actuellement sur des cas particuliers de boucles particulièrement intéressants qui sont apparus au cours de mes recherches antérieures.Une des applications auxquelles je m'intéresse présentement est le modèle des automates quantiques, qui combine les propriétés du calcul quantique avec la relative simplicité du modèle des automates finis. Diverses questions sont ouvertes dans ce domaine. Une des possibilités de recherche consiste à explorer la question de la taille que doit avoir un automate pour être capable de reconnaître un langage donné. Ceci fait suite à des résultats qui montrent que, pour certains langages, on peut utiliser des automates quantiques exponentiellement plus petits que leurs contreparties classiques, alors que dans d'autres cas c'est l'automate quantique qui est exponentiellement plus gros que son équivalent classique.""400140,""Beaulac, Rémi"
"388863"	"Beaulieu, Norman"	"Wireless communications and digital transmission"	"Wireless communications research has been given great impetus by the advent of cellular telephony, mobile satellite and portable personal communication services.  The exponentially growing user demand for services, together with the increasing demands for higher speed transmission of large amounts of data, as well as customer requests for multi-media services, create the need for new technologies.  In order to provide higher data transmission rates to more users without sacrificing the integrity of the received information, advances must be made in the transmission system designs and in the transmission system components.  In turn, achieving the best advances in wireless systems and components requires better estimation of the wireless channels in practical systems, and better modelling of the wireless channels in engineering design studies.The overall goal of the proposed research is higher capacity in broadband wireless communication systems at lower cost.  We want to offer more wireless services with higher quality to more users.  This is a significant challenge, scientifically and technologically, because the wireless spectrum (bandwidth) is a fixed resource, and the new wireless services, for example video cellphones, which are becoming increasingly popular require much bandwidth.  The primary thrust of this research is investigation into fundamental properties, limitations, and improvements in broadband wireless systems.  Without new discoveries in engineering science, the available spectrum will become overloaded and unable to provide wireless services to the anticipated number of future users.  A secondary thrust is the application of the research results to present and future systems.  New engineering science must be put into action in the development of new technologies for real world systems.  The field will benefit from new theories which provide insight into both scientific understanding and practical applications.  The average person, the user, will benefit by having more and improved wireless services.  Canada will benefit because leading-edge wireless technologies will be transferred to Canadian industries impacting product sales and job creation.""406678,""Beaulieu, Vincent"
"403642"	"Belostotski, Leonid"	"High-end wideband receiver design"	"This research will be targeting the design of a high-performance fully integrated wideband receiver system.  The immediate application for the receiver will be a new radio telescope.  The radio telescope will be the largest telescope ever built and is the main research focus of many radio astronomers at the present time.  The telescope is known in the astronomical community as the Square Kilometre Array (SKA).  This telescope will have a footprint of 3000km and will require tens of millions of receivers.  The fully integrated receiver, investigated during the proposed research, will have a significant impact on the telescope final cost.     Recent work by the applicant has shown that at least over a relatively narrow frequency range (0.7GHz-1.4GHz) for radio astronomy standards the latest wide-spread commercial integrated circuit technologies, Complementary Metal-Oxide-Semiconductor (CMOS) technologies, are able to nearly achieve performance requirements of the SKA receiver system.  This proposed research will build on the prior work and will investigate novel ways of increasing the receiver bandwidth to ultimately cover the full telescope frequency range from 0.3GHz to 25GHz.  The challenging aspect of this research is the desire to reduce any noise imposed by the receiver onto incoming signals to levels that are significantly lower than what is commonly achieved with commercial receiver systems.  The final solution will be based on commercially available technologies to control receiver costs as the anticipated number of the receivers exceeds tens of millions.    This proposed research will strive to completely integrate the SKA receiver into a single integrated circuit.  The results of the research conducted under this proposal will be applicable for commercial Ultra Wide Band applications and other applications requiring high-end wideband receivers.""398588,""Belov, Anton"
"396430"	"BenHamza, Abdessamad"	"Morse-theoretic topological modeling of 3D objects and applications"	"The importance of 3D object recognition is increasing rapidly in the field of computer graphics and multimedia communication due to the difficulty in processing information efficiently without its recognition. 3D objects consist of geometric and topological information, and their compact representation is an important step towards a variety of computer vision and graphics applications. One of the key mathematical tools used to study the topology of 3D objects is Morse theory, which studies the relationship between functions on a space and the shape of the space. The proposed Morse-theoretic framework aims to represent a 3D object with an invariant multiresolution skeleton graph to capture important structural object information using a mixture distance function. The key idea is to identify and encode regions of topological interest of a 3D object. The main motivations behind using the proposed approach are: (a) rotational invariance of the distance function makes it more adapted to object recognition than the Morse height function, and (b) we can show that a surface may be reconstructed from its intersections with concentric super-ellipsoids centered at the barycenter of the underlying surface. The topological changes occur as we change the level values of these concentric super-ellipsoids. In this research proposal, we will benefit form the studies performed during the past 3-year funding period to develop a Morse-theoretic framework for topological modeling of 3D objects, and to take advantage of the latter to develop a theoretically rigorous and computationally feasible methodology for 3D object recognition, retrieval, animation, and watermarking. In the course of developing our algorithmic tools, we believe it is necessary to investigate this promising Morse-theoretic formalism and provide in the process a more thorough and clear analysis framework.""391062,""Bénié, GozeBertin"
"406148"	"Bingham, Derek"	"The design and analysis of complex industrial experiments"	"The design and analysis of experiments have made important contributions to scientific discovery and innovation and will continue to do so for the foreseeable future.  The proposed research focuses on developing new methodology for the design and analysis of computer experiments for physical systems (computer simulators), and the design and analysis of (fractional) factorial experiments with randomization restrictions. In both cases, new statistical methodology will be developed.  The research is motivated by applications of key interest to scientists.  The research program is highly interdisciplinary, relying on collaboration with scientists in areas such as engineering, cosmology and numeric climate modeling. ""395103,""Bingham, Derek"
"393379"	"Biskri, Ismail"	"Applications linguistiques multilingues: apport des grammaires catégorielles"	"Notre projet, bien qu'il ne s'inscrive pas directement dans les recherches dédiées aux grammaires d'unification, reprend à son compte l'idée d'établir des niveaux d'abstraction de nature universelle. Une grammaire catégorielle, conceptualise la langue comme un système d'agencement des unités linguistiques comme desopérateurs qui s'appliquent à leurs opérandes. Aussi, la concaténation linéaire des unités syntagmatiques n'est pas l'opération fondamentale comme dans les grammaires chomskiennes voire syntagmatiques. C'est plutôt l'opération d'application d'un opérateur à un opérande qui est essentielle pour construire les unités linguistiques complexes. Cette caractéristique étant universelle est avantageuse pour faire émerger de ce modèle les opérations logiques et grammaticales élémentaires (règles grammaticales, métarègles, algorithme, etc.)invariantes par rapport à la diversité des langues et celles qui ne le sont pas et qui doivent être spécifique à chaque langue.Dans nos travaux antérieurs nous nous sommes intéressés au français et à l'anglais, dans le présent projet nous nous intéresserons aussi à la langue arabe.""399589,""Bisnath, Sunil"
"395381"	"Blanchette, Mathieu"	"Computational characterization of regulatory elements in the human genome"	"While the human genome is sequenced since 2001, the way it works remains poorly understood. Our group is designing computational and statistical approaches to identify regulatory regions - DNA regions whose role is to modulate the expression of a nearby gene. Gene regulation and mis-regulation is increasingly associated to diseases and disease susceptibility. The predictions made by our algorithms allow biomedical researchers to focus their resources on the most interesting candidate regions and suggests hypotheses that they can test. Our approach attempts to detect evolutionary signatures - specific patterns of mutations that are  indicative of specific functions.""411268,""Blanchette, Maxime"
"390348"	"Blostein, Dorothea"	"Document analysis systems"	"Documents are widely used in our information-rich society.  The proposed research streamlines the transition between paper documents and electronic documents, contributing flexible and general techniques for converting the information in scanned paper documents into electronic form.   One proposed project develops new methods for representing the decisions that are made by document recognition systems.  An explicit representation of decisions makes it easy to determine the causes of recognition errors, and supports automatic optimization of recognition algorithms.  The second proposed project improves document processing by combining an analysis of document text with an analysis of document figures.  The approach will be tested on a document classification problem: identify which technical documents are relevant for maintaining a biomedical database.  The third proposed project  develops methods of using external data to validate and correct recognition results.  For example, an internet search can be used to verify information recognized from a scanned business card, and to correct incorrectly-recognized digits in a telephone number.The proposed research will train ten graduate students and five undergraduate students.  These students will learn the theory and application of pattern recognition, including the use of mathematics and statistics.  They will gain experience in software development, technical writing and oral presentation.   Graduates of this program are in demand in both academic programs and in the information technology sector, often finding employment in software development or pattern recognition with firms like IBM.""403105,""Blostein, Steven"
"394198"	"Blustein, William"	"Augmenting human information processing through hypertext"	"Although this research is multifaceted the overall goal is to augments human cognitive and communication abilities.  My particular focus is on helping people to find and use information in complex `information spaces' such as many websites.  I believe strongly in the potential of hypertext to make documents that are clearly better than paper, but I do not believe that it is easy to do.""391843,""Blute, Richard"
"389349"	"Bock, Wojtek"	"Study of fiber-optic sensing system based on the laboratory-on-a-fiber concept"	"The principal research objective of this program is to conduct fundamental studies of novel fiber-optic sensing devices based on the lab-on-a-fiber (LOF) concept which could be used in safety and security or environmental monitoring as remote detectors of dangerous substances. These devices will be based on photonic crystal fibers (PCFs) and on a variety of functionalized coatings, for applications in photonic sensing of selected chemical targets. The program also includes a major applied thrust in technology development, prototyping, testing and implementation of an advanced universal LOF-based technological platform for fiber-optic sensor systems incorporating the sensing devices to be developed. The research plan includes specific short-term and long-term objectives. As the first objective at the sensing device level, studies and development of a novel generation of fiber-optic sensors based on PCFs will be undertaken to develop an entirely new class of devices with programmable properties. These devices will be specifically designed using a variety of approaches, involving long-period gratings, tandem gratings and task-specific fiber segments, in combination with few-mode and multimode interferometry. A practical objective is to selectively design and optimize the transmission, polarization, and light collection properties of these devices through appropriate microstructural modeling, and then to fabricate them through appropriate technological processes. A second objective is to subsequently equip these devices with a variety of functionalized coatings, in order to induce surface plasmon resonance, fluorescence quenching or other effects allowing for selective detection of a wide variety of intended chemical/biological targets. Consequently, we expect to design and manufacture fiber sensors with superior metrological properties, including selectively enhanced sensitivity to a chosen target, highly decreased sensitivity to temperature and negligible false positive ratio. Due to the flexible properties of the new sensors and the processing functionality/adaptability of our LOI system approach, they will be compatible with many specific sensor configurations.""412914,""Bock, Wojtek"
"391253"	"Boutilier, Craig"	"Robust preference assessment and online mechanism design"	"The assessment of user preferences is critical to the development of decision support and interactive optimization tools The ability to customize decisions to the needs or desires of specific users or organizations is of great interest in a wide range of domains, including medical decision making, financial planning, procurement and supply chain management, product recommendation, personalized Web search, advertizing, user interface configuration, and many others.  Making good or optimal decisions requires that a system know enough about a user's preferences (or utility function) to address the task at hand, and have the ability to trade off the costs of obtaining such preference information with the value of that information (i.e., its ability to improve decision quality).  The proposed research will develop new techniques for preference assessment and elicitation, both in single-agent and multiagent (or economic) contexts. The underlying methodology embodies assessment techniques that allow decisions to be made with only partial preference information, and elicitation techniques that obtain the most relevant information. Building on previous work using Bayesian and regret-based optimization and elicitation, we will explore two key themes that challenge current techniques. One is the assessment of preferences in sequential decision problems (such as Markov decision problems). Indirect (observational) and direct (querying) partial assessment methods will be developed, as will online methods that must act in the presence of an uncertain future (this is of special importance to online mechanism design). The second theme is the development of ""open-ended"" assessment and elicitation techniques, wherein the attributes or outcomes of interest are ill-specified or entirely unknown to the system. Feature identification/construction methods for utility functions will be developed as will techniques for handling vague features in multiattribute utility models.""411847,""Boutilier, Michael"
"402812"	"Bradbury, Jeremy"	"Empirical assessment and improvement of fault detection techniques for conurrent software"	"Ensuring the quality of concurrent programs is a very difficult problem that is becoming increasingly pertinent due to advances in hardware technology. Advances like multi-core processors have resulted in an increase in the need for concurrent software development. Unfortunately, developing correct concurrent code is much more difficult than developing correct sequential code. The objective of the proposed research program is to enhance our understanding of different fault detection techniques for concurrent software and to leverage our understanding to improve and customize the use of these techniques. The research is divided into three topics: 1) empirical assessment and comparison of different fault detection techniques for concurrent systems; 2) domain-specific customization of fault detection techniques like model checking and testing; 3) improving the combined use of different fault detection tools in large software projects. The underlying theme of all topics within the proposed research program is the use of empirical research methods to facilitate better tool usage as well as the development of more effective and efficient fault detection tools. The greatest significance of this work is to the field of software engineering and specifically software quality assurance. In the short term our research program will provide better understanding of today's fault detection tools. The long-term benefit of this research is the development of combined quality assurance approaches that use tools automatically optimized to a particular domain or a particular class of faults. ""409210,""Bradbury, Laura"
"395934"	"Bridson, Robert"	"Numerical methods for computational physics in animation"	"This research will develop new numerical algorithms for challenging problems in computational physics, e.g. simulating how air and water flow, or how solid objects collide and deform. The focus application is computer animation, building on this researcher's present and future collaborations with the film industry. Here many of the most difficult animation problems, such as producing photorealistic water or clothing, can be solved by simulating the underlying physics. However, this project is primarily concerned with the core algorithmic issues, which are of much broader interest in science and engineering, in particular the following:1) Discretization of Constrained Continuum Mechanics: many natural phenomena of interest to both animators and scientists are constrained: for example, incompressibility for fluid flow and biological tissue, developability (a restriction against in-plane deformation) for cloth and paper, inextensibility for hair, and collision/contact problems between solids. This project will create a new unified framework for handling all these problems in a computer simulation.2) Efficient Solution of the Resulting Optimization Problems: the bottleneck for these simulations is often solving a large optimization problem: e.g. how to find forces that minimize the energy of the system, subject to physical constraints. This research will create new, more efficient methods for solving these important classes of problems.3) Robust Geometric Methods for Physical Simulation: Another major challenge in simulating many physical phenomena is dealing with geometric problems, such as tracking the surface of a liquid. I seek to develop efficient and accurate techniques that are provably robust.""409012,""BridsonPateman, Evan"
"390665"	"Brlek, Srecko"	"Combinatoire des mots et géometrie discrete"	"Mon domaine de recherche principal est la combinatoire des mots, où le point de vue  algorithmique joue un rôle dominant  pour les aspects autant théoriques que  pratiques. Dans la continuité des travaux des  cinq dernières années, le projet s'articule sur la combinatoire des mots pour développer certains champs d'application. Plus précisément, dans l'ordre allant du théorique au pratique, je vais développer quatre thèmes:A. Combinatoire des mots: l'étude de la complexité palindromique et l'étude des mots lisses.B. Langages formels et combinatoire bijective : énumération de polyominos qui pavent le plan.C. Géométrie discrète: étude des propriétés géométriques des figures discrètes du point de vue de la combinatoire des mots; étude des mots liés au pavage du plan.D. Protocoles cryptographiques : les mots lisses ont des propriétés cryptographiques qui méritent une analyse plus poussée; Simultanément, les trois premiers thèmes contribueront à l'enrichissement d'un outil de calcul formel que nous développons depuis maintenant dix ans.Le projet est novateur sur plusieurs aspects, en particulier l'étude des interactions entre la géométrie discrète et la combinatoire des mots porte sur un domaine ayant des applications en  traitement d'images.""403300,""Broadbent, Anne"
"397966"	"Bui, HungTien"	"Robust high speed clocking circuits"	"Precise clock generation/synchronization is an extremely important part in the design of today's increasingly complex synchronous systems. For instance, when digital video is transmitted over coaxial cables, it usually follows the Serial Digital Interface (SDI) protocol which has stringent timing requirements. Frequency variations caused by temperature, supply voltage drop, or any other environmental fluctuations can cause timing problems and can result in transmission errors. Accurate and stable clock sources are therefore extremely important in modern systems.Clock generation/synchronization is typically done using analog/mixed-signal approaches, such as phase-locked loops (PLL) and delay-locked loops (DLLs). While these solutions provide good results, their design and verification process is usually long, difficult and requires a certain level of expertise. This is undesirable in a consumer market where the life cycle of a product is typically short. In an effort to improve productivity and reduce the time-to-market, several researchers have demonstrated purely digital methods of designing these same systems (eg. all-digital PLLs). In addition to these benefits, the digital designs are also technology-independent, are easily prototyped and are more robust to process and temperature variations. It is therefore significant to explore digital ways of accurately generating and synchronizing clocks.""389946,""Bui, Tien"
"389025"	"Cartledge, John"	"Signal processing for fiber-optic communications"	"The objective of the proposed research is to investigate techniques for all-optical regeneration in fiber-optic communication systems operating at bit rates of 40 and 160 Gb/s. All-optical regenerators improve the quality of signals degraded by transmission impairments. They have the potential to enable networks with enhanced functionality and increase the maximum achievable transmission distance. For a bit rate of 40 Gb/s, conventional electrical time division multiplexing (ETDM) is used to combine several lower bit rate signals into a single high bit rate signal.  A technique known as optical time division multiplexing (OTDM) is used for a bit rate of 160 Gb/s since the required components are not available for ETDM. Methods for the demultiplexing of OTDM signals will be explored as a topic of considerable interest itself, but also as an essential requirement for enabling the research on 160 Gb/s all-optical regeneration.     Three doctoral students will develop theoretical and experimental skills highly sought by industry as they explore (i) regeneration for amplitude modulated signals, (ii) regeneration for phase modulated signals, and (iii) OTDM demultiplexing using electrically- and optically-activated gates. Graduates during the current five-year grant are employed at Ciena, Communications Research Centre, Gennum, Nortel Networks, Optelian, and Stanford University.    The proposed research will advance the fundamental understanding of techniques for optical regeneration and demultiplexing. Thorough assessments of the system performance are topics of significant technical challenge and practical importance. If advanced technologies are to be deployed in fiber-optic communication systems, detailed study is required to fully understand the key issues that affect system performance. The potential impact of the research is significant given the role of global telecommunication networks in society today and the reliance of such networks on fiber-optic communication systems.""401115,""Carto, Shannon"
"391199"	"ChaibDraa, Brahim"	"Agent and multiagent computing for complex environments"	"Autonomous agents are computer systems that are capable of independent action in open, unpredictable environments. Nowadays, concurrent and distributed systems are very common, it becomes obvious in this case that a single autonomous agent is insufficient. Many applications, if not most of them, require multiple autonomous agents, called also Multiagent Systems. In such computing systems, knowledge, action and control are distributed among the agents, which may cooperate, compete or coexist depending on the context.    Agent and multiagent systems are considered as a significant area of interest for such applications as telecommunications, robotic systems, information elicitation and management, Internet and electronic commerce, computer games, information retrieval and filtering, user interface design, industrial process control, open systems, etc.  The successful adoption of these systems in all these areas will have a profound impact both on Canadian industries, and also on the way in which future computer systems will be conceptualized and implemented.    In the long-term, our project is engaged in a program of research aimed at deepening our understanding of autonomous software agents which can act and interact; and at developing practical methods for designing such agents. The major significance of such project is that it imports some powerful practical and theoretical ideas into the core of computer science.     In the short term, our main goals are: (a) to determine important issues relative to agent/multiagent planning and learning; (b) to develop methods and algorithms for those issues and, (c) to test and validate those methods and algorithms on realistic complex applications as: robotic assistant for persons with disabilities; robotic systems, intelligent vehicles and adaptive UAVs (unmanned aerial vehicles); resource management in rescue environments; and dialogues and communication between man and machines.""398566,""Chain, Frédéric"
"395102"	"Chalin, Patrice"	"Practical advances in the formal verification of security and safety critical software"	"It is estimated that faulty software costs the world economy 160 billion dollars yearly. Hence, efforts that can be made to help improve software reliability, even in a small way, will be worthwhile. Sir Tony Hoare and other eminent researchers recently determined that the ""time was right"" to revive Robert Floyd's 1967 Verifying Compiler (VC) project. They did so by recasting the project in the form of a Grand Challenge for Computer Science and Software Engineering known as Grand Challenge 6 (GC6), Dependable Systems Evolution. In summary, the GC6 is an international effort with a time scale of 15-20 years whose main deliverables consist of: (i) Unified theory of software analysis and construction; (ii) Verifying Compiler (VC), a tool that can establish the correctness of a program, relative to its specification, before it is run; (iii) Verified Software Repository (VSR) of industrial grade applications and their specifications. The overall research goal behind this proposal is to contribute to the development of theories, languages, tools and methodologies which can help the software industry be more effective at developing quality software. Work towards this goal will be through contributions to the GC6 with a focus on the Java Modeling Language (JML), a Behavioral Interface Specification Language (BISL) for Java because of Java's use in security and safety critical areas such as Web-based Enterprise Applications (WEAs) and embedded devices and controllers (such as smart cards). Specific projects within the scope of this proposal include: (1) compounding the benefits of Runtime Assertion Checking, Extended Static Checking, Full Static Program Verification and Model Checking; (2) unification of JML axiomatizations, mutliprover support, parallel verification; (3) enhancements to the language design and semantic foundation of JML; (4) industrial case studies. The combined advances set forth in the proposal are novel. They will help raise the bar on the size of applications that can be subject to formal verification using JML tools.""400071,""Challagulla, Krishna"
"395469"	"Chan, Adrian"	"Nonlinear methods of analysis for biological signals"	"We will be researching techniques developed for the analysis of nonlinear and chaotic systems and applying them to biological signals. In particular, we are interested in developing a robust system that is capable of monitoring muscular fatigue in a noninvasive manner. The current ""gold standard"" examines the spectrum of the myoelectric signals (electrical signals associated with muscle contractions), tracking the median frequency. As a muscle fatigues, the spectrum will shift towards lower frequencies, resulting in a decreased median frequency; however, the median frequency will also change when the force of the contraction and/or the joint angle is altered, which confounds this method and limits its practical utility. We are developing a novel method based on a Generalized Random Scaling Fractal model of the myoelectric signal spectrum. Initial results already indicate that this methodology can separate the different effects of contractile force, joint angle, and muscular fatigue. This will enable to construction of a system that is capable of monitoring muscular fatigue, not only for static contractions, but also for dynamic contractions. Research into reliable noninvasive methods for monitoring muscle fatigue has implications in application areas such as ergonomics, injury prevention, sports medicine, and human performance. These analysis methods also have utility in muscle activity onset detection. The onset time of muscle activity is a fundamental characteristic used in biomechanics and motor control research; however, analysis of the myoelectric signal for onset detection is often conducted manually, or if an automated system is used, manual correction is often still required. The problem is that either noise is falsely labeled as myoelectric data, or low levels of contractions are missed. This research will be leveraging the fact that myoelectric signals exhibits different parameters associated with its fractal geometry and persistence, as compared to its background noise. The ability to discern myoelectric signals from the noise will improve the accuracy and robustness of automatic onset detection. This will save time for researchers and clinicians, but also assist in the development of other real-time myoelectric signal analysis systems.""405351,""Chan, Alvita"
"406149"	"Chen, Jiahua"	"Statistical genetics, statistical finance and other statistical problems"	"Data on hundreds of thousands of genetic variations (e.g. gene expressions) of human beings are routinely collected by geneticists in the hope of identifying, among them, a handful that are responsible for various diseases having a genetic component, such as diabetes and cancers. Statistical methods are then used to analyze the data to complete the task. Yet such data analyses are far different from the ones that classical statistical methods are designed to deal with. With huge number of candidates under investigation but limited number of samples, many genetic variations can appear to be culprits purely by chance. Developing statistical theories and methods to determine the strength of evidence needed to identify the true set of responsible genetic variations is an area of intensive research. This proposal focuses on problems in this general area. We view the statistical problem from a model selection angle. Every group of genetic variations can be used to construct a statistical model to predict the disease status. A sensible criterion is used to judge their relative merits, and therefore, enables us to select an optimal one among a huge number of candidates. There are two important aspects in this development. First, the criterion must be adaptable to various statistical models with well understood good theoretical properties. Secondly, the criterion can be implemented without an exceedingly high computational complexity. That is, computational strategy should be developed together with the criterion. My research along these lines has just been started recently and has already met with some encouraging successes. I have continued research interests in statistical analysis for finite mixture models, problems in empirical likelihood, and in survey methodology. These research interests may appear very different from each other, yet the scientific principles behind them are tightly connected. Research problems in these areas form another important part of my research program and this proposal.""391349,""Chen, Jiahua"
"402614"	"Chen, Jun"	"Network data compression:  fundamental limits and practical schemes"	"Network data compression, also known as Slepian-Wolf coding, is a far-reaching generalization of Shannon's point-to-point lossless source coding to the distributed network scenario. However, the impact of the fundamental theory of Slepian-Wolf coding on practical network data compression techniques is still very limited. Most existing network data compression schemes are direct combinations of conventional point-to-point data compression methods, and cannot fully exploit the potential as promised by the theory of Slepian-Wolf coding. The proposed research is designed to enable Slepaian-Wolf coding theory to realize its potential as a theoretical foundation of network data compression. We shall explore the intimate connections between Slepian-Wolf coding and channel coding in terms of rate-error tradeoff and operational duality. The emphasis will be on obtaining guidelines for the design of practical network data compression schemes. We shall explore alternative formulations of distributed network data compression theory that incorporate variable-rate encoding, universal decoding, mismatched decoding, and asynchronous Slepian-Wolf coding. In addition to strengthening the theoretical foundation of network data compression, the proposed research will provide a systematic framework for practical Slepian-Wolf code construction that can meet the requirement of real world applications. The proposed research could find applications in many different areas. Besides its obvious usefulness in conventional networks, network data compression techniques can also be potentially used in video coding, secure fingerprint biometrics, secret key generation, data hiding and watermarking. The proposed research will be integrated with educational activities to attract talented students to work in this fertile interdisciplinary area and help them develop their engineering and mathematical skills in line with the needs of academia and Canadian information technology industries.""409055,""Chen, KeHeng"
"402850"	"Childs, Andrew"	"Algorithms for quantum computers"	"The computers in use today are based on a notion of information rooted in classical physics.  They represent data using bits, each bit taking the value 0 or 1.  But quantum mechanics, the physical theory describing the behavior of atoms at a fundamental level, offers a more general notion of information.  In quantum mechanics, data can take on a superposition of its possible classical values.  To take advantage of this new kind of information, we can envision constructing a quantum mechanical computer, an information processing machine that operates according to the laws of quantum mechanics.     It turns out that quantum computers can quickly perform certain tasks that seem to be intractable for classical computers.  For example, there is an efficient quantum algorithm for decomposing an integer into its prime factors.  No such algorithm is known for classical computers, and indeed, many of the cryptosystems in use today (for example, to encrypt financial transactions carried out over the internet) are based on the widely-held assumption that this problem is hard.  Thus, quantum computers overturn established notions of what can be computed efficiently, with potentially dramatic implications for technology.  However, only a few examples of quantum speedup are known, and the nature of the computational advantage provided by quantum mechanics is currently poorly understood.     My research program is focused on better understanding the power of quantum computers.  Specifically, I work to develop new quantum algorithms that achieve dramatic speedup over classical  computation.  This research serves both to illuminate the differences between classical and quantum computers and to find quantum solutions for problems of practical importance, such as search problems and cryptographic tasks.  I am especially interested in developing novel algorithmic techniques for quantum computation, expanding the toolbox of methods that can be used to design and analyze new quantum algorithms.""396872,""Childs, Sarah"
"402615"	"Cowan, Glenn"	"Mixed-signal VLSI for variability tolerant, low-power, and high-performance data communication"	"Scaling of integrated circuit process technologies has enabled orders-of-magnitude increases in the processing capabilities of microprocessors, however their input/output (I/O) signalling bandwidths have increased at a slower rate. Hence, computational and signal processing systems are not reaching their full potential, because data communication cannot keep up with data processing, and because data communication dissipates an increasingly large fraction of a system's total power dissipation. To address this, we are conducting research in the areas of chip-to-chip and on-chip data communication.To minimize cost, communication circuits (which are usually analog in nature) are often forced to reside and operate on predominantly digital integrated circuits. This imposes additional challenges on the design of the former, namely, a low supply voltage; an electrically noisy power supply and interconnection environment; a process technology whose transistors are optimized for digital logic performance; transistor models whose parameters may be accurate for logic simulation but inaccurate for analog circuit simulation; transistors whose model parameters are estimates, rather than based on hardware measurements, until after a preliminary design must be completed; and transistors whose electrical performance may vary significantly from wafer to wafer, and from device to device.The goal of this research is to propose and validate mixed-signal integrated circuit/system design strategies that improve the performance and manufacturability of data communication circuits. ""410365,""Cowan, HenryJames"
"389647"	"Cowan, William"	"Using applied perception to improve the quality of computer generated images"	"To create images computer graphics solves simulations of light and matter subject to physical laws that are most often real, but may be imaginary. These simulations are inexact, because the goal is a picture, not a model of reality. Thus, the criterion for success is not exact correspondence with reality, but the response of the viewer. This response is both situation-dependent and subjective. Applied perception, my research area, measures human response to images, visual, auditory or haptic, to provide an objective basis for situation-dependent quality criteria for computer-generated imagery.In this project I am concentrating on unattended perception, how a viewer receives and uses information without needing to focus attention on it. Visual input that maintains posture or adjusts gait is an example. The goal is to measure unattended information, which is tricky, so as to assess the factors that make it effective. The result will be rules of thumb that programmers can use when developing rendering algorithms for parts of a scene that are expected not to be attended. It is important that such parts be rendered well-enough but not so well that they consume too many resources.My approach to unattended perception is empirical. I approach it through calm technology, using dual task methods to measure which aspects of computer-generated images are perceived when attention is saturated with another task.The other aspect of this project concerns approximate methods for lighting and rendering. I have identified a large class of techniques as being examples of a general underlying algorithmic strategy, which simplifies the simulation by separating it into parts. I regard this class of algorithms, from which come most approximation algorithms used for unattended parts of a scene, as ripe for analysis and formalization. It is also the logical complement to my interest in unattended perception.""394652,""CoweFalls, Lynne"
"388195"	"Cox, Philip"	"Logic-based visual languages for manipulating structures"	"In many interactions with computers, users accomplish tasks by manipulating visual representations of objects. Having noted that logic programming provides a simple but powerful mechanism for manipulating complex symbolic structures (unification), we are investigating ways to apply variants of this mechanism to structures with concrete visual representations. Accordingly, we are applying logic programming in two domains where manipulation of visual structures is of key importance: designing complex parametrised structured objects, and spreadsheet programming and design.We have proposed a logic-based visual language for structured design (LSD), which integrates concrete representations of design components with operations that specify how components are assembled, and have shown how the definition of ""design space"", on which LSD depends, naturally captures the notion of the behaviour of complex components; how design synthesis and assembly of structured objects can be integrated; and how domain-specific operations on components might be animated. Our long term goal is to develop a formal basis for visual design languages, sufficiently general to apply to a wide range of industrial design domains. This has the potential to make an important contribution to industrial design.Spreadsheets, in their current form, have limited programming facilities, minimal abstraction facilities, and no sound software engineering tools or methodology. As a result, despite the fact that they are used for critical applications, they are among the most error-prone of programming tools. We have shown that by defining a form of unification that applies to arrays, logic programming can be added to spreadsheets to provide facilities for programming abstractions in the sheet interface, and specifying spreadsheet structure. Our goal is a spreadsheet model which supports the ""formula-in-cell"" programming familiar to spreadsheet users, while providing tools to help users build robust and reliable spreadsheet applications.""396846,""Cox, Richard"
"390889"	"Daku, Brian"	"Localization of microseismic events in mines"	"Microseismic events in underground mines are highly localized, low intensity earthquakes that are produced by shifting rock that is under high stress due to the mining operations.  These events produce short-time duration mechanical waves that propagate outwards through the rock from the source.  Microseismic waves are recorded using data acquisition systems connected to sensor (geophone) arrays located at mine level (generally, 6, 12 or 16 geophones are used).  The recorded signals are processed to estimate the originating location of the event.  This research primarily focuses on improving the signal processing to obtain better location estimates.     In an underground mining environment, accurately locating microseismic events is an important problem for both mine planning and safety, since microseismic events are indicators of potential unstable areas.  This is especially important in soft rock mines such as potash, which are very large mines with significant planning and safety requirements.  For example, in Saskatchewan there are ten potash mines.  These are approximately one kilometer below the surface, and they are relatively large, up to 20 kilometers by 20 kilometers at maturity.     The objective of the proposed research is to investigate methods to improve the localization of microseismic events.  The research will focus on using theoretical statistical signal processing tools to develop novel approaches for better localization.  The research is directly applicable to the underground mining industry, which is a significant component of the Canadian economy and will benefit in improved safety and efficiency.""412073,""Dalai, Ajay"
"406150"	"Darmon, Henri"	"Stark-Heegner points and algebraic cycles"	"My research is concerned with rational solutions to  elliptic curve equations: cubic equations of the form y^2 = x^3+ ax+b,   in which  x and y  are the unknowns and a and b are rational  parameters. Elliptic curves are distinguished by the fact that they are equipped with a rich algebraic structure: a composition law that makes it possible to generate new solutions from previously given ones. This composition law is at the heart of the many practical applications of elliptic curves to coding and public-key cryptography.  Solving elliptic curve equations-even in specific instances-can be a daunting task.  For example, the  simplest nonzero solution to the equation  y^2 = x^3 + 877x  is given byx =612776083187947368101^2/78841535860683900210^2,                y=256256267988926809388776834045513089648669153204356603464786949/78841535860683900210^3. Devising effective and efficient methods to find solutions like this  is closely related to the Birch and Swinnerton-Dyer conjecture, one of the seven Millenium Prize Problems for which the Clay Mathematics Institute is offering a million dollar prize. The Birch and Swinnerton-Dyer conjecture makes a number of   concrete, and  often quite startling, predictions about the existence of  rational or algebraic points on elliptic curves. My overarching  goal  is to bring to light the mathematical structures that could explain these predictions. I achieved some success in this direction around 2000 when I discovered the notion of ``Stark-Heegner points"", which led  to explicit, but conjectural, numerical methods for producing rational points in terms of integrals of associated modular forms. Another turning point in my research came this year with the discovery of  a related construction of rational points on elliptic curves, using   algebraic cycles on higher-dimensional generalisations of modular curves (known as Shimura varieties). Exploring the ramifications of this construction and how it fits into the broader context of Stark-Heegner points will be a theme of my research in the coming years.""391797,""Darmon, Henri"
"395851"	"Denko, Mieso"	"Data management techniques, routing and mobility management in wireless mesh networks"	"Wireless Mesh Networks (WMNs) are emerging multi-hop wireless networks that can provide low cost, high speed Internet access for public and commercial applications.  WMNs may be deployed as last-mile network access in many application areas such as community networks, intelligent transportation systems, municipality networks, public safety communications and surveillance systems. Despite research advances in the area in last few years and the network deployments, several research challenges still exist. Improving the network performance, efficient and fair use of the available resources requires further theoretical research and experimental studies. So far, research work focused mainly on the design of routing and Media Access Control protocols for pure ad hoc or infrastructure-based WMNs using strict layering approaches. Cross-layer design and optimization, intelligent techniques for data management, routing and mobility management schemes have not received much attention. In the past four years, my research team has been working on various topics in mobile and wireless multi-hop networks with focus on data management, mobility management, routing schemes and design of protocols for the integration of the Internet with mobile ad hoc networks. Our previous work has built a foundation for the work proposed in this research. The general objective of the current research work is to develop and evaluate data management techniques, routing and mobility management architectures, protocols and algorithms for infrastructure and hybrid WMNs.    The three topics data management, routing and mobility management are basically related.  Data management protocols and algorithms can exploit routing and mobility management architectures and protocols for their efficiency and effectiveness.  Furthermore, with cross-layer design and optimizations approach, lower and upper layer protocol stack information can be exchanged to optimize the overall network performance. These topics will be studied using analytical models, simulation techniques, implementation test-beds and statistical analysis. ""395584,""Denman, Kenneth"
"395353"	"Deschênes, François"	"Exploitation des effets visuels résultant de phénomènes physiques impliqués dans la formation d'images: applications à la surveillance et à la sécurité"	"Les caractéristiques visuelles disponibles dans une image dépendent, entre autres, du système de formation d'images (géométrie, optique, radiométrie, traitement du signal, etc), des conditions d'éclairage, des caractéristiques des objets de la scène (réflectance, forme, etc.) et du milieu dans lequel transite la lumière (type de milieu, opacité, particules en suspension, densité, etc.).  Ce programme de recherche s'intéresse à l'exploitation de telles caractéristiques afin d'exploiter adéquatement le contenu des images.  Plus spécifiquement, nous nous intéressons à l'influence de la propagation de la lumière à travers un ou des milieux donnés (eau, air, etc.) sur la formation d'images, c'est-à-dire à la prise en compte des phénomènes physiques généralement négligés et qui sont en relation avec la présence de particules en suspension (brouillard, gouttelettes, poussières, etc.) ou encore de milieux hétérogènes.  La présence de ces derniers dans le parcours rayons lumineux a des effets directs sur l'image résultante : variations d'intensité, augmentation/diminutation des contrastes, impression de déformation des objets, etc.  Une meilleure compréhension de ces phénomènes et de leurs effets sur les images ainsi qu'une modélisation plus adéquate nous permettront d'introduire de nouvelles techniques d'analyse d'images et de vision artificielle pour 1) détection automatique et à faible coût de fuites dans des conduites de gaz de taille variable, 2) assistance à la conduite automobile dans des conditions climatiques difficiles (pluie, brouillard, smog, etc.) et 3) identification automatique des zones d'une image ayant subi de modifications (e.g. incrustation/retrait d'un objet), ce qui est par exemple le cas des images falsifiées.  De nouvelles technologies pourront ainsi découler des recherches proposées.  Ces technologies trouveront application notamment dans les secteurs de la surveillance, de la sécurité et des transports.""408490,""Deschênes, JeanDaniel"
"402566"	"Després, Philippe"	"Streaming architectures for computation in medical radiation physics"	"Radiation therapy historically has evolved in close symbiosis with the development of faster and more powerful computers;  advanced delivery techniques such as intensity-modulated radiation therapy (IMRT) would not be possible without today's microprocessors.  The quest for better radiation delivery techniques continues to this day, and commands increasingly more powerful machines to handle the calculations required by complex treatment planning algorithms. In order to keep up with the processing requirements of modern algorithms, treatment planning systems (TPS) must sometimes resort to clusters of computers to provide clinical output in reasonable time.  This architecture, although efficient, requires substantial investments in time and expertise, and can hinder the deployment of better TPS for economical and logistical reasons.  It therefore appears that an economical and powerful, yet standalone calculation platform is highly desirable in radiation therapy, and in medical physics in general.  We propose here to develop such a platform, using innovative hardware material dedicated to massively parallel calculations such as Graphics Processing Units (GPUs). GPUs and similar stream processors such as the Cell Broadband Engine (CBE) have already been used successfully for general-purpose calculations in many scientific fields, achieving speed improvement factors of up to two orders of magnitude over traditional implementations on Central Processing Units (CPUs). We propose to implement medical physics algorithms on stream processors such as GPUs in order to significantly accelerate tasks such as image processing, optimization and dose calculations. This, in turn, will allow the integration of more complex, more accurate and more personalized algorithms in treatment planning systems. The development of such algorithms on stream processors will benefit not only the field of medical radiation physics, but any discipline where high-performance computing is desirable.""391392,""Desrochers, Alain"
"396576"	"DeSterck, Hans"	"Multilevel methods for scientific computing"	"This project develops new computer algorithms of 'divide-and-conquer'-type for two important scientific computing applications. The algorithms are made efficient by exploiting representations of the problems on multiple levels with successively coarser resolution.The first problem deals with the organization and retrieval of large amounts of information, for instance the importance ranking of web pages for search engine queries. Efficient multilevel methods will be derived for so-called Markov chain problems, which provide a model for importance ranking.The second problem deals with the simulation of highly nonlinear systems of equations, for example models of complex fluid flow around airplanes or in planetary atmospheres. Efficient multilevel methods will be developed for these nonlinear systems, by explicitly isolating the nonlinear singularities from the smooth part of the solution.The multilevel algorithms to be developed proceed in an iterative way by improving an initial guess of the problem solution over successive cycles. The improvements in each cycle are made efficient by reducing error components with different scales on appropriate progressively coarser levels in a recursive way. This approach can lead to a speedup of several orders of magnitude, compared with more conventional methods. The dramatically improved algorithmic efficiency of these multilevel methods will contribute to advancing technological capabilities in our increasingly information-based economy, and will help to extend the boundaries of our scientific understanding, for example in the field of extrasolar planetary atmospheres.""397651,""Desveaux, Darrell"
"397683"	"Dobre, Octavia"	"Emerging cognitive and intelligent radio systems"	"In a world of rapid growth of commercial wireless services, accommodating the explosive demand for spectrum access, efficiency and reliability becomes increasingly technically challenging. Furthermore, implementation of advanced information services for military applications in a crowded electromagnetic spectrum is a challenging task for communication engineers. A solution is provided by flexible cognitive and intelligent radios, capable of sensing and adapting to the environment. With software defined radio (SDR) products today, cognitive and intelligent radio is just the next step, and not a leap, in radio development.The objective of this research program is to develop a conceptual framework and a set of efficient algorithms for cognitive and intelligent radio systems, with both commercial and military applications. Dynamic spectrum sensing, opportunity sharing, radio access with high spectral efficiency, and blind signal recognition, which includes blind separation of multiple received signals, blind parameter estimation and equalization, and blind modulation classification, are topics proposed to be investigated. This work is crucial to cope with the scarce electromagnetic spectrum resource in commercial and military radio communications, and vital for decision involving electronic warfare in military operations. Results of this research will facilitate the understanding of methodologies and techniques which can be used to solve challenging open research problems pertaining to the cognitive and intelligent radio systems. The suite of algorithms devised in this research will serve as an engine for SDR to be used in such systems. The proposed research falls in the area of intelligent systems technologies for advanced telecommunications and security sectors, which, according to the federal Department of Industry, represents a priority direction of R&D investment in Canada.""406055,""Dobson, Jessica"
"396725"	"Domaratzki, Michael"	"Tools and formal models for biological research"	"Bioinformatics is the study of computational techniques for solving problems from biology. With vast amounts of biological sequence data now available for DNA, RNA and proteins, the need for fast computational tools is crucial.  There is a substantial research drive to develop new tools which aid in prediction of the function of molecules and construction of evolutionary relationships and histories, for example.Many of the projects to develop new computational tools for bioinformatics problems are developed with the sole vision of improving the state of the art of biological software. However, many innovations have shown that tools applicable to areas including programming language design, natural language processing and natural language translation are applicable to bioinformatics as well.This program of research will examine novel solutions to problems in bioinformatics by using existing tools from computer science. Using the theory of formal languages, where sequences of symbols are viewed as words in an abstract language, we will investigate problems from bioinformatics, including the prediction of RNA structure and the modelling of the rearrangement of DNA in single-celled organisms called ciliates.  By reusing existing tools in computer science for biological research purposes, this research program will help view new and established areas in bioinformatics as asking questions about similar problems and having similar solutions.""410514,""Dombroski, Jocelyn"
"396749"	"Eavis, Todd"	"Scalable methods for data warehousing and knowledge discovery"	"Over the past fifteen to twenty years, data warehouses have evolved from haphazard and often poorly understood repositories of operational information to become one of the cornerstones of corporate IT architectures. Not surprisingly, academic researchers have shown great interest in fundamental DW issues, as well as those related to the complementary technology of Online Analytical Processing (OLAP). One over-riding concern with the design and implementation of contemporary warehouses is the sheer scale of the underlying databases, the largest of which now regularly exceed 100 Terabytes. Given the astounding computational resources required to support direct, interactive analysis within these environments, there is clearly a pronounced need for innovative research targeting the architectures, algorithms, design models, and data structures in enterprise DW/OLAP spaces.  In the current program, we propose an algorithmic and architectural framework for the comprehensive parallelization of core data warehousing functionality. Specifically, we will explore new methods for scalable multi-dimensional caching and real time OLAP analysis, as well as the design of a more expressive and powerful OLAP query language. More importantly, these techniques - along with our previous methods for high performance OLAP processing - will be integrated into a fully parallelized OLAP server. In turn, this multi-CPU cluster will form the backbone of a grid architecture that will virtualize high performance computing resources across geographically distributed locations. By exploiting existing standards and technologies, including commodity DBMS platforms, embedded database systems, and grid development toolkits, our research group will be able to evaluate fundamental algorithmic research within realistic multi-user, multi-database, multi-Terabyte settings.  The end result should be a robust, flexible infrastructure that is uniquely suited to the massive analytical requirements on the horizon.""410698,""Ebadi, Diba"
"396050"	"Elbiaze, Halima"	"Architectures and algorithms for service oriented optical networks"	"Several emerging new applications and services, especially the ones based on massive data transmission, require capabilities non-offered by traditional Grid infrastructure. The later usually exploits packet-routed networks as non-deterministic resources. The constant evolution of optical network technologies combined with the dynamic provisioning mechanisms holds the promise to support these types of advanced applications and services. Nevertheless, the huge bandwidth offered by optical networks is still inefficiently exploited and needs appropriate killer applications. The global objective of my research program is to investigate new networking concepts and to propose innovative solutions in the context of service-oriented optical networks. I will study new architecture and protocols for the next generation of optical networks to support network and service virtualization. The emerging large scale applications require what have been referred to as ""deterministic"" network services that imply defined and guaranteed service-level parameters (e.g., minimum bandwidth, loss rate, latency and jitter). In addition, those services need to be provisioned across multi-domain heterogeneous networks. In order to create an optical network supporting virtualized sub-networks with different QoS-guaranteed end-to-end virtual links, I will first investigate a novel optical control plane that takes into account traffic dynamics. The aim is to provide a new intelligent control plane strategy with dynamic multi-granularity connections. The routing and bandwidth allocation for those connections will be performed under constraints: QoS guarantees interaction, computing resources and network state information availability. My new control plane will dynamically switch between high-level signaling for long-lived applications and low-level approach for finer applications granularity. My research project proposed in this application investigate three main tracks: 1) low-level signaling such as OBS (Optical Burst Switching) signaling 2) high-level signaling such as UCLP (User Controlled LightPath) and 3) multi-granularity resources optimization in service oriented optical networks to support network virtualization.""401817,""ElBoussaidi, Ghizlane"
"403747"	"ElSaddik, Abdulmotaleb"	"Infrastructure for collaborative hapto-visual environments"	"This proposal is a joint application of the MCRLab (Multimedia Communications Research Laboratory) the VIVA Research laboratory and the DISCOVER Lab (Distributed & Collaborative Virtual Environments Research Laboratory) of the University of Ottawa. It aims at enhancing and establishing an infrastructure of research for haptic audio-visual environments. Content creation for advanced simulations using hapto-audio-visual environments (HAVE) is a major challenge. Originally, HAVE have been purely hand-crafted with mathematical and graphical tools but recently data driven approaches have gained importance. The capture of highly detailed visual appearance models of everyday objects and subjects from the real world has developed into a major research area in computer graphics. The DISCOVER laboratory focuses on combining various research areas such as model acquisition, model display and manipulation and communication of these models in advanced applications. In order to strengthen the acquisition capabilities of DISCOVER, a facility to capture dynamic models of humans, objects and events is urgently needed. The facility would allow the capture of human dynamic motion data but also the acquisition of appearance models of humans including face, hand, and body motion, as well as, skin deformation. Beside human motion capture, the facility would also be suitable to model highly dynamic events, e.g., a dynamic model of a ball as it bounces, of water spray as an object enters it and even the rupture of a balloon. These research avenues are linked with applications in human motion analysis in sports and medicine, 3D computer animation, model acquisition for computer games, scientific applications and educational exhibits. The facility will also be used to capture image based representations of large complex real-world environments. New haptic devices and motion platforms are needed for advanced applications in medical training, health-oriented simulations and emergency training simulations.""394397,""ElSaddik, Abdulmotaleb"
"402827"	"Farzad, Babak"	"Algorithmic game theory and graph theory"	"Fundamental problems in computer science revolve around the discovery of efficient algorithms. In the past 5-6 years, computer scientists have witnessed an explosive growth of interest in algorithmic game theory which deals with models where there are no trusted coordinator and the outcome depends on the cooperation or competition by parties of different interests. One aspect of my research will focus on network routing and its generalizations as a non-cooperative game - a line of research in algorithmic game theory. As an example, this work includes a precise analysis of the extension of the classical selfish routing model to the model in which we incorporate the concept of ""time"" (when all users of a link do not experience the same latency).      At the same time, the widespread use of search engines on the Web and its ramifications have raised a variety of important algorithmic issues. In a second aspect of my research, I will consider those issues that concern the sponsored search advertisements. Finding optimum bidding strategies for advertisers, studying the convergence of such strategies (as both the search engine and the advertiser seek for a stable market) and bringing the concept of ""externality"" (when advertisers care who are the other advertisers on the page) to the existing models are parts of this work. The goods that are being sold in search ads auctions (the search queries) have two interesting characteristics: (i) they should be allocated to the buyers in a fraction of a second or they will perish instantly; and (ii) their total supply is unknown. I will examine the on-line nature of these auctions and in particular, focus on finding optimum ""reserve prices"" for keywords.      Designing fast algorithms or proving that no fast algorithm exists intimately relies, in many cases, on geometrical and graph theoretic characteristics of the problem. I also intend to work on some of the important problems in graph colouring - a traditionally important area of graph theory for computer scientists. Structure of colour-critical graphs, improper colourings and list-colouring of certain classes of planar graphs will be examined.""388976,""Farzaneh, Masoud"
"394987"	"Fayek, Dalia"	"Video traffic engineering for IPTV services"	"Video traffic is expected to occupy a significant portion of multimedia data in the future. This motivates the research in the delivery of broadcast TV over IP networks (IPTV). Video compression techniques are thus used to reduce the uncompressed raw video streams (~25 Mbps) to compressed streams (~4Mbps for MPEG-2 and less than 2Mbps for MPEG-4 codecs suite) to enable the multiplexing of several streams over current telecommunication infrastructure. Intuitively, this endeavour requires a strong synergy between the two research communities: the video compression and coding community and the networking community. This research aims to increase the dialogue between the two by building on the current literature and proposing (i) new video trace formats and (ii) new rate-control protocols for mass-delivery of video IP packets. The key challenge posed by the transmission of video traffic over IP networks is its high bit-rate variability and sensitivity to jitter. Unlike voice and data VBR models, characterizing the statistical behaviour of video traffic has been mostly based so far on empirical analysis and/or mathematical modeling that assume no packet losses in the transport network. The difficulty lies in the fact that video bit-rate variation depends on both (1) media content (e.g., amount of motion) and (2) compression technique used. Complications also stem from the lack of adequate mapping between transport network Quality of Service (QoS) parameters (packet-losses or bandwidth constraints) and decoded video quality as perceived by the end-user termed ""Quality of Experience"" (QoE).  The first part involves studying statistical behaviour of MPEG-4/H.264/AVC generated video streams in terms of marginal distribution and autocorrelation to derive mathematical models that capture the codecs' statistics. The second step is to experiment with network protocols that will tie application-level (QoE) to network-level metrics (QoS). Rate-control (RC) protocols can exploit this additional information to provide differentiated treatment to video flows based on network conditions.""400369,""Fayek, Mostafa"
"395967"	"Fernandez, JoseManuel"	"Performance analysis in computer security: theoretical and experimental methods"	"The main objective of this proposal is the improvement of our understanding of measures of performance in computer security through the study of the quantitative trade-offs between system and defensive performance, attack performance and characteristics of the environment in which they operate.On the one side, we hope that a better understanding of the trade-offs between defensive performance and environment conditions for particular attacks, will help us discover which parameters of the environment (partially under our control) can be optimised to reduce the effectiveness of certain types of attack.  Such adjustments could include changes in network or server configuration, network protocols, traffic shaping, etc.On the other, we hope that by studying the mutual influence between defensive and attack performance criteria, we will be able to predict what the ultimate ""equilibrium"" conditions are where both defences and attacks are optimal.  This provides a description of ""worst case scenario"" against for which we have to construct defences.This research programme involves both theoretical work involving mathematical modelling using stochastic simulation techniques, and experimental work involving experiments in controlled laboratory conditions, and validation using training and test data sets obtained from real networks.""392543,""Fernandez, Rachel"
"403218"	"Fischmeister, Sebastian"	"Model-assisted debugging of embedded systems"	"Software testing and debugging take up between 30 and 50% of the development cost in embedded systems. Despite this large percentage and the associated enormous costs, only little attention has been devoted to debugging of embedded real-time systems. Ad-hoc methods such as blinking lights to indicate errors and morsing error codes via beepers still dominate microcontroller-based system development.The proposed project will research methods and tools that provide a well-structured approach that assists the developer with tools and analysis methods during the debugging process. We will specifically study the following problems: detecting probe effects (changes in defect behavior because of program instrumentation), estimating system perturbation (changes in program behavior because of instrumentation), and interactive debugging. Other problems such as test generation, coverage, and program verification are outside the scope of this project.We will use program analysis, profiling, and developer specifications to create a holistic resource-consumption model of a given program. This model will allow us to implant debugging code at location which cause low system perturbation and possibly no probe effects. Using program transformation, we will change the program to increase the number and quality of suitable locations. Automata-based interfaces for peripherals will allow us to create interactive debugging of devices with physical sensor and actuators. The success will be measured by comparing the quality of our automatic instrumentation of example programs with instrumentation done by expert developers.The results of this programme will decrease the product development cost of embedded software. Canadian companies can use these savings to improve the software quality or add more features. Furthermore, reduced development costs lower the barrier of entry for start ups and will create additional jobs around growing industries such as automotive, medical devices, avionics, and consumer electronics.""407163,""Fiset, Dominic"
"391262"	"Freundorfer, Alois"	"Towards 3D high speed circuits"	"Information technology and consumer products such as cellphones, computers, video recorders, automotive parts etc. rely heavily on microsystems technology (i.e. microelectronics, photonics, wireless circuits and sensors). Future products will desire systems that are more compact, versatile, functional and less expensive.  Higher operating frequencies and wider bandwidths therefore become necessary. To achieve a higher level of integration, we will focus on 1) high speed electronics using advanced low temperature ceramics and 2) high gain beam-scanning planar antennae.     We have developed a low temperature hydrothermal ceramic process for depositing thin film, thick film and 3-D ceramics onto integrated circuits and printed circuit boards. Most ceramics have to be made at around 1000 degrees or more. This destroys the integrated circuit. We can do it at 150 degrees. No one else has achieved this. This will impact tunable electronics, miniature electronic resonators and filters with the development of appropriate masking methods. We will be the first to integrate 3-D ceramics onto integrated circuits. This will reduce product size and cost. We would also investigate new ceramic materials in our low temperature process.     High gain beam-scanning planar antennae are important in many applications such as automotive radar, satellite communications, room penetrating security/surveillance radars, and non-contact medical diagnostics. We have developed a new high gain antennae which operates at a single frequency whose  beam is steerable. Currently, the beam can only be scanned over a maximum  angle of 14 degrees. Our research aims to increase the scanning angle. Since the antennae is planar, it can be easily fabricated on a printed circuit board and  integrated in a product, thereby reducing costs.     These two areas of research will assist the Canadian industry in producing and manufacturing smaller, more versatile and less expensive products.  This will provide a substantial competitive advantage globally in automotive radar, millimeter wireless LAN and medical diagnostics equipment.""393311,""Frey, Brendan"
"396666"	"Friedlander, Michael"	"Large-scale numerical optimization"	"Optimization problems arise in a wide range of science and engineering disciplines.  The goal is to minimize (or maximize) an objective function while satisfying certain constraints.  Applications often give rise to optimization problems with significant structure that can be exploited to efficiently solve problems with thousands--sometimes millions--of variables.This proposal describes a five-year research program to develop innovative optimization algorithms for solving large-scale problems of practical interest.  I will address issues in algorithm development, analysis, and implementation. The research plan involves the following three main components: development of general-purpose algorithms for nonlinear optimization, development of specialized optimization methods for particular applications, and algorithm implementation.In many applications, optimization is a vital step in a larger procedure; it is therefore critical that optimization algorithms be fast and reliable.  Advances in efficiency and robustness are welcomed by optimization users in industry and academia.  I anticipate that the proposed research will have impact on three groups: researchers in numerical optimization, researchers in specific application fields, and industry practitioners.  The first group will benefit from the extension of current optimization theory and from general algorithm development.  Special-purpose algorithms and the development and release of open-source optimization software will make their impact on the latter two groups.""389709,""Friedman, Alinda"
"402849"	"Fung, Benjamin"	"Privacy-preserving RFID systems for data analysis"	"Radio Frequency IDentification (RFID) is a technology of automatic object identification. A typical RFID system consists of multiple tags and readers, and an infrastructure for handling the high volume of data. The tag is a small device that can be attached to an object, such as a person or a product, for the purpose of unique identification. The reader is an instrument, positioned in a strategic location, that communicates with the RFID tag. It broadcasts a radio signal to the tag, which then transmits its information back to the reader. Streams of RFID data are then stored in a database for analysis. Although RFID would be useful in a broad spectrum of applications, such as supply chain management, e-passports, and patient care and management, the uniquely identifiable tags pose a threat to privacy by tracking a person's movements or profiling an individual. Some privacy advocates have even labeled RFID as the ""worst thing that ever happened to consumer privacy."" Clearly, the privacy concern is an obstacle to the acceptance of RFID technology.     This research program overcomes the privacy obstacle in RFID information systems and develops a series of anonymization methods to transform the underlying RFID data into a version that hides sensitive information, yet keeps it useful for data analysis. RFID data poses new challenges to traditional database, data mining, and privacy-preserving technologies due to its unique characteristics: it is time-dependent, location-dependent, and is generated in large volumes of stream data. Previous works in privacy-preserving RFID technology mainly focused on the hardware and communication protocol layers instead of the database layer, in which a large amount of data actually resides.     The research results will contribute to the core data mining and privacy preservation domains, as well as the industries that employ RFID technology, sensor networks, and location-based services. The proposed research will result in a secure environment for employing RFID technology without sacrificing its information usefulness.""409233,""Fung, Carlen"
"393292"	"Gardner, William"	"Formal model-driven synthesis for concurrent systems"	"Concurrent systems present special design challenges due to their complex interactions, both with their environment, as in the case of reactive real-time systems, and internally in terms of synchronization and communication among their components. Such systems are encountered in diverse contexts -- single computers, distributed systems, or embedded systems with hardware and software components -- and some are safety-critical, that is, failures can endanger human life. Conventional design techniques are unable to insure that these systems will perform as expected when stimulated by inputs, and that they will not deadlock.     Formal methods, which involve writing the specifications for a system's behaviour using a mathematically rigourous notation, have been advocated as a way to verify system properties with the help of model-checking programs. However, industry practitioners have not been eager to adopt abstruse mathematical notations, uncommon programming languages, or additional costly engineering process steps. These approaches are also of limited value when the formal specifications have to be translated by hand into a common programming language, since the verified properties may be lost in the conversion.     This research makes the use of the algebraic formal method known as CSP, or Communicating Sequential Processes, more practical. System specifications written in CSP notation can be verified by means of a commercial model-checking program, then, using our design automation software, automatically translated into C++ and compiled into a running program. Additional components written in C++ can be ""plugged into"" the system's CSP control structure. The designer selectively applies the CSP formalism to critical portions of the system, leaving the non-critical portions to be written in C++, a popular programming language, by programmers without special training. This approach, called CSP++, will make it easier to design and implement reliable concurrent systems. We are extending CSP++ to allow controlling of digital hardware components, and to give the option of formally developing selected software components using B-Method.""402659,""GardnerCosta, Jesse"
"402823"	"Gasevic, Dragan"	"Towards semantic web-enhanced model-driven engineering"	"Today, we need a wider variety of software solutions than ever before. It follows then that we need novel methods that will allow for rapid development of software systems in order to be ready for any future technological challenges. This situation motivated researchers to establish the model-driven engineering (MDE) discipline to achieve three goals: increasing the productivity of software developers by focusing on domain-specific (non-technical) details; reusing the same design for several platforms; and allowing for early discovery of software errors. However, software is a knowledge repository where knowledge is largely related to the application domain, and not to software as an entity itself. So, we need to be able to share (trace, manage, interoperate) the knowledge encoded in software with the knowledge about all relevant aspects surrounding and influencing software (e.g., requirements and policies). With today's critical role of the Web and Web-based software systems, the need to be able to share the knowledge based on which a system is build is even higher. This knowledge sharing and management requires the use of an explicit definition of knowledge, as that is a basic need for machines to be able to interpret knowledge. It is natural then why the Semantic Web technology is a promising way to address these current problems.      This research explores the integration of the Semantic Web technology into software engineering by using MDE, with the final result producing synergetic effects for both technologies. These efforts can be in general classified into three general categories: (i) Integration of Semantic Web languages into the MDE theory; (ii) Model-driven engineering of Semantic Web languages; (iii) Semantic Web-enhanced development of domain-specific languages (for service-oriented architectures and e-learning). The major contribution is the semantic enhancement of the MDE process which should increase its efficiency and effectiveness, and assure that developed systems can precisely integrate the knowledge of domains under study. Moreover, the research should provide tools for developing software systems, leveraging benefits of the Semantic Web.""413753,""Gasevic, Dragan"
"402819"	"GhaderiDehkordi, Majid"	"Cross-layer optimization in wireless data networks"	"Data networks such as the Internet have traditionally been designed in a layered fashion where each layer communicates only with the layers directly above and below it through predefined interfaces. A layered architecture has also been implemented for wireless networks mainly because of inter-operability issues with legacy wireline networks. However, it is well known that such a layered design may not be appropriate for wireless networks due to the specific characteristics of wireless medium, i.e. high error rate and large rate variability, which can negatively impact higher layer network protocols. In particular, it has been suggested that some interactions across different protocol layers can significantly improve the performance of wireless systems.The objective of this research is cross-layer optimization of modern wireless data networks. Specifically, the goal is to improve TCP throughput by exploiting the information available at the protocol layers above and below TCP. The reason for focusing on TCP is that the majority of network traffic in both wireline and wireless networks is carried by TCP, and it is expected that TCP will remain the de facto transport protocol for a wide variety of applications in the future. Therefore, it is important both to understand its behavior, and to improve its performance in emerging wireless systems in order to provide better and more cost-effective wireless data services for end users.To achieve this goal, the following objectives are set for this program:(1) Characterizing TCP performance in modern wireless networks,(2) Developing practical mechanisms to address TCP performance problems in such networks,(3) Implementing the developed mechanisms to assess their performance experimentally.""408542,""Ghadie, Mohamed"
"391755"	"Giesbrecht, Mark"	"Sparsity, complexity and practicality in symbolic mathematical computation"	"This proposal describes a comprehensive program of research into the design, analysis and implementation of algorithms for foundational problems in symbolic mathematical computation.  These are the algorithms that make commercial mathematical software like Maple and Mathematica run.  They allow us to manipulate and solve large and complex sets of equations exactly, even when some quantities are unknown and left as variables.        My research will centre on three specific areas: solving sparse linear equations with exact rational numbers as entries, algorithms for approximate or ""noisy"" polynomials, and computing with ""lacunary"" polynomials (polynomials with high degree but only a few non-zero terms).  The common thread of the described projects is a focus on exploiting sparsity, the existence of lots of zeros or ""holes"".  In other words, if a very large problem we wish to solve has a very succinct description, we want to use this to make our algorithms run more quickly.        As an example, a system of thousands of simultaneous equations, where each equation involves only three variables, should be solvable more quickly than a system where each equation involves thousands of variables.  Our algorithms and software will allow us to treat the enormous and sparse equations that now challenge us in symbolic mathematical computation and its applications.  I will also consider ""approximate"" equations, where some of the data involved may be imprecise or noisy (say from experimental data).  I will show how to reconstruct equations to model such data which are of small size, thus making subsequent operations faster.  Finally, I will explore the exciting theoretical world of ""super sparse"" or lacunary polynomials. Computing with these pushes the limits of algorithmic science, but also may lead to useful methods in practice.        The algorithmic advances will be implemented in the symbolic linear algebra software library LinBox and in the computer algebra system Maple.  Both provide an avenue for broad dissemination of our methods.""400245,""Gieselman, Tanis"
"395372"	"Glaesser, Uwe"	"High level design of distributed embedded and mobile systems"	"My overall research objective is the development of a comprehensive methodological framework for computational and mathematical modeling of complex distributed systems at high levels of abstraction as required for the early software development phases to ensure that the key system attributes are properly established and well understood prior to actually building the system. Focusing on semantic rather than syntactic aspects, the identification and elimination of design flaws and undesirable side effects requires precise models of the expected system behavior and the use of such models for analyzing functional system properties, test case generation, and runtime verification. This research proposal focuses on formal aspects of software technology, industrial applications of formal methods, computational modeling and validation of public safety and security systems and spatiotemporal characteristics of crime patterns. Building on solid theoretical foundations, the emphasis is on applications of knowledge and skills to solve practical problems in a range of diverse application domains. A common challenge in all of the research activities is the need for bridging the gap between formal and empirical approaches. Addressing practical needs requires novel ideas for combining mathematical concepts and formal methods with computational techniques that work in practice. Exploiting the experience from extensive applications of the Abstract State Machine (ASM) method to the specification and validation of distributed and embedded systems, the proposed approach strives for a combination and systematic integration of the ASM modeling paradigm with other paradigms for analysis, modeling, validation, and verification of behavioral aspects of complex concurrent and reactive systems in real-life application contexts.""404993,""Glanfield, Joel"
"392744"	"Grahne, Gosta"	"Multifaceted data management on the web"	"The phenomenal success of database technology is based on the use of high level languages (such as SQL) and on the ability of the query processor to perform optimization tasks, i.e.  to render the query into a form that can be efficiently executed. The state-of-the-art technology works well in the traditional client-server relational or object-oriented setting. However, with the emergence of the {em World Wide Web}, as well as with the proliferation of electronically stored data, the client-server architecture as well as the relational or object-oriented approach are changing. The architecture is moving towards data sourc (databases, file-servers, web-pages, XML-document) interconnected through various mediators, integrators, peer-to-peer connections, and the data model is moving towards semi-structured data and XML. The new environment poses the multifaceted challenges of integrating data from different sources, while mapping schemas and data, reconciliating conflicting data, handling incomplete information and uncertainty, tracing the source of the data, and assigning reliability to it. Although each of these aspects have been studied in isolation before, their interaction poses some hard problems to be solved in order to guarantee semantically correct query answers and efficient query processing.  ""399936,""Grainger, Sue"
"403683"	"Grant, Kevin"	"Developing runtime structures for Bayesian network inference"	"Bayesian networks are a means of exploiting conditional independence information in joint probability distributions to facilitate both a compact representation of a joint distribution, and faster inferences from that distribution, without sacrificing the integrity of the original information.  This is crucial for probabilistic decision-making in settings where variables interact in complex ways, such as medical decision making. It also facilitates and improves the accuracy of classification, forecasting, and fault diagnosis. Any software that improves the efficiency of decision-making, without sacrificing accuracy, makes deployment on consumer hardware more feasible.  One of the major criticisms of using Bayesian networks in application is that their computational demands are often very high, both in terms of runtime and memory.  In our proposed research program, we intend to develop streamlined data structures for computing over Bayesian networks at runtime.  These data structures will be constructed according to application constraints on resources such as time and memory.Finding the optimal runtime structure for a particular set of resource constraints is a problem in combinatorial optimization, with a very large search space.   Evaluating the results of our optimizations will often require an exhaustive search over all possible representations, which is very time and space expensive.  To assist us in this research, we are proposing the purchase of a high-performance computer that will allow us to evaluate our optimization methods as efficiently as possible.  ""402841,""Grant, Kevin"
"394707"	"Guergachi, Aziz"	"Knowledge engineering principles for the effective management of complex systems"	"This research focuses on investigating advanced machine learning techniques with applications to engineering and business management. In the engineering area, we look at the analysis of the risk of failure of drinking water facilities. Contrary to what one may think, waterborne disease outbreaks in developed countries seem to continue to occur on a relatively regular basis. The reason for this phenomenon is not the lack of adequate treatment technologies. Rather it is the cumulative impact of a sequence of small individual events, each of which is relatively harmless on its own, that leads to the outbreak - hence the systemic nature of the causes of the failures, and the relevance of machine learning techniques such as kernels and pattern recognition algorithms (for the classification of a sequence of events as 'collapse-oriented' or 'survival-oriented'). In the business management area, the focus is on the topic of sales forecasting and management, which has a direct connection with the issue of job losses in the manufacturing sector in Ontario. This type of job losses is a direct consequence of the drop in manufacturing sales, which is itself due to the appreciation of the Canadian dollar against the United States dollar. Such an appreciation leads indeed to the odd situation where Canadian products suddenly become more expensive for American customers, while the value-adding features of these very same products have not changed at all - hence the drop in sales. Laying-off workers has so far been the quickest way, for manufacturers, to react to this drop in sales. There is, however, no evidence that the laying-off of workers is the optimal control action (in the control-theoretic sense of the term) to react to currency appreciation. The applicant and his co-workers developed a quantified system to help salespeople and managers analyze their interactions with customers, develop sales forecasts and design adequate action plans for the future. In this project, we intend to investigate both of these applications using the state-of-the-art techniques for machine learning, knowledge representation and pattern analysis.""399471,""Guérin, Brigitte"
"393608"	"Gulliver, Aaron"	"Wireless multimedia communications"	"People throughout the world are inherently and fundamentally mobile, they expect to be connected wherever they are. It is therefore not surprising that the wired world is rapidly becoming wireless. In addition, wired technology has advanced to the point where people expect very high speed, reliable real-time services in the home and elsewhere. It is natural that mobile users demand similar service levels. With this wireless revolution come many challenges to providing a positive user experience. Future wireless communications systems must be based on low cost, intelligent portable devices that can operate in a variety of hostile environments. The objectives of this research program are to develop innovative techniques for robust, low complexity, wireless multimedia communications systems. The goal is to produce future systems which can deliver high data rates; be efficient in both power and bandwidth; provide access anytime over a wide service area with multiple levels of reliable quality of service (QoS); and deliver diversified services with ease of use.This research program will focus on several key enabling technologies which will accelerate the achievement of the above goals. Orthogonal Frequency Division Multiplexing (OFDM) and Ultra-WideBand (UWB) will be considered. These modulation techniques will dominate future wireless systems due to their flexibility, simplicity and robustness to fading. Optical wireless is a very promising high bandwidth, low interference alternative to RF communications. Future system must adapt to their environments, therefore low complexity adaptive techniques will be considered within the framework of resource management, cross-layer design and QoS. To achieve the goal of a consistent user experience, applications will have to be more efficient, and more intelligent. Networks that can adapt to the wireless environment are essential.  Thus this project will consider the design of systems that can dynamically adjust resources according to channel and traffic conditions. One major application for this work is the delivery of IPTV/Video on Demand (VOD), voice and data to the home, the so-called triple-play of services.""413464,""Gulliver, Aaron"
"395061"	"Gunupudi, Pavan"	"Addressing simulation of electrical and optoelectronic circuits considering multidisciplinary issues"	"Growing markets for personal digital assistants, smart phones and personal computers in our present day information society has increased the need for greater processing power while maintaining miniature device form factors. This has placed an enormous burden on the design community to design faster circuits with better performance in a smaller area. In addition, owing to miniature feature sizes and massive integration, effects pertaining to distinct physics disciplines have started to surface together in modern devices. Commercial design tools built on traditional simulation engines are mostly restricted to analysis in a single discipline and are incapable of capturing the multi-disciplinary nature of these effects. Availability of reliable tools that facilitate multi-disciplinary simulation of devices and systems is essential in future design shops and can aid the design community to produce products with better quality in a cost-effective manner.The research objectives of this application are focused on generic strategic issues that address the multi-disciplinary aspect of design automation tools. The proposed multi-disciplinary simulation approach and methodologies are targeted towards a wide range of physical implementation levels. The developed algorithms will greatly benefit the design automation market as well as the electrical design community in general. This work will be performed under close collaboration with Canadian companies to reflect Canadian industrial needs and facilitate transfer of knowledge and technology.""390712,""Guo, Benqi"
"395093"	"Haarslev, Volker"	"Scalable description logic reasoning architectures for the semantic web"	"The semantic web extends the WWW from a collection of data and documents, that are designed for human consumption and are often hard to find and use, into a collection of knowledge that is understandable and very convenient to use by humans as well as computer programs. The knowledge available in the semantic web depends on wide-spread information (e.g., collections of web pages) and on the compilation of information into (often distributed) formal ontologies or knowledge bases. The Web Ontology Language (OWL) was approved in 2004 as a W3C recommendation. It is a response to the semantic web initiative and has become a de-facto standard in knowledge representation for many scientific communities as well as companies and governments. OWL is partially based on a family of knowledge representation formalisms called description logics. The appeal of description logics is the immediate availability of highly optimized reasoners offering a wide range of automatic inference services relevant for supporting the semantic web. Description logic reasoners are usually based on so-called tableau procedures requiring highly sophisticated optimization techniques due to their inherent runtime complexity. The first objective of this proposal is to achieve better reasoning scalability and to support persistency. This goal is due to recent developments in the semantic web community where (simple description logic) reasoning has been applied to very large ontologies consisting of up to millions of assertions. The challenge is to provide persistency and ensure scalability even if complex reasoning is required. This motivates the need to design and evaluate tableau optimization techniques which support scalable reasoning on persistent and virtually unlimited ontologies. The second objective is to develop novel optimization techniques for combinations of description logic constructors such as number restrictions, nominals, and inverse roles because known techniques are insufficient.""402330,""Haas, Carl"
"395063"	"Hall, Trevor"	"Ubiquitous and embedded photonic networks"	"The goal of the proposed Discovery Grant research programme is to contribute to the realisation of ubiquitous and embedded photonic networks capable of supporting new applications that push bandwidth and accessibility limits. The expected results of this programme are: new photonic network architectures applicable from the chip-scale to the continental scale; new packet switch architectures that maximally exploit photonic technology by retaining data in the optical domain; new wired and wireless digital signal processing empowered coherent optical transmission systems to extend the capabilities of access networks and embedded networked sensors; and to develop critical enabling photonic device technologies exploiting the latest developments in nanotechnology and integration technology. The programme is innovative because it combines state-of-the-art novel network, architecture and systems concepts with device integration platforms that are amenable to the low-cost volume manufacture critical for widespread adoption of ubiquitous and embedded networks. The proposed research will contribute directly to providing broad-band access to all, which is now essential to the economic and social well-being of developed societies. Embedded optical wireless networked micro-sensors will contribute to improvements in quality of life and will provide additional opportunities for commercialisation. The research results will help position Canada as a world-leader in photonic network technology and will create an opportunity for Canada to establish a dominant position in an expanding sector of the world-wide marketplace. Equally important, the research will develop highly qualified and skilled personnel promoting effective knowledge transfer and helping to fill the employment needs of the industry.""412923,""Hall, Trevor"
"396495"	"Haynes, Ronald"	"Implementation and analysis of adaptive algorithms for the numerical solution of partial differential equations"	"Mathematically, many models of interesting physical phenomena are written as partial differential equations (PDEs). Except for certain idealized situations, the PDEs which result are not amenable to analytical solution.   Instead we rely on numerical approximations.   I am particularly interested in time-dependent PDEs in two or three spatial dimensions whose solutions exhibit large solution variation, some type of singularity, blow-up or moving fronts. To efficiently track these regions of interest it is necessary to develop methods which adapt to the rapidly evolving features of the solution.Specifically, I am interested in both the practical issue of implementation and theoretical investigation of the natural marriage of two computational approaches which have received significant attention over the last 20 years - moving mesh methods and domain decomposition (in the context of Schwarz waveform relaxation).   These new  methods are known as Schwarz waveform relaxation moving mesh methods.  The strategy works by using moving meshes to track interesting features of the solution in space while the domain decomposition and waveform relaxation allows one to isolate difficult features of the solution in time. Domain decomposition allows a problem to be broken up into small pieces. Each piece may be solved independently, ideally in parallel,  and the pieces are then recombined to provide a solution for the whole problem.  This strategy is now attractive given the relatively easy access to high performance computing resources.  This topic will be of interest to numerical analysts as well as engineers and applied mathematicians whose work involves the simulation of complicated physical processes modeled by systems of partial differential equations.""400313,""Hays, Chris"
"402623"	"Hinzer, Karin"	"Use of nanostructured materials for greater than 40% efficiency concentrated photovoltaics"	"The goal of this program is to design and characterise novel high efficiency solar cells principally for applications in concentrated solar farms. The expected result is proposed device designs operating above 40% by utilising nanostructures in at least one of the multiple junction subcells. The program is innovative because increased efficiency, combined with lowered production costs, of photovoltaic systems is required for widespread deployment of both solar farms and localised off-grid energy generation. The research program will allow development of novel designs utilising epitaxially grown group III-V compound semiconductor triple-junction high-efficiency solar cells on germanium substrates, and eventually, on lower-cost silicon substrates. This program will use nanotechnology to improve subcell efficiencies and investigate physical parameters that determine the optimized operating conditions under concentration. Exploration of the design space for photovoltaic designs with three or more subcells will determine the best configurations for optimised cell efficiency in the 30-45% range, depending on sunshine conditions. With the help of additional funding, solar cells with optimum designs will be fabricated at the Canadian Photonics Fabrication Centre. These devices will be characterised using an artificial sun. Canada will benefit socially, culturally and economically. The proposed program will contribute directly to improving the environment and reducing dependence on non-renewable resources, through clean, green, alternative energy technologies. It will position Canada as a world-leader in high efficiency photovoltaic concentrator research and development. Equally important, the research will develop highly qualified and skilled personnel which will promote effective knowledge transfer and help fill the employment needs of an industry growing at 30% per year.""406058,""Hinzer, Karin"
"402813"	"Hoeber, Orland"	"Intelligent interfaces for next-generation web search"	"The use of Web search engines constitutes an important part of the activities that people perform on the Web. While significant effort has gone into the creation of Web search engines that can index billions of documents and return the results in fractions of a second, the interfaces used by the top search engines have remained essentially unchanged since the early days of the Web. Little support is provided for searchers as they attempt to craft a query. Even if they are able to provide a good query, the commonly used list-based representation does little to assist searchers in finding the information they seek.     The research program described in this proposal is a logical extension of Dr. Hoeber's recent Ph.D. work. The goal is to investigate the use of visualization and interaction techniques as a means for supporting the fundamental tasks Web searchers perform. Intelligent methods for displaying, organizing, and interacting with the information present in Web search tasks will lead to advancements in the interfaces used by Web search engines. These next-generation Web search interfaces will have a significant impact on how people search for and find information in the future. By taking advantage of our ability to visually process information, the cognitive burden associated with searching the Web can be reduced. Searchers will be able to recognize and explore elements of interest, rather than having to remember specific details.    Overviews of five research projects are provided in the proposal, focusing on personalization within Web search tasks, visual representations of Web search results, studies on interactive exploration of Web search results, visual representations of free-text queries, and using visualization to assist in re-finding information. This program of research illustrates the breadth and depth of investigations that can be undertaken in the area of visual and interactive Web search. The design, development, and study of prototype systems throughout these and other research projects will contribute to our understanding of how visualization and interaction can be used to support the human element in Web search.""398562,""Hoehn, Logan"
"390305"	"Hoover, James"	"Lightweight formal methods in agile software development"	"The world in which contemporary organizations operate is uncertain and rapidly changing.  Organizations depend on the software in their business systems to give them the flexibility to cope with this volatility and retain their competitive advantage.  The high-performance high-quality software development organizations that produce this software use agile development methods.  These methods develop software incrementally in close collaboration with the customer.  With agile, the purpose of the software, and the software itself evolve simultaneously.  Users do not have to wait until software is fully finished before they can start using it, and can change their requirements as new business opportunities arise.Modern software is complex, and seemingly simple changes can break it.  Agile methods cope with this through continuous testing.  Code and tests are developed concurrently, and tests are run automatically to validate that the software meets customer expectations.  However, certain kinds of critical requirements, such as ""orders are shipped only after payment"", cannot be completely tested.  Instead, a more formal approach is needed based on careful mathematical specification and reasoning about the software.  But formality is in general quite expensive, and unless it is lightweight and automated, it can interfere with the rapid change that is necessary in agile development.The goal of this project is to make agile development better by incorporating additional formal techniques and their associated automatic checking.  This way we maintain the flexibility offered by agile development while gaining the greater confidence offered by formal reasoning.""390333,""Hoover, Ratnajothi"
"395852"	"Hoyer, Peter"	"Quantum algorithmics"	"Can quantum computers be realized?  No-one knows, but if they can, the impactcould be enormous and the pay-off huge.  This fact has led leading researchgroups around the world to investigate the possibilities of realizing quantumcomputers.  This project is a part of this ongoing world-wide research.If quantum computers can be build, they will make errors, fail frequently, andrequire world experts to control for many years to come.  This is so becausethe physics is very involved and cutting-edge.  Quantum mechanics is onlybarely understood and building computers based on it is a very challengingtask.  It requires a joint effort of mathematicians, physicists, and computerscientists.Knowing that possible quantum computers will not be perfect, it is critical tofind and develop algorithms that will work even under these difficultconditions.  The objective of this project is to find and develop suchalgorithms.  The project aims at overcoming some of the physical challenges inbuilding quantum computers by applying ideas from computer science, and it aimsat utilizing quantum mechanical effects in constructing algorithms.  The projectaims at make experiments feasible and investigating prospective models.  Theproject also aims at increasing our understanding of quantum mechanics and whatit means to compute something.""403788,""Hoyt, Jeffrey"
"396715"	"Hung, Patrick"	"M-services computing security and privacy enforcement model with negotiation support"	"In this proposed research program, mobile services (m-services) can be seen as the integration of mobile and services computing such as wireless technologies and Service Oriented Architecture (SOA). M-services architectures are built on an insecure, unmonitored and shared environment, which is open to events such as security threats. It is because m-services inherit the known and unknown security weaknesses that are associated with the architecture of wireless technologies and SOA. As is the case with many other applications, the information processed in m-services might be commercially sensitive, so it is important to protect it from security and privacy threats such as disclosure to unauthorized parties.In the context of a long term research program (5-10 years) on mobile services computing security and privacy, this proposed research program focuses on the privacy access control theoretical model with negotiation support for protecting the sensitive information in m-services. The negotiation-enabled model provides for greater flexibility in supporting privacy access control decision-making processes in m-services. This model is particularly important for supporting m-services in a mobile dynamic environment. On the other hand, the major short-term objectives (within 5 years) of this proposed research program are to: (1) develop a theoretical model of privacy policies for m-services; (2) build a privacy access control negotiation model for m-services; and (3) demonstrate the completed model in an illustrative application, e.g., healthcare. This proposed research program aims to support one Ph.D. student and one Master student to conduct the research activities and experiments in the newly research infrastructure called ""Research Center for Mobile Healthcare Service Assurance and Privacy"" at University of Ontario Institute of Technology (UOIT).""399837,""Hung, Patrick"
"406151"	"Irani, Pourang"	"Navigation interfaces"	"There is a growing trend toward the use of ubiquitous devices (devices for information access anytime-and-anywhere) such as PDAs and cell phones. It is estimated that approximately 1.8 billion (and growing) users worldwide use mobile devices to perform day-to-day tasks and activities. However, software interfaces for these new devices are being deployed at a considerably slower pace. Many mobile applications employ the same interface widgets and controls as are used on desktop applications. This form of software interface transfer is inadequate given that mobile devices have numerous constraints, such as limited display size and resolution, different input mechanisms (pen and touch), and slower processing capabilities than typical computers. Furthermore such ubiquitous devices are also used in considerably different ways. As a result we need to reconsider how interfaces for ubiquitous devices are being developed.      A starting point in this proposal will consist of studying user tasks and strategies for navigating documents (e.g. text, hypertext, and maps). Current navigation interfaces for scrolling, panning or zooming that are commonly available are not the most effective techniques for navigating large workspaces on devices with small displays. With traditional interfaces on ubiquitous systems users can easily get lost in the navigation process and are required to manipulate complex widgets with pen- or touch-based input. Therefore new interfaces and techniques need to be developed. I will design techniques that take advantage of the expressive form of interactions available on these devices, namely with pen and finger interactions. The navigation interfaces will further be made effective by means of key visualizations that include animated transitions and off-screen cues. The novel interfaces will be evaluated in controlled experimental settings as well as in the field. Furthermore, I will develop predictive models for assisting designers to create novel navigation interfaces beyond the scope of this proposal. The outcomes from the proposed research will lead to results that, if properly channelled, could benefit millions of users worldwide.""395427,""Irani, Pourang"
"389895"	"Ito, Mabo"	"Towards improving internet performance"	"Recent work by the applicant has been concerned with dealing with some of the performance and QoS (quality of service) issues which exist in current Internet, particularly for inelastic applications that are sensitive to delay and packet loss. The current Internet was originally designed to support flat, elastic, applications which are relatively insensitive to delay and where packet losses could be handled by retransmissions. Modern applications, such as video streaming and collaborative work require timely delivery and a QoS assurance level. Long propagation delays, by itself, makes retransmission infeasible in many cases and forwarding delays due to congestion and poor routing compound the problem Furthermore, services on the current Internet are network centric, whereas modern needs are more user/application centric. Thus, in recent years the applicant has done work on web caching, load sensitive dynamic routing, multicasting, discard policies for QoS maintenance in transport of MPEG streams, routing improvement in DHT-based P2P networks, simulation tools for large scale networks, route failure recovery for reserved routing.     The proposed research will continue the research on improvement of routing performance and to provisioning of QoS. The results are intended to improve the performance of the current Internet as well as to experiment with ideas that might be used in the Internet of the future. The approach being taken is more user/ application centric than currently exists and also moving more intelligence inside the network, whereas the current Internet paradigm is to keep compute power and intelligence at the edges of the network.     For example, our current research on routing improvement in DHT based P2P networks is focused on fixed end points. Extension of the work will be sought to handle more mobile clients. The work on route failure recovery for reserved routing has focused on avoidance of a few simple classes of links. Extensions to other classes of links and the trade-offs between computational/state requirements and performance will be investigated""405905,""Ito, Maki"
"393855"	"Jacobsen, HansArno"	"The foundation for event management systems"	"Events underly and drive the operation of many large applications and systems.  However, little structured and foundational knowledge is available that helps developers design, structure, and build event-based systems.Events are state transitions of interest to applications, systems, and users in the environment. For example, the rise and fall of stock values constitute events of interest to algorithmic trading applications and brokers. The reading of an RFID tagged object is an event of interest to inventory management. The successful execution of activities in a business process are of interest to managers.These examples illustrate that the support for the modeling, the specification, the detection, and the management of events is of broad applicability. However, no universally accepted theory, model, language, approach, and reference architecture exists in supporting event processing based applications. The driving vision of this research is to develop an Event Management System to effectively support the afore-mentioned applications. This parallels the development of Database Management Systems almost half a century ago.  Unfortunately, Database Management Systems are ill-suited to support event processing, as events are about the future, while databases store, archive, and manage historic data. Databases were not designed to express, detect, and react to events.The objective of this research is to lay the foundation for the development of future Event Management Systems that would make up the core of event processing applications. To achieve this objective, this research aims to develop an event processing algebra, an event processing language, event detection algorithms, and a reference architecture jointly resulting in the Event Management System.""396477,""Jacobson, Christian"
"396515"	"Jin, Dean"	"Software system analysis and composition for integration and interoperability"	"This research addresses the need for effective models, methods and tools that support the development, composition and integration of software systems. There are four activity areas that this research targets:(1) Integration and interoperability as original design intent. This activity concentrates on specification, design and development as they relate to the integrated system construction process. Building an interoperable system from the ground up presents challenges that are fundamentally different from traditional software system development practices.(2) Integration and interoperability as an evolutionary necessity. The primary focus of this activity is the analysis of pre-existing systems with the intention of identifying opportunities for co-existence with newly developed or feature-augmented systems. This is an important factor in determining the long-term viability of many legacy information systems.(3) Assessing the 'composability' of systems. This activity focuses on evaluating the ability of distinct systems to work co-operatively in a 'system-of-systems' context. Examining the opportunities and limitations of system composition provides crucial information to organizations seeking to leverage the effectiveness of current software system assets.(4) Integrated system maintenance. This activity examines the long term effects of change on interoperable software systems. As integrated systems evolve they become much more difficult to document and maintain. The goal in this activity is to reduce the overall cost of ownership for software assets while continuing to maximize the benefits of the combined services they provide.In general, the aim is to increase the effectiveness of integration processes and reduce the effort involved in facilitating interoperability among software systems.""393080,""Jin, Jisuo"
"396620"	"Jourdan, GuyVincent"	"Distributed systems modeling and testing"	"This proposal is primarily about solving a set of problems related to distributed systems, with two main axes of research: -    )Distributed systems design inference: the automatic inference of suitable models from the observation or the behavior description of distributed systems, including questions such as detection of loops and irregular behavior; and-    )Distributed systems testing: the automatic generation of testing sequences for distributed systems for which a formal model is provided, including the verification of causal relationships, and the support for additional determinism in the implementation. ""390535,""Joy, Douglas"
"396637"	"Kaltchenko, Alexei"	"Estimation of information-theoretic quantities and algorithmic processing of classical and quantum information"	"Sequences of symbols or strings drawn from some (finite or infinite) alphabet appear in many applications where  objects can be encoded into strings in natural ways. Such sequences are often  viewed as realizations of stochastic processes also known as  ``information sources''.In many applications, when dealing with information sources, one naturally faces the following two tasks: analyzing/estimating sources properties and changing them in certain desirable ways. An important quantity characterizing an information source is its entropy (entropy rate). Entropy appears in many applications in digital and analog communications, biology, chemistry, physics, etc. For example, in studying chaotic dynamical systems, measuring the entropy of a (symbolic) dynamical system is important for deciding whether the system is chaotic. The long-term objectives of this research program are to develop methods  and design efficient algorithms for measuring  and estimating information-theoretic properties/quantities of information sources such as entropy,  relative entropy and conditional entropy; and to design efficient algorithms for source transformation.""405578,""Kaluarachchi, Harini"
"390192"	"Kaminska, Bozena"	"Flexible multilayer and multiparameter wireless microsystems for biomedical applications"	"The overall Aim of the proposed research is to develop an entire new generation microsystem embedded in a flexible material (polymer) as a 3D ultra low power or no-power structure containing new or adopted sensors, communication , processing, storage, and powering. The scientific and technological long-term research will aim at:- Identify new concepts and issues that may strongly impact the new generation of Microsystems, their integration and specific components.- Investigate new bio-micro-electromechanical and optical sensors (Bio-MEMS) - medical applications require dedicated detection of multifarious parameters.- Research and develop the 3D integration technologies based on polymer materials.- Develop robust techniques and technologies for active material based devices based on polymers (possible also ceramics and metals) and for surface functionalisation at the nano- and micro-scale in order to permit embedding of new functions into or onto a variety of structured surface. - Explore generic powering solutions following two tracks: low power functions and improvement of energy resources and management (integrated power sources).- Develop dedicated wireless communication components and application models (demonstrators).""406535,""Kaminska, Bozena"
"399378"	"Kapralos, Bill"	"Computational acoustics for virtual environments"	"Incorporating spatial sound in realistic simulations such as immersive virtual environments seems obvious.  In fact, doing so can be beneficial for a variety of reasons.  Spatial sound cues can add a better sense of ""presence"" or ""immersion"", can compensate for poor visual cues (graphics), and lead to improved object localization.  That being said, accurately recreating the potentially large number of interactions between a sound and the objects/surfaces it may encounter as the sound propagates through the environment while accounting for the human listener is extremely difficult and beyond our current analytical and computational reach, except perhaps for simple environments.  There is also a lack of established methodology for the detailed evaluation of the effectiveness and accuracy (e.g., how faithful the model is to the actual phenomena being modeled) of spatial sound systems in general.  Many evaluations involve making comparisons between various properties/statistics computed by the system and the corresponding theoretical/predicted values.  However, the human listener is the ultimate user of any spatial sound application and therefore, to determine the effectiveness of such systems, any method of evaluation must also include human listeners in order to account for human auditory perception.  This research program seeks to develop methods and techniques that will lead to more efficient, realistic simulations of dynamic, interactive, multi-modal virtual environments through the investigation of: i) the trade-off between accuracy vs. computational time; ii) whether human perception may be exploited to limit computational resources, and iii) the potential merger of human-based effects with acoustical modeling to facilitate efficient listener sound fields through the use of different algorithms and techniques.  The training of highly qualified personnel is an essential component of the proposed research program and will include the training of students.  While under my supervision, students will have the opportunity to work on unique, cutting edge research using state of the art facilities, while interacting and building ties with experts in acoustics.""391574,""Kapron, Bruce"
"395377"	"Keliher, Liam"	"Algorithmic analysis of symmetric-key cryptographic primitives"	"Cryptography is the field that uses mathematical techniques to secure information.  The rapid rise of computer and communications technology, especially the widespread use of the Internet, has greatly increased the demand for such techniques.  For example, trillions of dollars of currency exchanges are carried out each day, and these must be encoded in such a way that malicious parties cannot view or tamper with them (even if they are sent over publicly accessible networks).  Symmetric-key cryptographic primitives such as block ciphers, hash functions, and stream ciphers are fundamental components of the infrastructure that enables such transactions to be carried out  securely and efficiently.My research has been dedicated to the analysis of symmetric-key primitives for the past 10 years.  Over this time, I have developed a series of new techniques (algorithms) for analyzing the strengths and weaknesses of important classes of primitives.  This proposal is focused on the full development of my techniques into a cryptographic toolkit that can be readily used by other researchers who are studying existing primitives or designing new ones.  I also propose to extend the rigour of the standard analysis of such primitives by removing simplifying assumptions that can hide weaknesses.""405508,""Kellenberger, Lisa"
"395368"	"Keselj, Vlado"	"Harmonized string-based and unification-based methodology for text mining and processing"	"The importance of information availability and Internet is clearly evident in many areas, from a personal level to a general society effect, such as in economy, health care, or scientific research. While general search engines seem to provide a frequently satisfactory document and site retrieval, based on short ad-hoc queries, there is still an enormous time-consuming manual effort required to gather data, filter it, organize it, and use it in the actual tasks that a user wants to get accomplished.  Here, we propose to develop three different core natural language processing methodologies that will make a strong contribution to solving this information management problem.  Beside the theoretical results, we develop several tools that actually implement designed solutions, and we also apply these methodologies and tools to specific application areas.    Our focus is on the methods for (1) Common N-gram analysis, (2) Regular Expression based and finite state processing, and (3) unification-based processing and matching of typed feature structures; with an effort to harmonize these fairly different techniques. The developed software systems include tools for n-gram based text mining (Ngrams.pm and Swordfish), Starfish -- a text preprocessing and embedded programming tool, the question answering system Jellyfish, the system with typed feature structures in Java -- Stefs, and Shrack -- a peer-to-peer system for scientific information dissemination. The application areas include textual mining, authorship attribution, web usage and web content mining, dementia detection of Alzheimer type from spontaneous speech, phylogenetic tree generation, malicious code detection, bio-medical semantic text annotation, and knowledge management in eScience.    The N-gram model, regular expressions, and unification-base approach have been known concepts in NLP.  The novelty of our approach lies in a specific methodologies developed on top of these models: specific profiling and distance functions used with n-grams, iterative regular expression substitutions, and modifications to classical unification, such as relaxed unification.""401914,""Keshav, Srinivasan"
"393338"	"Kosseim, Leila"	"Answering opinion questions from blogs"	"If tonight, the president of the U.S. were to resign, only a few minutes later, opinions, beliefs and other comments would already flood the Web. As more and more textual information is available online, it has become necessary to develop natural language tools to help analyze informal texts quickly to make timely decisions. Recent developments in Question Answering (QA) have made it possible for users to ask a question in natural language and receive a specific answer rather than an entire document.  However, to date, most work in QA has been involved in answering fact-based questions from formal and information-rich sources such as newspaper articles.  However, as more and more people use the Web to express their opinions, the problem of answering opinion questions from informal documents will become an important issue.The objective of this research program is to address the problem of answering opinion related questions from non-factual and informal documents such as online blogs. To achieve this, I plan to take advantage of work done in the area of sentiment tagging and my previous experience in QA.The detection of opinions and the analysis of blogs is slowly becoming a popular topic in Natural Language Processing (NLP), and I believe that this program can make a significant contribution to this very young field.  Although most work in NLP is still being performed on formal and factual documents, blogs are very interesting because they provide a ""real"" application scenario and as blogs are created quickly and in mass quantity after an event, the automatic analysis of the content of blogs has become a necessity.   ""408228,""Kosta, Jacqueline"
"406153"	"Kribs, David"	"Mathematical synthesis of application and theory in quantum error correction"	"As computer chips and transistors shrink in size, engineers will eventually be forced to use objects such as electrons and atoms to encode bits of information, and hence will be forced to deal with the mysterious ""quantum world"". Scientists have recognized this as a central issue for several decades, but only within the past decade was it shown that quantum effects could be harnessed for a new kind of ""quantum computing"" that would quite simply revolutionize the world of computing and cryptography, as we know it. The experimental and theoretical progress of recent years has generated tremendous excitement within the scientific community, but there are still many fundamental issues to overcome if the promises of quantum computing and cryptography are to be realized. Extending these successes to large-scale many body quantum systems is one of the most difficult and intriguing problems in modern science. While the use of quantum mechanics in the context of computing enables fascinating possibilities, it also presents daunting new challenges. Of central importance is the need to mitigate the effects of errors induced by environmental noise that disrupt attempts to control and maintain features of quantum systems as they evolve in time. Even though substantial progress has been made on this front, the basic problem of ""quantum error correction"" remains one of the primary impediments to large-scale quantum computing. This research program goes directly to the heart of the matter. The overarching goal of the research is to find new techniques, and further develop established technologies, for error correction in quantum computing. The methodological approach is rooted in mathematics, and the interdisciplinary nature of the research provides opportunities to apply tools from physics. A significant goal of the research is to synthesize theory with experiment in quantum computing. Another important goal is to apply these investigations to other emerging disciplines of quantum information science, such as quantum cryptography and quantum gravity. ""395039,""Kribs, David"
"390513"	"Lachapelle, Gerard"	"Advanced GNSS signal processing techniques for indoor location"	"This project focuses on the development of innovative GNSS signal processing techniques that can be used in real-time in a software receiver to improve availability, accuracy and reliability of GNSS such as the Global Positioning System (GPS) indoors. During the past 10 years, new processing techniques have resulted in the use of GPS indoor and the commercialization of scores of new applications such as mobile phone location and pedestrian navigation devices (PND).  The characteristics of the new signals being deployed with the second generation of GPS satellites and the forthcoming Galileo and other GNSS makes these signals better suited for use under indoor attenuated signal conditions and are resulting in an urgent need to carry out the research proposed herein and to train related high quality personnel needed by the GNSS industry to continue its rapid growth.  The PND sector alone is expected to grow by 50% per year according to leading companies in this area.  The research will include the development of ultra high sensitivity techniques for signal acquisition and tracking that will necessarily rely on long time integration. Issues related to oscillator stability and tracking loop aiding from self-contained sensors will be given priority. The new processing techniques to be developed under this project will be optimized for real time effectiveness and implemented in a software receiver.  This will allow thorough testing and evaluation under a wide range of indoor attenuated signal scenarios, to better under the advantages and limitations of the new signals in term of performance in the indoors and contribute to the growth of the GNSS industry in Canada and abroad. Implementation in a software receiver is important due to cost, reconfigurability and the staged introduction of the new signals and related algorithms.""413596,""Lachapelle, Gérard"
"402822"	"Lam, Patrick"	"Software verification tools based on lightweight specifications"	"Software systems are notoriously difficult to get right. A key problem is that it is difficult to state just what it means for a software system to be ""right"" in the first place: while it is easy to point out particular undesired behaviours, particularly crashes, it is difficult to specify a software system's desired behaviours.The first part of my research programme therefore aims to develop new techniques for software developers to specify the behaviour of their systems. I am particularly interested in lightweight specifications, which concentrate on a particular aspect of a system's behaviour. I believe that their tight focus will make lightweight specifications much easier to work with than general-purpose specifications: focussed properties will be easier to state and to understand. To provide an initial set of specifications for already-existing software systems, I will also develop tools for automatically extracting specifications from software systems.Of course, software users would just like their software to work; they don't care that it has been specified. Hence, the second part of my research programme will develop techniques and tools that enable developers to verify that their software systems actually satisfy their specifications. My expertise in developing program analysis tools and techniques will enable me to develop novel techniques for verifying my proposed lightweight specifications.""407247,""Lam, PingHay"
"393002"	"Landry, René"	"Technologies de haute sensibilité et robustesse pour les récepteurs GNSS en environnement sévère"	"L'objectif général du programme de recherche est d'accroître la robustesse et la sensibilité des futurs récepteurs GNSS dans des environnements non idéaux (intérieur des bâtiments, forêts denses, villes à haute densité, tunnels et canyons, mines, sous l'eau, etc.). Nous souhaitons exploiter toute la redondance, les nouvelles propriétés et les possibilités des futurs signaux GNSS (modernisation du GPS, Glonass et nouveau système Galileo) à l'intérieur de nouvelles architectures numériques notamment pour accroître la qualité des mesures brutes, la sensibilité des boucles de phase et la robustesse globale du récepteur. Pour ce faire, le concept de radio logicielle, mieux connu sous le sigle SDR (Software Defined Radio), sera utilisé pour valider les nouvelles méthodes et architectures hybrides permettant le traitement en temps réel des nombreux signaux GNSS devenus larges bandes. Trois axes de recherche seront privilégiés pour atteindre les objectifs. Nous nous intéresserons en premier lieu à la quantification des performances de nouveaux filtres numériques anti-brouilleurs dits de pré-corrélation et aux techniques innovatrices contre les multi-trajets pour l'amélioration de la robustesse. Nos travaux mettront en oeuvre de nouveaux algorithmes d'hybridation des canaux ainsi que de nouvelles méthodes d'acquisition et de poursuite des signaux GNSS qui seront développés et validés dans le prototype temps réel du récepteur GNSS. L'originalité des travaux réside dans l'exploitation combinée des mesures de phase des différents signaux GNSS disponibles, d'aides externes et inter-canaux des boucles PLL et DLL et de la validation pratique des nouveaux algorithmes développés en simulation. L'exploitation de senseurs MEMS hétérogènes comme aide externe, ainsi qu'un lien de télécommunication sans fil, feront partie intégrante de nos recherches, alors que l'exploitation des technologies émergentes bénéficiera à la géomatique moderne. Au niveau du récepteur, la tête radio-fréquence sera remplacée par une conversion numérique directe du signal (Direct RF Sampling), ce qui simplifiera son architecture et réduira sa vulnérabilité inhérente. Nos travaux pourront également s'appliquer à d'autres domaines du génie.""400817,""Landry, Tommy"
"396530"	"Lang, Jochen"	"Interactive acquisition of the physical world"	"This proposal describes a research program which will investigate and develop novel methods for the acquisition of physical models of our environment. Increasingly, consumer applications on game consoles to cell phones utilize 3D models because of the ubiquity of performance graphics hardware. The computational power in displaying 3D models is currently not supported by an efficient process to acquire these physical models. Automatic model acquisition methods are often still too cumbersome, too specific and their results are hard to control. Most models to date are created by artists. Artistic modelling is suitable for characters in entertainment, games and advertising, but is inefficient for large-scale models and does not result in accurate physical models.My goal is to ease the acquisition of computational 3D models by integrating the human modeller into an interactive measurement-based process. Interactive methods allow direct human control of the acquisition process which will make the results of the measurement-based acquisition process predictable. Measurements of the physical world will lead to realistic models of our environment. Although 3D models are concerned with shape, the growing demand for high-quality images, animations and multimodal display requires that physical parameters are acquired as well. I propose to conduct research into interactive acquisition of 3D shape, physics-based deformable models and appearance models applied to virtual training environments, navigation and entertainment. Real-time interactive acquisition will greatly simplify 3D physical modelling and make it available to the non-expert.""400852,""Lang, John"
"395373"	"Laviolette, François"	"Majority votes and duality in machine learning"	"This program of research belongs to Machine Learning,  a subfield of Artificial intelligence. More specifically, this work is related to the Classification theory (also called Pattern Recognition), and to Reinforcement Learning theory  (a theory of sequential decision making under uncertainty). My long run objective is to contribute to mathematical fundaments of Machine Learning, using my expertise in other fields.In the last decades, Machine learning has produced spectacular advances of Computer Science. One can think of Neural Nets, Support Vector Machines, the Reinforcement Learning (RL) paradigm.Despite these advances that come from very interesting ideas, there is a lot left to do for the  mathematical fundaments of Machine Learning.   I  think that such fundaments not only will improve our comprehension of the efficiency of actual methods, but it will provide  tools that will allow us to invent more methods or improve the ones we have. In Classification, some of our new results already permit to obtain a novel explanation of state-of-the-arts algorithms, the objective now is  to imagine  new algorithms that could come out of these new results.The second objective of the program is to generalize those results to the RL framework. Theoretical guarantees are difficult to obtain here, because we turn from a single decision framework, as faced  in classification, to a sequence of decisions. It is possible to obtain a reduction from the latter to the former if, in the RL framework, we can get rid of the notion of the states of the process at a given time.  Defining a stochastic process without the notion of state  involves duality theory, which says that states can be equivalently  expressed as a set of properties that they possess. Some results already exist in that direction (e.g.: Predictive State Representation (PSR)). Moreover, such notions of duality exist in a variety of areas (namely in Mathematical Topology).  In those areas, duality ideas have proven to be useful already.   We expect to obtain a theoretical general framework of the notion of duality for probabilistic processes.""397385,""Laviolette, Steven"
"391074"	"LEcuyer, Pierre"	"Simulation of stochastic systems"	"This project concerns the simulation and optimization of complex stochastic systems (that involve uncertainty). Stochastic modeling and simulation are primary tools in science, engineering, economics, management, and several other areas. Simulation has become more attractive and less expensive than experimenting with the real systems, due to the decreasing cost of computing and the improvement in simulation methodology and software in the last decades. Simulation is often the only practical tool to deal with realistic models of complex systems, which are typically dynamic, stochastic, and nonlinear. However, designing efficient and reliable simulation methods, and using simulation to optimize decision/operation strategies in these systems, remains very difficult. Random number generators are a fundamental ingredient for stochastic simulation. They are also needed for computer games, gambling machines, and cryptology, for example. Their quality criteria differ across areas of applications, whence the need for different types of designs and for theoretical studies from different viewpoints. Quasi-Monte Carlo methods replace the vectors of random numbers used in a simulation by points that cover the space more uniformly than random points, to improve the accuracy of the simulation results. Other ways of improving efficiency include concentrating the sampling adaptively toward the most important events. My research focuses on the design and analysis of methods for: (1) random number generators for various types of applications, (2) high-dimensional numerical integration via randomized quasi-Monte Carlo, (3) improving the efficiency of simulations (e.g., by designing estimators with smaller variance) and dealing with rare events, (4) optimization of stochastic systems via simulation. These methods are applied in several areas such as finance, risk analysis, telecommunications, management, and so on. Both theoretical aspects (e.g., convergence analysis of algorithms and mathematical analysis of the structure of random number generators) and practical ones (e.g., empirical experimentation, software implementation) are covered.""413043,""LÉcuyer, Pierre"
"396624"	"Lemieux, Sebastien"	"Gene interdependence in microarray data analysis"	"DNA microarrays allows to quantify the transcripts of genes (mRNAs) in an organism and to identify the modulated genes between two conditions, which reports the modification of molecular mechanisms.  This technology has been heavily investigated since its inception and, recently, consensus is emerging regarding key features of an appropriate analytic pipeline.  One aspect of microarray data that is frequently overlooked is the impact of gene interdependency on the statistical methodologies used.  Since the integrity of the cell (through homeostasis and ordered differentiation) is dependent on a tight coordination of the expression of thousands of genes, it should be expected that variations in the concentration of their mRNA are highly correlated.  Widely used statistical methods requiring independence to preserve their accuracy are: normalization, multiple test correction procedures and gene ontology functional annotation.  Measuring the level of interdependency of genes on experimental data is still out of reach, and thus its impact on these methods have only be studied from a theoretical point of view.  The challenge of inferring regulatory networks from gene expression data have recently been qualified as an indeterminate problem.  In this proposal, we highlight the need to represent, at a higher level, global features of the dependency network.  These global features are likely to represent ""modes"" in which the cell operates depending on its level of differentiation or the level of complexity of the organism.  Understanding the high level of organization of this network will then allow us to have a better grasp at individual gene's role.  The main objectives of my research program are to 1) measure and take advantage of gene interdependency in genome-wide statistical analysis, and 2) expand the scope of meta-analysis of genome-wide experimental results.""411710,""Lemieux, Suzanne"
"388513"	"LeonGarcia, Alberto"	"Design of converged communications and computing infrastructure"	"This research project investigates and designs the infrastructure for next generation application-oriented networks.  The commoditization of computing and communications technology has changed the design approach to the next generation network.  New distributed computing models have appeared that fundamentally disrupt traditional models for offering services and applications in the manner they leverage commodity resources and use new business models.  Prime examples of players using these new models are Google, Amazon and BitTorrent whose distributed designs achieve entirely new points in the tradeoff between performance/reliability versus capital/operating cost. The of these new computing models must be incorporated into the architecture and infrastructure of the next generation of service and application providers. Our research investigates design principles to develop an architecture and infrastructure that has the reliability of existing telecom networks and is flexible and self-managing so that it can support a future applications environment that is very dynamic due to rapid churn in applications and traffic patterns.   The new network architecture must achieve entirely new levels of scale in terms of size, capacity, and diversity.  Our research explores the use of virtualization, service orientation and composition principles to create new applications as well as to manage network and computing resources.  We are developing approaches for dynamically managing and controlling resources in this future converged infrastructure.  Our work involves algorithm design, performance evaluation, as well as implementation and testing.  An important focus of our research is the design of highly scalable switches that combine electronic and optical technologies to provide an infrastructure that is power-efficient, compact, modular and growable.  Part of our research effort is to re-evaluate the architectural principles that govern how computing, storage, communications, and intelligence will be combined in the future information infrastructure.""400349,""LeonGarcia, Alberto"
"388363"	"Levesque, Hector"	"Representation and reasoning for autonomous agents"	"This research concerns the reasoning that a robot would have to do to interactproperly with a world that is changing and incompletely known. Almost all ofthe research in robotics today concerns basic level tasks such as dealing withnoisy sensors (like sonar), controlling manipulators (like robot arms), ordetermining how to move without colliding into obstacles. Higher leveldecisions, such as when a robot needs to move at all, in what order it shouldtackle its jobs, or what it should try to do if something goes wrong, oftenend up being pre-programmed in advance. The goal of the research here is toinvestigate what it would take for a robot to be able to make such decisionsfor itself, based on what it is trying to do, what it knows about its world,what it has been able to find out using its sensors. To get these decisions tocome out of a general reasoning process, however, it will be necessary toexpress in a language suitable for automated reasoning, enough of what theagent needs to know about its world, goals, actions, ability, the cognitivestates of other agents, collaborative task execution, and so on. The researchproposed here would examine various aspects of these problems, and attempt todevelop high level agent controllers of this sort.""407907,""Lévesque, JeanSébastien"
"402626"	"Levi, Ofer"	"Optical nanostructures for high sensitivity bio-sensing"	"The ability to detect biologically active molecules is of crucial importance for fundamental studies in biochemistry, as well as applications ranging from bio-defense to drug development and point-of-care diagnostics.  Medical diagnostics often still rely upon labor-intensive, slow, and expensive bulk optics laboratory techniques.  The miniaturization of biological analysis systems promises to facilitate portable detection of common diseases and to greatly enhance our diagnostic abilities and reach.In this research, we will develop novel optical nano-structures for high-sensitivity bio-sensing and combine them with miniature optical readout devices and flexible approaches for bio-sensing and bio-diagnostics. We focus on localization of a ""capture"" molecule (peptides, antibodies are typical examples) on a treated surface and optical detection.  Our optical bio-detection methods for the inspected molecules will rely on changing a local parameter such as absorption, index-of-refraction, or scattering.  The research progresses from electromagnetic theory and analytical device design to biological applications in 4 main stages: (1) design of optical nano-structures with analytic theory and numerical simulations; (2) fabrication of dielectric and hybrid dielectric/metal optical nano-structures using standard semiconductor fabrication tools (electron beam, optical lithography, etching); (3) evaluation of optical properties for the nano-structures and comparison to theoretical predictions; (4) demonstration  of bio-sensing utilizing these innovative designs within a ""lab-on-a-chip"" miniaturized sensor platform.""403121,""Levin, David"
"393540"	"Li, Xun"	"Design, simulation and fabrication of low-cost single mode Fabri-Perot laser diode for fiber-to-the-home networks"	"The tremendous growth in broadband access services has propelled photonics technology to a new frontier where fiber-based solutions enable not only converged voice, data, and video applications, but also a range of value-added services in local access networks. As a fast-emerging technology that utilizes the vast bandwidth available in optical fibers, these fiber-based access solutions, known as fiber-to-the-home (FTTH) networks, will revolutionize the entire global information network for reaching residential homes and business premises. A major barrier to the widespread deployment of FTTH technology, however, has been the cost. The objective of this proposed research program is to develop cost-effective single longitudinal mode (SLM) semiconductor laser diodes for optical network units (ONUs) in FTTH networks. The program will focus on the investigation of the alternative methods to control the lasing mode in semiconductor lasers with no grating involved. The main idea of achieving the SLM operation is to insert a passive band-pass filter at the rear end of the conventional Fabri-Perot (FP) cavity through etching. It is well known that the total internal reflection (TIR) may happen at a slanted trench introduced across the waveguide. Once we put two such trenches in parallel, tunnelling happens at a set of particular wavelengths at which the reflected waves through the two TIR interfaces cancel out. Therefore, within our interested wavelength range, the slanted double trench structure shows a band-pass filter feature with its spectral peak positions and pass-band width all controlled by its geometrical dimensions. By further aligning the pass-band with one of the targeted FP cavity modes, we achieve the SLM operation. With enhanced immunity of the laser performance from the external feedback and with increased yield, the new design targets a substantial reduction of the fabrication cost of such laser diodes for ONUs which typically account for 70-80% of the cost of a FTTH network. The proposed research, if successful, will therefore have significant impact on the development of the key components of FTTH networks.""403716,""Li, Yang(Eileen)"
"403649"	"Litoiu, Marin"	"Adaptive and Self-managing systems"	"The software industry is faced with one main challenge:  the high cost of software maintenance and operation.  One of the main reasons for this is the usage and operation complexity of the software, together with its lack of run-time flexibility and adaptability to the workload and environment conditions. There seems to be two complementary solutions to this problem: adaptive and self-managing systems and software as a service computing model.The goal of this research program is to investigate methods, patterns, algorithms, architectures and methodologies that lead to adaptive software systems in the context of software as a service. Adaptive and self-managing software is software that can easier adapt to changes in user requirements, is more autonomic, more evolvable, that is software that can be easier upgraded, released and deployed. We are looking at the entire lifecycle of the software:  analysis, modeling, development, deployment, operation as well as to specific run-time artifacts that enable adaptive systems. A specific goal of the research program is the advancement of model based feedback computing, in which the adaptation and self-management decisions are based on a model of the software. The models capture specific quality aspects such as performance or reliability or can capture partial functionality of the systems.""405538,""Little, Alexander"
"390345"	"Little, James"	"Robots, maps, and tasks"	"Currently we have mobile robots that can map their environs both indoors and outdoors, and then reliably recover their metric position while moving around in their space by solving the well-known simultaneous localization and mapping problem (SLAM).  But robots cannot yet tell us where they are, nor easily take our directions.  Bridging the gap between dense accurate geometric information and the semantics of the space we share with robots is the critical next step in building usable robotic assistants.  I will investigate how to supply a world description to the robot, learned from examples or adopted from ontologies that characterize prototypical spaces.   The relations between places and robots support our interactions with them and enable robots to work with us.  Moreover, I will ground descriptions in robust visual features that identify particular locations, to which place identifiers are associated.Assistive technology, in particular stereo-guided wheelchairs, is an application where stereo sensing-based mapping technology can sense obstacles and nearby walking patients.  Moreover, vision can provide situation awareness, by giving information about contexts and activities, beyond simple geometric occupancy maps.  Recently our mobile robotics group teamed with David Lowe's group in recognition to create ""Curious George"", a mobile robot for the  semantic Robot Vision challenge at AAAI 2007.  The robot learns to find objects in a room from Web images.  Our next generation robots will use maps to guide viewpoint selection and use context to find objects, and assist people in everyday tasks.""410599,""Little, Jarrett"
"393185"	"Lounis, Hakim"	"Towards an automatic decision making system for software design and coding"	"Building high quality applications require an efficient access to specific knowledge. It is the case especially for software design and coding, where, software practitioners need to make decisions; they clearly need support for deciding what alternative is better for a specific context, and for identifying the effect of their decision on the quality of the software product.In recent work on software quality, we have used machine-learning techniques to capture knowledge from data (e.g., fault prevention knowledge, impact knowledge, etc.). However, in general we have to consider informal (or semi-formal) knowledge (texts, diagrams, statements, observations, behaviors, etc.) and then we have tostructure it in order to finally formalize it and make it available for an automatic processing. A knowledge acquisition framework has to support this variety of sources; the ultimate goal of such a framework is to build knowledge bases that will support decision making for future projects.Considering knowledge implied in the whole development process is a vast and complex challenge. Thus, the objective of this proposal is to go towards an automated solution for manipulating knowledge for (only) software design and coding. We propose a knowledge-based architecture for decision making; it assumes (i) a knowledge representation language based on objects -allowing at least to express constraints and rules, but also other formal representations-, and (ii) inference mechanisms for reasoning on this knowledge. On the other hand, this system will be built on top of an existing framework for object-oriented applications analysis. Such an architecture will allow (1) software designers/programmers exploit knowledge bases during software construction and help them taking ""right"" decisions during design/coding. It will also allow (2) more experienced software designers/programmers manage the knowledge they have learned over their practice, and store it in formal and reusable schemes. Finally, it will help (3) capture and acquire knowledge from past software development projects, by analyzing their data with appropriate techniques.""391087,""Lovasik, John"
"395383"	"Lu, Jianguo"	"Web service collection, searching and composition"	"Web is not only delivering data for humans to read, it is also emerging as a platform that provides machine-readable information, thus enabling the creation of new functionalities composed from existing web services and web programmable APIs. In order to support the systematic and disciplined development of such compositions, there is a need to study the methods and tools oriented to such web applications. Moreover, many web services compositions and customizations are closely related to our daily lives, hence they can be carried out by ordinary computer users as well as professional programmers. Since these computer users have little programming knowledge, it is more important to provide automated or interactive support to produce composite services. The project will study the problems related to the construction of a web service programming environment for novice programmers. First, we will construct a repository of web services and other reusable web components, based on which we can carry out the experiments. Then various service search methods are investigated, in order to facilitate the reuse of the components, either manually by programmers or automatically by service synthesizers. Finally, those search methods will be used to construct interactive and automated web service composition systems. For interactive web service composition, we will develop a service broker that has a good command of the services on the web, as well as the programming requirements from the service assembler. Service broker will watch over the shoulders of the service assembler and acquire the knowledge of the context where the service will be used, hence will be able to recommend the services to be used in the next step. This work will be based on various software component reuse techniques such as source code searching and recommendation. For automated web service composition, we will focus on services wrapped from legacy database systems and annotated with SQL queries. With those services we can synthesize the implementation of a web service using database query rewriting.""407277,""Lu, Jie"
"395924"	"Lucet, Yves"	"Computer-aided convex analysis and optimization"	"The goal of this proposal is to develop efficient algorithms, to implement them in a software package for validation and wide dissemination, and to apply them in a variety of fields, especially optimization, with a special focus on the intensity modulated radiotherapy treatment planning (IMRT) optimization problem.       The research focuses on Computer-Aided Convex Analysis, which consists of investigating new algorithms to compute and visualize fundamental transforms in convex analysis. The resulting framework will allow researchers to perform experimental mathematics in convex analysis (and related fields), and to improve current optimization algorithms. To facilitate research investigations, and the training of graduate and undergraduate students in convex analysis, the algorithms will be released as a toolbox. They will allow fast visualization of convex transforms, the quick validation of conjectures, and the clear illustration of theorems.      The results, both theoretical and numerical, will be used on deterministic global optimization algorithms. The improvements will then be applied to the problem of intensity modulated radiotherapy treatment (IMRT) planning. IMRT uses geometrically shaped and intensity modulated radiation beams to treat cancer tumours with more accurate and more effective doses of radiation.  So the research has the potential to improve cancer treatment by providing more accurate treatment plans in a shorter time. It will take advantage of an ongoing collaboration with the BC cancer agency through the Kelowna cancer clinic, which offers radiotherapy treatment to the population of the Okanagan.       Two other fields that use the same convex analysis calculus, and so will benefit from more efficient algorithms, are Network Communication and Image Processing. The improvements resulting from the new algorithms will be applied to speed up current calculations in these fields.""404104,""Lucht, Benjamin"
"395402"	"Lumsden, Joanna"	"Investigating innovative evaluation techniques for mobile technologies"	"For emerging mobile technologies (e.g., new generation cell phones, handhelds, etc.) to reach their full potential across as many sectors of society as possible it is imperative that they are considered usable; this is not, unfortunately, the case at present.  Methods for accurately and meaningfully evaluating mobile device usability are currently lacking, and are the topic of considerable debate in the emerging field of Mobile Human-Computer Interaction.  The proposed research will investigate novel methods for meaningfully evaluating the usability of mobile technologies, and explore how best to conduct lab-based usability studies of such technologies such that the evaluation results reflect the real world context of use.  Various means by which to effectively mimic real world mobile contexts of use in a controlled lab setting will be explored such that mobile technologies can be meaningfully evaluated relative to their intended usage scenarios.  The research will involve a series of evaluations of mobile technologies relative to various contexts of use; each experiment will explore novel lab set-ups using appropriate combinations of physical environmental configuration and the use of passive, active, and interfering distractions to incorporate abstractions of real world contexts in the lab.  Where possible/relevant, related field trials will be conducted and the results obtained from the lab and field settings will be compared to determine the strengths of each evaluation approach.  Based on the research findings, a set of guidelines and practical procedures will be derived to support effective mobile evaluations of mobile technologies.  This research will not only contribute to knowledge in the field of mobile interaction appropriateness relative to given usage scenarios, but it will lead the way in terms of mobile evaluation strategies.  This research is essential to realizing the potential of mobile technology; it will be useful to researchers and technology developers alike, with the result that more usable technologies will hopefully reach, and be widely adopted by, the end user market.""406404,""Lumsden, John"
"403435"	"Lyons, Kelly"	"Tools and methods for modeling, managing, and implementing service systems"	"The long-term goal of this research is to design and develop novel tools and methodologies for effectively modeling, managing, and implementing ""service systems"". The tools and methods will combine existing service delivery platforms, architectures and protocols (web services, service oriented architectures, etc.) and emerging social computing environments such as blogging, wiki's, FaceBook, and virtual worlds.Service systems are networks comprised of technology, people, and organizations through which services are delivered and deployed. Services are interventions that transform state and co-produce value.  A ""service"" is often defined as any task that one organization or individual beneficially performs for and with another. Examples of service systems include IT outsourcing, education, IT service delivery centers, and call centers. Service systems are highly complex systems (especially if they contain a large variety and number of technologies, people, and organizations) and are difficult to model, manage, and implement because of the complexity of integrating people, their knowledge, activities, and intentions within the systems. Through this research, we will create novel tools and methodologies for extracting and building models of complex service systems. We will design and create interconnection methods and tools for integrating existing protocols and technologies with social computing environments to better connect the human and software services in the system, and we will investigate business policy management using knowledge available on the Web and within social networks.""398069,""Lyons, Peter"
"389865"	"MacCaull, Wendy"	"Verification and automated generation of adaptive workflows for complex and distributed processes"	"As research and development in science and technology continue to improve the lives of Canadians, there is an increasing dependence on computing technology.  As the organization of people, resources and information involved in many activities (ranging from business and manufacturing processes to public health and health services delivery) becomes more complex, and tasks are distributed over various locations, several problems emerge:  (1) how do we generate plans that allow us to organize the people, the resources, and the information, to get the job done in an optimal fashion; (2) how can we develop a system that can take in information, sometimes from several different sources and in several different formats, and adapt or modify the process automatically; (3) how do we ensure that the process as a whole is behaving as we wish - that is, verify, or guarantee, that at all times and under all situations, the process has properties we want and does not display any undesirable properties.  Workflow technology is designed to provide a reliably repeatable pattern of activity for complex processes, enabled by a systematic organization of resources, defined roles and information flows. Our research focuses on the mathematical foundations of workflow technology that will enable us to find solutions to the above three problems.  We use and develop methods from mathematical logic to automatically generate plans that can be mathematically proved to have properties that we want. Such proofs are especially important in processes or systems where human lives are at stake.  We are working with industry and health authorities to develop prototypes for systems to improve health services delivery.  Interfacing these systems with Canada's emerging electronic health records will provide benefits to all Canadians. In the long term, the results of our research will be applicable to many other economic sectors.""395162,""Macchi, Arturo"
"396802"	"Magen, Avner"	"Geometrical approaches to approximation algorithms: new avenues and analytical limitations"	"Computers rely on algorithms, pre-definied sets of rules, to solve problems. But some problems are simply too difficult and do not allow for an exact solution in a reasonable amount of time even when the input size is modest. In such cases researchers have resorted to ""approximation algorithms"" that work in reasonable amount of time and yet do not compromise too much in the accuracy of their solutions. As the field of approximation algorithms matured, the algorithms that were being designed tended to fall into one of a relatively small number of algorithmic-paradigms.My particular research focuses on some of these approaches, mainly ones that are based on geometric considerations. The first is the ""semidefinte relaxations"" approach, in which we look at a problem with relaxed conditions that is solvable, and the challenge is to relate it with the original one. This technique was shown to be very powerful and has gained much popularity in the last decade. In addition, I study the area of ""metric embedding"" where geometric simplifications are sought in order to transform a hard problem to a simpler one using distances as important parameters.I am also interested in showing limitations these and other algorithmic paradigms, such as dynamic programming, greedy algorithms and backtracking algorithms have. Such insights may indicate new ways of finding algorithmic methods that overcome the obstacles the existing ones suffer from. Last, I am interested in finding sublinear-time algorithms to problems where the typical input size is too large even to be read completely. ""400829,""Magesan, Easwar"
"390660"	"Majumdar, Shikharesh"	"High performance distributed systems: resource management and middleware"	"The availability of computing systems and communication networks at a reasonable cost is rapidly increasing the deployment of parallel and distributed systems. A grid that provides on-demand access to computing, storage, communication and various other types of resources that include wireless devices and sensors is receiving a great deal of attention in various fields including scientific, engineering as well as enterprise computing. Achieving a desired level of quality of service, scalability, high resource utilization and robustness are important for an effective deployment of such systems. Effective management of resources is crucial in achieving these system objectives. With the rapid evolution in both hardware and software technologies inter-operability on a heterogeneous environment is becoming an important requirement for distributed applications that utilize the system resources described earlier. Middleware is the glue that provides such inter-operability.  Due to the multi-billion dollar investment in Web Services (WS) technologies by major software companies, many inter-operable systems of the future are expected to be WS-based. Although WS-based systems can provide inter-operability, if not designed carefully, the concomitant system overhead can give rise to severe performance penalties. Research is proposed for resource management on grids as well as on high performance WS-based systems. The research results will be useful in application design, operating systems, and middleware for grids and other web services-based parallel/distributed systems.The applicant is involved in research on resource management and middleware for distributed systems for a number of years and his work includes pioneering research on parallel job scheduling. His activity in the past six years is reflected in 10 journal papers, 3 contributions to books, a number of refereed conference papers and invited contributions, chairing conference and program committees, and selection as a Distinguished Visitor for the IEEE Computer Society. The applicant's experience, as well as the high caliber graduate students and the facilities available at Carleton University will be useful in achieving the project objectives.""405410,""Mak, Anthony"
"399361"	"Mandryk, Regan"	"Sensing and adapting to user context in human-computer interaction"	"Despite progress in graphics capabilities and processing power, current interactive applications still have considerable usability problems. One main reason for these problems is that applications do not understand or adapt to users' context, such as their location, situation, expertise, or emotional state. As a result, applications often act inappropriately: they provide inappropriate feedback, interrupt the user at the wrong time, increase frustration, and overwhelm the user with unsuitable information. In order to solve these problems, we must make significant advances in two key areas: first, a set of mechanisms for gathering, interpreting, and modeling user context; and second, a set of techniques and strategies for adapting user interfaces and system behaviour based on contextual information. My research program investigates three main themes in sensing user context and adapting user interfaces. The first theme will focus on the detection and modeling of user emotional state when engaged with interactive technologies. The second theme will focus on the detection of user expertise and the provision of intelligent help based on this information. The final theme will focus on the detection of cognitive interruptibility, and the subsequent treatment of interrupting notifications in computer interfaces. Contributions will be made in the areas of user modeling, interface design, intelligent and adaptive interfaces, affective computing, and context-awareness. ""403756,""Mandryk, Regan"
"395714"	"Mao, Zhiwei"	"Next generation wireless communications:  cognitive radio and wireless sensor network"	"The proposed research will investigate different aspects of next generation wireless communications. We propose to investigate cognitive radio technique, which is a new spectrum management framework where unlicensed secondary radio devices are capable of sensing spectrum utilization and effectively adjusting their transmission parameters in order to utilize the large amount of unused spectrum in an intelligent way while not interfering with licensed primary users. We propose to investigate intelligent opportunistic spectrum access in a cognitive radio framework, to develop efficient transceiver designs and error control coding schemes and to develop theoretical analyses of the performance in cognitive spectrum sharing systems. In addition, we also propose to investigate the spatial sampling design in wireless sensor network. The scientific approach will be based on literature survey, analytical modeling and extensive computer simulations. Implementation complexities will also be examined.The importance of the proposed research lies in the demand for the tremendous market for different low-cost high-speed wireless services and applications, and the need to produce skilled graduates to exploit the state-of-art technologies for our economic benefits.""409907,""Mapiour, Majak"
"389587"	"Mark, Jon"	"Wireless communication networks:  network interworking, and resource, mobility and traffic management"	"The proposed research will investigate into resource, mobility and traffic management issues in hybrid wireless/wireline networks (e.g., wireless LANs, cellular and Internet) and interworked wireless mesh networks such as WiFi (IEEE 802.11 series) and WiMAX (IEEE 802.16 series).  The long term objective is to develop in-depth knowledge towards the construction of a flexible and robust cross-domain internetworking architecutre, i.e., an information transport platform, for handling multimedia services in fixed and mobile environments.  The short term objectives are to (i) investigate cooperative diversity with partial channel state information, to be acquired via channel estimation, (ii) investigate dynamic spectral access in cognitive radio, (iii) investigate cross-layer design and optimization for interworked wireless mesh networks, and (iv) develop modeling and analysis methods for wireless Internet.  The results from the short term objectives will form the building blocks toward fulfillment of the long term objective.Communication over wireless interconnected networks has a number of challenging issues, chief among which is the bottleneck created by the wireless propagation channel, the dynamics induced by user mobility.  The research program laid out by the above four objectives addresses very challenging issues, each of which embodies a rich paradigm for contributions of new knowledge, novel signaling techniques, enabling networking technology, etc., and a rich environment for the training of highly qualified personnel.  Protocol design making efficient use of elements and traffic profiles across adjacent layers can lead to flexibility and robustness.  A flexible and robust information transport platform that allows for information transmission from anywhere at anytime with user QoS satisfaction will indeed have a great impact on the social and economic welfare of Canada.""400611,""Market, Brenna"
"392008"	"Massicotte, Daniel"	"Métaheuristiques pour le traitement des signaux à précision finie en communication"	"Le projet vise la proposition de métaheuristiques pour l'annulation des interférences dans les systèmes de communications mobiles offrant le meilleur compromis performance/complexité pour une mise en oeuvre efficiente en arithmétique entière. L'un des problèmes majeurs et communs à tous les systèmes de communications est la présence d'interférences multiples qui nuisent à la qualité des communications. Dans la littérature, plusieurs propositions apportent des solutions théoriques performantes pour reconstituer les informations dégradées, mais peu tiennent compte d'un développement conjoint de l'algorithme et de son architecture en vue d'une intégration sur silicium. En fait, la complexité et le type de structure algorithmique des solutions développées pour ces applications rendent peu efficace leur mise en oeuvre physique en raison des contraintes de temps de calcul, d'exactitude, de consommation de puissance, de coût... La méthodologie proposée consiste à voir le calcul des paramètres du filtre comme un problème d'optimisation difficile plutôt qu'un problème de minimisation d'une fonction de coût comme la descente du gradient. L'originalité du projet réside dans la proposition de métaheuristiques pour l'estimation des informations désirées dans un système dynamique linéaire ou nonlinéaire, optimisant le fonctionnement en arithmétique entière pour un compromis performance/complexité adapté à la technologie de mise en oeuvre cible. L'idée est de pouvoir poser des fonctions multi-objectives capables d'être résolues par des métaheuristiques. De ce fait, en basant le calcul des paramètres, des coefficients et de la structure du filtre sur une métaheuristique discrète, le calcul à précision finie devient intrinsèque à l'algorithme et permet d'optimiser l'adéquation algorithme-architecture selon une fonction multi-objective. Pour valider l'approche, nous proposerons des solutions aux systèmes de communication mobile CDMA, OFDM, MIMO, et Beamforming. À long terme, cette approche mènera au développement de solutions optimales en termes d'adéquation algorithme-architecture pour le traitement numérique des signaux.""391001,""Massicotte, Guy"
"406154"	"McCann, Robert"	"Optimal transportation: geometry and dynamics"	"This proposal focuses on the analysis of a family of optimization and evolution problems having applications in physics, geometry, medical imaging, partial differential equations, and economics.  Linking them together is a transportation problem --- which comes into play whenever the dynamics conserve a local quantity.  It can be caricatured as follows: Given a distribution of iron mines throughout the countryside,  and a distribution of factories which require iron ore,  decide which mines should supply ore to each factory in order to minimize the total transportation costs.  Here the cost per ton of ore transported from the mine at x to factory at y is specified by a function c(x,y).  However, when the mines and factories are distributed continuously over a curved landscape, then the problem has a rich structure and deep connections to geometry, nonlinear partial differential equations and the topology of the cost function, which have only begun to be explored.Incarnations of this problem embed in current models of surprisingly diverse phenomena. Included among the subjects of proposed research are questions related to atmospheric weather patterns, groundwater drainage,  pollutant diffusion, population spreading, and economic decision problems facing informational asymmetry.In addition to these applications (and motivated by them),  this proposal includes a continuing investigation of the structure of solutions --- maps or otherwise --- to the transportation problem in a variety of geometric settings.  Despite much recent progress,  basic questions still remains:  classify all costs for which solutions are unique;  understand when production location will vary smoothly with consumption,  and,  if this is not the case, identify the nature and location of discontinuities.  Although the research is directed primarily at gaining scientific understanding, it has potential applications for resource management, engineering design, weather prediction, improved microeconomic and policy decisions, and logistics.""393066,""McCann, Robert"
"393189"	"McInerney, Timothy"	"Interactive visual analysis of multidimensional medical data"	"The goals of the research are to explore and develop novel software tools, techniques and systems to allow simple, intuitive, efficient, interactive visual exploration, measurement, and analysis of medical data sets, useable by both radiologist and surgeons. The focus will be on tools for 3D and 4D (i.e. time series) medical data, including newer image modalities such as 3D ultrasound. Specifically, tools and techniques will be developed for object extraction and modeling, visual comparison of two objects, object feature emphasis, object manipulation in 3D, object measurement and annotation, tracking changes in objects, object interrelationship analysis, volume image navigation, and sharing and recording of multimedia diagnostic reports and surgical plans. The integration of user collaboration mechanisms is also a major focus, allowing multiple users to view and interact with the image data or with diagnostic reports.""397762,""McIntire, Eliot"
"402629"	"Mi, Zetian"	"Antimony-based self-organized quantum dots:  epitaxial growth, characterization, and photonics at the nanoscale"	"The driving forces for future technologies have increasingly relied on the exploration of novel nanoscale materials. Antimony (Sb)-based self-organized semiconductor nanostructures, such as InAs(Sb)/InGaAsSb and InSb(As)/InAs quantum dots and nanowires, a class of novel materials with tremendous importance for infrared and terahertz photonics, have remained largely unexplored. In this program, the PI will investigate the growth and fundamental characteristics of such nanostructures and develop near-, mid-, and far-infrared nanophotonic devices, providing enabling solutions for critical technologies in communications, spectroscopic sensing, medical imaging and future quantum computing. In the short-term, the PI will develop 1.55 µm InAs(Sb)/InGaAsSb quantum dot lasers and amplifiers on GaAs that can significantly outperform their quantum well counterparts, for applications in fiber optical communications. In the medium-term, the PI will investigate the epitaxial growth and characterization of Sb-based mid-infrared quantum dot heterostructures and extend, for the first time, the demonstrated InAs quantum dot technology developed by the PI and other researchers, to the mid- and far-infrared frequency ranges. The third focus is the development of novel quantum dot and nanowire heterostructures with precisely controlled properties by utilizing the novel technique of selective area growth on nano-patterned substrates, with the long-term goal is to realize electrically-injected ultrahigh-speed nanoscale lasers and high-efficiency single photon sources, fundamental building blocks for future quantum computing and information processing systems. The proposed research program extends across several frontiers of science and engineering and will provide an exceptional opportunity for students to acquire knowledge and gain expertise in epitaxial growth, nanofabrication, and characterization of nanophotonic devices and, therefore, prepare them well for future careers in emerging areas of nanoscience and nanotechnology.""393740,""Miadonye, Adango"
"395521"	"Michelson, David"	"Propagation and channel modelling for wireless communications"	"As wireless communications moves to higher frequencies, new environments, and more demanding applications, and as end user expectations for performance and reliability increase, it has become increasingly necessary to either develop new propagation models and/or re-estimate the parameters of existing ones so that appropriate techniques for mitigating the effects of fading, delay spread, and time variation can be devised.We propose to develop and apply the instrumentation, tools and techniques required to characterize the propagation channel in three environments of extreme interest to wireless system developers: (1) fixed and pedestrian channels in urban and suburban macrocell environments, (2) fixed and pedestrian channels in industrial and health care microcell environments and (3) Ka-band links from earth to satellites in low earth orbit. The propagation and channel models that we develop will capture our knowledge and understanding of airlink impairments in a form useful in design, simulation and test. As such, they will play important roles in the software and hardware simulators and RF planning tools used at all stages of the product development life cycle, from system engineering to implementation to manufacturing to deployment. The results will broadly benefit the Canadian wireless industry, including manufacturers, consultants, operators and end users.""388591,""Michener, Gail"
"394411"	"Miri, Ali"	"Secure distributed information systems"	"Advancements in computer and communication technologies are rapidly changing how users and computers interact with each others. In this new computing paradigm,  information can be gathered from a network of tiny sensors, processed by portable and wireless devices with various communication capabilities, and stored across distributed databases.  Privacy and security represent a major challenge in such  distributed information systems. This proposal describes our plan to design better and more efficient sensor and Radio Frequency IDentification (RFID) networks, where information is transmitted between the nodes in a secure fashion, and intruder and malicious activities are easily identified. We will design new light-weight cryptographic tools ideal for use in resource-constrained devices, and build algorithms which allow sharing information across different databases while ensuring the privacies of parties involved.""393018,""Miron, Gilles"
"402851"	"Modersitzki, Jan"	"Constrained imaged registration"	"Image registration is a commonly used technique to align images or data acquired from different positions at different times or with different devices. The objective of image registration in a clinical environment is to align these views in order to produce a meaningful diagnosis or treatment planning. For example, the diagnosis and treatment of female breast cancer may require a time series of three-dimensional magnetic resonance images (MRI), visualizing the contrast uptake in the tissue. This uptake, divided into the so-called wash-in and wash-out phases, is of particular interest, since it characterizes tissue properties. Assuming the female breast to be static, a point based analysis would provide the uptake characteristics as a function in time. However, the acquisition time is usually around ten minutes and due to motion of the patient, like heart beat or breathing, the breast is not static and the interpretation of the data is weakened and complicated by motion artifacts.Registration techniques are used to automatically compensate for image differences that are related to motion. However, common schemes do not prevent a change of volume of the processed data. This is very delicate, if tumor growth is under consideration. For these applications, only a volume preserving scheme guaranties that changes of volume is due to measurement and not to the alignment. Unfortunately, there are only two approaches to deal with volume preservation. The first one is suggested by Torsten Rohlfing et al and uses a spline based parametric transformation and penalty based on the integral over volume changes. This approach has some disadvantages. An alternative approach has been suggested by Eldad Haber and myself and is based on a more general non-parametric transformation and explicit constraints on a voxel basis. The essential advantage of the our approach is that it guarantees volume preservation, as has already been shown. However, this approach is computationally quite demanding and since a rigorous implementation is still missing, it can only handle low-resolution data. More research is needed to make this method clinically available.""395670,""Modesto, Sean"
"402630"	"Moez, Kambiz"	"Development of millimeter-wave CMOS transceivers for gigabit wireless personal area networks"	"The unlicensed 60-GHz band offers a large seven-gigahertz bandwidth and great interoperability around the world for development of high-data-rate short-range wireless communication systems. However, there are two major obstacles in the way to widespread use of 60 GHz links. First, as these systems have traditionally been implemented in expensive compound semiconductor processes, the high cost of fabrication has prevented the 60 GHz links from being an economical alternative to low-GHz wireless systems. Second problem is the high absorption of material, especially oxygen, at millimeter-wave frequencies, leading to high path loss and making the transmission of 60 GHz signals very difficult. To address these problems, we define our research proposal as the design and implementation of fully-integrated low-cost millimeter transceivers in a nanometer CMOS technology. Very recently, aggressive CMOS scaling into the nanometer regime has produced transistors with cutoff frequencies well over 100 GHz, enabling the CMOS technology to be used for implementation of 60-GHz transceivers. The full integration of mm-wave transceivers minimizes the fabrication cost by reducing the dimension of a 60-GHz wireless radio to a few square millimeters as well as eliminating the cost of packaging. Therefore, 60 GHz transceivers can be implemented at the lowest possible cost in CMOS technology.  On the other hand, as CMOS offers highest level of integration of all semiconductor technologies, several transceivers and antennas can be integrated on a single CMOS chip. A multiple-input-multiple-output scheme complemented by a smart antenna array structure can effectively compensate for the path loss of material, making the detection of 60 GHz signal easier than for a single-transceiver single-antenna system. However, there are several challenges  in the design of circuits at frequencies very close to limitation of the technology and implementation of high quality passive devices on lossy silicon substrate. These challenges will be tackled in this research project through discovery of new modeling methods, innovative circuit techniques, and efficient system architectures.""388309,""Moffat, Anthony(Tony)"
"406156"	"Mohar, Bojan"	"Topological and algebraic graph theory"	"This research involves study of interconnections between graphs, topology, and geometry, graph minors, colorings and nowhere-zero flows, and development of algorithmic and computational tools in these areas. The research program is very broad with a wide selection of open problems and conjectures in the following areas: structural and geometric theory of embedded graphs, algorithms and obstructions for graph embeddings, crossing numbers, graph minors, conjectures of Negami and Barnette, circular colorings and flows, and eigenvalues of graphs. As diverse as these problems are, they have much in common, and this research is aimed at finding new links between them, leading to solutions of open problems.""396585,""Mohar, Bojan"
"406157"	"Mori, Gregory"	"Recognizing human figures and actions"	"In this proposal we describe approaches to tackle the problem of humanactivity recognition.  The grand goal in this area of computer visionis to build systems that can automatically find human figures ineither still frames or video sequences, and determine what action theyare performing.  The recognition of human figures and actions is animportant problem, due to its many practical applications.  Thistechnology is directly applicable to human-computer interaction, imageand video retrieval and search, security and surveillance, videomotion capture, and automated vehicle driver assistance systems.""396736,""Mori, Gregory"
"406158"	"Morin, Patrick"	"Algorithms for robust multivariate statistics"	"Mathematical statistics provides many useful analysis and interpretation techniques that can be used in any field where experimental or measured data is present. However, some of these techniques require large amounts of computation that make them difficult to use on large data sets that occur increasingly often. The proposed research project will focus on finding more efficient ways of using a computer to apply power analysis techniques from mathematical statistics. This will allow the application of these techniques to massive data sets in order to perform analyses that were previously not possible. Such data sets occur, for example, in geographic information systems, remote sensing, satellite imagery, medical imagery, business management systems, and bioinformatics.""394789,""Morin, Patrick"
"393192"	"Mosca, Michele"	"Quantum computing and cryptography"	"In the beginning of the 20th century, scientists realized that the classical framework for physics was wrong. This led to the creation of a new mathematical framework for physics, called quantum mechanics.  A broad array of physical theories have been developed in this new framework and tested with remarkable accuracy. By the end of the 20th century, scientists were not just observing quantum phenomena; they were also controlling them with greater and greater precision. This was the beginning of the ""harnessing"" of the quantum world. The degree to which quantum phenomena have been controlled to date is still rather limited in comparison with what we believe is actually possible. One of the greatest challenges of the 21st century will be to fully harness the power of quantum mechanics. We already know some of the consequences, like some unbreakable cryptographic primitives, and efficient solution to some problems widely believed to be intractable on conventional ""classical"" devices. Just as we had very little idea about the applications and impact of classical computers in the 1940s, we likely have only scratched the surface of the capabilities of this fundamentally new form of information processing. There are three main themes in my proposed research program, with a strong inter-relationship.""Quantum Algorithms and Complexity"": The first objective is to continue to discover the capabilities and limitations of quantum computation.""Quantum Cryptography and Testing"": Another objective is the development of new cryptographic tools that can be used in the next generation information security infrastructure. ""Implementation of quantum information processing"": Lastly, I plan to apply computer science ideas and approaches to develop more tools that will advance and accelerate the development of scalable quantum information technologies.""413403,""Mosca, Michele"
"395361"	"Munzner, Tamara"	"Building and evaluating information visualization systems"	"My area of interest is information visualization, or infovis. I design, build, and evaluate systems that use interactive computer graphics to help people understand the structure of large, complex datasets in order to carry out particular tasks. My long-term research agenda is the development and validation of a more complete set of design guidelines for the field.A balanced visualization research portfolio can have many aspects. In system design, I pursue a mix of user-driven and technique-driven projects. In evaluation, I am interested in both the low-level characterization of human visual response, and the higher-level comparison and characterization of specific visualization techniques.In the future, the need for information visualization systems will become increasingly acute. One of the few quantities increasing faster than the CPU speeds predicted by Moore's Law is the amount of data that we generate to process. This explosion of data comes from many sources: processors with the ability to log events have become interwoven with the fabric of daily and business life; sensors have become small, cheap, and networked; and the growing feasibility of simulation allows the gathering of data about virtual rather than real-world events. Data collection is not an end of itself, but a means to the end of helping humans deal with the world. Computer-based visualization allows humans to wend their way through these mountains of data, making decisions based on understanding.""394130,""Muraki, David"
"395426"	"Nayak, Ashwin"	"Efficient quantum algorithms and protocols"	"Quantum mechanical computers, when built, will considerably alter the information processing landscape. They offer provably efficient solutions to problems believed to be beyond the capability of current computers. At the same time, they compromise the security of widely deployed cryptographic schemes.We plan to design new quantum algorithms for a variety of problems in combinatorial optimization and statistical physics. These algorithms, based on novel properties of quantum walks and quantum Fourier transforms, would be provably faster than those known today.  We also plan to devise efficient protocols for fundamental cryptographic tasks that are secure in the strongest possible sense---even in the presence of quantum computers.  In the process, we hope to discover new constructions of combinatorial objects akin to randomness extractors, as well as ways of analysing strategies of malicious parties in cryptographic scenarios with methods from convex optimization.  Finally, we wish to develop techniques in quantum information theory to address important questions regarding quantum and classical communication.The study of quantum information and computation has revealed powerful concepts whose consequences are percolating into classical computing as well as other areas of mathematics. We expect that our research will have further such impact.""411972,""Naylor, David"
"406159"	"Neufang, Matthias"	"Abstract harmonic analysis - beyond the classical realm"	"My research explores the fruitful interactions between functional analysis and abstract harmonic analysis, which naturally take place in the framework of Banach and operator algebras. The latter field originated historically from quantum mechanics, and has since then maintained close connections to this and other areas of mathematical physics. One of the most recent developments is the theory of operator spaces and quantum groups, which provides an exciting blend of functional analysis, harmonic analysis, algebra and topology, and has applications to quantum information theory and quantum computing. The proposal mainly aims at thoroughly investigating the structure of certain natural Banach algebras arising in abstract harmonic analysis, i.e., algebras over locally compact groups, as well as their 'quantized' analogues, defined over quantum groups. The key idea is to follow a representation theoretical approach: the objects of interest can be viewed as completely bounded maps on the algebra of bounded linear operators on a suitable Hilbert space. This enables us to use the powerful machinery of von Neumann algebras and operator spaces. In recent joint work with M. Junge and Z.-J. Ruan, we have thus succeeded, for instance, in characterizing one of the most fundamental concepts of Fourier analysis - Pontryagin duality - in terms of a commutation relation, even in the general context of locally compact quantum groups. Moreover, we have been led to exciting discoveries in quantum information theory, namely the construction of new quantum channels with highly desirable properties (in particular, in the setting of finite-dimensional quantum groups, the completely bounded entropy of these channels can be calculated explicitly). One of the main goals pursued in this line of research is the construction of a certain locally compact group associated with every locally compact quantum group, which is an invariant of the latter, and allows us to transfer ideas from classical harmonic analysis to the realm of quantum groups - a fascinating program that I have just started to develop with one of my Ph.D. students, M. Kalantar.""395274,""Neufang, Matthias"
"390816"	"Neufeld, Eric"	"Smart visualizations of multivariate knowledge"	"William Playfair (1759-1823), considered by many to be the parent of statistical graphics, would likely have been dazzled by modern consumer spreadsheet software. In a single program, one can place thousands of pieces of data, and with a few mouse clicks, compute summary values, and display distributions of data in pie, bar, line, and scatter graphs. Furthermore, a change to a single data point can be propagated immediately through the statistics and to the graphics, letting lay persons easily produce the graphs. But he also might have been surprised at how many of the visualizations were ones he developed in his lifetime, and at the fact that current software generally manipulates relationships between only a few variables at a time. I believe we are at a point in the research frontier where it is possible, in real time, to automatically generate complex conjectures about probabilistic and causal relationships among large numbers of variables from observational data, to visualize such relationships in an intuitive way, to identify weaknesses and strengths in human generated conjectures (where computer-generated conjectures are ambiguous or incomplete), and to use these diagrams effectively to understand the potential consequences of policy changes. The present work draws on and extend results from my previous work, and the work of others, in the areas of artificial intelligence, graphical models, data visualization, information visualization, and computer graphics to produce algorithms and techniques that will give ordinary users of data (i.e, persons who are not trained statisticians) an intuitive visual interface for productively exploring the conjecture space of datasets, interacting as necessary. A proof of concept software implmentation already exists, and preliminary results have already been published.""401737,""Neufeld, James"
"395087"	"Nguyen, UyenTrang"	"Efficient and secure group communications in wireless mesh networks"	"In a wireless mesh network (WMN), wireless routers provide multi-hop connectivity from a host to either other hosts in the same network or the Internet, enabling networking capability where a wired infrastructure is unavailable or expensive to deploy (e.g., rural areas, water surfaces, hard-to-wire buildings).   Group communications (or multicast) are a form of communication that delivers information from a source to a set of destinations simultaneously in an efficient manner.  Multicast supports many important applications such as distribution of stock quotes, financial data, billing records, software, newspapers, pay-per-view movies; audio/video conference; distance learning; and distributed online games.  Although multicast plays an important role in these applications, research on multicasting in WMNs is still in its infancy.   Few well-designed and well-tested solutions are currently available, and they focus on only one aspect of communications (e.g., routing).   In this long-term project, we propose a comprehensive suite of protocols to support efficient and secure group communications in WMNs, which addresses the following main problems:  routing; flow/congestion control; quality-of-service guarantees for real-time and multimedia applications; reliable multicast; network security provision; host mobility management; and inter-operations with the Internet and other types of wireless networks .   Wireless mesh networking has proven to be a key technology for next-generation wireless networks.   This project, upon completion, will enable multicast services to be extended to areas served by wireless mesh technologies such as remote areas, small towns,  water surfaces, and transporation systems.   The success of the project not only contributes to the advancement of wireless networking, but also makes significant impacts on Canada's economy (distribution of financial data, advertising), health care system (remote medical diagnosis), technology (distributed interactive simulations, multimedia communications), education system (distance learning), social communication (news distribution, tele-conferencing), and entertainment industry (pay-per-view movies, online games).""401786,""Nguyen, VanThanhVan"
"406160"	"Nie, JianYun"	"Adaptive and efficient inferential information retrieval"	"Current search engines are limited in their capability to infer the implicit information from a user query. They only use word matching to retrieve documents.This research proposes to exploit different types of contextual information to infer the user's intention behind a query. Contextual information includes user's characteristics or profile, his search history, his domain of expertise, etc. By exploiting contextual information, we can arrive at a better understanding of the user's information need, thus retrieving better documents for the user.""406486,""Nie, JianYun"
"391144"	"Nie, JianYun"	"Adaptive and efficient inferential information retrieval"	"Current search engines are limited in their capability to infer the implicit information from a user query. They only use word matching to retrieve documents.This research proposes to exploit different types of contextual information to infer the user's intention behind a query. Contextual information includes user's characteristics or profile, his search history, his domain of expertise, etc. By exploiting contextual information, we can arrive at a better understanding of the user's information need, thus retrieving better documents for the user.""407420,""Nie, Xiaoxi"
"392063"	"Noumeir, Rita"	"Integration of healthcare information"	"Prompt access to relevant clinical information is crucial when making healthcare decisions. The Electronic Health Record (EHR) promises better care by providing centralized access to different sources of data that make up the patient medical record. Such data includes the patient medical history, diagnostic reports from various care domains, images and other clinical data in electronic format. In addition to enabling timely access to relevant clinical information, EHR creates totally new opportunities for assisting in clinical tasks and for guiding medical decisions. As access to information originating from various sources is becoming possible, care decisions can be enhanced from information integration. Information integration aims to obtain a unified comprehensive view by integrating information that is extracted from data gathered at various sources. The extracted information needs to be semantically integrated, organized, and presented in order to help in the clinical investigations. On the other hand, with the EHR, the issue of data security has become increasingly important. Confidentiality needs to be ensured by controlling the access to personal health information as expressed by the patients' electronic consent. Within the framework of the proposed research program, we will explore a new access control framework based on the patients consent. We will also explore novel approaches for information integration, organization and presentation to help in information navigation. Furthermore, we will investigate innovative methods to identify and co-locate features from multi-modality images, such as X-rays and Magnetic Resonance Images, in order to assist in breast cancer assessment.""407016,""Nouraei, Hirmand"
"391206"	"Opal, Ajoy"	"Algorithms for analysis of analog circuits"	"This proposal develops new algorithms for the analysis of nonlinear analog circuits, such as, amplifiers, filters, switched capacitor, switched current and oversampled delta-sigma modulators. These circuits are currently used in consumer products, such as, cell phones, telephones and audio equipment. A common method used in the design of these circuits is to simulate them on a computer using specialized software for their analysis. Computer simulation is used because it is cheaper and faster to perform, as compared to fabricating a test circuit. Computer simulation also allows the testing of many what-if scenarios to choose the best design for a given situation. Thus, it is important to analyze or simulate these circuits accurately and with minimum cost.These circuits can be analyzed by general purpose analog circuit simulators, such as Spice. However, general purpose simulators require considerable execution time, or can not simulate these circuits because they do not take into account the specialized structure of these circuits. The aim of this research is to make use of the specialized structure of these circuits and to provide fast and accurate algorithms for their simulation. The analysis of these circuits will be done in the time and frequency domains, including the calculation of the response, sensitivity, noise and distortion. It will lead to the design of better circuits with higher performance, such as, lower power consumption or higher speed of operation.Graduate students trained in the analysis and simulation of nonlinear analog circuits will be capable of developing specialized software used in designing high performance circuits used in consumer electronics.""401145,""Opapeju, Florence"
"406161"	"Panario, Daniel"	"Mathematical analysis of algorithms, and computations in finite fields"	"The overall area of my research is the design and analysis of algorithms. My recent research has centered around two lines: the study of algorithms when randomness is considered, and the investigation of algorithms in finite fields and their applications. Probabilistic analysis of algorithms comprises the study of algorithms when randomness is used. In particular, average-case analysis of algorithms focus on the understanding of the behaviour of algorithms on random inputs. An essential goal of the area is to advance in the understanding of randomness. Immediate goals are better understanding of particular algorithms, and the development of new efficient algorithms based on this knowledge. The scientific approach is based on mathematical proofs involving generating functions for counting the properties of interest for the analysis of the algorithms, and asymptotic analysis when exact counting is not possible. My goal in this area is two-folded: to develop algorithms for fundamental problems like sorting and searching with better performance than previously known algorithms, and to contribute to the mathematical understanding of random polynomials over finite fields. My second line of research involves computations in finite fields and their applications. Algorithms that work with finite fields play a fundamental role in practical applications in cryptography, coding theory and information theory. My goal in this area is the design of efficient algorithms in finite fields, and the usage of elements from the theory of finite fields in Engineering and Computer Science applications. Examples are efficient implementation of arithmetic using normal elements and irreducible polynomials; constructions of certain types of polynomials over finite fields (irreducible, primitive, and so on) and their applications to fast LFSR (Linear Feedback Shift Registers) and codes constructions.""394048,""Panario, Daniel"
"402859"	"Pasquier, Philippe"	"Theory and practice of argumentation based-negotiation"	"Automated negotiation provides an important foundation for technical services in the information society (pulled by applications like e-commerce and pushed by technologies like web-services and grid computing). Mechanisms in which software agents exchange potential agreements (offers) according to various rules of interaction have become very popular in recent years. These mechanisms try to accommodate the agents' preferences and include: game-theoretic analysis and heuristic-based approaches. A growing body of research is now emerging that points out limitations in such mechanisms and advocates the hypothesis that agents can increase the likelihood and quality of an agreement by exchanging arguments that influence each others' mental states (and thus preferences). This is the idea of argumentation-based approaches to negotiation. While this idea is not completely new, no work has provided evidence of the advantage of argumentation-based negotiation so far.The long-term aim of that research is to prepare the next generation of negotiating agents that would subsequently integrate argumentation-based negotiation techniques. In order to do so, we will develop formal and computational models of argumentation-based negotiation of increasing complexity and provide for each of them an analytical (when possible) or empirical evaluation along with a comparison with traditional approaches to automated negotiation.""398395,""Pass, Brendan"
"402858"	"Patriciu, Alexandru"	"Automatic manipulation of soft tissue under image guidance"	"Robotics is a multidisciplinary research field requiring knowledge from several disciplines including computer science, electrical engineering, and mechanical engineering. Robotics researchers searched for new application fields and identified the medical interventions as one with many potential applications. Although promising results have been achieved, medical robotics still pose a number of challenges that can be answered only by the joint effort of engineering and medical researchers.Commercial master-slave medical robots have been developed and they have already a well defined place in the medical field. On the other hand the systems for image guided interventions are still in research stage. A robotic system for image guided interventions is required to automatically perform an intervention using a plan defined by a physician using medical images. Therefore the 'intelligence' required from the robotic system is higher then the one required from a master slave - manipulator which moves under surgeon control.Our research program focuses on enhancing image guided medical robots with the ability of performing basic procedures on soft tissue. The robots will be able to execute a soft tissue intervention defined using medical images and compensate deformation incurred during the execution. Examples of such basic procedures are image guided needle insertions, soft tissue manipulation, soft tissue cutting. This research is expected to have an impact on medical robotics by enabling automatic soft tissue interventions. The control algorithms that we will develop have the potential of enabling long distance telesurgery. In this approach the remote robot will carry out automatically using image and force feedback operating plans sent from the master controller. This may overcome the inherent problems caused by the transmission delay in the case of traditional master-slave control over long distances. Last but not least the control algorithms are expected to be instrumental in other fields that require deformable object manipulations like food industry and deformable object assembly.""390965,""Patrick, George"
"395219"	"Pavel, Lacra"	"Control algorithms for robust and adaptive photonic networks"	"Recent years have seen an increase in demand for broadband services like video, voice and data transmission, while users increasingly demand anytime network connectivity. To satisfy these needs, the communications industry is moving towards dynamic networks for multiple transmission media - cable, wireless and optical fiber. As the backbone of the internet, photonic networks must become dynamic themselves, from the core to the access points, and must be adaptive to changing traffic and link conditions.      Our research is focused on how to bring intelligence into photonic networks: how to develop rigorous algorithms that will make networks robust, flexible and self-adapting. We will address the challenge of how to make them robust by uniquely treating optical networks as dynamical systems. We will create new algorithms for automatic network control and self-management, by using system theoretical methods. These algorithms will allow photonic network capacity to be set up, configured and re-adjusted with minimal human intervention. As an alternative to ad-hoc approaches, the system theoretic approach of our research will provide a rigorous basis for photonic network algorithm design.    Why is this so important? Because a solid theoretical foundation is required upfront if we are to develop scalable tools and techniques for dynamic photonic networks, that move beyond heuristics and ensure network performance and stability.  When successfully deployed, these capabilities will bring benefits to time-critical and high-bandwidth applications, hence benefits to Canadian information technology economy.""391671,""Pavelka, Mary"
"395630"	"Pearce, Trevor"	"V-Sim framework and methodology"	"The new Carleton University Centre for Advanced Studies in Visualization and Simulation (V-SIM) is a CFI-funded centre for interdisciplinary research involving the extensive applications of modelling and simulation. The targeted research typically includes interactions with human and/or specialized hardware components, and the simulations are deployed on distributed systems. The resulting style of simulation is a blend of traditional simulation components with distributed real-time systems. The Reality in the Loop lab is located in V-Sim and is focusing on the engineering of this style of simulation. The proposed V-SIM Framework and Methodology (V-FAM) research will be based in the Reality in the Loop Lab, but will be useful across a spectrum of projects having different simulation goals. The framework will provide guidance for the underlying organization of the simulation, and will prescribe interaction patterns among simulation components. The development methodology will emphasize component reuse while migrating from general-purpose to target-specific platforms, yet will accommodate the need to integrate and extend components that were not originally designed for a specific simulation's needs. Experiments in V-SIM will provide both the motivation, requirements and setting for this research. Ideally, V-FAM will be an exemplar of best practices for the timely and practical development of interactive distributed simulations.Modelling and simulation is touted as an influential direction in future applications like national safety, entertainment, rapid prototyping, education, training, and biomedicine. These applications are highly interdisciplinary, yet their success hinges on the engineering capability to develop and deploy distributed simulations. V-FAM research will illuminate and advance the engineering state-of-the-art in this area. The results will be of broad interest to the modelling and simulation community, and of direct interest to DND. ""410184,""Pearlman, Joshua"
"406162"	"Pei, Jian"	"Analyzing and Mining Large Uncertain Data Sets: models, methods and applications"	"Uncertain data are inherent in some important applications, such as environmental surveillance using sensor netoworks, market analysis using customer surveys, and quantitative economics research. Uncertain data in those applications are generally caused by factors like data randomness and incompleteness, limitations of measuring equipment, delayed data updates, etc. For example, sensors are often used to detect presence of endangered, threatened, or special concern risk categories of animals in remote or preserved regions. Due to limitations of sensors, detections cannot be accurate all the time. Instead, detection confidence is often estimated to model uncertainty. Analyzing and mining uncertain data collected from such sensors can enable biologists to capture and understand interesting behavior patterns of wild animals. As sensor network technology becomes mature and economical, in the near future, many large sensor networks will be deployed to monitor our broad homeland and environment, including forests, glaciers, and bodies of water. The knowledge from such uncertain data will be essential for Canada and BC to deal with many grand environmental and social challenges such as global warming and its consequences, environment and natural resource conservation, homeland security, and safe working environment. Analyzing large collections of uncertain data has become an important task. The objectives of this project are to systematically investigate the models of effective and practical analytic queries and mining tasks, develop efficient methods, and apply such techniques to emerging uncertain data intensive applications such as sensor networks and Web mining. Doctoral and Master's graduate students will be trained to answer the emerging demand for highly qualified personnel equipped with the expertise in uncertain data processing. Uncertain data processing will also be introduced into advanced undergraduate and graduate databases courses. Research results will be widely disseminated by publications in major conferences and journals. Tutorials will be delivered to major conferences, too.""396712,""Pei, Jian"
"393301"	"Pike, David"	"Combinatorial algorithms, structures and applications"	"This program of research studies computational and theoretical problems in graphs and networks. Graphs and networks each consist of a collection of nodes, some pairs of which are connected by edges. As one example, consider the network that we can construct from the World Wide Web: let each webpage be represented by its own node and then join node A to node B if webpage A has a link to webpage B.  This example, known as the Web Graph, contains the structural information of the web, but none of its content.  One key question that we now want to ask is whether it is possible to find collections of webpages that share related content, when all we have to work with is the network structure of the Web Graph.     Another research goal involves studying decompositions of graphs. As an example of a problem that can be modelled as a graph decomposition, suppose that N delegations, each with an equal number of delegates, are in attendance at a convention.  During the convention there is a banquet at which there are only round tables, each able to seat T people.  A seating arrangement of the delegates so that each pair of delegations has two delegates seated side by side, and also so that no pair of people from the same delegation sit at the same table is the same as a type of graph decomposition known as a T-cycle system of order N. One kind of question that we want to study is: how many different seating arrangements are there for particular values of T and N?  Supposing that the delegations have been grouped into alliances such that each delegation belongs to at most one alliance, we can also ask: how many different alliances do there have to be so that there is a seating arrangement where no table has all T of its people from the same alliance (in addition to the previous requirements for the seating arrangement)?  This type of question can be solved by studying colourings of graph decompositions.     In addition to theoretical advances involving graphs and networks, this research program has applications that range from seating arrangements and scheduling, to intelligence gathering based on analysing networks, to studying genetic and ecological connectivity in marine species, and to aspects of human population genetics.""411432,""Pilcher, Christopher"
"406163"	"Pillay, Pragasen"	"Energy efficiency and renewable energy"	"Motors constitute the largest % of loads in the electrical power system (over 50% in developed countries). While their initial requirements were for fixed speed applications, they are increasingly required to work at higher speeds and designed to work with non-sinusoidal excitation. The core losses at high frequencies and during non-sinusoidal excitation, is poorly understood. Recent work has been conducted by the applicant on advanced instrumentation to measure the losses, but a theoretical foundation still needs to be developed. One component of this work is to examine the widely used core loss formulas and to understand their limitations. Then both an analytical and a numerical approach will be taken, using Maxwell's equations, to produce better, more accurate formulas for loss calculations in electrical machines of different designs.Wind generators have come of age, particularly at the utility level MW range. A smaller number of choices exist in the small range (less than 10kW), yet there is a significant market in the medium range (10-300kW). While the machine type at the utility level and the small range have been largely agreed upon, in the medium range this is still an open question. This research will examine the design and control strategies that are competitive in this range.Of particular concern to the operators of wind utility generators are reliability and the detection of faults. Advanced signal processing and mathematical algorithms will be applied to detect machine, gearbox or other faults to improve protection, reduce serious damage and even predict time to failure. This will improve the reliability and reduce maintenance costs.""403280,""Pillay, Pragasen"
"393197"	"Pizzi, Nicolino"	"Classification methodology for mitigating the effects of unreliable class assignments in high dimensional biomedical data"	"Classifying biomedical data involves finding a mapping (relationship) from patterns (e.g., data relating to some type of tissue) to a set of classes (e.g., disease states). Patterns are represented by features (e.g., concentrations of biological compounds) and class labels are assigned using a reference test (e.g., a medical expert's analysis of tissue being ""normal"" or ""abnormal""). This process often suffers from three significant challenges: the number of features in a pattern is high; the number of patterns is low; and the reference test may be unreliable. The first two challenges, known collectively as the ""curse of dimensionality"", cause an inability to find robust, general solutions. This is often addressed by reducing, in some fashion, the number of features; however, a direct correspondence back to the original features is necessary for medical experts to make informed judgments about the mapping's predictive power. While reference tests may be well-established benchmarks, they are seldom perfectly accurate and sometimes improperly applied. Nevertheless, any strategy that compensates for this imprecision must ensure that the mapping is correctly validated against the benchmark.Techniques using Computational Intelligence (CI) - adaptive, learning, or evolutionary algorithms - have demonstrated their effectiveness in data analysis. This proposal involves the development of a comprehensive CI based Classification Methodology to mitigate the effects relating to many features, few patterns, and an unreliable reference test. Its novelty and significance is in its tightly integrated approach, where the enterprises - accurate classification, effective feature reduction, unreliable class assignment mitigation - incrementally inform and drive each other in a unified manner. This methodology will be instrumental in the analysis of complex biomedical data and will lead to more reliable classification systems. Further, while this investigation involves methodological and foundational concepts, it will simultaneously deal with ""real-world"" data. This synchronization of the fundamentals with their use in the specific application domain will ensure an efficacious and coherent comprehension of the underlying processes and will generate tangible practical outcomes.""404427,""Pjontek, Dominic"
"406164"	"Plett, Calvin"	"Radio frequency and broadband circuits"	"The proposed research consists of the exploration of fully integrated ultra-low power, short-range wireless communications circuits These circuits will be used to communicate to and from sensors, and potentially for radio-frequency identification (RFID). To achieve ultra-low power, these circuits will communicate with pulses which allows the system to be turned off easily between a set of pulses.Novel aspects of this research include the exploitation of on-chip antennas and of a fully integrated power source making use of scavenged power, where scavenged power sources could be ambient light, radio frequency fields, thermal gradients or vibration. While pulse mode communications systems, fully integrated antennas, and on-chip power sources have been demonstrated individually, the novelty here is that they are all used together, and the circuits are being designed with the antenna and power source in mind. As a result, the overall transceiver has the potential to be lower in power, smaller (of the order of a few square mm), and being fully integrated, to be lower cost. The small size is of obvious benefit to bio-medical sensors where large size can cause discomfort to patients to whom biomedical sensors are attached. As one example, in the case of sensors used in radiation therapy, small size and the absence of a battery results in less blockage or deflection of radiation. In RFID applications, small size will allow smaller items to be tagged, for example, jewelry. Benefits of low cost could allow these devices to be considered disposable in biomedical applications. In RFID applications, lower cost means it is more likely these devices would be used to identify lower cost items, for example to replace the bar codes in a grocery store. The design of the proposed low-power, low-cost communcations systems as applied to biomedical sensor would benefit the medical community. If the final goal of replacing barcodes is achieved, such a system would be of enormous benefit to all consumers. And finally, achieving lower power, smaller size and lower cost are universal goals in the electronics industry, so aspects of this research could be applicable to other fields, for example, consumer electronics, communications, and entertainment.""391124,""Plett, Calvin"
"396800"	"Poupart, Pascal"	"Prior knowledge elicitation and policy explanation for decision-theoretic planning and learning"	"Consider spoken-dialogue managers, mobile robot controllers, automated monitoring/prompting systems for seniors with dementia or any other complex system that must accomplish a fairly complicated task.  The conception of such systems is particularly challenging due to the noisy nature of the sensors (e.g., noisy speech recognition, noisy sonars) as well as the uncertain and interdependent effects of system actions (e.g., uncertain effect of prompts on seniors, interdependent and noisy motor controls in robotics).  As a result, it is generally impossible to design complex robust systems by hand coding control policies.  The fields of decision-theoretic planning and learning have made significant advances in the development of automated techniques to generate robust control policies that could revolutionize the next generation of computer systems.  Instead of programming a policy directly, an algorithm is used to optimize a policy based on a model or simulator of the system and its environment.  However, eliciting the domain knowledge necessary to specify a model or simulator, and validating/explaining the resulting policy are two major bottlenecks ignored by the research community that are holding back the adoption of this disruptive technology.  Knowledge elicitation and policy explanation are particularly challenging since non-technical domain experts tend to have partial and imprecise knowledge, and often need high-level explanations of the policy where technical details are abstracted away to better convey the intuition.  Hence, the objectives of this research are i) to design general and principled techniques to elicit and encode partial/imprecise domain knowledge about the system, the environment and the desired policy, ii) to develop algorithms that can exploit as much domain knowledge as possible to improve scalability, and iii) to create generic tools to validate and explain the decisions made by a policy at an appropriate level for developers and non-technical experts.""410795,""Poupore, Jessica"
"391362"	"Prusinkiewicz, Przemyslaw"	"Computational models of plant development and form"	"How organisms acquire their form is a fascinating basic research problem in developmental biology. The form of an organism is largely determined by its genetic makeup, yet molecular-level phenomena do not control form directly. Instead, they set the stage for a cascade of developmental processes, which take place at different levels of plant organization and eventually yield the final form. These emergent processes are increasingly studied using data-driven computational models, simulations, and visualizations, which complement and help interpret the results of biological experiments. In the general scope of these studies, my research is focused on:A. Model-based studies of plant development.  In collaboration with experimental biologists, I am seeking common mechanisms that underlie diverse developmental processes in plants.  Our work is driven be the hypothesis that several key aspects of plant development and form can be explained in terms of the pattern-generating properties of the plant hormone auxin.  B. Advancement of mathematical and computational techniques for the modeling of development.  In this part of my research program, my students and I devise techniques that can be incorporated into simulation software. This software can then be used to create visual simulation models that provide insights into the mechanisms of plant development in nature.I am also organizing elements of biology, mathematics, and computer science into a coherent framework, the ""computational biology of plant development.""  This synthesis is needed as a systematic account of the results obtained to date, as a stepping stone for further research, and as a vehicle for transmitting ideas to the next generation of researchers and students.       ""404380,""Pryor, JenniferElaine"
"394224"	"Rafiei, Davood"	"Time-aware querying of the web"	"Web-based information retrieval systems must contend with continuously increasing volume of data and limitations of human time and capacity to read and analyze more than a handful of search results. Querying and filtering based on some natural constraints (such as 'time' in our case) is an intuitive and effective approach to data management in this complex paradigm.  The objective of this research is to investigate the interaction between ""time"" and ""user search"" on the Web and to advance the ability of a search engine to better understand time-sensitive queries with the general goal of improving the quality of the search experience for the Web users. The specific components of the research project are (1) advancing automatic extraction of time sensitive cues and signals from resources on the Web (e.g. pages, news articles, blogs), (2) supporting scalable annotation and indexing based on time-sensitive signals extracted in the earlier step, and (3) integrating the notion of time in both queries and search results. Together, the three research components provide a solid foundation for building scalable time-aware Web information retrieval systems. This work relates and builds on top of our earlier work on querying historical data, data extraction from natural language text and efficient approximate streaming algorithms.Because of the nice mix of theory and practice and the current popularity of search engines, the proposed work is expected to have wide impact on research and education. The project provides a number of Ph.D.- and M.Sc.-level research for graduate students as well as research experience for a number of undergraduates. The results of this research will be disseminated through publications, presentations and demonstrations at large forums and venues. Also the software developed will be either made available for download or accessible through a Web interface (as done in the past).""408043,""Rafo, Natasha"
"402938"	"Rekleitis, Ioannis"	"Enabling robotic autonomy in challenging environments; an algorithmic approach"	"My work is centred on deriving algorithmic solutions that enhance an agent's autonomy through intelligent decision-making, collaboration and information exchange with the other members of the computational group. The agents may be nodes in a sensor network, mobile robots, or robotic spacecraft. Autonomy capabilities are especially important when robots operate in challenging environments such as underwater or in space. In particular both planetary exploration and underwater robotics suffer from the same limitations as communication with a human operator is intermittent and with low bandwidth. In order to facilitate autonomy any robotic system must address the following fundamental problems: estimate their state with respect to the surrounding environment; model the world around them; and plan their tasks in an optimal manner. In the proposed research I am planning to extend my work on state estimation for robots moving in three dimensions, using a variety of techniques such as Particle filters and Extended Kalman Filters. I am also planning to develop new techniques for environment modelling using irregular triangular meshes. The previous two components are usually termed together as Simultaneous Localization and Mapping (SLAM) in the robotics community. Moreover, it has been demonstrated in my earlier work, as well as, by other researchers that collaboration between intelligent agents greatly enhances their capabilities.  Therefore in my future plans is the extension of single robot planetary exploration algorithms to multiple robots. In general, my research reflects the belief that, as robotic systems become more common, the need for autonomy is very important, and collaboration among them becomes necessary. Collaborative agents are able to better understand their environment and perform complex actions robustly and efficiently. Many open problems remain in the field of multi-agent/multi-robot systems, and I am looking forward to solving them.""411397,""Relkov, Tonia"
"396439"	"Robillard, Martin"	"Evolving software project knowledge"	"A significant amount of data gets produced as part of software development activities. In addition to numerous deliverables (e.g., source code, test cases), an active development organization will typically generate numerous internal artifacts that contain knowledge about the software under construction. Internal project artifacts can be generated manually (e.g., how-to guides), or automatically (e.g., execution traces). Considered as a whole, the artifacts produced as part of software development activities constitute an important repository of knowledge about a software project. Given the knowledge-intensive nature of software development, effective use of accumulated software project knowledge has the potential to help software development organizations increase their productivity and the quality of their products.However, in an environment where software artifacts are modified on a daily basis, the consistency between a piece of software project knowledge and the current state of the project is continually at risk. Open-source projects provide plenty of examples of programming systems for which the only tutorial available is out-dated by several versions. Although certain types of information can easily be re-generated (e.g., test coverage data), many types of information can be expensive to generate. In general, systematically ensuring the relevance and correctness of existing software project knowledge is a major challenge.The goal of this research program is to investigate how we can efficiently maintain and evolve software project knowledge, and to elaborate a set of modeling and traceability techniques that will allow software development organizations to cost-effectively maintain the knowledge they have accumulated about a software project. This program will innovate by focusing on the problem of maintaining the relevance and accuracy of existing knowledge, as opposed to generating additional knowledge (e.g., through data-mining, or static analysis).""388199,""Robillard, Pierre"
"389932"	"Rudnicki, Piotr"	"Toward a practical, mechanically checkable reasoning environment"	"Rigorous mathematical techniques find more and more applications in specification and development of complex systems.  Automating the process of checking that the mathematics has been properly used is desirable not only in applications but in conducting mathematical research.  This leads to the grand idea of building a computerized verification system that represents all important mathematical knowledge and techniques.  For more than three decades, I have been involved in a project named Mizar that has attempted to build a modest, trial version of such a system.  Mizar is based on a relatively rich formal language, designed by A. Trybulec, for recording reasonings (definitions, theorems, and their proofs). Mizar texts can be checked by the Mizar verifier and then included into the Mizar Mathematical Library.  The library is founded on set theory and at present consists of almost 1000 formally verified articles with about 50 articles added annually.  All components of the Mizar system evolve; the changes are driven by the needs of authors contributing to the library.  These contributions are of lasting value as the contents of the library is mechanically transferable to other representations and to some other proof-assistants.  I propose to continue my participation in the Mizar project by contributing to the Mizar library.  In the next few years I would like to continue development of algorithmic graph theory in Mizar.  I will cooperate with two colleagues from our department who are interested in algorithms for recognizing special classes of graphs. I hope that after formalizing enough background material we will become able to employ the Mizar system in assisting original research.""401718,""Rudolph, Alena"
"402973"	"Sabati, Mohammad"	"The use of fuzzy logic in magnetic resonance imaging"	"Magnetic resonance (MR) is a powerful imaging modality in which spatially encoded raw data is acquired from the object being imaged. A computer program then processes these data to obtain the desired tomographical, or cross-sectional, image. In the past two decades, numerous data acquisition methods and image reconstruction techniques have been introduced to ""efficiently"" collect the necessary raw data and to generate diagnostic and informative medical and non-medical images. A variety of methods were required to target different MR applications (like imaging of stationary or moving objects, vascular imaging, measuring flow, and imaging specific tissues) and to overcome MR limitations (like lengthy acquisitions, artifacts from subject motion, and blurring).     In MR imaging, the relationship between the imaged object, the data acquisition, and the information content of the reconstructed image is a complex process where exact knowledge about this relationship has been only established using numerical and limited non-numerical information. This complexity arises because of unknown object's content, unpredictable motion artifacts during imaging, and unpredictable dynamics such as perturbations, external noise and disturbance. Hence, the information gathered before or during the imaging process is never complete, sharp or comprehensive.    In this research, we will investigate the use and suitability of relatively new informational logic, known as fuzzy logic, to incorporate linguistic expert knowledge into novel data acquisition and reconstruction algorithms for MR. We anticipate that fuzzy logic can address some MR current limitations and can provide accurate information in a shorter time. Our proposed study is directed towards experimental and computational techniques that can be used to prevent and/or minimize artifacts during imaging; rather than looking at post-imaging solutions.""394590,""Sabbagh, Mark"
"402861"	"Sadat, Fatiha"	"Towards a hybrid approach to cross-language information retrieval"	"Cross-language information retrieval (CLIR) allows a user to formulate a query in one language and to retrieve documents in one or more languages. The current research proposal concerns a unified approach of query translation and document translation with an application on morphologically complex languages such as Arabic, Chinese, French and English. The query translation approach translates queries into document languages using available linguistic resources; while the document translation approach translates the retrieved documents into the target language. This research proposal is based on four major parts and objectives, as follows:1. Investigate on query translation using the dictionary-based approach. An investigation on the disambiguation of polysemous lexical items using word co-occurrence graphs will be pursued. Furthermore, efficient algorithms for the transliteration of cognates and named-entities using a hybrid spelling and phonetic information will be developed. 2. Design and implement efficient methods for translating large sets of retrieved documents. Three methods based on rules, statistics and their combination, respectively, will be explored. Special data such as named entities, dates, numbers and currencies will be investigated carefully with both rule-based and statistical approaches. Furthermore, linguistics information, such as syntax and morphology are needed in the pre-processing step. Arabic and Chinese languages suffer from the problem of word segmentation. 3. Continue the research on exploiting comparable corpora for bilingual dictionary enrichment and machine translation. One of the major bottlenecks in the development of portable and robust NLP applications with broad coverage is the lack of linguistic resources.  4. Develop efficient and comparative algorithms for query expansion using an ontology-based approach and relevance feedback in both pre- and post-translation phases.""404942,""Sadavoy, Sarah"
"402846"	"SafaviNaieni, Rei"	"Authentication and identification for next generation communication systems"	"Authentication of information is arguably the most important security goal in modern electronic communication systems. Receiving a message on a computer terminal needs assurance that the message is coming from the claimed sender and it has not been tampered with. This assurance does not exist for most communications over the Internet today.  Inadequate security for identification of users, devices and services, and authentication of messages has made attacks such as identity theft and fake websites, a major security concern. Authentication systems provide assurance for communicants about entities involved in the communication and the messages that are exchanged. Authentication systems must also meet the challenges resulting from the advancement of technologies and the introduction of new services. Applications such as social networking and instant tele-conferencing require secure authentication in dynamic groups.  Multimedia data is produced at very large volumes and multimedia authentication systems have special requirements such as tolerance to small changes in data.   A growing concern in authentication systems is users' privacy and the assurance that users' data cannot be later used to breach their privacy.A useful authentication system must be provably secure and also sufficiently efficient to be acceptable by the users.   The two most important approaches to proving security are information theoretic security that provides an unlimited and everlasting security guarantee, and computational security that guarantees security for the   lifetime of the data. The main advantage of this latter approach is the efficiency gain. In this  research program we will,  (i) explore,  model and analyze  the problem of entity and message authentication in today and future communication systems, and  (ii)  construct  efficient authentication systems with provable security that satisfy extra properties such as privacy of the users, or tolerance to small changes in multimedia data that are byproducts of commonly used multimedia processing. The proposed program will produce highly qualified graduate students equipped with knowledge of theoretical and practical information security.""407438,""SafaviNaini, Arghavan"
"396450"	"Salavatipour, MohammadReza"	"Approximation algorithms, approximablity, and algorithmic graph theory"	"My current and planned research focuses on the following sub-areas in theoretical computer science:i) design of efficient approximation algorithms for problems that naturally arise in applications such as network design and computational economy, (ii) study of computational complexity and hardness of approximation for such problem, and (iii) algorithmic aspects of graph theory and graph coloring problems on planar graphs.Most of the real world optimization problems, such as the ones on graphs that arise from applications in the design of networks, are NP-hard. Therefore, under widely believed assumptions, we cannot solve these problems in a reasonable amount of time. Given this, it is typically acceptable to compute a near optimal solution efficiently. So research has focused on the study of approximation algorithms; these are algorithms that run fast and produce a solution that is within a guaranteed factor of the optimal one. Among these optimization problems, those related to graphs and network design are particularly important and have attracted more attention recently. Another class of such problems arise in computational economy (for example how to price bundles of items to maximize revenue and or customer satisfaction). It is also  important to study the hardness of approximation (computational complexity) of these problems. This helps to put the quality of the proposed algorithms in perspective, and have a better understanding of the level of difficulty of these  problems which in turn could be used to design better algorithms.A significant amount of my research will involve the study of coloring and other problems on graphs and in particular on planar graphs. Some of these problems (which have theoretical importance) have also applications in frequency channel assignments in a cellular network. The question of minimizing the number of colors in a specific coloring problem translates to minimizing the frequency bandwidth required in such a network.""413505,""Salcudean, Septimiu"
"403688"	"Samaan, Nancy"	"A testbed for wireless mesh networks"	"Wireless mesh networks (WMNs) represent the next wave in wireless technologies due to their ease of deployment and economy. Nonetheless, being a nascent technology, WMNs still lack many of the development tools, available to older wired/wireless technologies, and needed to exploit their full potential. An essential tool to pursue research in this area is a software simulator or an experimental testbed. However, a reliable simulator for WMNs does not exist due to a lack in understanding the characteristics peculiar to these networks (e.g., dynamic capacity, interference and fairness). This fact makes an experimental testbed the only possible avenue for developing and evaluating new research ideas before they can be applied to commercial products.    )This application seeks to acquire an experimental WMN testbed and utilize it for a two objectives. The first objective has a long-term nature and aims at constructing a fully autonomic carrier-grade WMN. The second short-term objective is focused on establishing adaptive quality-of-service (QoS) provisioning solutions for WMN networks. The short-term activities in the proposed research involve two main phases.  The first phase will be directed towards characterizing WMN performance. The requested testbed will be applied to address many questions needed to proceed with this characterization such as: what is the capacity of a given WMNs? What are the effects of varying cross-layer parameters such as transmission power, modulation rate, and the use of multi-channels on capacity? How does fairness affect flows throughput? Measurements collected during this phase will represent a significant research output in its own right, allowing researchers to reproduce testbed properties in simulation. An important outcome of this phase also is the development of a new metric for QoS unique to WMNs. The second phase will evaluate various developed QoS schemes for topology selection, admission control, traffic scheduling, and network self-configurability.  Finally, long-term research activities will address advanced WMN problems such as mobility and handoff and will investigate emerging scalability issues by extending the size of the testbed.""403669,""Samaan, Nancy"
"389857"	"Sankoff, David"	"Mathematical genomics"	"We study the mathematics, statistics and computing of genome rearrangements, the evolutionary processes by which segments of chromosomes are inverted, moved around and exchanged.  There is an algorithmic technology for inferring a minimizing sequence of operations necessary to transform one genome into another, where the operations are modeled on processes such as inversion, reciprocal translocation, chromosome fusion and fission, transposition of chromosomal segments. Once these inferences are made, however, there is a need for some way to statistically validate both the inferences and the assumptions of the evolutionary model.     Our approach is to use models of random genomes to see to what extent there is an signal remaining in the comparative structure of the two genomes, or whether evolution has largely scrambled the order of each one with respect to the other.     We work on the problem of the rearrangement median for three genomes as the basis for gene order phylogeny based on rearrangements, and apply it to chloroplast and mammalian data sets.      We study the ""genome halving problem"" for ancient tetraploids and extensions to mixed ploidy gene order phylogenies. We apply this to the study of the ""Saccharomyces complex"" of yeast species.    In comparing genomic maps, we introduce the notion of partially ordered genomes to take into account missing and unresolved data, and use this to compare the genomes of various cereal species.  We also work on distinguishing mapping errors from genuine rearrangements of the genomes and we apply this to the comparison of the rice and sorghum genomes.     We have defined a new measure of ""generalized adjacency"" as a useful tool in computing ancestral genomes within a phylogeny.  This leads to many algorithmic and statistical questions we propose to investigate.""412897,""Sankoff, David"
"395454"	"Sarkar, Anoop"	"Semi-supervised learning for natural language processing"	"Natural language is the main repository of human knowledge, and with the advent of large amounts of readily available machine-readable natural language text such as the web and newswire feeds, there has been a steady increase in the number of computational applications that attempt to extract information from these sources. The field of natural language processing now has a well-developed suite of approaches that use supervised machine learning. However, adapting to a new domain or a new language, especially a language with insufficient data, is still a major bottleneck when building software applications that process natural language. This proposal aims to address this crucial issue by exploring the use of semi-supervised machine learning methods for natural language, which can learn even when only a small amount of training data is available.Here are two examples of benefits from this research: First, by using domain-general information about English and then adapting to a specialized language, such as bio-medical research papers, natural language applications can be used to extract information in this new domain. Second, machine translation systems are trained on sentences previously translated by human translators, but such translations are only available in large quantities in a domain like parliament proceedings, e.g. the French-English Canadian Hansards. But this domain is not well suited to build a machine translation system for another domain, e.g. political weblogs. Our research enables translation systems to adapt to this new domain. Our research also enables machine translation in a domain where only small amounts of training data are available, e.g. the Nunavut Hansards.""400318,""Sarkar, Niladri"
"389680"	"Schaeffer, Jonathan"	"General game-playing programs"	"For over 25 years, I have been working on applying heuristic search and knowledge techniques towards building high-performance artificial intelligence (AI) systems. I often use game-playing and puzzle-solving programs as my experimental test domains. This has led to success in numerous domains (chess, checkers, poker, sliding tile puzzles, Rubik's Cube - all yielding valuable insights). However, each program is ""one of a kind"", with limited carryover from domain to domain. Although some of the algorithms apply to classes of applications, each high-performance AI system represents a complete re-implementation.   Humans are very good at transferring their experience in one domain (a game/puzzle in this case) to another. The state of the art in computing science is that the program designer transfers their experience from building a program for one domain to that of building a program for another domain. Ideally, the human should be factored out of this process.   There has been movement in the AI community to build general systems that can handle a broad class of applications. In this particular case, can we build a program capable of taking any game and, based solely on the rules and past experience, build a new game-playing program? Such a program would be a step towards the realization of ""human level"" AI.    This project will investigate the research issues around building a high-performance (high level of demonstrated skill) general game-playing program. The intent is to have a program that is a) capable of learning to play a broad spectrum of games and b) is scalable in its ability to learn (more learning time equates to better performance).""412878,""Schaeffer, Jonathan"
"402853"	"Schroeder, Bianca"	"Understanding and coping with failure at scale"	"The funds requested in this proposal will serve to provide the resources required to carry out the PI's new research program on ""Understanding and coping with failure at scale"". The goal of this new research program is to address the age-old problem of building highly reliable computer systems in a novel way. The key observation is that, although system reliability has been a key concern since the first computer systems were build 50 years ago, embarrassingly little is known about basic characteristics of failures in real systems, since virtually no data on failures in production systems is publicly available. As a result, much research is based on hypothetical and often simplistic assumptions.The broad goal of the PI's new research program is to enable the creation of more reliable systems through a detailed understanding of real-world failures. The motivation for this research program comes from the PI's initial results on failure data analysis, which show that many common assumptions about failures are not realistic and that making those assumptions can lead to highly sub-optimal solutions. These results indicate a strong need for a better understanding of failures in the real world and more realistic failure models, based on empirical data. The PI plans to analyze large amounts of failure data from real production systems in order to derive insights into the root cause of failures, statistical properties of failures and more realistic failure models, and then use this knowledge to develop better methods for avoiding, coping with and recovering from failures.""404490,""Schroeder, Cheryl"
"393220"	"Schulte, Oliver"	"Machine learning for entity-relationship databases"	"My research develops new mathematical concepts to extend statistical principles so that they can be applied to large and complex modern databases. The computational challenge is to develop efficient programs that can apply these algorithms to rapidly sift and identify useful information-essentially revealing diamonds in the rough. Vast amounts of data are gathered daily for different purposes. As more and more organizations and companies accumulate data, a crucial competitive edge comes with the ability to efficiently find informative patterns in the data and to learn from it to improve decision-making. This process requires computational tools for extracting information from data records. The proposed research combines fundamental time-tested statistical concepts with new machine learning technology to provide practical data analysis tools that have a strong theoretical foundation. The most successful learning programs are based on statistical concepts. The problem is that these concepts were invented for relatively simple domains, like a group of patients, and are not well-suited to handle the rich structure of modern databases. In modern databases we may have many different types of entities that interact in different ways; for example planes, passengers, pilots, flight attendants, and airports are all related to each other in different ways. Computer models that integrate relationships form the basis of learning programs that make queries possible and predict intelligent mathematical solutions from information profiles. The answers will help human beings perceive and evaluate risks and opportunities in complex situations from healthcare to helicopters.""412057,""Schulte, Patricia"
"406165"	"Schuurmans, Dale"	"Learning structure models from complex data: principles and algorithms"	"Almost all data is now digitally stored, with unprecedented computing power available to process it.  These developments have created new opportunities to advance computer interpretation (e.g. natural language processing, computer perception) and intelligent data-analysis (e.g. bio-informatics) by analyzing the massive amounts of complex data stored in text, multimedia, and scientific repositories.  My research addresses the fundamental challenge of synthesizing predictive models from data, focusing on the problem of learning structured predictors in the presence of latent variables and incomplete observations.  These challenges are motivated by the fact that predictions in complex domains are not just simple class labels or scalar values, but are complex structured outputs---such as parse trees, scene labellings or graph labellings---that involve multiple outputs to be predicted in a coordinated fashion, often with intervening latent variables.  The key problem is training complex predictors when some of the output or intervening latent variables are unobserved.   To tackle these problems, I employ two key strategies.  First, I exploit probability models, which specify optimal interpretations from noisy, incomplete and conflicting evidence.  Thus, one aspect of my research focuses on developing new algorithms for learning and inference in structured probability models.  Second, I formulate convex training principles for the implied learning problems.  Convexity decouples parameter optimization from model quality: a poor result arises from poor model structure, not a poor local minimum.  A further key insight is that supervised training can be extended to unsupervised or semi-supervised training by following a self-supervised approach, where missing data components are treated as auxiliary variables to be optimized (i.e. inferred) simultaneously with parameter optimization.  A natural convex formulation of joint training can then be obtained by working with equivalence relations over missing components.  This approach has led to fundamental advances in unsupervised and semi-supervised training, including state of the art extensions to support vector machines, maximum margin Markov networks, and expectation-maximization.""393221,""Schuurmans, Dale"
"390600"	"Sesay, AbuBakarr"	"Bluetooth-based radio access with asymmetric processing"	"Hospitals are demanding Wireless networks in order to reliably access/transfer information to/from the point-of-care, allowing healthcare professionals to make faster decisions and diagnosis.  In warehouses, homes, office, etc., information, products and documents need to be distributed and tracked. The objective of this project is to develop an asymmetric wireless network experimental system with centralized processing and distributed access points that provide wireless service to picocells (cluster of room, etc.) using Bluetooth devices and sensors. Access points are fitted with multiple antennas (MIMO) and are connected to the processing centre via optical fibers or cables. The access points are generic (non-standard specific) and therefore enables the network to support other standards such as IEEE 802.xx. Signals on the reverse links are down converted, amplified and sampled at the access point, and transmitted to the processing centre where the bulk of the processing is done according to signal format or standard. On the forward link, signalling format such as modulation, error correction coding, space time coding, beamforming, encryption, etc. are generated at the processing centre and transmitted to the access points. Optical fibers or cables provide the wide bandwidth required for data transfer to/from the access point. This asymmetric configuration provides numerous opportunities. The access points need not be replaced as standards evolve. There is more flexibility with respect to processing power requirement, ease of signal format or antenna reconfiguration and security processing.  The test bed is, therefore, useful for multi-standard operation and for testing new standards and algorithms. Novel, low complexity, low power MIMO processing algorithms will be developed to improve transmission rate, co-existence with other wireless systems and user capacity. These solutions will lead to a new design of local/personal area networks and multi-standard access points that will significantly impact, for example, how Canadian healthcare professionals handle and share patient inormation for speedy diagnosis and treatment.  Other Canadian services that will benefit are warehouses, offices, schools etc.""391637,""Seshadri, Rangaswamy"
"402633"	"ShahHeydari, Shahram"	"Methods for protection and restoration from large-scale failures in telecommunicaton infrastructure"	"The explosive growth of communication demands in recent years has attracted special attention to the issue of reliability of telecommunications infrastructure and backbone networks. The importance of large-scale failure problems in communication networks cannot be overstated. Such failures could paralyze the backbone networks and completely disrupt essential network-based voice and data services across the country. Fast and cost-efficient recovery of communication service is essential in today's world. A number of different options for network restoration exist, including local (link-based) restoration or end-to-end (path-based) protection, using shared or dedicated capacity, offering different restoration times and redundancy costs. The main objectives of this research proposal are to analyze the impact of large scale failure scenarios arising from security attacks or natural disasters on the telecommunication infrastructure, and to design multi-layer restoration schemes, methods and algorithms for integrated protection and traffic restoration of access, metropolitan and backbone layers of the network. Survivable topology design, cost analysis, and administrative and management issues related to large-scale failure recovery in the communication infrastructure are among the topics that will be tackled in the proposed research program.  ""404413,""Shahi, Arash"
"390988"	"Shallit, Jeffrey"	"Descriptional complexity, combinatorics on words, formal languages and number theory"	"I propose to investigate problems combining aspects of theoretical computer science, combinatorics, algebra, and number theory, along the lines of the topics in my recent monograph with Jean-Paul Allouche, ""Automatic Sequences:  Theory, Applications, Generalizations"", published by Cambridge University Press in 2003.  In particular, I plan to work on problems involving the topics of descriptional complexity, conbinatorics on words, formal languages, and finite automata.In descriptional complexity, we are concerned with how succinctly objects can be described (as opposed to the resource consumption of algorithms, which is the domain of computational complexity).   The techniques used in this area are taken from algebra, number theory, and combinatorics, and problems frequently make use of deep results in these areas.Combinatorics on words is concerned with the properties of strings of symbols.    For example, given two words, when do they ""commute"" (that is, read the same no matter which word is taken first)?   Although of interest in its own right, recently there are also some applications to cryptography, where it is used in constructing hash functions resistant to certain kinds of attacks.Formal languages is the study of the mathematical properties of languages, which are sets of words.  This area is of particular interest because of applications to parsing and compiling.Finite automata are simple models of computation.  By studying the properties of these simple models, we hope to gain deeper insight into the nature of what can and cannot be computed.""390468,""Sham, TsunKong"
"396728"	"Sharlin, Ehud"	"Exploring physicality in interaction"	"We are physical beings. We evolved through interaction with natural physical surroundings and elements and much of our experience relies on physical objects and environments. Computers on the other hand were designed initially as abstract generic machines that crunch numbers and do not have direct physical function or meaningful physical form. An important effort in current human-computer interaction research attempts to bring computers closer to our physical reality by allowing them to exploit the users' physical environments as well as humans' innate and learned physical abilities. I explore how humans interact with physical environments, entities and artefacts, I implement these lessons in the form of physical computer interfaces and evaluate how these interfaces affect user interactive experience and behaviour and which lessons can be learned for future interaction design.In this research I approach two emerging physical interaction themes: human-robot interaction and physical games interfaces. Robots are physical interfaces, capable of sensing their environment as well as of acting and altering it. With robots becoming more common, fundamental questions of physical interaction with humans need to be investigated and answered. I will design intuitive physical robotic interfaces and tools for rapid prototyping and evaluation of collaborative human-robot interfaces. I will also develop high-level interaction techniques that will allow robots to provide users with sociable cues about their state and function.Current computer games and entertainment experiences are rapidly introducing meaningful physical interaction to the home environment as was recently manifested by the commercially successful Nintendo Wii and its novel mappings between inexpensive accelerometers and gameplay. More evolved physical interfaces will allow an even more immersive gameplay experiences. I will explore ways in which physical touch between players and physical interfaces can enhance gaming experience, and in which computerized games can be played as if they are an integral part of the user's real physical surroundings.""399493,""Sharma, Abhay"
"395076"	"Shen, Weiming"	"Agent-based distributed dependable systems"	"An increasingly important requirement for software based systems is the ability to deliver reliable, correct, and safe services in a timely manner. This kind of ability is usually called system dependability which is demonstrated through features like self-managing, self-maintaining, and self-repairing/self-healing. These features are becoming a requirement of software based systems in many industrial sectors, e.g., in some manufacturing environments (such as steel mills and chemical plants) and critical infrastructures/facilities (such as power grids and nuclear power plants) where system failures or unexpected shutdowns will cause significant economic losses or even destroy entire plants/facilities. The objective of this research is to develop scientific and engineering foundations, innovative technologies, and practical solutions to support distributed, automated, dependable computer systems that are able to handle faults and uncertain changes at real time without (or with reduced) degrading of services. The emphasis will be on understanding, modeling, designing, and implementing complex software systems in dynamic manufacturing environments and critical infrastructures/facilities. We will particularly investigate the application of intelligent software agents in this area, by integrating with other concepts, principles, and technologies, including active event-based concepts, optimal control theory, scheduling and optimization, data mining, and distributed sensor networks.""399022,""Shen, Xuemin(Sherman)"
"406166"	"Siddiqi, Kaleem"	"Geometric flows for shape analysis"	"This research program will concentrate on two recent developments in the field of shape analysis. The first has to do with an outward flow by which the boundary of an object is recovered from a medial manifold. Very recent results in mathematics show that it is possible to recover the differential geometry of a bounding surface from appropriate calculations on its corresponding medial manifold. Furthermore, my group's work on hamilton-jacobi skeletons, and in particular our average-outward flux based formulation,  has been shown to be a special case of a more general medial integral. In fact, it is now possible in theory to obtain integrals of measurable functions on surfaces or volumes directly in terms of their medial integral counterparts. I will focus on translating these theoretical results to a computational setting, grounded in my own expertise in the development of algorithms for obtaining and manipulating medial representations. In the second development, my group has been investigating the geometry of 3D streamline flows due to approximately parallel curves as they arise in computer vision, medical imaging and graphics. We have developed a differential geometric characterization of such flows, leading to three curvature measures, and have used minimal surface theory to motivate the choice of a generalized helicoid as an osculating object. Thus far our work has concentrated on the use of these models for Diffusion MRI analysis and for the characterization of white matter fibre tracts. Several computational questions remain to be addressed, and the use of this mathematics for other applications, e.g., texture analysis in vision and hair pattern modeling in graphics, is entirely unexplored. The second major aim of this proposal is to develop the computational aspects of 3D streamline flows. Research progress in these two directions will have important consequences for our understanding of the geometry of object surfaces and of streamline flow patterns, and will impact applications in computer vision, medical image analysis and graphics.""392220,""Siddiqi, Kaleem"
"394226"	"Smith, Spencer"	"Improving the quality of scientific computing via software engineering methodologies"	"The objective of the proposed research program is to improve the quality of scientific computation (SC) software.  Although many high quality algorithms have been implemented, especially with respect to the qualities of correctness, reliability and performance, there are other qualities, such as usability, maintainability, verifiability, productivity, reusability and portability that are often neglected.  To address these quality concerns, the proposed approach is to adapt software engineering (SE) methodologies that have been successfully applied for other types of software.  One new idea that can be adapted from SE is the development of SC code as a program family.  In this approach similar software products are developed together, with the common parts reused and the variable features identified and systematically handled.  This research has the potential to significantly improve SC code with respect to reusability because the common code only has to be implemented once.  Testing is another area of SE practise that will be adapted with the goal of improving the quality of SC software.  The testing research will be challenging because most testing methodologies assume that for each set of test inputs an expected output can be identified, but this is often not the case for SC software.  This challenge will be addressed through techniques of parallel testing with independent algorithms, comparison to closed-form solutions, the use of interval and infinite precision arithmetic, a posteriori error estimation and systematic identification and documentation of known characteristics of the solution.""401426,""Smith, Stacey"
"396989"	"Smyth, Tamara"	"Physics-based sound synthesis and model parameter estimation"	"This research will continue to focus on several issues related to improving real-time interactive physics-based computer music instruments, including continued development of measurement techniques for obtaining musical instrument transfer functions, improved sound synthesis quality, extending the variety of sounds of which instrument models are capable (by ""generalizing"" synthesis models), and improving playability, involving primarily the estimation and mapping of model parameters.A relatively inexpensive and robust measuring technique is underdevelopment, allowing for accurate estimation of reflection functionsfrom wind instrument bores.  These functions may be incorporated intoparametric synthesis models, either in the context of a classic waveguide model, or in the construction of a parametric impulse response to be used in our currently in-progress synthesis technique dubbed, ""Convolutional Waveguide Synthesis"".  This serves both to improve the quality of the sound synthesis, but also to provide an accurate test signal for observing parameters and inferring actual values of the model parameters.  It can also serve to create hybrid models no longer scientifically constrained, though still physically based, where physical attributes required for the production of sound (i.e. to achieve oscillation) are no longer solely responsible for the sound's quality.""403596,""Smyth, Thomas"
"399762"	"Sobot, Robert"	"Biologically inspired mixed-signal integrated circuits"	"Mixed-signal approach to integrated circuit (IC) design, where digital and analog logic share the same semiconductor die and seamlessly work together, has been employed by both industry and academia for a number of years now.In recent years, however, the main focus of mixed-signal IC research and development has shifted towards architectures and systems inspired by biological organisms and biomedical applications, where the traditional telecommunication circuitry, which is providing data-link path, now represents minority within the overall system. Moreover, either internal IC functions are being implemented and optimized so that operations of biological organisms are being mimicked, or the overall IC specifications are targeted for applications related to, for example, medical implants and biomedical research. Some of IC specifications which are nowadays assumed, include ultra-low-power of operation, small physical dimensions and self-power capability, for example by means of power scavenging or wireless power transfer. Research being proposed deals specifically with mixed-signal integrated circuit theory, design and implementation issues which are overlapping with other disciplines such as biomedical engineering, physics, robotics, earth and space sciences. Emerging fields such as neuroprosthetic and brain-computer interface (BCI), bionics and genetic algorithms, with support of material sciences, such as nanotechnology, open a large number of possibilities for interdisciplinary research and further advances of science. It is goal of this research to further explore the novel possibilities at the borderlines between the field of integrated circuit design and the above fields. The objective is to contribute to improvements in electronic systems which are meant to enhance human lives and well-being in general.""410822,""Socholotuik, Mandi"
"390587"	"Sorenson, Paul"	"Software quality assessment and improvement in service-oriented environments"	"The emergence of SOA (Service Oriented Architecture) as a general architectural pattern for modern software systems and the creation of a series of universal standards for web services delivery provides an excellent opportunity to explore how the quality of software can be used as a major factor in the selection of IS (Information System) services. These new SOA-based systems expose service capabilities and make more evident to users the importance of delivering quality systems and associated support services.  My primary objective is to uncover methods (processes and tools) that measure and improve upon the quality of IS services delivered to users. Specific projects undertaken will be focused upon are i) defining a model for measuring and advertising service quality, ii) determining and assessing how this model can be incorporated as part of an extended registry capability for service-oriented applications and iii) exploring approaches for testing IS services to help ensure defined levels of service quality.""401713,""Sorge, Jason"
"393722"	"Spiteri, Raymond"	"Optimized time-stepping methods for the numerical solution of differential equations"	"Evolutionary partial differential equations (PDEs) are used to mathematically model many important physical processes, ranging from to the interactions of sub-atomic particles to the motions of galaxies. These and other real-life problems are complex. Their solutions can only be simulated using a computer. However, a major advantage of using simulations in scientific discovery is the ability to generate data and analyze complex systems ""in silico"". This is particularly useful when real experiments are infeasible to carry out.High-fidelity simulations often require complicated equations, complex geometries, and high resolutions in order to obtain meaningful data. Despite the advances in computing technologies, simulations of such models continually push the limits of our computing abilities. Moreover, it is discouragingly easy to produce simulations that produce spurious predictions such as negative values that are supposed to represent population sizes. Such solutions are deservedly viewed with skepticism. This combination of computational challenges makes the numerical methods employed a key factor in the efficiency and reliability of a simulation.Accordingly the overall objective of this proposal is to discover, analyze, and implement optimized time-stepping methods for the numerical solution of evolutionary PDEs that model challenging physical phenomena.  Specifically we are interested in simulation-based investigation of high-speed compressible gas flows, electrical activity in heart tissue, and catalytic reactions for reduction of greenhouse gas emissions. To achieve this objective, I also propose to develop problem-solving environments (PSEs) in which the research can be conducted. There is an increasing need for such PSEs as computing paradigms evolve to use massively parallel, multi-core, and/or hybrid computer architectures.""393674,""Spivak, Gregory"
"402834"	"Stanley, Kevin"	"Heterogeneous sensor network deployment and monitoring for scaler fields"	"Spurred by the growth of wireless networking, enabled by advanced in MEMS design, and inspired by robotic platforms like Spirit and Opportunity, heterogeneous sensor networks have moved to the forefront of many research agendas.  In heterogeneous sensor networks, the hardware of the nodes or motes is not uniform, and classified by power consumption, computational capacity and mobility.  Homogeneous networks achieve spatial resolution through high mote densities, while mobile heterogeneous sensor networks (MHSN) employ fewer more expensive mobile motes to reduce the node density of the network.  While even a randomly deployed sensor network will generate more data than a single sensor, distributions which are based on the expected behavior of the area to be measured are more effective than distributions which are deployed blindly.  However, this raises a dilemma: how do you estimate the expected behavior of the system you are about to measure?  Presumably if detailed information on the system is available, there is no need for a sensor network to make measurements.  Recently, researchers have found that using statistical representations which estimate how one sensor's measurements correlate to one another; they can deploy nodes with more confidence.  This technique can be extended to a sequential deposition of sensors, building the sensor network piece by piece, or destination targets for mobile robots to rove from point to point, making measurements.  This research proposal aims to add levels of sophistication to MHSN deployment strategies, which will allow the integration of physical models, the management of complex multivariate data and the inclusion of human experts in the measurement loop.  It will balance the additional implementation complexity against maintaining the statistical rigor of simpler techniques.  By adding physical models, multivariate representations and human experts, MHSNs can be developed into powerful tools for agricultural and environmental monitoring.""402988,""Stanley, Nancy"
"390394"	"StDenis, Richard"	"Controller synthesis for infinite-state systems / synthese de controleurs pour des systemes a espace d'états infini"	"``Software must run closed loop.'' This statement from Robert Laddaga emerges from thinking about software agent evolution from conventional programs. Software agents require more monitoring and control because of their greater degree of autonomy, adaptation and mobility. The control of dynamic systems is a discipline that is born at the beginning of last century. Many solutions founded on powerful mathematical theories have been proposed for continuous systems and more recently for discrete event systems as programs or software agents. In the last case, the real impact is less significant, although several efforts have been made to elaborate a substantial theory called Supervisory Control Theory. Controlling programs during their execution implies, amongst other, solving fundamental problems related to their representation in a mathematical form. A natural way to represent a program is to use an automaton with a stack of unlimited size as well parameters that symbolize unbounded values. It results from this modelling pushdown systems and parameterized systems with infinitely many reachable states. Therefore, the control of infinite-state systems raises new important issues, which have been study very little in the past in the context of the Supervisory Control Theory, and constitutes the main subject of the present research program.""398655,""Stead, Brent"
"391790"	"Takahara, Glen"	"Deployment, distributed inferance, and modulation problems for energy efficient wireless sensor networks"	"Wireless Sensor Networks (WSNs) are the focus of intense research activity due to a myriad of potential applications in, for example, health and environmental monitoring, security, smart agriculture, disaster relief, and military information gathering, with economic and social impacts that are anticipated to be profound. Many opportunities and challenges exist for intelligent application designs which exploit the distributed computing potential of such networks. The challenges are mainly due to the fact that WSN nodes typically run on non-replenishable batteries, making energy efficiency of paramount importance.     The proposed research will focus on three aspects of energy efficiency in WSNs. The first is the design of optimal node placement schemes to efficiently achieve given tasks. Specifically, in addition to satisfying coverage and connectivity requirements of an application and the network, our proposed node placement solutions will be optimized for the communication requirements imposed by application dependent distributed computing algorithms in order to increase the maximum usable lifetime of the network. The second aspect is the design of distributed statistical procedures appropriate for WSNs. Our procedures will be designed for energy efficiency while maintaining a given precision of inference, and will provide specific requirements for our node placement strategies to ensure balanced energy consumption across the network. The third aspect is the design of energy efficient adaptive modulation schemes to enable nodes to transmit information reliably under changing source and channel conditions using as little energy as possible.     The research will directly benefit startups and large companies alike who wish to position themselves in the burgeoning WSN services market by offering customized turnkey application solutions, as well as benefitting government sponsored WSN projects aimed at improving the health and security of Canadians. The support for this research will mainly go towards the training of highly qualified personnel, funding two Ph.D. students, one postdoctoral fellow, and three Master's students per year.""412038,""Takahata, Kenichi"
"391643"	"Thibeault, Claude"	"Test de très haute qualité et diagnostic de circuits intégrés à l'échelle nanométrique"	"Cette proposition de recherche s'intéresse principalement aux défis que représentent le test et le diagnostic de haute qualité des CI à l'échelle nanométrique. L'intégration à cette échelle amène de nouveaux défis à tous les niveaux, y compris le test et le diagnostic, qui nous obligent à réévaluer et revoir les techniques existantes afin de maintenir la qualité actuelle. Pour certaines applications plus critiques (ex.  l'automobile et le biomédical), le niveau de qualité exigé est davantage élevé. C'est la raison de notre intérêt pour le test et le diagnostic de haute qualité, ce qui nécessite davantage d'optimisation afin de garder les coûts du test/diagnostic à des niveaux acceptables. De plus, la tendance actuelle au niveau du coût de la fabrication et du taux de succès de la première tentative d'intégration (qui est inférieure à 40% selon [3]) devrait éventuellement rendre plus populaire l'utilisation de plateformes génériques contenant des matrices de logiques programmables, telles celles que l'on retrouve dans les circuits de type FPGA. Ceci explique notre intérêt, à moyen et long terme, pour ce genre de circuits. La présente demande contient deux volets :-Le premier volet vise à améliorer 2 techniques de test de très haute qualité, développées dans un contexte ASIC, et à les adapter à des structures programmables.-Le second volet cible l'optimisation du test/diagnostic via l'exploitation de nouvelles informations, au départ  dans le processus de diagnostic des ASIC. Cette utilisation devrait permettre un profilage des circuits et une adaptation plus personnalisée des stimuli en fonction de la sensibilité estimée de ces circuits à différents types de défectuosités (par exemple les courts-circuits et les circuitsouverts). Nous prévoyons étendre par la suite cette utilisation du test des ASIC. Finalement, cette utilisation devrait à long terme pouvoir être appliquée au test des FPGA.""407729,""Thibodeau, Roxanne"
"394793"	"Tizhoosh, Hamid"	"Oppositional concepts in population-based problem solving"	"Population-based optimization techniques such as genetic algorithms, differential evolution and ant colonies have diverse applications. These optimization methods have proven to be useful in many cases where conventional optimization methods encounter their applicability limits. However, population-based schemes have their own limitations. Specifically, they may often need considerable computational time to find a solution. This disadvantage becomes more visible the larger the population is, which is (almost) always the case when we deal with high-dimensional and/or complex optimization problems. Hence, methods by which to increase the speed of these techniques have been under investigation for quite some time. The main focus of this research will centre on development of methods to increase the speed of differential evolution and ant colonies. Oppositional concepts will be employed to accelerate the convergence of the methods while maintaining the necessary level of solution accuracy. Opposition-based approaches to optimization generally incorporate the simultaneous consideration of the solution and the opposite solution (chromosome and anti-chromosome, path and opposite path). Recent achievements in the successful design and use of opposition-based differential evolution encourage us to seek some fundamental answers with respect to a mathematical formalism for these techniques and to exploit the potentials of oppositional schemes for all population-based algorithms. Standard benchmark functions and metrics will be used to verify the better performance of opposition-based extensions of methods under investigation. As a real-world test case, segmentation of medical images, specifically breast and prostate ultrasound images, will be undertaken as well. Population-based methods have been used to extract objects from digital images in different ways. Their results, as reported in literature, are in some cases impressive. However, processing images with these methods are extremely expensive. This has restricted their use in practical cases. Any level of speedup is desirable here. Image data sets along with radiologist's ground-truth are available for experimental performance verification.""407036,""Tjostheim, Kristen"
"396505"	"Tran, Thomas"	"Developing intelligent information systems to enhance E-Commerce"	"The long-term goal of this research is to enhance electronic commerce (e-commerce), which is presently operating under its expected capacity. I approach this challenge by: (i) designing an effective trust management system; (ii) developing an intelligent recommendation system; and (iii) constructing a feasible architecture for mobile e-commerce.    One main reason why e-commerce has not yet realized its full potential is that traders find it very difficult to trust one another online. It is, therefore, important to have a trust management system that can effectively assist e-commerce traders to make good trust decisions. I will consider the social, psychological, and computational approaches to analyze the trust decision making process, identify the components involved in trust and study how these components interact with one another in order to construct a useful trust management system.    Encouraging consumers to participate more actively in online shopping is a valid way to boost e-commerce. This can be achieved by developing intelligent recommendation systems that help consumers make their best purchase decisions. I would like to explore suitable techniques from artificial intelligence, data mining and  information retrieval to develop a system that can intelligently analyze several sources of appropriate information available online (e.g., ratings, comments and reviews of users and experts, websites of vendors, etc.) to suggest the most suitable products and vendors to consumers. The proposed system should also provide consumers with good justification for its recommendations.    Mobilizing online trading is also a promising direction to improve e-commerce. I plan to construct a feasible architecture for mobile e-commerce that would allow users (both consumers and vendors) of mobile devices, such as PDAs or mobile phones, to conduct business anytime and anywhere. This architecture would enable inter-operability among different e-marketplaces, while taking into consideration the inherent difficulties (e.g., security issues, etc.) as well as the limitations of mobile devices (e.g., expensive connection costs, etc.).""403027,""Tran, TuHao"
"388373"	"Tropper, Carl"	"Parallel simulation of complex systems"	"There are two components of this proposal (1) distributed Verilog simulation (2) parallel particle  simulatlon. The proposed research on distributed Verilog simulation is a continuation of an existing project the goal of which is to  develop a scalable distributed gate level Verilog simulator for both distributed and shared memory platforms.  Our previous research has resulted in the development of a distributed Verilog simulation environment, DVS,  using Time Warp as the simulation engine and the open source Icarus Verilog as a front end.  We have developed a highly efficient gate level simulator, XTW, for a gate level simulator which makes use of a multi-level queueing mechanism.  We propose (1) developing a simulation environment which links Modelsim sequential simulators executing on individual  nodes together via Time Warp (2) porting XTW  to a shared  memory environment. We will try  to take advantage of shared memory for improved memory management. (3)  continuation of our work on module based load balancing. Preliminary results indicate that partitioning  based upon modules has benfited from the locality. We propose the development of efficient algorithms for  re-partitioining the circuit during the course of the simulation's execution (4) applying reinforcement learning  techniques to optimistic simulations.  Our  second research area focuses on the integration of continuous methods for the solution of pde's and discrete  event algorithms in the context of cosmological simulators.  We are  making use of a standard technique for the solution of interacting (cosmological) bodies-an oct tree solver for  Newtonian gravitational equations. Our goal is to incorporate colliding bodies into the picture by making use  of discrete event techniques. We have implemented a Time Warp engine and integrated it with the oct-tree  solver. The work is being done in conjunction with an astro-physics group from the University of Washington. Our first proposed task is the verification of the combined algorithms and an  evaluation of their scalability using realistic scenarios provided  by the astrophysicists.  We then intend to develop  partitioning algorithms.""400739,""Trost, Brett"
"395583"	"Truong, Kevin"	"Development of computational tools for studying protein sequences, structures and signaling networks"	"Cells are composed of protein signaling networks that perform biological functions such as regulating cell growth or catalyzing biochemical reactions.  As a result, the malfunction of proteins often causes human illnesses such as Alzheimer's disease, heart disease and cancer.  My long term research goal is to create synthetic protein signaling networks that will allow us to one day manipulate cell biology with the same precision as electrical circuits and computer networks.  To accomplish this goal, my proposal will focus on developing computational tools for studying protein sequences, structures and signaling networks.  First, to infer the function of a protein sequence, the Smith Waterman (SW) algorithm is used to find its similarity to proteins of known function.  As sequence databases grow larger, faster sequence comparison approaches are required such as using accelerated field programmable gate array (FPGA) hardware.  To make the FPGA solution more affordable, I will develop FPGA hardware for accelerating the SW algorithm using fewer resources while maintaining a comparable speed.  Next, to study the protein signaling kinetics within cells, fluorescent protein biosensors are powerful tools but the design of these biosensors is often trial and error.  Using a computational tool to model the conformational space of protein biosensors, I improved the design however the tool was not quantitative.  To address that problem, I will include molecular factors that select preferred biosensor conformations.  Lastly, to design synthetic protein networks or model larger existing networks, I will develop a computational tool for simulating the spatial and temporal kinetics of protein signaling networks.  Together this work will yield insights into protein sequences and their networks that will ultimately aid in developing therapies for human illnesses.""397706,""Truong, Khai"
"393324"	"VanBreugel, Franck"	"Concurrency: semantics and verification"	"Probabilistic transition systems can be exploited to model randomized algorithms, systems that interact with physical devices, etc. I plan to study probabilistic transition systems and variations thereof.  In particular, I plan to develop quantitative models of these systems.  Such quantitative models are more robust than qualitative models, since minor variations in the probabilities have little effect on the former type of model whereas they may have drastic effects on the latter type.  Also, I aim to develop verification algorithms for these systems based on the developed models.                                                                                Java is the most used programming language these days.  Numerous verification tools have been developed for Java.  Java PathFinder, JPF for short, is such a tool.  JPF can check numerous properties of Java programs, like uncaught exceptions, integer overflow and deadlocks.  JPF has been designed in such a way that it can easily be extended.  I plan to improve and extend JPF.                                                                                The business process execution language for web services, BPEL for short, is the de-facto standard to program web service compositions.  Various verification tools have been developed for BPEL.  With my students, I also developed some.  All existing tools work with models of the business process, rather than the BPEL code itself.  My plan is to develop a verification tool, along the lines of JPF, that works directly with the BPEL code and, hence, will detect different errors than all existing tools.""393430,""Vandal, Alain"
"405765"	"VanOorschot, Paul"	"Topics in Computer Security and Usability"	"The general area of the research program is computer security and usability, or more concisely, usable security. The main focus is on selected problems related to authentication and identity management in computer systems. End-users (humans) need to identify themselves so that they may gain access, as authorized users, to computer resources and online services. All others who are unauthorized, such as those attempting to fraudulently gain such access, should be unable to do so. Problems of specific interest include authentication mechanisms which are usable for everyday tasks and ordinary people (vs. security or computer experts) using the Internet, for example for online banking or access to personal web sites. Mechanisms for authentication must be easy to use, otherwise they are bypassed by everyday users for whom security is not the primary goal; they must also be secure. However such ""usable security"" mechanisms have remained elusive, and only in the past few years is this beginning to be a significant focus of academic research. Practical problems that usable security mechanisms directly address, with respect to authentication and identification, include identity theft (which can result in significant personal losses in terms of time and dollars, and large collective economic losses), and loss of privacy (e.g., others may gain access to personal details intended to be available only to selected personal acquaintances). As one specific example, widespread use of social networking sites, without proper access controls, has lead to loss of privacy with serious implications. Similarly, dangers exist for other remotely accessed information, such as databases containing medical histories of patients. Usable authentication or access control mechanisms to address this will be explored and designed, as one example. The overall goal is to narrow the gap between usability and security, so that rather than having a choice between the two, convenient mechanisms can be employed which provide both simultaneously. The research involves analysis of existing mechanisms, identification of problems they exhibit, proposals of new mechanisms, and analysis thereof.""395374,""VanOorschot, Paul"
"395024"	"Vechtomova, Olga"	"Resolving complex information needs by means of natural language processing and interaction with user"	"Users, in particular those doing professional research on a certain subject, frequently have complex information needs. By a complex information need we mean a user's requirement to obtain information pertaining to a specific type of relationship between two or more entities, such as ""What are the causes of inflation?"", ""What can prevent osteoporosis?"", ""What effect does glucosamine have on arthritis?, ""What do people think about stem cell research?"". Some examples of users who might have complex information needs are business analysts, intelligence analysts, investigative journalists, medical doctors, police detectives, lawyers, and scientists. For instance, a business analyst might want to find information about the effects of credit card fraud on the Canadian banks. Currently available information retrieval technologies do not provide adequate support to users with such complex information needs: firstly, interfaces do not encourage or assist users in formulating complex queries, and secondly, search algorithms do not attempt to find instances of specific types of relationships between entities. In the proposed research we aim to develop methods to help users find information in response to complex information needs. This will be achieved by, first, developing methods that identify and rank information items (documents, passages, sentences) based on the evidence that they refer to the specific type of relationship between the entities mentioned in the query. Secondly, we will work on developing methods that will interactively help users to express complex information needs. The developed methods will be evaluated through test collections and by means of user studies.""399098,""Vederas, John"
"397079"	"Vlajic, Natalija"	"Self-healing and fault-tolerant wireless sensor networks"	"In recent years, wireless sensor networks (WSNs) have attracted great attention of the research community thanks to their tremendous potential in various application fields, including environment monitoring, security surveillance and disaster management, combat operations. A typical WSN setup assumes one or more of the following:(1) sensor nodes are randomly scattered throughout the deployment field, e.g. by being disseminated from a plane;(2) the deployment field is a region of irregular geographic composition, possibly comprising natural obstacles (lakes, cliffs, etc.) or man-made obstacles (buildings, overpasses, etc.);(3) sensor nodes are small, inexpensive, wireless and battery-powered devices prone to failure due to: component malfunctioning, battery depletion, environmental factors (e.g. extreme heat, flooding, freezing), man-caused factors (e.g. interference, accidental damage, explosion).Based on the above, the WSN topology inevitably gets plagued by serious irregularities and/or areas completely void of functioning nodes - a.k.a. holes. Clearly, the existence of such topological anomalies has a major negative impact on both, the network function (sensing of certain phenomenon) and operation (routing of data).Given that WSNs find their use in a number of critical applications, it is clear that the inability of these networks to operate correctly (or efficiently) in the presence of topological anomalies may cause damage to the human health and safety, and result in considerable financial loss. Accordingly, the main objective of the work proposed in this document is to advance the current state of research and application of WSNs, by devising new effective techniques of self-healing and fault-tolerance.""397077,""Vlajic, Natalija"
"396753"	"Vukovic, Aleksandar"	"Virtualized network for next generation internet"	"Imagine yourself sitting in a restaurant and by touching the surface of your table (display) you order your food and drink. While you are waiting for your order, by touching the same screen at your table, you can find out sports results of your favorite team or talk with a travel agent to book your hotel and flight for next month's business trip. Or maybe, you would like to watch an HD movie or send a 3D virtual birthday card.  You are living in a virtualized world where the ability to transform physical assets into logical ones, or to create associations of many-to-one and one-to-many is an every day reality.       The proposed ""Virtualized Network for Next Generation Internet"" program is all about that. Over the next 3 to 5 years, we will see a network paradigm shift - from a server-oriented network to service-oriented, from personal resources to shared resources, from an operator managed network to user managed, from partial availability to global access. This will require a completely new set of network functionalities. The network will become an open and transparent medium for forwarding application packets. The control of such a network will move at the edge of the network, towards end-users.      The proposed research program covers a 5-year term, aiming to: develop and demonstrate the capabilities of a virtualized network, conceptualize network agility for efficient use of bandwidth on demand, and empower end-users to control it. Practical benefits of developing such a network are as follows: a) Enabling services based on a private network created on-demand; b) Allocating dedicated bandwidth to access available shared network resources (e.g., processing, storage and memory), as needed; c) Using bandwidth efficiently (i.e., reduced network operational expenses); and d) Enabling practical applications in education, environment, health, genetics, biotechnology, entertainment, energy, manufacturing, space exploration, etc. ""406601,""Vulej, Walter"
"402312"	"Wahid, Khan"	"New architectures for real-time video and bio-medical imaging applications"	"Most real-time multimedia-based applications employ digital video and image processors that are built on computation of irrational transform coefficients (such as cosine, sine, or wavelet functions), which cannot be exactly implemented due to lack of finite binary representation. Hence, in implementing them, we invariably introduce heavy truncation errors; as the process continues, more errors are incurred, which eventually results in inefficient design, and degrades reconstruction quality. In the recent past, error-free scheme has been shown to be an effective way in eliminating such truncation errors, where the basis functions are exactly encoded with integers. This integer implementation is very simple, and enables building efficient VLSI architectures.      The research expands and refines the concept of ""error-free computation"" by first proposing an investigation in a novel multi-dimensional level that allows more implementation-friendly features (such as parallelism, pipelining, simple additions, etc.), resulting in low-area, low-power, and high-speed designs. Secondly, by taking advantage of these attributes, we aim to build efficient and performance-enhanced intellectual property cores such as: (a) transform units for JPEG, JPEG2000, and H.264 compliant encoders/decoders with high quality data reconstruction for real-time applications, (b) video compressor for biomedical imaging applications such as capsule endoscope and telemedicine, and (c) image compressor for information security applications such as digital watermarking. These architectures will be implemented on both field programmable gate arrays (FPGA) and standard-cell platforms so that the performance improvements in hardware and application levels can be demonstrated. Finally, we aim to develop a novel ""generalized error-free encoding"" that will encode virtually any discrete transform coefficient error-freely so that other designers may benefit from this scheme. The results of this research will significantly enhance the efficiency and the accuracy of many video-related commercial applications, which can be transferred to Canadian industry, enhancing Canada's strong role in defining future video products.""393904,""Wahl, Linda"
"395394"	"Walker, Robert"	"Unanticipated reuse"	"According to the federal government, Canada's software industry generates $28.6 billion in annual revenues, growing at a rate of 22 percent annually. Despite such rapid gains, productivity and quality remain serious concerns. While industry embraces attempts at managing quality, a prominent researcher argues: ""The concept of quality assurance and quality management systems is failing industry, government, academia and consumers. This is because quality systems and procedures have become a means of avoiding blame rather than a means of delivering an excellent product or service.""Unanticipated reuse is a means for taking the functionality in some existing software system, that is known to be of high quality, and integrating it into the software system under development, in the attempt at leveraging that high quality and reducing the development cost of the new system.  This idea has been hampered by the lack of infrastructural support, and the lack of scientific knowledge about the consequences of the detailed approaches needed to make unanticipated reuse a practical reality. This research program is focused on developing that infrastructural support while evaluating the strengths and weaknesses of its use.  The results of this research program will lead to unanticipated reuse that is better understood, better supported, and therefore less risky in industrial practice.""408220,""Walker, Sean"
"388892"	"Wand, Yair"	"Theory-based methods for information systems analysis and design"	"Information systems (IS) analysis focuses on understanding a business domain and defining the requirements for an IS to support this domain. System design is the creation of a plan for an IS that will fulfill the requirements identified in the analysis. A proper process of analysis and design is critical to the success of information systems. Most techniques in use for systems analysis and design are based on practice, not theory. Consequently, it is difficult to evaluate and choose appropriate techniques. This project is part of a research program to develop theoretical foundations for systems analysis and design with the following long-term objectives:1. Ground systems analysis and design in theoretical foundations,2. Develop practical methods based on the theoretical foundations,3. Develop computerized tools to support the theory-based methods.   The research is based on a view of an information system as a representation of an application domain (a work-system). Accordingly, the purpose of analysis and design is to develop a faithful and usable representation of the domain. The definition of s a faithful representation requires a characterization of what needs to be represented. In philosophy, the area that deals with modeling the world is ontology. Hence, the research is based on ontological foundations.    The specific objectives of the proposed research project are to develop:1. A method to model work systems in terms of business processes and interacting roles, 2. A method to identify IT-related control requirements in business processes,3. A method to link the IT enterprise architecture to work systems models, 4. An IS design that enables explicit representation of embedded business knowledge.""412113,""Wanderley, Marcelo"
"402313"	"Wang, Xianbin"	"Efficient adaptive communication technologies"	"Recent development in spectrum opportunistic communication and cognitive radio brings significant interests in the design of robust and adaptive transmission technologies for operation in hostile electromagnetic environment with strong interference and fast channel variation. In addition, it is envisioned the emerging of global wireless information infrastructures will be characterized by the convergence of a variety of wireless platforms, existing simultaneously and supporting diverse multimedia services. These combined trends in wireless systems and networks bring fundamental challenges for the design of efficient adaptive communication technologies.The primary goals of the proposed Discovery Grant program is to: (a) study the requirements on the transmission technologies that support spectrally efficient communication as well as the diverse multimedia data streams, (b) develop, analyze and validate the proposed adaptive communication technologies for the abovementioned purposes, (c) improve the efficiency of future wireless systems and networks through transmitter-receiver coordination and cross-layer design, and (d) system integration and performance evaluation of emerging wireless communication systems.It is expected that the project will bring significant benefit to the Canadian society through high qualified personnel (HQP) training. Parts of the proposed research program will be supported by a recently approved CFI infrastructure grant awarded to the applicant (Leaders Opportunity Fund for Canada Research Chair) at the University of Western Ontario. The students working on this project will be involved in the design, analysis, simulation and prototyping of emerging communication systems using the CFI facilities. The experience and knowledge gained from this project by the students will prove beneficial to the Canada by enhancing the competitiveness of the research and industry communities.""413524,""Wang, Xianbin"
"402855"	"Wang, Zhou"	"Perceptual image processing"	"The early years of the 21st century have witnessed a tremendous growth in the use of digital images as a means for representing and communicating information. Digital images are subject to a wide variety of distortions during acquisition, compression, transmission, processing, and reproduction. Eventually, most of these images will be consumed by human eyes. However, the most widely used objective image quality assessment (IQA) approaches have been found to be poorly correlated with perceived image quality.    The objective of this project is to explore new theories and methodologies in the field of perceptual image processing, which aims to develop IQA methods that are consistent with visual perception and use them to benchmark and optimize a wide spectrum of image processing algorithms and systems. The last 5 years have seen a sudden acceleration in progress and interest in the area, and it is anticipated that the next 5-10 years will be the critical defining period of this immature but fast-evolving field. The short-term goals of this project include: 1) development of novel IQA methodologies and extension of their scope of application to pattern recognition tasks; 2) investigation of a new framework for perceptual image coding; and 3) exploitation of a new paradigm for perceptual image restoration. The long-term goals of this project are to establish a complete theoretical foundation and to provide a set of fundamental methodologies for perceptual image processing. Great effort will also be made to transfer the research outputs into commercially viable technologies.     The PI's recent related work has been frequently and widely cited by both academic and industrial researchers and implemented in publicly available software packages. The success of the proposed research will not only make significant impact to the academic communities of signal/image/video processing and computer/human vision sciences, but also provide new technologies for many industrial, medical and military applications. In the process of achieving the scientific and technological objectives, we will train highly qualified personnel in information technology for Canada.""403735,""Wang, Zhuo"
"391911"	"Watters, Carolyn"	"managing complex tasks on the web"	"Our reliance on the Internet for an expanding proportion of our information and communication needs calls into question the design of the applications we use for a growing variety of tasks. That is, not only do we seek information, we plan holidays, conduct research, buy goods and services, provide goods and services, execute financial transactions, have fun, and process our email.  How then do we model the user and user tasks in order to incorporate that modeling into the design of tools and functions that improve both user effectiveness and user experience for increasingly complex tasks.In this research we extend previous work on user modeling in the context of the Web browser application. First, we will explore what types of tasks users complete on the web and how they currently perform these tasks to develop behavioral models. Following this analysis we will design new tools and functions based on these models  to try to improve not only the effectiveness and efficiency of completing these types of tasks but also to improve the user experience.  An examination of subtasks and the use of current tools to ""cope"" can be used to predict features of tools that can be tailored to task as well as to individual preferences. The methodology of this research includes, analysis of current practices, modeling of behavior, design and implementation of new tools and methods, followed by laboratory and field user experiments to validate hypotheses. ""397866,""Watterson, James"
"391282"	"Williamson, Carey"	"Wireless Internet Performance"	"Wireless network technology continues to advance at a phenomenal pace. Wireless Local Area Networks (WLANs) such as WiFi are now widely deployed, and have become a vital part of our networked information infrastructure, providing users flexible and ubiquitous wireless Internet access.Despite the prevalent deployment and use of WLAN technologies, wireless networks are not without performance problems. These problems arise because of multi-layer interactions between Internet protocols and the characteristics of wireless network environments (e.g., mobility, transient connectivity, time-varying channel conditions). Many of these protocol performance issues will pervade new wireless technologies too.The general objectives of the proposed research program are to identify protocol performance problems in broadband wireless networks, and to propose and evaluate innovative solutions to these performance problems. These objectives will guide the research exploration across a range of wireless technologies, including WiFi, WiMax, ad hoc, wireless mesh, and hybrid wireless/cellular network technologies.The research will use a combination of experimental, simulation, and analytical approaches. Empirical measurement and experimental evaluation will be a mainstay in the research, using the facilities provided by the Wireless Internet Performance Laboratory at the University of Calgary.Training of HQP is a key part of the research plan. Strong partnerships and industrial collaboration are anticipated throughout the duration of the research program.""412710,""Williamson, Carey"
"402857"	"Woelfel, Philipp"	"Applications of probabilistic methods to algorithms and complexity"	"Probabilistic methods belong to the most important tools in theoretical computer science. While the design of efficient randomized algorithms is one of the obvious applications, probabilistic methods are also extensively used in other areas, such as combinatorics or in complexity theory. In this research program, I intend to develop and apply probabilistic methods in three areas of theoretical computer science.(1) Algorithms: For hashing and dictionary problems, there is a gap between the type of algorithms that are used in practice and algorithms that have good theoretical properties. The best randomized dictionaries provide lookup and update operations in constant time (with high probability). But often the constants in the time and space bounds are huge, or the algorithms are too complicated to be accepted in practice. My goal is to find simpler algorithms, mainly for hashing applications, that have good running time guarantees, but where the constant factors in time and space are such that the algorithms become interesting for practical purposes.(2) Distributed Computing: Problems on asynchronous shared memory models have been studied extensively, but with respect to randomized computation, many questions still remain unanswered. On the other hand, probabilistic methods seem to be a suitable tool to overcome the problems associated with asynchrony. My goal is to devise new randomized shared memory algorithms for selected problems such as mutual exclusion, or the simulation of strong atomic operations by weaker ones.(3) Complexity Theory: The ""number on forehead"" multiparty communication model is an important computation model, that is not well enough understood. Good lower bounds in this model would have significant consequences for circuit, propositional proof, or branching program complexity. I intend to apply probabilistic methods, especially those based on universal hash families, in order to develop better lower bound methods and to gain a better understanding of the multiparty communication model.""402688,""Wohl, Gregory"
"395979"	"Wu, Dan"	"Uncertain knowledge representation and reasoning in mobile multi-robot system"	"Probabilistic robotics is the current state-of-the-art in mobile robotics research. However, the research on probabilistic robotics mostly focus on a single mobile robot, this makes one to speculate how the probabilistic robotics approach can be scaled up to deal with multiple mobile robots. Some initial efforts have been tried. Itis realized in this endeavor that the choice of uncertain knowledge representation is crucial in the design of any probabilistic algorithms. It is also emphasized that the development of new representation of uncertainty is absolutely essential for the probabilistic robotics paradigm to scale up to more complex problems, such as the problem of multi-robot coordination and collaboration. The long-term objective of the proposed research is to advance the probabilistic robotics research from a single robot oriented paradigm to a multi-robot oriented paradigm by (a) providing an uncertain knowledge representation and reasoning framework that is capable ofmodeling multiple mobile robots and their environment; (b) using the proposed representation framework to study problems in multi-robots coordination and collaboration.The anticipated significance of the proposal is twofold. From a theoretical development perspective, the proposed research advances the current research on probabilistic robotics from a single-robot paradigm to a multi-robot paradigm. From a practical perspective, it is not uncommon nowadays to see service robots appear in commercial market at affordable prices. The market for personal and mobile robots grew to $5.4 billion in 2004 and become larger than the industrial, non-mobile robot market; by 2010, that figure will approach $17 billion. How multiple service robots can cooperatively and collaboratively meet consumes' need have to be thoroughly studied before projected market value can be realized.""404168,""Wu, Di"
"394999"	"Wu, Kui"	"Perception-based wireless sensor networks"	"This project aims to study the impact of perception-based computing model on the lower-layer networking design and to obtain efficient networking solutions to support intelligent applications with wireless sensor networks (WSNs), including for example greenhouse control, environmental and natural resource monitoring and management, and ""smart homes"" to aid seniors' daily lives.Recent progress in wireless communication and Micro-ElectroMechanical System (MEMS) makes it feasible to build tiny wireless sensor nodes that integrate sensors, processors, memory, and wireless transceivers within the size of several cube centimeters. Such tiny sensor nodes, when clustered together, automatically create highly flexible, low-power wireless sensor networks (WSNs) with numerous new applications ranging from environment monitoring and building control systems to smart entertainment devices that adjust audio and video quality based on changing environments. Many applications with WSNs require human interaction and perception, for example, a sensor-based intelligent entertainment system that provides good audio/video quality to audience, a sensor-guided navigation system that provides useful information to visitors, an unmanned aerial vehicle based on visual perception and environmental feedback, a home control system that supports comfortable living conditions. In these systems, rational decisions have to be made in an environment of imprecision and uncertainty. The computational theory of perception has become a useful tool in perception-based reasoning and has been applied in many application areas. Very little research, however, has investigated how the perception-based computing model at the application layer impacts and guides the network design at the lower layers, e.g., resource allocation, data processing, sensor scheduling, and routing. This project is to fill the vacancy by exploring the advantages of perception-based computing model in the network design and resource utilization of WSNs. ""394019,""Wu, Lang"
"395348"	"Xu, Changqing"	"Advanced nonlinear optical materials"	"Despite the advance of the laser technology, wavelength that can be reached by the conventional solid state and gas lasers is still very limited. Wavelength conversion in periodically poled crystals is considered to be a promising technology to obtain new laser wavelengths that are not accessible to conventional solid state and gas lasers. The proposed program aims to maintain the leading position of the research and education programs on advanced nonlinear optical materials and devices in Dr. Xu's group at McMaster University. The preliminary objectives of the proposed program are to develop efficient periodically poled (PP) crystals with large thickness (up to 3 mm) and short period (down to 1 um), which are required in generating new wavelength over a broad wavelength range (350 nm - 5000 nm); to study photorefractive and thermal effects related to the PP-crystals; to develop compact laser sources at wavelengths that cannot be reached by the commercial semiconductor laser diodes; and to explore new applications of the PP-crystals in the fields of laser display, bio-instrumentation, quantum communications, environment monitoring, and spectroscopy. The long-term objectives of the proposed program are to establish and maintain the research team as one of the leading players internationally in this area, enhance collaborations with photonic companies and institutions in Canada and worldwide, provide Canadian industry with highly qualified personnel and new/improved products, and therefore, contribute to the growth of Canadian economy. The scientific knowledge generated is expected to bring to market a new family of photonic materials and devices. The graduate students trained will meet the need for highly qualified personnel and contribute significantly to the deployment of photonic material processing and laser technologies.""409604,""Xu, Chen"
"396701"	"Yang, LaurenceTianruo"	"Testing methodologies with power minimization for embedded systems"	"Driven by the rapid growth of the Internet, communication technologies, pervasive computing, automobiles, airplanes, wireless and portable consumer electronics, embedded systems  have moved from a craft to an emerging and very promising discipline in today's electronic industry. Testing of a fabricated chip is a process that applies a sequence of inputs to the chip and analyzes the chip's output sequence to ascertain whether it functions correctly. As the chip density grows to beyond millions of gates, embedded system testing becomes a formidable task. Vast amounts of time and money have been invested by the semiconductor industry just to ensure the high testability of products. On the other hand, as design complexity drastically increases, current gate-level design and test methodology alone can no longer satisfy stringent time-to-market requirements. The proposed High-Level Test Synthesis (HLTS) system will develop new systematic built-in, self-testable techniques to integrate testability consideration into the synthesis process, and make it possible for an automatic synthesis tool to predict testability of the synthesized circuits accurately in the early stage. It also optimizes the designs in terms of test cost, as well as performance and hardware area cost.Additionally, the proposal targets another problematic consideration affecting the design of systems-on-chip and embedded systems, namely the power consumption. All the existing research, including my previous work on high-level synthesis approaches, has considered low power and testability as two separate optimization objectives. I am planning to extend the above built-in, self-testable synthesis system for low-power dissipation. It is anticipated that a data path allocation and integrated synthesis algorithms (with scheduling and allocation) will be developed to achieve a fully self-testable design and low power dissipation without resulting performance degradation and with minimal area overhead.""410174,""Yang, Menglin"
"406171"	"Yang, Oliver"	"Research and applications of traffic congestion control in computer networks"	"Congestion control has been an essential element to the robustness of the Internet, and to the provision of QoS (Quality of Service) to its services. We have systematically explored many potential approaches using the classical control theory because it can provide a theoretical basis for the stability of a control algorithm, instead of relying on any heuristics from intuition or experience that cannot be guaranteed. In this new phase of research, we would like to 1)    )explore a few more outstanding congestion controllers using classical control theory.2)    )research on traffic controllers using the latest modern control techniques.Our theoretical research so far has helped us to produce some good controller designs.  We are seeking collaborations to apply these designs in the photonic or wireless networks via testbed demonstrations. The implementation of a testbed would bring  i) more practicality to our research; ii) a stronger collaboration with academics and industry; iii) publicity for our research; and  iv)  opportunities for new research.Results from this research project could stimulate further leading-edge research, both fundamental and applied. The experience accumulated in this research will give students a competitive edge in their industry jobs upon graduation.""390504,""Yang, Oliver"
"390456"	"Yuan, LiYan"	"Concurrency control and grid database management systems"	"Concurrency Control and Grid Database Management SystemsThe proposed research is to develop a new architecture for grid database management systems (GDBMS) with the following distinct features:  1. the system will run on a cluster of commodity (PC-based) servers;  2. all the data will be stored on the direct-attached RAID disks, each of them will be accessed     only by its dedicated server;  3. the system will adapt the staged architecture; and  4. the logic formula protocol will be used for concurrency control.Because of the advantages of the logic formula protocol and the staged architecture design, we anticipate that the system will greatly increase the performance and scalability of database management systems, and signicantly reduce the total cost of the database systems. ""402421,""Yuan, Tao"
"402314"	"Zhang, JianKang"	"Diversity space-time modulation for MIMO wireless communication systems with fading channels"	"The recent arrival of the Information Age has created an explosive demand for knowledge and information exchange in our society. This demand has triggered off an enormous expansion in wireless communications in which severe technical challenges, including the need of transmitting speech, data and video at high rates in an environment rich of scattering, have been encountered. A recent development in wireless communication systems is the multi-input multi-output (MIMO) wireless link which, due to its potential in meeting these challenges caused by fading channels together with power and bandwidth limitations, has become a very important area of research. The importance of MIMO communications lies in the fact that they are able to provide a significant increase in capacity over a single antenna system. To take advantage of the MIMO communication channel and combat fading and cross-talk, a space-time block code (STBC) has been developed by spacing the transmitter antennas sufficiently and introducing a code for the transmitted symbols distributed over transmitter antennas (space) and symbol periods (time). The research program presented in this proposal is to develop a first-rate research and training environment for the MIMO wireless communication systems. The proposal aims at establishing a novel theory and solving the following three open and challenging problems: (1) The design of an optimal STBC for the MIMO systems with the optimal coherent receiver (i.e., the channel knowledge is available at the receiver); (2) The design of an optimal STBC for multi-input single-output systems with simple coherent receivers. Such a design is often useful in mobile downlink communications; (3) The systematic design of a STBC for the unique identification of the MIMO fading channels when channel information is completely unavailable. The proposed research program will create an essential research and training environment appropriate to impart requisite skills to highly qualified personnel (HQP) who will fulfill the needs of Canadian academia and wireless communication industries. The program will attract talented and experienced researchers and HQP in the world and retain them in Canada.""402488,""Zhang, Jianping"
"397769"	"Zhang, JohnZhong"	"Visibility-based polygon search problems"	"We propose to study polygon search problems in computational geometry and robotics. Computational geometry is the study of algorithms to solve problems related to geometry. In many applications of the discipline, visibility problems play a fundamental role. In essence, given a polygon, if the line segment connecting two given points is within the polygon, we say that the two points are mutually visible. Despite its simple definition, there are many intriguing problems related to it. Motion planning in robotics is one of the applications related to visibility problems. Basically, it asks how to plan the movement of robots in a given area such that any intruders inside the area will be detected. Our work studies motion planning for robots at an abstract level. It was coined as the polygon search problem, in which the given area is delineated as a simple polygon and the robots (also called searchers or guards) and intruders are represented as mobile points. Our research centers around three directions of the polygon search problem. (1) Characterization of searchable polygons, in terms of the geometric patterns, under different settings of searchers, such as a 1-searcher, who is equipped with one flashlight; (2) Generation of a search schedule that the searcher(s) can follow to search a polygon, once the polygon is determined to be searchable; (3) On-line search in a polygon with unknown geometric properties, such as its edges and vertices. Our study will improve our understanding of polygon search problems, provide further improvements to the current results, and solve new proposed problems.""402758,""Zhang, Junfeng"
"395352"	"Zhao, Dongmei"	"Radio resource management in a reconfigurable wireless network"	"The evolution of software radio has enabled wireless network devices to dynamically reconfigure their air interfaces. As a result, network protocols and resource management should fully exploit the advantages provided by this flexibility. Although extensive work has been done to enable network reconfiguration at the physical layer, little has been done to study issues related to quality of service (QoS) provisioning and resource management in a network where stations can dynamically reconfigure their air interfaces. In the proposed project we will study important issues in allocating radio resources for supporting multimedia services in reconfigurable wireless networks. The objective of this research is to provide solutions and technologies so that mobile stations can keep their traffic always best connected by dynamic and adaptive reconfigurations, and the precious radio resources can be efficiently utilized. Specifically, we will propose strategies for mobile stations to adaptively select their air interfaces, design admission control and transmission scheduling schemes for networks to effectively support the QoS of multimedia traffic, and provide solutions to ensure smooth connections during and after reconfigurations.  The basic approach is to take into consideration both horizontal and vertical information in a reconfigurable network in order to optimize the resource allocation performance, where horizontal information comes from sub-networks using different air interfaces, and vertical information is from different layers of the network protocol stack.  The outcomes of this research will contribute towards future always-best-connected networks and benefit network operators, mobile users, and equipment manufacturers. This work will benefit the Canadian telecommunication industry by attracting more investments in wireless networks and encouraging more mobile wireless users.""395523,""Zhao, Dongmei"
"402845"	"Zhu, Bo"	"Towards privacy-preserving security data sharing within large-scale collaborative detection systems"	"Recent reports show a trend of increasing professionalization and commercialization of maliciousactivities, as well as the increase in terms of the number of attacks. Malicious users are collaboratingwith each other so as to launch attacks more efficiently and effectively. For example, they have builtunderground forums for exchanging information about new exploits and underground economy serversto trade compromised resources. Unfortunately, on the other side of the attacking-defending competition,there are relatively less collaborations going on among the defenders. The major barrier is the concernthat, shared security data may contain sensitive information, which may be disclosed and misused at alater time. And such information may be originally difficult to obtain by adversaries.To address this concern, in the long term, the proposed research program will develop techniques that canpreserve privacy while sharing security data within collaborative detection systems, and at the same timeensure the usability of such systems, in a realistic adversarial environment. In the short time, the proposedresearch program will aim at achieving an appropriate balance between privacy and other properties that arecritical to the practical usages of the system. Such properties include availability, accuracy (dependent onquality and quantity of shared security data), and efficiency. The proposed research will have significantimpacts on boosting a variety of collaborative security applications, e.g., zero-day malware detectionand spam filtering, through strong protection of the system's privacy and usability as well as incentivemechanisms. As a result, both security and cost-effectiveness (in terms of defending attacks) of computersystems, no matter within an Enterprise network or national-wide, will greatly benefit from the proposedresearch.""409343,""Zhu, Chenchong"
"391693"	"Zhuang, Weihua"	"Resource allocation in spectrum agile wireless networks"	"Limitation of radio spectrum has been a major hurdle in meeting the increasing demand for mobile multimedia communication services. However, a large portion of licensed radio spectrum is not used efficiently. This project is to investigate and develop control algorithms and protocols for efficient radio resource allocation and quality-of-service (QoS) provisioning to secondary users in a wireless network, where the instantaneous spectrum unused by licensed primary users can be properly identified via cognitive radio technologies. By modeling the instantaneous radio resource availability as a random process, we will study three fundamental issues in resource allocation to the secondary users: 1) medium access control, which specifies how each user contends for and shares available resources with its neighboring users; 2) link-layer statistical channel modeling and call admission control, which is essential to facilitate the development of spectrum sharing rules; 3) distributed and dynamic routing for end-to-end QoS provisioning. The research will create new ideas and generate new knowledge for efficient radio resource allocation and effective QoS provisioning in the spectrum agile wireless networks. The novelty and significance lie in three aspects: 1) Resource allocation based on cognitive radios is just at its embryonic stage. QoS support to the secondary users poses significant technical challenges due to the random nature of resource availability; 2) The statistical channel modeling and analysis such as traffic arrival and service processes will provide more insights on how the networks behave under distributed control and will help to develop more efficient and effective network control mechanisms than simulation and/or measurement based approaches; 3) As distributed network control is mainly based on limited or even inconsistent network state information, the modeling and analysis is much more complex than those for networks with centralized control, especially when the network topology, resource availability, and traffic load change dynamically with user mobility and multimedia traffic. The research will contribute to open up a broad range of innovative applications for economical ubiquitous communication services to mobile users.""392846,""Ziada, Samir"
"393226"	"Zilic, Zeljko"	"Design tools for microsystems with post-fabrication debug, correction and tuning"	"The integrated microsystems of the future will become significantly more complex and heterogeneous. To realize their sensing, computing and wireless communication functions, they will employ microelectronics substrate and embedded software, along with microfluidics, MEMS and emerging technologies. Even now, the fraction of first-time-correct microelectronic devices has been drastically reduced from just a few years ago - the staggering complexity, compounded with the time-to-market pressures, has made the time-consuming task of pre-silicon verification increasingly unfeasible. Robust and scalable solutions are therefore needed to help in salvaging and improving the integrated system usability, following the incomplete pre-manufacture verification. Failures discovered after manufacturing integrated microsystems are increasingly caused by interactions of multiple blocks, possibly implemented by different technologies. The new techniques are needed to facilitate post-fabrication verification, the detection and locating of design errors, and eventually their correction. The proposed research program will investigate debugging and the design for debugging, i.e., techniques that facilitate discovering and locating faults, together with finding suitable means for their correction, especially in the realm of post-fabrication integrated microsystems debug. Since only the holistic approach will suffice for heterogeneity and complexity reasons, this research program includes a rich set of sub-projects, building on our recent advances in relevant verification and fault detection schemes for targeted implementation technologies and system-level techniques. The specific subprojects include the use of assertion-based techniques for debugging, in addition to the debug techniques for arithmetic circuits, mixed-signal systems, and transaction-level modeling, as well as the exploratory techniques for the emerging technologies""407595,""Zilinskas, Gregory"
"402862"	"Zimmermann, Thomas"	"Mining software repositories across projects"	"At the beginning of the last century, the philosopher George Santayana remarked that those who could not remember the past would be condemned to repeat it.  In other words, to achieve progress, we must learn from history.Recently, researchers and practitioners realized the potential of historical information to understand and support software development.  So far, software repositories (such as version archives and bug databases) were used to build recommendation systems for developers, identify experts within projects, classify changes into good or bad, and to predict the number of future bugs.  Mining repositories works best on large projects with a long and rich development history and is therefore actively used by companies such as Microsoft and SAP AG.Smaller and new projects, however, rarely have enough data for the above techniques.  In this research, we hypothesize that such projects can learn from the repositories shared by similar projects.  As part of this research program, we will develop techniques that mine across projects and allow transferring knowledge from one project to another.  Our research will support both open-source and closed-source software development, as well as small, medium, and large businesses.""406471,""ZincirHeywood, Nur"
