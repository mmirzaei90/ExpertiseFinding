"id"	"researcher_name"	"application_title"	"application_summary"
"427364"	"Abhari, Ramesh"	"Enabling ultra wideband applications through innovation in design of interconnects, passive elements, and antennas"	"Electronic industry has dramatically advanced growth during the past few decades but still struggles to achieve the extreme specifications needed in many emerging applications that require ultra-wide band and high-speed signal transmission. The passive back bone of electronic system including interconnects and passive components have been identified as the main factors limiting system performance. Recently published technology roadmaps acknowledge the bandwidth limitation of interconnects and call for dramatic solutions to enable development of next generation electronic systems. In this proposal, an alternative solution for signaling in electronic systems by using bandpass interconnects instead of the conventional lowpass ones is proposed. These bandpass interconnects include substrate integrated waveguide-type structures, wireless interconnects that use antennas for launching and retrieving information and AC coupled interconnects that employ capacitive or inductive coupling for signal transfer. The bandpass feature of these interconnects brings new system capabilities such as wider bandwidth and lower transmission losses for the case of waveguides, and reduced delay and flexibility for the case of wireless interconnects. The AC coupled interconnects provide promising compact system integration capabilities. Each group opens new directions of innovative research for increasing data handling capacity of electronic systems. In this research program, system design considerations such as customizing driver and receiver blocks, dispersion compensation and signal and power integrity evaluations will be investigated. Development of enhanced passive components and integrated antennas, their miniaturization and signal integrity evaluations form another thrust of this research program. Electromagnetic bandgap structures continue to be a key element in engineering new components and power integrity solutions. The ultimate goal of this research is to address the design challenges in deployment of ultra wideband electronic systems. ""427048,""Abid, ZineEddine"
"427413"	"Abugharbieh, Rafeef"	"Novel paradigms for computational analysis for structure and function in medical images"	"Over the past two decades, medical imaging has become one of the main pillars of modern healthcare. The increasingly rich spectrum of available structural and functional imaging technologies provides exquisite medical data that offer tremendous opportunities for non-invasive visualization and quantitative analysis of anatomy and physiology. Applications, some of which are clinical realities, vary from basic computer-assisted diagnostics to elaborate image-guided interventions. Exploiting the enormous amounts of information embedded in such medical image data is, however, a very complicated task. In fact, the lack of efficient, accurate, and robust computational analysis techniques poses huge challenges that seriously hinder optimal extraction and use of image information in practice. The main objective of this research project is to advance the field of in-vivo imaging-based characterization and monitoring of human health. Specifically, it aims to enable quantitative assessment of morphology and function and to facilitate better understanding of the effects of various diseases and therapies. This will be achieved through the development novel techniques for automated analysis of anatomical and functional regions of interest, particularly in magnetic resonance imaging (MRI) and functional MRI (fMRI) data. The applications targeted will focus on neurological (brain) and musculoskeletal (knee and hip) imaging. Advancing research applications in these important areas has tremendous impact and benefits to large sections of the population, considering our aging demographics where diseases like Parkinson's, stroke, and arthritis, are common, debilitating, and constitute a large fraction of the healthcare burden.  ""429757,""AbuGhazaleh, Haitham"
"427235"	"Adams, Carlisle"	"Techniques for secure and private interactions in online environments"	"Online environments are communities of interest whose interactions take place primarily, or perhaps exclusively, over communications infrastructures (including wired and wireless Internet, telephone, and sensor networks). Examples of online environments include e-health, e-government, e-business, e-commerce, and e-learning. Security and privacy are essential services that must be tightly integrated with the design and implementation of such environments.This research program will focus on the use of combinations of cryptographic and non-cryptographic techniques to create security / privacy technologies that are effective and that can be widely deployed. Three areas are of particular interest: access control (making use of policy-based cryptography), credential infrastructures (making use of anonymous proofs of attribute ownership), and secure electronic voting (making use of end-to-end verifiable e-voting technologies). In all three areas, the concepts are built on firm underlying mathematics and proofs of security, but questions of efficiency, usability, and abuse prevention still need to be explored for a number of environments, as well as determination of appropriate infrastructural components and operational procedures.Our approach will be to combine cryptographic techniques with policy infrastructures, biometrics, authorization mechanisms, semi-trusted hardware, and other technologies to create deployable solutions for specific environments.""441302,""Adams, Carolyn"
"420166"	"Agathoklis, Panajotis"	"Multidimensional system design and implementation"	"Multidimensional systems are used to process signals which have more than one independent variable.  They have applications in space-time processing, image and video processing.  The proposed research focuses on the following problems:1. Beamforming and Direction of Arrival Estimation:  Algorithms for Direction of Arrival (DOA) Estimation will be developed for broadband applications such as ultra wideband communications or speech detection using sensor arrays.  The approach will be based on using subarray beamforming which estimates the DOA from the phase difference between a reference signal and its phase shifted version.  The proposed approach is expected to lead to a computationally simple DOA estimation technique for broadband signals.2. Multidimensional Filter Design and Implementation: Design techniques and implementation efficiency with respect to computations and roundoff errors for multidimensional (2-D and 3-D) filters will be studied and evaluated. The possible application areas to be considered are filtering of broadband signals propagated by plane waves and filters for video standards conversion.3. Applications of Image Registration: The work of the applicant and his students on image registration will continue by focusing in the application of a new and efficient image registration technique to possible applications such as content based image retrieval.""424889,""Agbossou, Kodjo"
"427394"	"Ahmed, MohamedHossam"	"Design and analysis of cooperative-diversity wireless networks"	"There is a growing demand for efficient techniques for next generation wireless systems. Hence, it is timely to address new technologies that can revolutionize wireless communication systems in terms of transmission rates and quality of service. Cooperative diversity is a very promising technology which is expected to be one of the underlying technologies for future wireless systems. The main concept of cooperative diversity is that intermediate nodes between the source and destination nodes can forward the source signal to obtain multiple copies of the same signal at the destination. Combining these multiple copies can bring many benefits such as better signal quality, improved coverage and higher capacity. However, cooperative diversity also brings some challenges and complexities. The main issues to be addressed in this program of research are the following:1) Novel cooperation and relaying techniques 2) Cross-layer resource allocation algorithms for cooperative-diversity networks3) Synchronization and channel estimation in cooperative-diversity networks4) Incorporating cooperative diversity in existing and emerging wireless systems5) Developing policies and incentives to encourage users cooperationThe study will start with defining a framework for cooperative-diversity networks. Next, the main challenges will be identified. Novel techniques and algorithm will then be proposed. Then, the proposed algorithms will be analyzed using computer simulation, analytical methods and experimental work. Results from this research are crucial for gaining deeper insight into the cooperative-diversity concept. Also, the results can be utilized to facilitate its adoption in existing and future wireless systems. In addition, this research will enable the training of several graduate and undergraduate students.""420367,""Ahmed, Nasiruddin"
"431880"	"Aiello, William"	"Enterprise security"	"It is a truism that the Internet has become integral to nearly all aspects of modern life, from science to  entertainment, from government to industry, from the economy to our culture.  But while the open architecture  of the Internet has been the chief enabler of it's explosive growth, this same openness, combined with the  abundance of insecure commercial software installed in end-hosts, has provided attackers with a nearly  inexhaustible supply of vulnerable hosts to exploit, either as primary victims or as intermediate steps in a chain of exploits.  These threats comprimise the safety and integrity of commerce and activity on the Internet.   We propose here research into techniques and tools for enterprises to better secure their networks, hosts, and data resources.  Two themes recur throughout this research.  The first is an emphasis on the anomaly detection approach rather than the misuse detection approach.  In the later, models are made of illegitimate traffic and alarms are raised when traffic or  behaviour matches a model.  In the former, a model is made of legitimate traffic and an alarm is raised when traffic or behaviour deviates from the model.  The majority of security tools rely on misuse detection as there is a wide-spread belief that the behaviour of computing systems are too complex to model for use in anomaly detection.  Two of the projects described in this proposal involve modelling complex domains.  The second  theme is an emphasis on measuring and improving the tradeoff between security and usability/availability.""437668,""Aikens, Kathleen"
"422917"	"Alajaji, Fady"	"Joint source-channel coding theory with applications to wireless communication networks"	"In this application, we seek funding to continue our research program in information theory and wireless communications. The research program concerns the development of advanced joint source-channel coding (JSCC) technology for improving the reliability, connectivity and speed of wireless communication networks and meeting the rapidly growing demand  for seamless and ubiquitous wireless multimedia services. JSCC provides the  right approach to construct the best codes for the efficient transmission of data sources over noisy channels, particularly under stringent delay and complexity constraints and for multi-user networks where Shannon's principle for the separation of data compression and channel coding fails to hold in general. Building on our experience and extensive achievements in JSCC, we will focus on the following four research objectives: (i) the information theoretic investigation of the JSCC error exponent for multi-user networks; (ii) the development of sophisticated discrete channel models to judiciously represent the behavior of soft-decision demodulated correlated fading multi-antenna channels; (iii) the design and analysis of powerful low-complexity JSCC techniques for wireless networks; (iv) the application of the above JSCC methods to wireless multimedia communications and sensor networks.This research project, which will feature the training of five graduate students (3 doctoral and 2 Masters) and one postdoctoral fellow per year, is anticipated to provide a deeper understanding of both the fundamental principles and the practical aspects of JSCC, thus positively contributing to a sustainable improvement of wireless communications and multimedia technologies.""434899,""Alam, Jahrul"
"424803"	"Alencar, Paulo"	"Recombinant and evolutionary high-level software architectures: Design, integration, evolution and analysis"	"One of the fundamental goals of software engineering is to enable predictable and modular construction and evolution of software systems by assembling software architecture components. However, due to the increasing complexity, distributed, and dynamic nature of current information systems, undetected integration failures from internal violations may be ultimately revealed only as costly accidents that often cause undesirable system crashes. In addition, interface and configuration issues frequently arise from misunderstanding, incompatibilities, or differences resulting from independent development practices, heterogeneous environments, or different vendors. As a result, since maintaining and changing this class of (Web-based) distributed and dynamic software systems usually requires the examination and modification of components and component interactions, system integration becomes difficult to maintain as system components are extended or upgraded. The main goal of this project is to define and evaluate a new recombinant and evolutionary software architecture approach, and supporting methods and tools, for component-based design, integration, evolution, and analysis. The approach is based on formal abstract models of distributed event-based recombinant components, extended component interfaces, concern and process-based evolutionary extensions, and integration and evolution contracts that can provide a more expressive definition of component-based system design representations and an enhanced analysis of the interaction of architectural components, their extended functionality and behavior, and their compositions.  In a general way, the results of this research can impact individuals or (Canadian) organizations that are struggling to produce large, complex (dynamic) systems and applications, and can help push the drive to improve productivity, software quality, reduce maintenance overheads, and impact on the establishment of a marketplace of contract-based software components. In addition, we are convinced that providing a non-ad-hoc and comprehensive component integration and evolution theory and its automated support will radically improve software engineering efforts.""442443,""AlEnzi, Mustafa"
"425402"	"An, Aijun"	"Resource-adaptive learning of classification rules from data streams"	"With the recent advances in hardware and software, data streams have become ubiquitous as many sources produce data continuously and rapidly. Mining data streams is to extract knowledge structures or patterns in a non-stopping stream of data. Such patterns can be used for effectively monitoring, analyzing and predicting the changes in the application environment where the data stream is collected. Over the past few years, models and techniques have been proposed to extract patterns from data streams. However, many open issues still remain in data stream mining.     The main objective of the proposed research program is to provide solutions to the open issues in data stream mining and develop a resource-adaptive framework for learning classification rules from data streams. We will work on the following issues. An efficient data structure will be designed that aggregates the data on the fly by making trade-offs between memory consumption and accuracy according to resource conditions. We will develop anytime rule learning and classification algorithms that can produce a best possible answer according to the real-time constraints. We will also systematically investigate the types of concept drifts and develop methods for detecting and handling different types of concepts drifts. In addition, we will design dynamic online discretization methods for use with rule learning from data streams.     Mining data streams is still in its infancy state. The proposed research program will advance the field by providing solutions to its open challenges. Although we focus on classification rule learning in this research, the proposed data structure and algorithms can be adapted for use in other data stream mining tasks. With the growing proliferation of handheld devices, mobile phones, wearable computers and tablets, it becomes ever more important to develop online resource-adaptive data mining systems to assist people in decision making. The techniques developed in this research program will have a wide range of applications in areas such as networking, telecomminucations, scientific experiments, finance and medicine.""432352,""Anaka, Matthew"
"425356"	"Anderson, John"	"Flexible team formation and task allocation for multi-robot systems in complex domains"	"The exploration of an unknown environment to gather information (e.g. mapping) is a problem naturally suited to the use of multiple intelligent robots. In highly challenging and dangerous environments such as urban search and rescue (USAR, where robots are used to search for human victims after a disaster) significant benefit derives from using robots with differing motor, sensory, and control abilities (heterogeneous robots). While a robot might find a crevasse it cannot enter, another team member might be small enough to move through this, for example.  Similarly, different terrain is amenable to different robot types (e.g. humanoid vs wheeled).  In such domains robots must also be considered expendable, meaning that team structure will not be static: new types of specialized robots may be called for, and robots that are lost or damaged must be replaced.  Ideally multiple teams should be released across a broad area and collaborate to build a shared map.  Existing approaches to this typically require centralized control (leaving the system vulnerable if a controller is lost), static team structures, and a shared external representation for space (e.g. GPS, which may be unavailable).  Similar issues to USAR exist in space exploration, military, and security domains, as well as area coverage tasks (e.g. robotic harvesting).  This work will involve the decentralized coordination of multiple teams of heterogeneous robots (wheeled, treaded, humanoid, and microrobots) in a USAR domain.   It will support the gradual development of a shared representation of the world and one another's abilities through local encounters between robots (without broadcast communication).  This will allow better decision making as to which tasks should be distributed to which robots (e.g. adapting to partial failure of robots, or knowing when a newly-encountered robot type might be useful).  Teams will adapt their structure as exploration unfolds, adding members to acquire new skills, for example, and removing unneeded or poorly performing members (including their leaders), allowing these to be picked up by other teams (along with the knowledge they contain). All robots will function autonomously (i.e. not rely solely on a leader) to the degree their abilities allow.""429231,""Anderson, Jonathan"
"427037"	"Anis, Mohab"	"Maximizing yield in nanometer circuits using a design-for-manufacturability approach"	"As the silicon industry moves towards nanometre designs, one of the most important design challenges is the increasing variability in device characteristics. Device variations significantly impact the chips' performance, power budget and ultimately its yield. The proposed research investigates the design of novel integrated circuits and design automation methodologies while maximizing chips' yield.The innovative feature of this research is offering design techniques and methodologies to design circuits while having an understanding of lithography-induced variations, which is critical to chips' yield. The idea is to back-annotate the lithography-induced variations to the circuit and high level design with appropriate models that can be calibrated to achieve adequate accuracy, significant power reductions and high yield values. Such a design methodology is imperative to exploit the full potential of performance and power management techniques in chips while meeting highest levels of functional yield. The research will demonstrate that cooperative lithography/circuit/CAD co-design will have a profound impact on the way robust chips are designed in future nanometre generations. ""428307,""Anisca, Razvan"
"431897"	"Ansermino, Mark"	"An integrated expert system for physiological monitoring"	"Engineering solutions offer a promising solution to preventing unintended harm to patients in health care. Advances in technology have brought about an exponential increase in the amount of physiological data collected in many health care settings. Continuous physiological monitoring is widely used during surgery, in the intensive care unit, in hospital wards, and in private homes to provide timely warning of changes in a patient's condition. Despite major advances in sensor technology, the means of advising clinicians of significant events is still underdeveloped. More effective use of the information produced by currently available and new physiological sensors can be used to vastly improve patient safety through enhanced monitoring, diagnosis, and treatment of patients. The successful introduction of intelligent monitoring and automated control promises to harness this information to enhance safety, as it has in other industries such as aviation. Such intelligent data analysis requires a multifaceted approach. A range of techniques can be used to present the salient information and combine information from multiple sources. ""420710,""Anstee, Richard"
"428965"	"Antle, Alissa"	"Investigating embodied interaction in embedded computational environments"	"As computation plays an ever larger role as an embedded part of the environment, research that seeks to understand the embodied nature of human's interactions with computation becomes increasingly important. Embodied interaction is an approach to understanding human computer interaction that seeks to investigate and support the complex interplay of mind, body and environment in interaction. This approach is relevant for a broad class of systems that rely on direct physical interaction with computation embedded in the physical environment including mixed reality, tangible computing, responsive environments, pervasive computing and physical computing. Physically-based forms of child computer interaction including body movements, the ability to touch, feel, manipulate and build sensory awareness of the relationships in the world is crucial to children's cognitive development. The focus of this five year program of research is the investigation of how child users create meaning through action and interaction with embedded computational environments. Specifically, I will focus on three interrelated research areas. First, I will continue to study how embodied conceptual metaphors can be used as foundations for the design of interactive environments for both children and adults. Secondly, I will continue to study how interactive environments and tangible objects can be designed to augment cognition and accelerate learning by supporting children to offload difficult cognitive activities to direct physical interaction. Thirdly, I will support such investigations through the research and development of enabling technologies and resulting prototypes of new forms of embedded computational environments and tangible objects. Taken together, these three interwoven streams of research form a comprehensive and innovative research program in human computer interaction.""427916,""Antle, Michael"
"428768"	"April, Alain"	"Process model for software maintenance and evolution"	"The proposed research program is specialized in the area of software maintenance process modeling. Software process models have been extensively investigated in the context of the forward engineering software life cycle, but only a few authors have investigated process models in the context of software maintenance and evolution. The short-term objectives of this research grant is twofold: (1) to design the advanced practices of the process model; and (2) to develop support methods and tools, to support software maintenance process improvement. To do so methods to assess current strengths and weaknesses of software maintenance processes, activities and practices have to be investigated and, from there, process models to describe low to high maturity processes have to be investigated. This research will highlight the software maintenance concepts that need to be addressed by the process model. Experimental research will be carried out to complement the initial verification and validation of the advanced practices using case studies. Case studies on the use of the S3m as a tool for continuous improvements in software maintenance could contribute to the development of a better understanding of the problems of the software maintenance function. Feedback of case studies will be analyzed to improve the process model, including the addition of process attributes and indicators to support the maturity rating process. Industrial partners will be involved in experimental analysis in real software maintenance improvement programs where improvement decisions are made. In addition, output from this research will be used to update and improve the international standards ISO14764 and ISO/IEC TR 19759 content. Novelty of this work is demonstrated by the results of this research recently published in two books 'Improving software maintenance' by the publisher Loze-Dion in 2006 and 'Software Maintenance Management: Evaluation and Continuous Improvement' by Wiley in 2008. Masters and Ph.D. students will address various research issues on the basis of the funding granted. Bachelor and Masters students have already initiated research work.""422718,""Aquino, Manuel"
"427397"	"Assi, Chadi"	"Resource allocation and cross-layer optimization in wireless networks"	"Wireless technologies have seen tremendous success over the years and wireless networks have now become increasingly popular due to their fast and inexpensive deployment and their capabilities of providing flexible and ubiquitous Internet access. While the majority of existing wireless networks are still exclusively confined to single hop access, it is the ability of these networks to forward packets over multihop wireless paths which enabled them to easily extend the coverage area. This convenience has led the IEEE 802.11 to emerge from individual homes to large scale deployments in environments covering medium to large enterprises, apartment complexes and housing developments, as well as public area hot spots (e.g., airports, highway rest areas, coffee shops, etc.). As more users depend on such networks as their primary source for Internet access, there is an increasing expectation that these networks should provide reliable and high end-to-end throughput. Unfortunately, achieving good multihop throughput has been challenging due to factors, such as lossy wireless links caused by interference from concurrent transmissions, and intra-path interference caused by transmissions on successive hops along a single path.  The proposed work will provide a fundamental understanding of the capabilities and limitations of multihop and wireless mesh networks and will present solutions to improving their capacity through improving the spectrum spatial reuse and using smart antennas. We will therefore develop practical algorithms with strong theoretical foundation which will achieve significant performance improvements for multihop and wireless mesh networks that are essential for providing reliable wireless access to the Internet.""440780,""Assi, Hamza"
"435044"	"Asudeh, Arshia"	"Constraint-based syntax and semantics"	"Computational natural language processing has typically been caught between two complementary forces: 1. the desire for broad coverage, which is useful for natural language technologies like internet search, and 2. the desire for deep understanding, which is useful for natural language technologies like text summarization, translation and dialogue systems. Automated natural language understanding ultimately relies on the development of computational semantics. The work supported by this grant focuses on computational semantics for Lexical-Functional Grammar (LFG), a constraint-based theory of natural language syntax. Constraint-based syntactic theories have formed the heart of the deep grammar engineering approach for nearly thirty years.The LFG grammatical architecture provides a number of unique opportunities for pursuing both theoretical and computational issues about natural language syntax and semantics and the relationship between them. First, the architecture supports hybrid approaches that have the potential to combine advantages of two distinct traditions in how semantic representations are built. Second, LFG's syntactic representations use a rich vocabulary that encodes syntactic cues that are directly relevant to certain important semantic-pragmatic categories, such as presupposition and implicature. The computational treatment of these sorts of pragmatic effects is of growing importance, since they are pervasive in natural language and any system that aims for deep textual understanding must be able to handle them. Third, the long-standing Parallel Grammar and newer Parallel Semantics projects (http://www2.parc.com/isl/groups/nltt/pargram/) not only provide a broader natural community for the research, they also bring to the fore questions of cross-linguistic semantics, which relate the proposed work to machine translation. ""426218,""Ata, Athar"
"434944"	"Atrey, PradeepKumar"	"Privacy-aware automated multimedia surveillance for public safety"	"Recently, due to the significant increase in various security threats, multimedia surveillance systems are being increasingly used at many public places to monitor/record people's activities and avoid any potential untoward incidents. Although the current-generation surveillance infrastructures have proved to be highly useful from a security perspective; there has been a major apprehension among people in regard to their privacy safeguards. Recent reports also suggests that the privacy concerns have increased. It is worth mentioning that although there has been a significant progress in the field of surveillance research, the issues related to people's privacy have often been overlooked in the past and have only begun to attract the attention of researchers very recently. Achieving the goals of security and privacy simultaneously is quite challenging as it requires a mechanism that can automatically determine in real time what to monitor/record for safety purpose and what not to monitor/record for privacy concerns. For the effective design and development of privacy-aware multimedia surveillance systems beyond today's state of the art, many interesting research questions related to privacy aware sensory data management, processing, analysis, fusion, and retrieval need to be answered. Building on the applicant's extensive expertise in multimedia surveillance, this research is aimed to investigate scientific solutions to perform effective and automated surveillance keeping the people privacy in tact.""440282,""Atwater, Aaron"
"419651"	"Avis, David"	"Polyhedral computation and quantum information"	"I am proposing to work on polyhedral computation, and in particular, its applications in quantum information and discrete optimization.Quantum Information.In collaboration with researchers at ERATO and the University of Tokyo, we have made extensive advances in recent years in understanding Bell inequalities and, to a much lesser extent, quantum correlations. Our future goals are to provide efficient procedures (where possible) to compute maximum violations of Bell inequalities and to provide feasible quantum setups that give such violations. These setups are natural candidates for exploiting optimally the additional power given by quantum information processing. Discrete Optimization.We are currently studying discrete optimization problems related to mining optimization, in particular the solution to the so-called ""optimum pit"" problem with additional knapsack like constraints. We are adopting a polyhedral approach, which eventually should yield strong cutting planes that can be used in a branch and cut solution (these problems are NP-hard). We will also study LP and semidefinite approximations. The related polyhedra are directed analogues of the well studied cut and semimetric polytopes.Polyhedral Computation.I will continue the development of the lrs library. This freely distributed and widely used code employs the reverse search technique and exact arithmetic to compute the vertices or facets of high dimensional polyhedra and related problems in polyhedral computation. New theoretical advances will be needed to improve the performance of the code on highly symmetric and highly degenerate combinatorial polytopes. Recent additions to the package includes a program to compute Nash Equilibria by means of two nested reverse searches on related polyhedra.""434415,""Avramidis, Stavros"
"435060"	"Bahreyni, Behraad"	"Design, modelling, and manufacturing of integrated multi-transducer systems"	"Due to an intensive research thrust over the past 15 years, Micro-ElectroMechanical Systems (MEMS) have replaced, or can replace, many macro-scale transducers. These micromachined transducers use a variety of transduction mechanisms to convert different physical quantities to each other in order to provide the main system with the necessary data. However, fabrication techniques for micromachined devices are mainly optimized around a specific transduction mechanism. As a consequence, the growing number of systems that require multiple transducers need to use several individually fabricated devices.   The objective of this research is to reduce the cost, size, complexity, and power consumption of a sensory system and improve its performance and reliability by integrating several of the necessary transducers on one chip. The challenges at the design, fabrication, and interfacing steps for these Integrated Multi-Transducer Systems (IMuTS) will be addressed with an emphasis on the integration of resonant transducers. The research will include the development of versatile fabrication process flows and their characterization. Several transducers will be designed and tested for given applications of IMuTS and their thermal responses will be characterized. The electrical, mechanical, and thermal behaviour of these devices will be modelled in order to facilitate the optimization of transducer designs. Moreover, resonant transducers will be designed with the aim of using a single resonator to infer information about more than one physical parameter by exploiting either the material anisotropies or the difference in the responses of various resonant modes of the structure. Interface circuits will be developed in standard CMOS processes with emphasis on low power design.   This research plan, in addition to advancing the technological forefront, provides a learning framework for training of PhD and MSc students. These students will conduct the ground breaking research that will directly benefit numerous scientific and technological fields such as wireless sensor networks, implanted biomedical devices, structural health monitoring, and automotive safety.""425044,""Bahturin, Yuri"
"435363"	"Baker, Christopher"	"Knowledge-navigation infrastructure for the life sciences: mutations"	"Semantic technologies facilitate knowledge management and data integration. Their application in life science depends on exhaustive modeling of biological knowledge as well as robust deployments of the technology. The goal of this project to establish a scalable, ontology centric knowledge management infrastructure to support the reuse of information (the aggregation, integration, navigation and management of knowledge from distributed sources, legacy document formats, and data types). This project aims to leverage this advanced infrastructure to facilitate knowledge discovery in life science. The working use case will be data integration for to support the reuse of annotations from unstructured scientific documents that describe the consequences and impacts of protein and DNA mutations. ""419525,""Baker, Curtis"
"427368"	"Beaulieu, Christian"	"Technical development of sodium magnetic resonance imaging of the human knee"	"Engineering advances of hardware and software over the last 25 years have made magnetic resonance imaging (MRI) indispensable for the diagnosis and improved understanding of many human diseases and disorders. Routine clinical MRI measures signal from the hydrogen nucleus (i.e. proton, 1H) of water and it is unique differences in the water properties of tissue that yields image contrast. However, with the appropriate engineering, MRI can acquire signal from a different nucleus, namely the naturally abundant sodium (23Na) ion within the body. Sodium is linked directly to proper cell function, and may provide evidence of tissue injury despite normal appearance on conventional MRI. Sodium MRI is inherently very challenging because of low sensitivity due to sodium's low concentration in tissue and complicated physics responses. Therefore, sodium MRI is a non-standard imaging technique not available on most clinical 1.5T (i.e. strength of the magnetic field) MRI scanners because of the lack of appropriate electronics and specialized imaging pulse sequences. Imaging is more sensitive if the MRI scanner has a very strong magnetic field. This proposal will take advantage of a very strong ""triple strength"" MRI scanner (4.7T) located at the University of Alberta.The goal of this technical grant is to develop the hardware and software for sodium MRI of cartilage in the knee as sodium concentration is directly linked with cartilage degradation in osteoarthritis, a debilitating condition which affects millions of Canadians. On the other hand, current standard MRI of water is not specific to cartilage damage. Imaging the sodium ion in the knee will require us to design, build in-house, and test new MRI electronic components (i.e. radiofrequency coils). Computer simulations will be used to predict the parameters that will yield the best signal-to-noise ratio given a desired image resolution and scan duration. Once we engineer, optimize, and test our novel MRI scans on healthy people, the noninvasive assessment of sodium in cartilage could be applied in the future to people with osteoarthritis of the knee.""438981,""Beaulieu, Claudéric"
"427025"	"Behjat, Laleh"	"New algorithms and techniques to implement scalability in VLSI physical design"	"Advances in the Integrated Circuits (IC) technology have resulted in tremendous growth, making ICs prevalent in our lives. Today's ICs contain over a billion switching elements, transistors, which are connected through a complicated network of wires. The total length of these wires is estimated to be around three kilometers for every square centimeter of manufactured IC. All these transistors need to be placed (placement stage) and their interconnecting wires need to be routed (routing stage). The placement and routing stages need to be performed optimally and efficiently.  The main objective of the proposed research program is to develop a new framework for placement and routing stages that is able to handle the sizes of circuits that will be encountered in the future. This framework will rely on new heuristics that use circuit structure and characteristics to improve the efficiency of the placement and routing stages. The proposed research has two focuses: developing algorithms that use mathematical modeling and circuit structure to implement hierarchy in design, and developing congestion reduction-based algorithms for routing.  ""442258,""BehnamDehkordi, Mohammad"
"427122"	"Benesty, Jacob"	"Speech enhancement algorithms for multimedia applications"	"Speech enhancement plays a fundamental role in many applications such as teleconferencing systems, speech recognizers, human-machine interfaces, hands-free communications, mobile phones, and hearing aids. Indeed, the noise is around us and everywhere. So when a signal of interest (usually speech) is picked up by a microphone, it is always contaminated by noise, reverberation, and other undesired signals. The objective is then to clean up the noisy signal with digital signal processing tools without damaging much the desired speech signal.The problem of speech enhancement is an old one and has been around for more than 40 years. This should not come as a surprise since, contrary to what some people may believe, it is a very difficult problem for mainly two reasons. First, the nature and characteristics of the noise signals can change dramatically in time and application to application. It is therefore laborious to find versatile algorithms that really work in different practical environments. Second, the performance measure can also be defined differently for each application. Two perceptual criteria are widely used to measure the performance: quality and intelligibility. While the former is subjective (it reflects individual preferences of listeners), the latter is objective (it gives the percentage of words that could be correctly identified by listeners). It is very hard to satisfy both at the same time.So this research proposal aims at developing the next-generation speech enhancement algorithms with one and, especially, multiple microphones (or microphone arrays) with the clear objective to reduce the noise and other interference signals as much as possible with little (or no) distortion of the desired speech signal. In other words, we will try to improve the quality as much as possible without affecting the intelligibility. We will also try to develop practical algorithms that can be easy to use and easy to tune in the real world and for most applications. To achieve this goal, we will rely on some strong ideas and results we have developed recently.""434396,""Benfey, Tillmann"
"435039"	"Benkoczi, Robert"	"Models and algorithms for facility location with applications to wireless networks"	"Wireless service providers operate efficiently when they deploy their resources in an optimal or near-optimal manner. Optimizing wireless networks, such as cellular, sensor, or mesh networks, means selecting the position and the capabilities of certain wireless nodes in order to minimize cost and maximize the network performance.  Problems of computing a set of locations to optimize a given objective have been studied in logistics and industrial engineering already. The goal in such a context is to locate facilities to provide equitable service to clients, to minimize the transportation cost, or to capture the largest market share.  However, the performance of wireless networks depends on specific factors, such as interference and radio propagation, not considered relevant for industrial operations. This research proposes to design and solve location problems that accurately reflect the behaviour of wireless networks in order to better support the planning decisions of service providers.""429892,""Benlamri, Rachid"
"422039"	"Bergeron, Anne"	"Algorithms in comparative genomics, transcriptome assembly, and metagenomics"	"In this program, we will investigate three problems. Genomes of different species often share many genes, but these may occur in different orders in the genome sequence. Based on the principle that any two species share a common ancestor, the question of how to reconstruct the evolution of one genome into another, based on gene orders, have been studied intensively in recent years. One of the lingering problems is the difficulty of discriminating between the multitude of different evolution scenarios that are proposed by the current models. Based on previous results, we plan to find new answers by studying properties of special graphs associated to genome comparisons. In addition to this long term goal, we plan to rapidly develop  software tools that can aid the construction of evolution scenarios.Our second problem is to develop algorithms to characterize all RNA molecules that are present in a cell at a given time. Recent technology allows us to easily get partial information on these molecules, but there is still a lack of software tools that can transform this data into sound results. Recently, we designed theoretical models that showed the limits of these approaches. Our next goal is to construct software tools that can deal with real situations.Our third goal is somewhat a mix of the first two, and probably the most challenging. In metagenomics surveys, many species are sequenced together because they live in the same environment, or are closely related species. We plan to generalize the approaches developed to characterize RNA molecules in the cell  to this more complex problem, where rearrangements between close species will be identified.""443066,""Bergeron, François"
"422368"	"Bergler, Sabine"	"Composing meaning"	"The work proposed here for the next few years builds on all of my previous research in spirit, and in particular sentiment analysis, causality, hedging, and (dream) emotion analysis. The unifying feature of all these topics is that they are concerned with (a) differentiating factual information from embedding meta-information in different text genres and (b) decoding this meta-information and making it accessible for automatic text analysis as an equal component in text meaning. That the factual content of a text contributes only one part of its meaning has been largely ignored in Natural Language Processing (NLP) systems to date. But recently, the possibility of commercial exploitation of sentiment and opinion-laden texts from abundant on-line sources such as blogs for product improvement has shifted attention to consider sentiment and even emotional content using shallow means. Now, valence shifters (language constructs that can invert the meaning of a clause, such as negation) are considered more widely. But still, the predominant technique for, for instance, sentiment analysis, is to simply count the number of positive vs. negative sentiment words in a sentence with a winner take all overall sentiment score. The limitations of the simplistic approach are obvious: a long string of positive statements about acting and directing in a movie review can be undone with the simple concluding statement: ""And yet the film bombs."". Stochastic techniques fall short, because (a) the linguistically relevant features have not been identified and thus there are no common annotation schemes nor training corpora yet (b) their compositional behavior is too complex for simple annotation schemes. With better understanding of the nature and behavior of all of the different meta-propositional devices will come better annotation schemas that will lead to more refined stochastic systems. In considering the linguistics of non-factual aspects of sentence meaning and how they contribute to the meaning of the text, I propose to develop an annotation scheme and a compositional semantics for texts that treats different meaning components coherently.""437883,""Bergmann, Karel"
"425041"	"Berry, Daniel"	"Requirements engineering and electronic publishing"	"Requirements engineering (RE) is gathering information from stakeholders of a client organization toproduce a coherent requirements specification (RS) for the system the client desires. This work's goal is todevelop methods that help producing this RS.  Requirements elicitation is gathering information about thesystem to be built from any source, the client, users, organization at work, interviews, video recordings,organization policy documents, vision documents, RFPs, etc.  Requirements analysis is refining thisinformation into a RS by deriving what is possible from the information and validating it with the client andusers.  My focus is on understanding the RE process to know what works and what does not, and armed with thisunderstanding, to propose and validate methods and tools that improve RE's effectiveness:  1. understanding the various roles in the RE process: The participants in RE are at least the customer,     users, requirements engineers, and system developers. Unless each performs her role properly, the RE     process can lead to a system that meets no stakeholder's real requirements even if it meets all     specified requirements.  2. dealing with requirements that are expressed in natural language (NL): Even if there is a formal     mathematical specification, an overwhelming majority of all documents involved in RE are in NL, which is     inherently ambiguous.  An ambiguous NL requirements specification can lead to incorrect implementations.     Moreover, each ambiguity requires asking stakeholders to resolve the ambiguity.  Another area of research for me is electronic publishing. Development will be donein Multi-Lingual Formatting and Dynamic PostScript Fonts as case studies in RE.""434552,""Berry, Daniel"
"427361"	"Beznosov, Konstantin"	"Security engineering for large-scale distributed software applications"	"Security is an essential feature and foremost concern to Internet and enterprise distributed software applications. However, the adoption of secure distributed applications by commercial and government organizations is considerably hampered by the prohibitively high cost of ownership and the inability to support real-world applications and infrastructures. The general goal of my research is to explore new methods for designing and managing security mechanisms for large-scale distributed applications. Specifically, I will investigate new authorization architectures for medium-to-large enterprises (from hundreds to thousands of computers) that will be able to sustain frequent infrastructure failures and will reduce delays associated with making access control decisions. The proposed research will investigate an approach to increasing reliability and efficiency of enterprise-scale authorization solutions by flooding delivery channels with speculatively pre-computed authorizations and actively recycling them on a just-in-time basis.   The approach consists of three key elements. First is decoupling applications from their authorization servers using publish-subscribe technology. The administrative and operating overheads associated with reconfiguring access control systems to accommodate component and infrastructure failures should thereby be reduced. Second is ""active recycling"" of authorizations by searching through the publish-subscribe authorization streams and distributed caches in order to compute the ""best approximate"" (as opposed to  ""precise"") authorizations. Third is speculative pre-computation of authorizations and their distribution to policy enforcement points  through the publish-subscribe channels. The expected benefits of this approach are (1) improved resilience of authorization infrastructures in the light of the high failure rates observed in large enterprises; (2) reduced delay (latency) in requesting and obtaining authorizations; and (3) reduced human resources required to maintain enterprise-scale authorization infrastructures. Preliminary results demonstrate a 30% increase in the number of authorization requests that can be served without consulting the authorization server.""429278,""Bezuidenhout, LouisWentzel"
"425046"	"Bigras, Pascal"	"Optimisation des systèmes de commande robustes de manipulateurs robotiques en présence de contraintes matérielles, logicielles et de sécurité"	"Plusieurs tâches nécessitant une très grande précision ou une force contrôlée, peuvent être difficiles à automatiser de façon optimale et robuste à l'aide de robots industriels. En effet, ces forces et ces positionnements précis nécessitent l'ajout de boucles de commande à l'unité de contrôle du robot. Or la mise en oeuvre de ces boucles ajoutées requiert une réorganisation matérielle et logicielle de l'unité de contrôle, qui devient possible uniquement lorsque son architecture est ouverte. Malheureusement, les fabricants de robots industriels d'aujourd'hui ne permettent qu'une ouverture partielle de leurs contrôleurs. La mise en oeuvre des boucles supplémentaires est alors possible en imposant toutefois un ensemble de contraintes structurelles causées par des restrictions matérielles, logicielles et de sécurité. L'objectif à long terme du programme de recherche proposé est le développement et l'optimisation d'architectures de contrôle robotique qui tient compte de cet ensemble de contraintes structurelles. Même si la prise en compte de ces contraintes complexifie considérablement les problèmes d'optimisation et d'études de robustesse soulevés, il existe depuis quelques années plusieurs approches qui permettraient de formuler et de résoudre ces problèmes de façon efficace. Le programme de recherche proposé permettra notamment d'assurer de meilleures performances, une plus grande robustesse et une sécurité accrue  lors de la mise en oeuvre de nouveaux procédés robotisés et de systèmes de diagnostiques et d'interventions médicales s'appuyant sur des robots.La subvention du CRSNG contribuera à la formation de quatre nouveaux chercheurs dans le domaine de la commande des systèmes robotiques : deux à la maîtrise et deux au doctorat. ""436689,""Bijeljanin, Milan"
"420523"	"Bodorik, Peter"	"Privacy compliance for e-commerce in web services architecture"	"Due to improvements in various fields of Information Technology (IT) and electronic devices, much data has been and is continually being collected on customers/citizens through various channels, such as monitoring devices or eCommerce channels.  Many news items, dealing with security breaches and inappropriate use of private data, have attracted a lot of attention and created pressure on governments to regulate the use of private data.  Governments have responded with enacting privacy laws that regulate the collection and use of private data in various domains, such as HIPPA in the US for health data and PIPEDA in Canada for data collected online.   Most companies today inform users under which privacy policies data is collected and tools have also been created to provide users/customers with automated agents that fetch the web site's privacy policy and compare it the user's privacy preferences.  However there is urgent need to provide appropriate tools for enforcement of privacy policies, that is, to ensure that private data is indeed used that satisfy the privacy policy under which it was collected.  This is a very challenging problem because  eCommerce/eBusiness is complex due to integration of various enterprise systems within organizations, and due to increased B2B integration, cross border e-commerce, e-business, mobile commerce, etc.  Moreover, dealing with privacy issues has not been designed into software applications, but rather ""bolted on"" after applications are installed.  The focus of this research project is develop new scientific methods and tools to enforce privacy policies, specifically to create mechanisms to (i)  monitor the movement of private data between the various integrated subsystems and storage of data by the subsystems; (ii) determine how the private data is used by subsystems and applications; and (iii) ensure that the data use and storage of private data satisfies the privacy policies under which they were collected.""422326,""Bodwell, Graham"
"426948"	"Boley, Harold"	"Scalable rule interchange techniques"	"The Web was initially used to distribute documents for direct human consumption. The Semantic Web is additionally distributing formalised knowledge for machine consumption to assist humans. Formal knowledge extends databases by expressive vocabulary definitions (ontologies) and policy specifications (rule systems). Since machines can interpret such formal knowledge, they can use it to logically prove knowledge that is new to the assisted humans. However, not all formalisations can be processed efficiently on the Web (scalability issue) and not all pairs of formalisations can be translated into each other (interchangeability issue). Challenges for Web-based knowledge interchange arise from syntactic and semantic heterogeneities of ontology and rule languages above the XML-infrastructure level. This proposal explores scalable rule interchange over a network of the size of the Web, which will be enabled by: (1) an agreement on a rule language standard (e.g., a dialect of W3C's Rule Interchange Format) to drastically reduce the number of translators between interchange partners; (2) the efficiency advantage of rule interchange involving rule translation over rule querying; (3) the use of a Mediator Architecture connecting interchange partners in domain-specific subspaces of the Web. With (1)-(3) in place, distributed rule creation, deployment, and maintenance become feasible. This will be evaluated by: (4) use cases in symposium planning, opportunity alerting, and digital rights management. Work on these four components has been converging, in Canada and abroad, toward scalable rule interchange. The proposal takes up this window of opportunity to discover a viable synthesis by systematically researching the component techniques and their interactions.""435496,""Bolic, Miodrag"
"421209"	"Bornemann, Jens"	"Microwave component design for modern wireless applications"	"RF and microwave design engineers rely heavily on commercially available software packages which are based on electromagnetic principles. Especially with respect to modern frequency assignment schemes, the choice of an initial design is certainly the most crucial part of any component or system development. Ultra-wideband (UWB), dual- and/or multi-band operations of wireless systems require frequency-agile transmit and receive capability (both on the circuit and antenna levels) to adapt to changing propagation scenarios and a rapidly increasing pool of new applications. Therefore, the principal objective of this research program is to develop new frequency-agile circuits, including their design guidelines, for tunable and/or switchable dual-, multi- and ultrawide-band filters. This activity will go along with new antenna configurations which allow the designer to directly integrate filtering capability into the antenna for interference suppression or service selection. A general objective of this research is also associated with applications other than communications. The increasing use of UWB technology in bio-medical applications is of particular interest. Special emphasis will be placed on the trade-off between circuit size, sensitivity and manufacturing tolerances. Prototyping is expected to utilize simple and commonly available printed-circuit fabrication. Existing equipment and facilities at the University of Victoria will be used to experimentally validate components and design procedures. This research will assist the Canadian wireless industry to seize competitive advantage in modern wireless applications.The applicant has more than twenty years of experience in this general area of research. He is an IEEE Fellow and an internationally recognized expert in the areas of microwave CAD, filters and UWB antennas.""433604,""Bornn, Luke"
"424421"	"Boutaba, Raouf"	"Fundamentals in network and service management"	"With the rising scale and complexity of networks and services, there is a pressing need for scalable, efficient, and reliable management solutions.  Autonomic management is envisioned to be the ultimate solution.  In working towards this goal, my previous research on self-management has revealed a number of fundamental issues.  First, current management systems collect only rudimentary data about network devices and flows, reporting only a few basic quality-of-service metrics. There is little information processing, or correlation among the metrics. The lack of comprehensive knowledge support and formal treatment of knowledge generation impedes the effectiveness of many network and service operations.  Second, the diverse distributed applications (e.g. P2P, overlay networks, tube streaming, social networks) supported by today's Internet can interfere with vital network planning and engineering operations, such as inter-domain routing and traffic engineering.  There is an inability to consolidate network and service operations, as little is understood about their interactions, resulting in critical management issues, including stability and optimality.  Third, malfunctions and misconfiguration caused by today's manual (and in the future by automated) configuration and administration of networks is a primary source of service disruption, and creates susceptibility to malicious attacks.  A strong sense of reliability must be introduced into the design and development of the entire management system.  This includes the use of formal methodologies, for verification and evaluation of the system during design and at runtime, as well as the application of self-stabilization and fault tolerance to maintain correct operation in adversarial environments.     )This research draws on our extensive experience in automated network management and autonomics research to address these critical issues.  By seeking effective and practical solutions, this work will have both immediate and far reaching impact on the management of networks, by advancing the state-of-the-art in order to enable autonomic management.""443464,""Boutilier, Bryan"
"421288"	"Bridges, Gregory"	"Nanoprobes for mm-wave device measurement and sensors"	"Advances in the development of high-frequency integrated circuits and devices are being made at a phenomenal pace. Much of this is driven by recent novel materials and by nanofabrication technologies, enabling unique structures and making possible exciting new devices and sensors. In this research program we will develop new nanoprobe tools for testing these devices and investigate new sensor and imaging applications. There has recently been significant effort in the use of mm-wave and sub-mm-wave frequencies for broadband information communication technologies, for radar and imaging, and for material characterization and bio-diagnostic applications. Being able to gain information on the internal operation of the circuits and devices operating at these frequencies is crucial for their development and is especially needed as when employing novel materials or MEMS based technologies. However, there are currently few viable tools for providing both high-spatial resolution (nanometer) and high-frequency (mm-wave) capabilities. We will be developing new scanning probe microscopy tools to address this regime. This will be achieved through custom fabrication of shielded scanning probes with integrated transmission lines that are capable of delivering localized mm-wave signals to a scanning probe nanometer sized tip. The ability to image, stimulate and sense at high frequencies with a shielded nanometer sized probe makes possible many novel applications. We will employ this for high-frequency stimulation and microwave impedance imaging in single cell biodiagnostic systems.""436169,""Bridgewater, Courtney"
"427660"	"Brooks, Stephen"	"Rendering and presentation of spatial data"	"Much of computer graphics remains in the domain of the specialist since it requires an uncommon set of skills, both artistic and technical.   The digital designer must possess a natural aptitude for artistic aspects such as composition, colour, and visual flow, and also engage in esoteric technical tasks such as object transformations or non-linear manipulations. Yet, even for those with the requisite skill sets, many tasks remain complex and labour-intensive. The overarching theme of this research is therefore to reduce the complexity of graphical applications through semi-automation and the incorporation of aesthetic constraints, thereby facilitating the democratization of computer graphics.  Within this broader context, the proposed research will allow those working with spatial data, who may not possess artistic training, to semi-automatically create dynamic renderings of their data.       The past decade has seen a vast increase in the use of spatial data for analysis, research, planning and popular use.  This work will therefore be important as a facilitation of spatially-based knowledge to the broader public for decision making, education, and engagement.  Public communication is critical for many industrial and academic projects. For example, if one examines online materials available for large research projects, one finds that the primary vehicle of communication to the public is visual illustration, animation and images.  It is the automation of such visual presentations that motivates this research. Through automation, allocated budgets for visual media production could be reduced, and for smaller scale projects that lack such budgets, automation would enable the production of such materials.       To achieve these goals, I aim to design and develop algorithms for the simulation of illustrative rendering styles, for the presentation and enhancement of spatial datasets.  ""436876,""Brookshaw, Marcus"
"429025"	"BurtonJones, Andrew"	"Making conceptual models clearer and more understandable"	"In the early stages of developing an information system, systems analysts often create conceptual models.  Such models are primarily diagrammatic, and include constructs and rules that analysts can use to describe a domain.  Examples include entity-relationship models and business process models.  Conceptual models are vital because they help analysts to understand users' requirements and to communicate them to designers.  Research shows that understanding user requirements is the key to successful information system development. Although conceptual models are vital, research shows that they are often underutilized and used poorly.  The motivation for my research is that conceptual models will be used more often and more effectively if they can be made clearer and more understandable.  To this end, I propose to develop a theory-based method and modeling tool that will help analysts to create good conceptual models.  The research has four phases with each phase focusing on a different element of 'goodness' described in a theory of representation: clarity of semantics, consistency of semantics, quality of decomposition, and accuracy of dynamics.   The research will involve theoretical work (applying theories of representation and ontology), design (creating and implementing rules in a system), and testing (evaluating the system).  The intended contributions of the research are: - For research:  New methods for developing conceptual models, new evidence regarding the usefulness of conceptual modeling theories and principles, and a software tool that can be extended in further research. - For practice:  A tool that analysts can use to create conceptual models that are more clear, more understandable, and therefore more useful in practice.""440241,""Buruma, Melle"
"422322"	"Butler, Gregory"	"Knowledge-based bioinformatics"	"Bioinformatics is an interdisciplinary area concerned with how best to apply information technology to exploit the huge amounts of data in genomics and in related fields such as proteomics, systems biology, drug discovery, population genetics, and pharmacogenomics. Our aim is to make advances in the biological sciences and information technology. We do this by studying the needs of genomics, providing solutions for bioinformatics that draw on current best-practice, and identifying aspects of information technology that require improvement.At Concordia, I work closely with biologists and biochemists to study fungal genomics. We search for new degradative enzymes within fungal species that will have applications in industrial processes, such as the pulp and paper industry, in biofuels, in environmental bioremediation, and in sustainable development. The fungal genomics work covers a broad range of bioinformatics from sequence quality control, sequence analysis and annotation, genome-to-EST mapping, proteomics,  biochemical pathways, protein families, and high-throughput assays for enzyme characterization.Key areas of research in bioinformatics are the development of algorithms and software tools; the management and integration of data; the extraction and management of scientific knowledge; and the intelligent use of knowledge to answer biological research questions.I am investigating all of these key areas in collaboration with colleagues at Concordia and across Canada with particular emphasis on database management, data mining, data integration and intuitive access; ontologies, workflow, and the semantic web.  As well, I am assisting colleagues with interests in natural language process to investigate extraction of knowledge from the scientific literature.This is highly innovative, practical research with the potential for very significant impact on genomics.""419781,""Butler, Ian"
"419573"	"Cameron, Robert"	"Parallel bit stream technology"	"The proposed research is into tools and technology for high-performance text processing using parallel bit stream technology on commodity multicore processors. Current research results have established that parallel bit stream technology can provide order of magnitude performance improvements over traditional byte-at-a-time character processing on modern pipelined processors in applications such as string search, character set transcoding, lexical analysis, regular expression matching and XML parsing. In effect, the parallel bit stream technology makes substantial use of intraregister parallelism in order to process up to 128 characters at a time using 128 bit registers. However, this involves new forms of programming for which high-level tool support (e.g., high-level programming languages) are not yet available. Furthermore, current trends in commodity processor design involving multicore processors are creating demands to find new forms of parallel programming to take advantage of intrachip parallelism. By developing new techniques to leverage intraregister parallelism in support of intrachip parallelism, further performance advantages of these parallel text processing techniques over sequential byte-at-a-time techniques can be expected. By developing high-level programming tools that map to implementations based on parallel bit stream technology, these performance benefits can be made broadly available to programmers without the need for tricky and error-prone low-level programming.""423152,""Cameron, Robin"
"435116"	"Chang, Liuchen"	"Development of new passive anti-islanding method for distributed generators"	"There is a growing interest in adding distributed generators (DGs) that are driven by renewable energy resources such as wind and solar energy to the electric power systems (EPS).  However, adding a large numbers of DGs to the EPS may compromise the stability and reliability of the EPS.  DGs operating as islands are not permitted by utilities as a result of risks to safety and equipment.  Anti-islanding is a critically important component of the interconnection system between a DG and the EPS, and is also a challenging technical problem that has been recognized as one of the technical barriers preventing widespread deployment of DGs.       Although numerous anti-islanding methods have been investigated, a universal anti-islanding method that possesses adequate robustness and reliability is yet to be developed.  The objective of the proposed research is to develop a methodology that allows the best attributes of existing schemes as well as new schemes to be seamlessly incorporated into an innovative classification model that identifies whether an island condition exists or not.  The proposed technique will be universal and hence, applicable for DGs that interface directly to the EPS through rotary generators and also for DGs that interface to the EPS through power electronic converters.  The success of this project will enable higher penetration of renewable energy in electricity markets and facilitate greenhouse gas reduction and economic development.       The research will proceed along a timeline with well defined tasks that include: developing a simulation model, developing a database of training events, developing and validating a classification model, performing a comparative analysis with other approaches, constructing an interconnection apparatus and conducting field verification tests.  The interconnection apparatus will be tested on both a photovoltaic DG installation (power converter based) and a wind turbine DG installation (induction generator based) that reside in our research facilities.  This project will provide research and laboratory based training for two Ph.D. students and one M.Sc.E. student with skills that are in high demand by Canadian industry.""429493,""Chang, Liuchen"
"427363"	"Charlebois, Serge"	"Study of superconducting Josephson junctions fabricated using a novel nanodamascene process for quantum computing and RSFQ application"	"In 2007, a Université de Sherbrooke team reported an innovative new tunnel barrier fabrication method that allows greater dimensional control on metallic single electron transistor (SET). This breakthrough innovation is now referred to by the semiconductor industry's roadmap (ITRS 2007). The proposed research program will use this novel fabrication technique to realize nanoscale superconducting Josephson junctions (JJ). Low temperature superconductor JJs have been made using mainly 2 methods: shadow evaporation (e.g. Al based quantum computing); planar metal-barrier-metal (e.g. Nb based rapid single flux quanta electronics, RSFQ). This novel technique will be the first major change in Josephson junction fabrication methods in the last 2 decades. Two significant outcomes are expected from this new technique: smaller area junctions could be achieved in niobium thus giving a greater freedom to balance charging and josephson energy in qubit designs (quantum computing bit); higher operation frequency (~500GHz) and integration density for RSFQ large scale circuits as expected by the RSFQ roadmap. Furthermore, this technique will allow tunnel barrier engineering even at ultra small area junctions, which is hardly possible with the conventional methods. The 1st phase of the program is to explore material issues related to fabrication and to superconducting properties of the resulting JJs. Niobium will be a candidate of choice but other materials will be considered. The 2nd phase will be to implement barrier engineering and study its effect on junction properties. The 3rd phase will aim at fabrication small scale superconducting circuits based on the developed technology. Applications to quantum computing qubit designs and RSFQ are considered. This research program has a unique potential for innovation by challenging the conventional fabrication methods. The required nanofabrication environment is already available and well supported. The program will also greatly benefit from my extensive expertise in superconducting nanostructures and from my extended local and international network in the field.""420309,""Charles, Anthony"
"434597"	"Chau, Kenneth"	"Integrated plasmonics amd Electro-optics"	"Currently, a fundamental bottleneck in information technologies is the delay associated with electrical interconnects. Optical interconnects, on the other hand, are superior to electronic ones in terms of speed and bandwidth, but the diffraction limit of light restricts miniaturization and high-density integration.  Interfacing optical interconnects and electronic devices represents a major challenge in the advancement of information technology.  A promising solution is to use waveguides based on metallic surfaces that support surface plasmon polaritons (SPPs).  SPPs are electromagnetically coupled charge density oscillations on the surface of metals.  Subwavelength metallic structures can confine SPPs over extents smaller than the diffraction limit by ""squeezing"" electromagnetic energy near the surface of the metal.  Thus, plasmonic devices based on SPPs are capable of carrying optical frequency signals over a wide bandwidth without the size constraints faced by conventional light waveguides.  The Subwavelength Optics and Plasmonics Laboratory in the School of Engineering at the University of British Columbia Okanagan aims to develop a new class of integrated plasmonic device for information and computation technologies.  This will be accomplished by developing:1)    )highly efficient, compact SPP waveguides and passive integrated devices based on photonic crystal concepts;2)    )microcavities for efficient SPP energy confinement; and3)    )on-chip SPP sources and detectors.Achieving these objectives is important because integrated plasmonic devices, which combine the speed and bandwidth of optical interconnects with the miniaturization capabilities of electronics, hold the potential for revolutionizing information and computation technologies.""421428,""Chau, SiuCheung"
"425139"	"Chau, Thomas"	"Intelligent systems for pediatric rehabilitation"	"The proposed research will contribute to Canada's leadership in the emerging area of academic rehabilitation engineering. In particular, our work will focus on:(1) determining the relationship between subtle ""fractal"" patterns in human locomotion and energy expenditure; and(2) studying the patterns of involuntary movement in children with cerebral palsy.To realize the first objective, we will measure both energy consumed during walking and the timing of each step. Using nonlinear signal analysis methods and statistical comparisons, we will determine what relationship exists, if any, between the timing patterns (fractal dynamics) and energy expended. This work will generate new knowledge about how humans regulate quasi-periodic activities and why these fractal patterns are important to human movement.      Regarding the second objective, we will engage children with cerebral palsy who have uncontrollable movements of the limbs and head. We will measure the motion of these body parts non-invasively while the child participates in a communication activity. We will subsequently mine the movement data for fractal patterns and changes thereof when the child responds to or initiates communication. We will develop new methods to detect subtle changes in motion signals, both from the fractal and conventional statistical perspectives.      This research will lead to the development of new quantitative and objective assessments or outcome measures for rehabilitative interventions. Trainees will be drawn from many different engineering and science disciplines to work on exciting interdisciplinary problems, from developing new change point detection algorithms to relating energy parameters to timing patterns.""434716,""Chau, Thomas"
"428959"	"Chen, Yuanzhu"	"Exploration and utilization of broadcasting nature of multi-hop wireless networks"	"Multi-hop wireless networking studies a wireless data communcation scheme, where messages are forwarded multiple times over wireless links before reaching the destination for delivery.  Multi-hop wireless networks are a powerful extension to the Internet that will allow it to be a truly ubiquitous infrastructure.  Since the pioneering Packet Radio Network in the 1970's, research on such a networking paradigm has been fruitful.  Not only have different enabling technologies been developed, they are also starting to be integrated to the Internet seamlessly.  Achieving the tangible goal of harnessing the flexibility of such networks with high network operation efficiency is on the horizon.  However, we should not be contented with the fact that multi-hop wireless networks are to be ""as good as"" the Internet. In fact, research on multi-hop wireless has been primarily focused on addressing issues emerging from a networking framework which are more or less deviations of or add-ons to the Internet.  The most fundamental axiom of the Internet data transportation, i.e., packet forwarding among a group of nodes with multiple essentially point-to-point links, has been intact until recently.  This is to be changed by recognizing and embracing the broadcasting nature of wireless communication channels of  multi-hop wireless networks.  Indeed, a wireless link in this case is a limited-scope broadcasting and multiple-access channel at the same time, which is a much more general and potentially more powerful model than the point-to-point channels in the Internet.  Departure from practice and confinement of the Internet is the way to transcend it.  To see further than the Internet giant, we must get onto his shoulders.  The long term objective of the proposed research is to investigate the inherent properties of multi-hop wireless networks in order to overcome disadvantages of such networks induced by translating operating methods from the Internet and to fully unleash their potentials.  It will advance the state-of-the-art data communication, so that we are ever closer to the ultimate goal of ubiquitous computation and information automation.""442843,""Chen, Yulin"
"431935"	"Cheng, LinOi(Irene)"	"Perceptually enhanced graphics and multimedia"	"My research goal over the next five years is to introduce new perceptually motivated techniques to enhance the effectiveness of graphics, visualization and multimedia applications; as well as continuing to explore just-noticeable-difference (JND) based techniques proposed in my earlier research, and strengthen their robustness. My research will include the following novel advancements:(i) JND techniques and perceptually motivated skeletonization algorithms to generate a skeleton scale-space which can be used to support data visualization, registration and matching. Extend the skeleton technique to measure shape and volume changes in medical data.(ii) Continue research on scale-space filtering and extend the applications to visualization and simplification of large 3D point clouds, e.g., geographic, archeological and urban scene data.(iii) Apply human computer interaction techniques and perceptual factors to enhance online multimedia education. This involves the study of student behaviors and the development of student models using machine learning methods. The objective is to improve individualized performance.(iv) Advance multimedia education to a mobile environment by examining the impact of packet loss when transmitting data over an unreliable network, optimizing the utilization of limited available resources.(v) Compare different 2D segmentation algorithms on images and videos, and extend existing, or develop new techniques for perceptually guided 3D segmentation of medical data. (vi) Visualization of medical data in a multimodal context, integrating functional MRI, CT, 3D geometry and texture; and analyzing the changes over time.(vii) Research on multimodal perception to discover the impact on human response and student learning when different multimedia content, e.g., audio, video, animation, stereo and graphics, are presented in a virtual scene.""443636,""Cheng, LiZhen"
"422302"	"Cheriyan, Joseph"	"Approximation algorithms for NP-hard problems in network design"	"My current and planned research focuses on outstanding open questions in the area ofapproximation algorithms for problems in network design, graph connectivity, and networkflows. These topics occupy a central position in Theoretical Computer Science and Combinatorial Optimization. Moreover, they arise in many practical contexts such as the design of fault-tolerant communication networks. Many of these problems are NP-hard. This means that optimal solutions cannot be computed in a reasonable running time, modulo the conjecture that P is not equal to NP. Hence, research has focused on approximation algorithms, that is, algorithms that run efficiently (in polynomial time) and find solutions that are within a guaranteed factor of the optimal solution.I have attacked some of the outstanding open topics in the area (jointly with co-authors),and we have achieved substantial advances. The problem of designing networks thatsatisfy vertex connectivity requirements is a hot topic. Together with Vetta, I havegiven constant-factor approximation algorithms for some key special cases of practical importance, for example, metric-cost subset k-connectivity. Another topic of high interest pertains to packing Steiner trees in a given graph. Together with Salavatipour, I have obtained both approximation algorithms and hardness results that serve to classify the tractability of the different problems (models); these results have implications for network coding, which is an exciting emerging area of practical and theoretical significance.My current and future research focuses on these topics, and the high-level goal is to achieve algorithmic advances and to obtain a better understanding of  the fundamental issues.""421465,""Cherkaoui, Omar"
"420524"	"Chiu, David"	"Pattern-discovery and related algorithms"	"Identifying significant relationships in an uncertain environment, using data from multiple sources, is extremely important in discovering novelty in pattern discovery. For example, linking genotype and phenotype attributes plays a vital role in understanding the biological processes, under normal or novel conditions involving genetic modifications and changes, in problems such as the engineering of novel bioproducts. We propose to study biological processes as an information system that includes identifying relationships of functional segments and molecular sites of biomolecules, and aim at understanding their effects on biomolecular structure and functionality. We study the problem as a data-mining problem using the paradigm of granular computing and propose analysis methods based on different types of multiple-value (multi-attribute) association patterns extracted from an alignment of data. The patterns are based on analysis of statistical testing from a prior model. The goal is to provide a very flexible analysis/synthesis through selection, partitioning and attribute clustering etc. of the data. The paradigm incorporating hierarchical and modular analysis aims at a more generalized model than contiguous sequential pattern analysis and is more descriptive than multiple-variable analysis. After detecting significant patterns, further pattern-mining methods to understand the inter-pattern relationships will be developed. The proposal plans to apply to many real world problems, including whole molecule and sub-molecule analysis, transgenic species comparisons, metagenomics and comparative proteomics analysis in the production of biomaterials. Other applications in engineering such as image and visual analysis, text analysis will also be considered.""437773,""Chiu, Derek"
"419697"	"Chvátal, Vasek"	"Branching rules and resolution search in combinatorial optimization"	"Planning and scheduling problems are solved by methods of combinatorial optimization. These fall mainly in one of two categories: local search heuristics and so-called (for the contrast) exact algorithms.  The umbrella of local search comprises heuristics aimed at quickly finding a reasonably good solution. Starting from such a solution, the exact algorithms proceed, often through a lot of hard work, to find the best solution and to prove that it is indeed the best. Heuristics have been the subject of intensive study by researchers in constraint satisfaction programming (often classified under ""artifical intelligence""); exact algorithms have been the subject of intensive study by researchers in mathematical programming (often classified under ""operations research"").  Recent years have seen increasing interaction between these two initially separate communities, but the gap is still far from closed.Many exact algorithms begin by splitting a problem into two or more subproblems,  which are then solved separately. Branching rules are directions for choosing the split. Branching right is imperative for the efficiency of the overall solution process. The first part of the proposal aims to take a step towards a better understanding of branching strategies.The second part of the proposal concerns refinements and testing of a combinatorial optimization algorithm called resolution search. This algorithm has been designed to mitigate the harmful effect of bad branching decisions. In addition, it can act as a vehicle for bridging the gap between the heuristic algorithms and the exact algorithms.""420756,""Ciampi, Antonio"
"427505"	"Coady, Yvonne"	"Exploring scalable systems through federated infrastructures"	"Now that machines with millions of cores have arrived, we must fundamentally revisit what we mean by scalable systems.  Besides the ability to handle load gracefully, we must consider the increasing expectations and guarantees that now need to be extended to consumers of services across a spectrum of platforms.  Scalable systems must seamlessly grow to incorporate increasing populations of entities embedded in various communication fabrics.  Broadly considered, this concept applies to processors communicating on a bus or across a network, and services communicating through compositions of components.  Ironically however, attempts to add more resources---whether they are physical processing elements or higher-level services---do not always meet with success as they cannot be easily incorporated into existing systems.We propose to explore means of achieving scalability in this broad context by considering the interaction between key infrastructures often responsible for scaling individual system concerns.  Our goal is to innovate in the area of federated infrastructures that work together according to agreed upon objectives. We plan to consider the interactions that exist between fidelity, virtualization, heterogeneity and concurrency.  By fidelity we mean expectations such as privacy or even quality of service at the user level, virtualization refers to the abstraction of hardware and platforms, heterogeneity considers a wide spectrum of commodity devices, and concurrency reflects the growing state-of-the-art in terms of cores and even clouds of computational elements.  We believe that federation holds promise as no one part of the system alone can resolve the current barriers that prevent systems from scaling.  For example, though virtualization provides a means of coping with heterogeneity, it simultaneously introduces tradeoffs in fidelity, and concurrency.   We propose to explore means of explicitly composing federated infrastructures to reconcile these demands, enabling incorporation of the increasing spectrum of resources and services now available in commodity settings.   ""426653,""Coates, Mark"
"422374"	"Crépeau, Claude"	"Post-quantum and quantum cryptography"	")Post-Quantum Cryptography is a keyword coined by Dan Bernstein to refer to (cryptography related) computational assumptions that are not known to be broken by a potential Quantum Computer. This field is currently gaining considerable momentum and several new improvements are making use of such computational assumptions continuously more appealing to users. The main three sources of such hypothesis are the fields of coding theory, real lattice problems and multivariate polynomial with low degrees. Because of the threat generated by the potential existence of a quantum computer, a lot of attention is now moving onto these alternative assumptions.    )Quantum Cryptography refers not only to the traditional applications such as Quantum Key Distribution due to Bennett and Brassard, but to any cryptographic task where quantum information is used in one way or another: either, to use quantum communication to solve classical cryptographic tasks (such as QKD), or more involved cryptographic situations where Quantum Data is actually at stake. It is a tradition around my research group to focus on alternate cryptographic tasks such as Bit Commitment and Oblivious Transfer. These two primitives are of highest importance for constructions of more general cryptographic tasks.         The spirit of this research is to continue the study of cryptographic protocols in the context of quantum information processing. Very many new related questions will arise in the next five years and most of them will be of interest to my research group.""421988,""Crespi, Bernard"
"435297"	"Cui, Bo"	"Nanoscale structure and device fabrication by nanoimprint lithography"	"Nanoscale science and technology is one of the fastest growing research areas in recent years. Nanostructures or devices are typically fabricated by lithography, etching and thin film deposition techniques. Among various nano-lithographic technologies, nanoimprint lithography (NIL) is the choice for high throughput patterning of nanostructures. Similar to CD/DVD duplication process, it is based on mechanical hot embossing principle onto thermoplastic polymers that become soft when heated (thermal NIL). Another popular form of NIL is based on ultraviolet light curing and hardening of a mixture called resist that consists of pre-polymers and additives (UV-NIL). Besides its high throughput, NIL has ultra high resolution of sub-5 nanometers, which is beyond the limitations set by light diffraction as in photolithography or beam scattering as in electron beam lithography. Therefore, NIL is considered as the most suitable lithography for mass production of many nano-devices.The objectives of this proposal is to design, fabricate (by NIL) and characterize the following devices: 1) chemical and biosensors based on surface enhanced Raman spectroscopy (SERS) and extraordinary optical transmission through nano-hole array in noble metals, which are both label-free with high sensitivity; 2) optical negative refractive index metamaterials offering unique optical properties absent in normal materials; 3) nanostructured plastic solar cells having enhanced energy conversion efficiency; 4) transparent and conductive metal nanostructures suitable for electrode for organic semiconductor devices; and 5) bit-patterned magnetic recording media capable of areal density well beyond 1 Terabit/in^2.""422755,""Cuillière, JeanChristophe"
"425249"	"DAmours, Claude"	"Data and coset dependent spreading for DS-CDMA systems based on coding techniques"	"Many wireless communication systems employ code division multiple access (CDMA)technology, most notably 3rd generation cellular.  Unfortunately, these systems are not able to meet the demand for wireless services in some markets and new techniques are being developed to improve the amount of data that can be simultaneously transmitted by multiple users over the limited available frequency spectrum.  Many 4th generation proposals call for a completely new technology to be used.  Such a solution requires replacing much of the existing wireless infrastructure.In this research proposal, we propose improving existing CDMA systems rather than replacing them.  In CDMA all users are assigned a spreading code which is used for signal identification.  However, since all users transmit simultaneously over the same frequency spectrum, tey interfere with one another.  The interference limits the amount of users that can simultaneously transmit.  In our research, we propose using the spreading code for more than just signal separation.  The spreading code will also carry redundant information and act like an error control code without the power loss associated with code rates.  This technique is referred to as data dependent spreading (or coset dependent spreading).  Initial results that we have published show that the technique is more resistant to the interference caused by other users and thus allows more users to access the common channel.  In previous data dependent spreading research, many simplifying assumptions are made, such as perfect channel gain knowledge at the receiver.  The goal of the proposed research is to examine practical systems employing data dependent spreading.  Thus we need to look at channel estimation techniques, integration of additional coding, reduced complexity receiver algorithms etc.""436459,""DAmours, Guylaine"
"434594"	"Daneshmand, Mojgan"	"Emerging technologies in Millimeter-wave MEMS devices: From aerospace to biomedical applications"	"The wireless revolution is evident in every aspect of our daily life, from satellite communications to human health. Integration and miniaturization is an obvious next step which pushes technology limits beyond those of conventional ones. Micro-Electro-Mechanical Systems (MEMS) and nanotechnology provide the opportunity to realize millimeter-wave wireless devices in much smaller size, superior performance and lower cost to satisfy the market demand. This research plan is dedicated to advancing the millimeter-wave MEMS technology for various targets covering from aerospace to biomedical applications. Primary focus of the project will be on developing a new category of MEMS devices that would be called 3D monolithic millimeter-wave MEMS. The idea is based on monolithic integration of MEMS actuators and micro-fabricated waveguide. The concept will be implemented to develop single device as well as integrated circuits and systems for satellite and automotive applications.  The proposed approach promotes a new category of millimeter-wave MEMS devices that offer superior performance with much higher power handling capability compared to the existing MEMS devices. To illustrate the wide application range of the proposed concept, the developed 3D MEMS basic building block will be expanded to a scanning switch network for biomedical dental imaging. This is used to substitute X-ray imaging which is harmful to human body tissue. This research will also investigate on conventional planar MEMS technology. For instance a MEMS switch network will be implemented to change the electrical size of an antenna slightly and generate a phase shift at the far field. This can be used in phased array antenna as a source of phase shifting and avoid external phase shifters which reduces the cost considerately.""424383,""Daneshmand, Mojgan"
"425978"	"Dansereau, Richard"	"Joint audio-visual signal processing for video conferencing"	"We are seeing the emergence of new signal processing techniques that combine multiple physiological modalities such as hearing and in vision.  These modalities can be in the form of speech, facial movements, and gestures and used to improve signal processing results and human-computer interaction.  Areas that can take advantage of multimodal processing includes audiovisual speech recognition, audiovisual speech enhancement, audiovisual speaker detection, audiovisual speech separation, audiovisual affect-sensitive recognition (emotion recognition), and gesture-speech recognition for human-computer interaction.The broader scope of the research program I propose is to develop multimodal techniques for advanced video conferencing.  The initial focus is on single channel audiovisual speech separation and underdetermined multi-channel audiovisual speech separation where one or more microphones are placed within a room to record the speech along with one or more video cameras to record lip motion.  Other aspects such as audiovisual speaker detection and tracking, and audiovisual speech enhancement will also be part of the research.The problem to address is the ""cocktail party effect"".  Consider that you are at a cocktail party where many smaller groups of people are talking.  Even though we hear other people talking in the room, humans are quite adept at focusing on one particular speaker and ignoring the speech of the other talkers.  The goal of the speech separation is to develop signal processing techniques that can filter out the speech of the other talkers will keeping the speech of just one talker, like in the ""cocktail party effect"".  It is expected that including information about lip motion will improve the speech separation over acoustic-only solutions, which degrade when any room reverberation and high levels of noise exist.""433123,""Dansereau, Véronique"
"427985"	"Das, Olivia"	"Modeling and design of secure software systems"	"Poor security of software systems (in form of high vulnerability to intrusions) may allow unauthorized takeover of the critical functionalities of those systems, leading to events ranging from bank fraud to terrorist activities. Since late fixing of security flaws tends to be more time consuming and expensive, it is important to have faster modeling techniques that would assist designers to analyze designs quickly and easily, early in the design cycle. All designs involve tradeoffs and if we simply optimize for the security attribute, we stand the chance of ignoring other attributes of importance, for example performance and dependability, which are affected by security solutions. My proposed research is concerned with developing modeling techniques and tools that would help in analyzing multiple attributes - performance, dependability and security - in a unified way, and would aid in better understanding of design tradeoffs. Key features of this proposal include: (i) development of novel modeling techniques for probabilistic validation of security of software systems, (ii) integrated models to analyze performance, dependability and security, (iii) training of research students in the engineering of dependable and secure software. The research will employ stochastic models for representing system behavior.""435489,""Das, Rahul"
"434964"	"Daudjee, Khuzaima"	"Data freshness in P2P replication"	"Peer-to-peer (P2P) systems often need to service data requests from telecommunications and web-basedclients around the globe.  To boost performance and to increase data availability, unstructured P2P systemsemploy replication to bring data closer to where it is needed, or to increase system or data resources.  Acommon replication strategy is to use result caching, which distributes replicas of query results in theP2P system. This strategy, however, does not provide any data freshness guarantees since cached resultcopies will become stale as the primary copy is updated. Thus, stale replicas or copies can preclude theiruse by P2P applications that require fresh data.     A key goal of this proposal is to provide data freshness guarantees in P2P systems.  There are severalproblems with providing freshness guarantees that this proposal seeks to address.  First, a replication strategythat determines which peers in the system should cache the result is needed, keeping in mind that a largenumber of choices exist.  Second, an update propagation strategy needs to be designed that would determinehow, and which, replicas in the P2P system should be refreshed given that refreshing all replicas is likely tobe costly and unfeasible.  Third, due to network churn and failure, peers may become unavailable or newpeers may join the system.  Thus, techniques for supporting replication need to work gracefully in the faceof this potential dynamism in the P2P system.  Another key goal of this research proposal is to design anddevelop a set of techniques that can provide a spectrum of design choices that take data freshness andperformance into consideration.  These techniques would quantify the trade-offs between freshness andperformance, given that providing fresher data is likely to be more costly but the storage and managementof available but undesired stale data is likely to waste system resources.""419374,""Daugulis, Andrew"
"425254"	"Davidson, Timothy"	"Advanced multi-user systems for wireless communication networks"	"The increasing demand for ubiquitous broadband wireless access with differentiated services, such as voice, email, and interactive multimedia, is a key driving force for advances in wireless communication technologies. The availability of multiple antennas at one or both ends of a wireless link provides the potential for substantial increases in the achievable rate and the reliability of the link, and hence these multiple-input, multiple-output (MIMO) systems appear, in some form, in the physical layer of almost all proposals for future wireless communication systems. However, the wireless medium is fundamentally a shared environment, with an inherent broadcast capability, but also mutual interference. The communication resources provided by the environment can be used more efficiently if the physical layer of the wireless network (or networks) is designed to take advantage of the shared nature of the environment, as distinct from optimizing the performance of each link independently, and if it does so dynamically. This dynamic allocation of resources offers the potential for a substantial increase in the quality of service that can be offered, both in the case of networks with some form of centralized control, such as the uplink and downlink of single-cell systems, and networks in which control is distributed, such as cooperative communication systems, spectral overlay (""cognitive"") systems, and ad-hoc networks. The goal of the proposed research program is to develop computationally-efficient design techniques for multi-user transceivers that enable wireless networks to make efficient use of the available resources. The resulting systems will form some of the building blocks with which the visions of future wireless networks will be realized. The development of the design techniques will be guided by insights from (and contributions to) multi-user information theory and optimization, and the implementation of the designs will take advantage of the flexibility provided by the ""software radio"" transceiver architecture. The focus of the work will be on the development of fundamental design principles, but specializations of these principles could be applied to systems developed by particular standards bodies or companies. ""424676,""Davidson, Timothy"
"429664"	"Davies, Jim"	"Visuospatial scene generation"	"I propose to create a system that finds statistical regularities in databases of labeled images, as well as quantitative ranges of qualitative descriptors (e.g. fairly tall men are between 5'10"" and 6'4"" in height)  to create a visuospatial scene generation (VSG) model.  The model will output a symbolic scene description that includes the following information: 1) what objects are present in the scene, 2) which particular images to use to represent those objects, 3) where those objects are, both in terms of absolute location and relative to other objects, and 4) quantitative magnitudes of input adjectives and prepositions (such as ""large"" or ""above"").  More specifically, it will output these things in terms of probability distributions.    This model will be used in several ways: 1)  it can be used as a part of a generate-and-test algorithm for computer vision, as other generative models have done. 2) it can be used to suggest to computer vision systems what other objects might be likely to appear in an image, and indeed where they might be in the image.  These suggestions will improve the vision system's final classification decisions. 3) the system can be used to create actual images for people to view.  In the longer term, tools such as these would provide inspiration to creative illustrators working on open-ended problems: for example, those who create posters, illustrations, and other kinds of graphical layouts.  Additionally, it will be a first pass at a model of human visual imagination.    The system will be evaluated with use of object recognition programs, and through user testing.""422075,""Davies, Julian"
"425245"	"Davies, Robert"	"Adaptive networks for managed home telecommunication services"	"The service demands from residential home networks are rapidly surpassing the capabilities of the access networks which connect the customers to the internet. Telco ISP's find themselves in this unenviable position through the forgivable albeit short-sighted approach of upgrading their service platform by building siloed services. All routing, QOS, content, and other services are managed within the silo with no compatibility or interaction with other business units in the Telco entity. Thus the notion of more efficient composition of services is extremely difficult to implement.The IP Multimedia Subsystem (IMS) is a standard whose proponents (the 3GPP and 3GPPV) claim contains the roadmap out of silo architectures toward a future of service oriented functionality. This true to a point but it is questionable that IMS will place Telco service providers in a optimal position to compete in a purely IP world. IMS is inherently incompatible with service architectures like web-services and requires special connectivity structures to enable web-services. Also IMS is complicated and viewed with suspicion by Telco ISPs.In the proposed program, the researchers will address 3 main issues of confusion in ISP evolution to service oriented functionality:1. The viability of the IMS in Enabling the Telco ISP to migrate from a silo architecture to a service oriented architecture.2. Hybrid IMS/Service oriented models that may ease the transition for the large ISP.3. Verification of the new service models on a functional core/access network testbed.The results from the research will be immediately transferable to telcos and other ISPs involved in service based business development. The research project will build on a residential services testbed that has been built up over 5 years at TRLabs. The project will be undertaken with regular consultation with TRLabs Telco industry partners.""437201,""Davies, Sarah"
"422962"	"Denidni, Tayeb"	"Advanced antennas for wireless communication systems at millimeter-wave bands"	"The objective of the proposed research program is to investigate and design innovative advanced antennas to achieve a high gain, reconfigurable radiation pattern capability and multi-band operation for future generations of wireless communications at millimeter-wave bands. This research program focuses on tree inter-related projects: millimeter-wave dielectric resonator antennas, electromagnetic band gap (EBG) materials and their applications in the design of agile EBG antennas with multi-band operation, and beamforming antenna array systems. (i)  I will pursue the theoretical and experimental studies  of high-gain and broadband dielectric resonator antennas operating in millimeter-wave bands, this topic includes antenna design, modeling and development using new dielectric materials and new architectures, ii) I will continue to investigate agile antennas with reconfigurable radiation pattern using new materials such as EBG structures or metamaterials, and we  also expect  to extend this  concept to dual- and multi- band agile EBG antennas in order to simultaneously serve different systems at different frequency bands, and (iii)  I also propose to perform theoretical and experimental investigations for developing new beamforming antenna array systems for wireless applications at millimeter-wave bands.  In this research program, I intend to capitalize on my previously successful work and continue to excel in designing new advanced antennas for millimeter-wave wireless applications.  In conclusion, this program will contribute (i) to better understand the physical and electrical mechanisms involved in the design  of millimeter-wave dielectric resonator antennas; (ii) to foster focused research activities in electromagnetic band gap materials and their applications in advanced agile EBG antennas; (iii) to develop innovative beamforming antenna arrays, consistently with specific needs of Canadian high technology  industry; and (iv) to train highly qualified personnel to respond to the scientific and technological challenges of our modern society.""421748,""DeNil, Luc"
"434613"	"Deslandes, Dominic"	"Intégration tridimensionnelle de microsystèmes en ondes millimétriques"	"La loi de Moore, qui stipule que le nombre de transistors par unité de surface double tous les 2 ans, a prédit avec précision la révolution microélectronique des 40 dernières années. En 2006, un groupe de recherche du Georgia Institute of Technology a constaté que l'intégration des microsystèmes respecte une loi similaire à celle de Moore. Il est donc prévu que le nombre de composants par centimètre carré passera de 50, en 2004, à plus d'un million en 2020. Ainsi, le domaine de l'intégration des microsystèmes électroniques subira de profonds changements au cours de la prochaine décennie. Un des principaux défis à relever est l'inclusion des composants en ondes millimétriques dans la création de nouvelles applications utilisant une bande de fréquence aux particularités très intéressantes. Conséquemment, ce programme de recherche a pour but fondamental de développer des technologies d'intégration destinées aux microsystèmes en ondes millimétriques. Ce programme portera sur deux volets complémentaires et inter-reliés, soit  le développement d'une technique d'intégration volumétrique en ondes millimétriques et l'étude d'une nouvelle plate-forme d'intégration surfacique. La combinaison de ces deux volets permettra de créer une véritable plate-forme d'intégration en trois dimensions qui sera utilisée pour concevoir des microsystèmes à forte intégration sans compromettre les performances. Les applications potentielles de cette technologie sont nombreuses : la réalisation de stations de base pour des systèmes de communication point-multipoint, l'intégration de réseaux de capteurs pour des applications de surveillance et de télédétection, la conception de radars ultra-compacts pour l'industrie automobile, etc.""419627,""Deslongchamps, Pierre"
"435072"	"DeZanche, Nicola"	"Technologies for large-channel-count MRI array detectors"	"This research program aims to develop technical solutions that will improve the quality and acquisition speed of magnetic resonance imaging (MRI). One way to achieve these improvements is to make the large magnets on which MRI is based much stronger. However this solution comes at great financial cost. This research program aims to take advantage of recent improvements in data acquisition systems that have allowed MRI scanners to receive using greater numbers of multiple antenna-like sensors (""array coils"") than ever before (think of megapixels in a digital camera). Array coils with large numbers of elements make optimal use of existing MRI field strengths and offer improvements in image quality and acquisition speed over those from arrays with fewer elements. However, building such large arrays is a challenging task due to the need to connect individual cables to each of the array's loop-shaped elements. The additional cables further contribute to ""crosstalk"" between the loops (think of trying to listen to one conversation from a distance in a room crowded with people). As the loop dimensions shrink the amplifiers connected to the loops are no longer ideal as current design methods assume. The objective of this research program is to optimize the performance of large arrays for MRI and take full advantage of existing MRI field strengths by improving the design methods and using fiber-optics to eliminate the problems caused by the increasing number of cables.""428233,""Déziel, Eric"
"435088"	"Dong, Min"	"Resources-constrained communication and networking through adaptation and cooperation"	"Despite the desire to expand the benefits of the tremendous advance in wireless communications, the growth of today wireless technologies and their applications faces an increasingly challenging reality of resource limitations. The limitations mainly concern spectrum and energy availabilities, which are becoming the core issues of future wireless technology evolution. These necessitate a shift of focus on wireless design principles to the resource cognitive paradigm. It will impact the design methodologies of signal processing, communication, and networking for wireless systems. Surrounding the central theme of resource utilization efficiency, new challenges are arising for wireless network and system operation to be control efficient, link adaptive, and nodes collaborative. In response to these challenges, this proposed research program aims at developing the enabling technologies and methodologies to maximize resource utilization efficiency per node and across the network for improved wireless communications and heterogeneous networking. This proposed research will discover the fundamental limitations and trade-offs, as well as develop effective schemes, for link adaptation under feedback exploration, energy-constrained signal processing through distributed and collaborative schemes, and cognitive scheduling policies in resource-limited networks, such as wireless mesh networks and sensor networks. This research will be vital to the future advance of wireless communication technologies that overcome resource limitations and foster the widespread use of infrastructure-less networking in areas of public safety, education, and sensing, across Canada and around the world. In addition, the proposed research will contribute to the training of highly qualified personnel by providing excellent opportunities for students to conduct their advanced study under this program.""426202,""Dong, Mingzhe"
"427663"	"Dounavis, Anestis"	"Design automation algorithms of integrated circuits and microsystems"	"With rapid advances in technology, system complexity and increased signal speeds, significant demands are placed on Electronic Design Automation (EDA) tools to provide the same efficiency and accuracy. Innovations in fabrication process technology have significantly reduced the feature sizes of integrated circuits and increased packing densities of chips. Aggressive design objectives such as system on chip or system in package coupled with increased operating frequencies require multidisciplinary design methodologies such as electrical, thermal and electromagnetic analysis to accurately model high-speed integrated circuits. Furthermore, advances in process technology have introduced novel integrated sensors and actuators, popularly referred to as the micro-electromechanical systems (MEMS). The design of MEMS requires knowledge of multiple engineering principals such as electronics, mechanics, optical, thermal and fluids. As a result of these technological advancements, the underlying algorithms of traditional EDA tools have become ineffective and in some cases obsolete in addressing the multidisciplinary nature and computational complexity of modern designs.     Currently, multidisciplinary systems such as integrated circuits and MEMS are not handled appropriately by simulators and stand as one of the major bottlenecks in design. The proposed research will develop advanced modeling and simulation algorithms to reduce computational complexity of system design. This will be achieved by extending the scope of macromodeling and model-order-reduction algorithms to efficiently link distributed, electromagnetic and MEMS modules in a unified simulation environment.""433379,""Doutre, Colin"
"427326"	"Dubé, Danny"	"Improving the implementation of high-level languages"	"My past, current, and future research contributes to improve the implementation of high-level programming languages, and that of functional languages in particular.  Programming has slowly been moving towards higher-level languages, that is, towards simpler, more expressive, more powerful, more intuitive languages.  Programming languages are work tools, and choosing a powerful tool for challenging projects leads to many benefits.  Development and testing times are reduced.  In addition to reductions in times, the resulting software has a tendency to be more reliable.  Consequently, choosing a high-level language is not simply a question of taste or style, but it is also an economically sensible choice.  I am also interested in improving the implementation of other languages too, not exclusively that of high-level ones, and, more generally, I am interested in making software more reliable.  Means of achieving those goals include encouraging the use of high-level languages, of course, but also developing and improving direct reliability- and security-enhancing techniques---like programming with contracts and static and dynamic formal software verification---or, in other words, focussing on the parts of software engineering that are based on programming language technologies.Projects that I describe include: research on adaptive static analysis, a static analysis whose precision and cost is not fixed but rather parameterized by the abstract model it uses and that is able to progressively refine the abstract model in order to increase precision while keeping the cost relatively low; research on abstract profiling, a static analysis that aims at computing execution behavioral statistics similar to those obtained using conventional program profiling, this static analysis being a good candidate for benefitting from adaptiveness; research on a fast version of a control-flow analysis, with the intent of making it more attractive and favoring its use in compilers; research on software security that consists in applying our new Extended Proof-Carrying Code framework to oracle-based proof reconstruction, for instance; and research on a data compression project, called bit recycling, which is unrelated to software but that has been particularly successful in the recent years.""438879,""Dubé, Dany"
"435075"	"Duchesne, Simon"	"Neuroimaging in dementias: advanced technique development"	"My long-term research is dedicated to developing groundbreaking analysis techniques for image-based computer-aided diagnosis (CAD) within the main applicative context of dementias (e.g. Alzheimer's (AD)). The focus of this NSERC Discovery research program is to develop and validate robust, automated magnetic resonance imaging (MRI) feature extraction methods, combined with novel high-dimensional data analysis techniques, to increase CAD accuracy. Specific Objective 1: to develop and validate a) image harmonization techniques; b) automated large scale image quality control methods; and c) a control-theory framework for image processing pipeline optimization, for the extraction of robust, automated MRI features in multi-scanner data. The targeted performance is to achieve <5% inter-scanner intensity and registration features variability on the human scan-rescan data, with optimal pipelines and embedded automated quality control resulting in <5% failures. Specific Objective 2: to develop and validate a) nonlinear high-dimensional models; b) quantum classification techniques; and c) an unsupervised learning framework for phenotype identification from large-scale datasets, for the generation of a novel high-dimensional analysis framework. The targeted performance is classification of subjects according to multi-modality phenotypes with higher accuracy than previous approaches. We will base these efforts on our medical data warehouse comprised of more than 2,404 subjects 20 to 96 years of age, translating to more than 6,000 individual MRI datasets and accompanying clinical/cognitive estimates.  We know of no other database of this magnitude in the world. With potential for multiple graduate-level projects, this multidisciplinary science and engineering research program strongly benefits the training of HQP. Its successful completion will provide a framework for the analysis, data mining and classification of massive, high-dimensional data, increasing the performance of computer aid to diagnosis in AD.""431564,""Duchesne, Sophie"
"421260"	"Dueck, Gerhard"	"Synthesis of reversible logic functions"	"Over the last forty years the manufacturing of electronic circuits has achieved enormous speedups for their devices. The number of gates on a chip has been doubled approximately every two years. It is clear that such an exponential growth can not be sustained indefinitely. The physical boundaries of the miniaturization will be reached in the near future. However, the demand for more computing power is still increasing and there are no indications that this trend will not continue. An analogous trend can be seen in the increase in low power devices (such as cell phones) where the computing power is not the primary concern-power consumption is a more critical factor. It has been shown that the loss of information consumes energy. To achieve computations with no power loss, the process must be reversible. It is not surprising that reversible logic has gained interest in recent years. Quantum computation is gaining attention, because some problems can be solved more efficiently in the quantum domain, then with traditional computers. All quantum operations are by necessity reversible. Therefore, quantum computations will benefit from new research results in reversible logic. The proposed research project will advance the theory of reversible logic. Applications for the new results will be found in emerging nanotechnologies as well as in low power devices and quantum computations.""439934,""Dueck, Stuart"
"426252"	"Duntsch, Ivo"	"Algebraic and logical methods for data analysis"	"Most methods of knowledge-discovery and data mining assume parameters outside the observed data such as prior probabilities, belief functions or statistical distributions, the origin of which is not always clear or justifiable. On the other hand, there is a high demand for ""non-invasive"" methods of data analysis (such as rough sets, concept lattices, or knowledge structures) that take into account only the given data without stringent additional model assumptions. Using fewer model assumptions results in more generality, wider applicability, less decontextualization, and reduced costs and time. A powerful tool of non-invasive data analysis is relational reasoning, based on ideas developed by Alfred Tarski in the 1940s and which incorporate both relational and algebraic aspects of reasoning procedures. The main aim of my research is to study the algebraic and logical foundations of relational reasoning and to supplement it by statistical and information theoretic methods that enable it to function as a reliable tool of non-invasive data analysis. The main application areas are qualitative temporal and spatial reasoning, theories of visual perception with a particular emphasis on JJ Gibson's work, and the development of structural tools for diagnostic assessment.""428029,""Duong, Franck"
"420068"	"Dussault, JeanPierre"	"Solution numérique de problèmes d'équilibre"	"Mon programme de recherche s'articule autour de l'étude d'algorithmes d'optimisation, de méthodes numériques de la programmation mathématique. Mes travaux actuels portent sur la robustesse, fiabilité et efficacité des algorithmes, en particulier pour des problèmes comportant diverses formes de dégénérescence. Je suis également activement impliqué dans des projets d'applications en reconstruction tomographique, en optimisation de réseaux de fibre optique, ainsi que dans le développement d'un joueur de billard virtuel.En plus des applications, les objectifs de mon programme de recherche visent donc à l'amélioration d'algorithmes d'optimisation, et se regroupent en quelques thèmes. Les méthodes de second ordre (Newton) étant prises comme référence, deux objectifs  sont d'exploiter des variantes d'ordre supérieur et d'utiliser  des méthodes d'ordre 1 dans les algorithmes de poursuite de trajectoires. Le traitement particulier de problèmes MPEC et variantes constitue un autre objectif. Un autre objectif est le développement d'un cadre de travail informatique facilitant la mise en oeuvre d'algorithmes numériques d'optimisation.""438883,""DussaultBelleau, JeanFrançois"
"427388"	"Dziong, Zbigniew"	"Resource management for next generation heterogeneous networks"	"The next generation networks (NGN) are envisioned to be an inter-working environment of heterogeneous transport networks that will provide a service platform that can integrate traditional and innovative IP services. These services should be available at any user location that can be also changing with time due to the user mobility. The heterogeneity of NGNs will have several dimensions that can be divided into two categories associated with technology and ownership, respectively. In particular the technology heterogeneity will require coopearation between wired and several wireless acces technologies such as 3G, 4G, WLAN, WiMAX, satellites, including wireless mesh. The resource management in such heterogeneous networks can be quite complex and to a large extent heuristic, which can lead to large inefficiencies, unless new models are developed that can address the issue in an automated and simple to understand manner. Therefore we propose to develop an integrated and consistent framework and mechanisms that would significantly facilitate NGN resource management and make it more efficient. The framework will be based on an economical model that can integrate and automate several timescales of operations and therefore can further reduce the network operation costs. Here we will use our experience and achievements with an economic model that was developed for homogeneous wired networks.  In this proposal we address three main elements of resorce management: connection/flow admission and routing, adaptation of network capacity, and service pricing. The main challenges are related to extension of the economic models to wireless access networks and to encouraging coopeartion between network operators. To overcome these challenges we will use models derived from Markov decision and game theories. We expect that the results of this research will provide algorithms and knowledge that will be used by Canadian operators to improve service quality and resource utilization in a cost efficient manner.""420177,""Eadie, Reginald"
"423110"	"Eagleson, Roy"	"Surgical telerobotics visualization and performance evaluation"	"This research proposal is presented within the context of our recent results, and those of other researcher in our lab, as part of the field of the design of Human-Computer Interfaces for Computer-Assisted Surgery, Tele-Robotics, and Surgical Simulation.  In order to address some issues in the design of these systems through a more focussed set of studies, we have identified four clinical procedures that are significantly relevant to Canadian Healthcare: Minimally-Invasive Cardiac Surgery, Ultrasound-Guided Prostate Biopsy, Robotically-Assisted Urological Surgery, and Laparoscopic Cholecsytectomy.  From a research perspective, these serve to provide well-defined requirements for our system designs, and allow us to address the specification of objective metrics for their verification and evaluation. This proposal stresses the need for a critical viewpoint on the evaluative procedures involving their usability, and argues for conducting these in parallel with Basic Science Research in Human Visualization and Spatial Reasoning, in order to inform the design of these human-computer interfaces.""439317,""Eamer, Jordan"
"425225"	"Easterbrook, Stephen(Steve)"	"Engineering the software for understanding climate change"	"The proposed research will investigate the contributions that software engineering can make to the study of climate change, including the development and validation of large scale numerical simulations of climate, tools to foster the scientific collaboration among inter-disciplinary teams studying climate change, and the development of decision support tools to assist in evaluating policy responses for mitigation and adaptation of climate change. Our goals are:  - To investigate, from a software engineering perspective, how climate modelers get their ideas into working code, and deal with the long term evolution of their software.  - To identify suitable methods and tools to improve productivity, software reliability, and team coordination among the scientific communities who contribute code to climate simulations.  - To investigate the use of requirements analysis and data modeling techniques to improve accessibility to the huge volumes of data generated from climate simulations.  - To investigate the use of multi-stakeholder problem analysis techniques and decision support tools to compare and assess competing strategies for reducing the emissions of greenhouse gases, at the individual, community and governmental levels.This research will produce results useful to a number of different disciplines. In software engineering, it will contribute to a growing set of empirical studies of scientific computation, helping to characterize the distinct software development methods used by scientific communities. For climate science and global change studies, we expect to improve the ways in which simulation models are developed, and provide tailored software tools, thus improving software quality and productivity. Finally, our conceptual modeling tools will make a contribution to how people reason about sustainability and how they choose among competing ideas for reducing their environmental impact. ""438604,""EastLavoie, Simon"
"427471"	"ELDarieby, Mohamed"	"Ubiquitous and participatory social networking for ambient situational awareness"	"This proposal promotes viable communities through allowing City residents the opportunity to collaboratively develop a collective situational awareness through an assembly of individual perspectives. This proposal focuses on empowering residents of Canadian cities to take a vital part in enhancing the quality of service delivered out of municipal infrastructures especially during real life events such as emergencies. We take the quality of this infrastructure for granted until it fails in some way (e.g., traffic congestions and vehicle incidents are becoming Canadian 'household' concerns). This makes the management of the civil infrastructure key to our societal and economical metabolism. In the platform, city residents use their mobile devices to report infrastructure status information; this requires minimal deployment of sensors for the applications considered. Situational content is transported to city headquarters using municipal wireless mesh networks (WMN) to enhance their ROI. City servers take advantage of the social relationships between city residents as well as their real-time physical location to filter and aggregate user-generated situational content. The WMN is used to transfer back situational knowledge to city residents (on their portable devices) roaming across the city. In more time critical applications, such as incident management, situational information (e.g., incident notifications) may ripple across the WMN covering highways notifying travelling cars and diverting traffic to avoid vehicle build up. In addition, City residents may query databases and locate real-time emergency information in an accurate and timely no matter where they are (e.g., evacuation scheduling and destination choice to the nearest safe shelters). The platform exploits the convergence of social computing and networking disciplines and relies on constructs that integrate features of municipal infrastructure and social relationships among city residents. This proposal contributes to the wireless cities vision, adopted in more than 25 Canadian cities, by enabling new types of value-added applications that supports emergency evacuation planning, traffic congestion management and vehicle incident management applications. ""423339,""Elder, James"
"419438"	"ElHawary, Mohamed"	"Optimal and economic operation of electric power systems"	"This research deals with emerging problems in optimal operation of electric power systems (vertically-integrated and deregulated) using advanced optimization, estimation, and computational intelligence techniques.  The work includes enhancements to computational intelligence tools to solve operational problems competitive open transmission and possibly distribution access environment where many unexplored operational strategies exist. We research optimal mechanisms for pricing active and reactive power and ancillary services assigned to quality of power supply while respecting environmental and security constraints. We formulate and solve short-range optimal operational problems for systems with multiple ownerships of transmission, generation and distribution assets to allow power market participants to make well-founded decisions.We propose to enhance load and market clearing price (MCP) forecasting and optimal model parameter estimation capabilities for transmission and distribution systems to improve computational efficiencies along with improved accuracy. We deal with short and long term load and MCP forecasts. We also treat unresolved issues in cost and performance model estimation.We deal with distribution system aiming reduce losses in primary feeder systems by optimal deployment of fixed capacitors and FACTs-based reactive devices. A non-explored area deals with the beneficial effects and impact of distributed and embedded generation assets on the conventional distribution system.  This research will improve economic savings for power system participants, and enhance social welfare functions involving environmental impacts and continuity of service and quality of power. Canadian electric power industry is enjoying a resurgence of requirements for highly skilled personnel and this research will help in this direction.""425935,""Elhedhli, Samir"
"420505"	"Elliott, Robert"	"Non linear filters and hidden Markov models"	"The central theme of the research is the optimal estimation of signals observed in noise. The most successful algorithm for this is the Kalman filter. Extensions of the Kalman filter when the system jumps around a set of nominal operating modes have been developed, but they have been ad hoc procedures. Indeed, the IMM technology appears to be the current standard in the United States. The research will develop an exact filter obtained in previous work which provides improvements on the IMM, investigate Viterbi modifications and derive related smoothers. The results potentially have applications in many areas of communications, tracking and signal detection.We shall also develop estimation results for point processes. With the development of optical communication technology this modeling is of increasing interest. Some applications arise quite naturally from observed data that is modeled by counting processes, for example, the arrival of photon packets, or even the arrival of email packets. Further, several counting processes admit a marked point process representation, where each count event is coincident with another event of interest. An example is an email process, where its point process could be the times of arrival and the mark process the number of bytes in each email event. Immediate communication applications here include observing email processes to estimate traffic character or parameter values.Hidden Markov Models have been successfully applied in the sequencing of genes, and more recently proteins. We shall investigate further applications of our results in this area.""436949,""ElliottDonaghue, Irja"
"435095"	"EnrightJerger, Natalie"	"Semantically rich networks for many-core architectures"	"Computers have become pervasive throughout our society. By increasing productivity and mobility, as well as providing new avenues of entertainment, computers have been commonplace in professional and personal lives. The wide array of uses and applications of computers present numerous challenges to hardware designers.  In the last decade, the computer industry has begun to successfully integrate multiple processing cores on a single chip through improvements in manufacturing technology.  In previous decades, purchasing parallel computing equipment represented a massive financial investment for a corporation.  Today, parallel architectures are cheap enough to produce that they can be found in cell phones and laptops.  As a result, parallel architectures are rapidly becoming ubiquitous.  To leverage the computation power of these multiple cores, communication between cores or devices is essential.  As in any teamwork activity, the best progress is made when all team members are communicating frequently and working together; it is much the same with parallel computer architectures.  The work in this proposal looks at streamlining the communication between cores to increase its efficiency.  Communication between cores should be natural and intuitive; this work proposes to improve the mechanisms that carry out that communication. Improvement in communication for computer architectures will have a wide impact across Canadian industry; software productivity and application development time will be improved substantially.  Our recent research has shown that communication performance plays a critical role in overall system performance.  Further, we have demonstrated that adapting the communication infrastructure to the specific communication behaviour of applications can be beneficial.  The long term goal of this project is to extend the hardware communication support to encompass a broader array of applications and behaviours.""437550,""Ens, Joel"
"427529"	"Ergun, AyseFunda"	"Algorithms and lower bounds for sublinear processing of long sequences and large data"	"As more and more applications are generating and storing massive amounts of data, the processing of large data is becoming crucial in many fields.  In this proposal we are interested in processing large amounts of data in the presence of resource constraints. In particular, we would like to develop algorithms in the property testing, streaming, and sensor network aggregation models, allowing time and/or space sublinear in the size of the input.  Clearly, such tight restrictions imply that the solutions provided by our algorithms cannot be fully accurate in many cases; one of our goals is to explore the tradeoff between accuracy and efficiency in these models. We will focus on problems related to properties of sequences, but will explore other areas as we develop more techniques.""429170,""Erickson, Crystal"
"426041"	"Far, Behrouz"	"Distributed knowledge management using ontology learning"	"There has been increasing demand for Knowledge Management (KM) solutions for intellectual assets in companies and governmental organizations. Conventional KM solutions are mainly based on centralized architectures comprised of central databases or Web repositories accessible using browsers and semi-formal queries. Experience shows that such architectures cannot be used effectively in heterogeneous computing environments. Modern KM solutions concentrate on three key research areas: representation, distribution and management of knowledge resources. On the distribution and management side, run-time ontology resolution has recently received much attention.  The objectives of the proposed research are: (1) to devise methods for run-time ontology resolution based on concept learning from multiple peers with individualized ontologies; (2) to devise methods for semantic search using thus mentioned concept learning; and (3) to build a semantic search engine (SSE) that can be used in a KM solution. The SSE consists of a systematically developed set of modules that can be combined to conduct simultaneous search and ontology learning at various level of abstraction.  The main purpose of the SSE is to produce effective returns with regards to defined queries based upon the collected pieces of information with semantic diversity that are distributed physically in multiple repositories.  This is possible even if partially overlapping sets of features are used to represent a concept in different repositories. The results of the searches will be aggregated into learned patterns that will efficiently serve future queries. Such a system has several potential applications in information retrieval, medical diagnosis and battlefield planning. Since the proposed research aims to deliver a proof-of-concept, it will focus on supporting theory building linked to a restricted class of techniques and tools, by focusing on KM for software engineering project repositories.""439525,""Farag, AhmedMostafaTawfik"
"425279"	"Felty, Amy"	"Machine-assisted theorem proving:  Proof techniques and applications"	"The principle objective of the proposed research is to advance techniques in formal proof of properties of programs, programming languages, and policies.  For individual programs, proving that they meet certain properties contributes to a very high level of assurance that they will behave as expected.  For properties of programming languages, it is well-known that those that are formally proven to be sound can better provide a solid basis for building software systems that are reliable and secure.  Examples where policies play an increasingly important role include assuring that software that processes health or other personal data respects privacy requirements, and assuring that software that is used in the administration of businesses respects government financial and security regulations.     The techniques I will focus on to meet this objective include the use of higher-order abstract syntax as a representation technique for languages and programs, and the automation of proof search.  These techniques will be developed in the context of targeted applications.  In particular, I will focus on applications where it is most likely to be possible to scale such techniques and apply them in practice to industrial software systems, and on applications where it is most crucial that policy constraints are met with a high degree of assurance.     To meet my objectives, the work will be broken down into four projects, each with subprojects that will vary from shorter term to more speculative longer term research: (1) reasoning using higher-order abstract syntax, (2) foundational proof-carrying code, (3) a theorem proving approach to privacy-sensitive information flow, and (4) analysis and verification of policies.""443174,""Fenech, Marianne"
"431933"	"Fiala, Mark"	"Development of computer vision based localization and mapping for robotics and augmented reality"	"The ability to self-localize with vision is an important capability that humans and animals possess, there is a need to implement vision in automatic systems such as robotics and Augmented Reality (AR).  Mobile AR holds the promise of providing new ways of viewing data and electronic content annotated over the real world,and has been recognized by many including MIT's Technology Review as one of the top ten emerging technologies.  For a mobile robot to navigate or an AR system to operate in an unknown and unprepared environment, the pose (position and orientation) must be constantly measured.  Computer vision can be used to build a model of the environment and localize within it, this model can be shared with other robots or AR devices.  Computer vision has seen major recent developments that suggest a breakthrough in these areas to produce real world robotics and AR systems may soon be possible.  The goals of this research are to develop such a vision based localization system, specifically in the context of mobile devices.  Aspects of feature detection in images, matching between viewpoints, and fusion into an automatically generated map will be addressed.  The research will focus on two aspects for mobile devices: processing power and devices, and compact scene map representations.  The former will look for theory and algorithms that can run on lower processing power devices such as mobile phones, or can be implemented in custom programmable devices or Graphic Processing Units (GPU).  Secondly, research will attempt to find compact representations of the map of vision features for efficient download to other robots or use by mobile users.  This compact data will allow location-based AR content viewable on consumer camera cell phones when a user enters an area previously mapped and aligned with virtual content by others.                                                                       ""438617,""Fiander, David"
"422043"	"Fiume, Eugene"	"Scale-sensitive representations for realistic computer graphics"	"Computer graphics is among the most pervasive of all information technologies.  Progress in the area of realistic computer graphics has been at once fast and sluggish.  Certainly the interactive depiction of some realistic phenomena is becoming increasingly feasible.  Yet despite remarkable strides forward, the convincing depiction of realistic environments involving even moderately complex geometry, materials and lights, and physical or biological simulation, is a slow offline process in which the validity of the outcome is unclear.  Paradoxically, the ease with which our eyes can be convinced makes it all the more difficult to see that computer graphics has little basis in reality.Computer graphics is now blending with results from other fields.  What assurances can we give users of our visual simulations as they integrate them with real-world data from medicine, science and art?  How can we guarantee our images are accurate?  How can we reason about fast approximations, the assumptions they take, and the errors they incur?  How can we deal with ever increasing visual complexity?Being clear about what we are doing requires more precise mathematical characterization.  My research programme for the next few years will focus on putting computer graphics on a sounder mathematical footing, and then applying the results to develop fast, reliable, accurate algorithms for illlumination and animation.  The first step will be to reformulate the fundamental equations of light transport to allow multiple geometric representations with embedded physical dynamics.  The next step will be to demonstrate how this more precise view of realistic computer graphics will lead to more efficient and accurate algorithms suitable for embedding in next-generation graphics systems.  We will see that a crucial component is the development of techniques in realistic computer graphics that are sensitive to the scale of the input and the output.""439496,""Fiume, Marco"
"426679"	"Fraczak, Wojciech"	"Multi-tape automata and formal verification"	"Formal methods in system verification is an important field of theoretical computer science. In my researchproject I am particularly interested in formal methods involvement in the context of the safety critical systems.Such systems are often described as a collection of communicating asynchronous sequential processes. The objective of this proposal is to study methods based on automata and language theory for formal verification of such systems.The objectives of the proposed research program are the following:1. Define and study mathematical models for representing asynchronous communicating systems in terms ofcollections of tape sharing multi-tape transducers. In such models, sequential processes are seen as relations on words expressible by finite automata. We will study their fundamental properties such as: decidability and complexity of membership problem, equality problem, inclusion problem, various closure properties, etc.2. Develop an assertion language supporting a hierarchy of abstraction layers. The assertion language will be based on ""simple-functions"" for which there exists a unique representation in terms of canonical ""Concatenation State Machine"". The language will allow the specification (at different levels of abstraction) of local state transformations of sequential processes and represent them by finite automata. Such representation would permit the use of model checking methods in the process of formal verification of the system.3. Develop a process algebra in order to establish a concise syntax, together with formal mathematical semantics for asynchronous communicating systems. Such a process algebra would constitute the foundation for the development of various proof methods in order, for example, to perform hand-crafted proofs over complex (e.g., infinite state space) systems, for which model checking methods do not apply.""441062,""Fradet, Mathieu"
"426982"	"Franks, Greg"	"Reverse engineering performance models from trance data"	"Distributed computing systems are becoming increasingly important for information access and for administration as organizations in education, health, government, and business move into networks of interacting computers.  Constructing performance models of these systems has shown to be beneficial during all stages of a project. For example, a model can be used to set performance budgets for components of a system.  The model can also be used to find feasible designs and locate potential bottlenecks.  Finally, the model can be used to plan the resources required for installed systems.  In all of these case cases, the use of models can lead to substantial cost savings for a performance-sensitive project.The major problem impeding the use of performance models is the actual generation of the models themselves.  The architecture of the system being studied must be extracted and the resource requirements of the components in the system must be specified.  Getting this information can be time consuming and difficult.The purpose of this project is to simplify the entire model building, solution and analysis process.  Performance models will be created automatically based on the automatic instrumentation and tracing of pre-existing software.  The user need only to execute her software, possibly as part of normal system testing, to generate the performance model.  The instrumentation and tracing will be driven by high-level design models where possible because the design model can provide hints on how to filter and translate the event logs generated by the system under test.  Further, measurement results from tracing will be imported back into the design models.   Performance modeling will then be moved out of the realm of experts and into everyday use.""419257,""Franks, Ian"
"423097"	"Frappier, Marc"	"Formal reuse and validation of information system specifications/Réutilisation et validation formelle des spécifications de systèmes d'information"	"Information systems (IS) are prevalent in today's economy.  A typical example of IS, and probably the most important one, is the Enterprise Resource Planning system (ERP), with its 18 billion $ worldwide market.  To manage costs, enterprises are buying COTS (commercial of the shelf) IS like ERPs, but that forces them to adapt their business process to the COTS, reducing their flexibility and ability to develop innovative business strategies, and locking them with a single vendor with little bargaining power for controlling maintenance costs.  This proposal aims at providing a more flexible way of designing IS that better match the rapidly evolving environment that organisations are facing.  We want to build on the platform that we have developed over the past five years, called APIS, and extend its capabilities for building an IS, from the user interface to services and databases.  There are three axes we want to address: reuse, validation and instrumentation.  Reuse is a key productivity and quality issue in IS building.  For instance, Hertz's car rental system is probably very similar to Budget's, but not exactly the same.  We want to study reuse mechanisms at the specification level, so that one can build a generic specification, e.g., for a car rental system, and instantiate it and tailor it for a specific enterprise environment, while preserving the properties and the capabilities specified at the generic level.  The APIS platform eliminates most of the need for design and implementation, which means that unit, integration and system testing are also eliminated.  However, there is still a need to perform validation and acceptance testing, to make sure that the system specification is the appropriate one, that it satisfies business requirements.  This can be done manually, by designing tests scenarios.  We could go a step further by using modern verification technology, either by using theorem proving or model checking.""441825,""Frappier, Vincent"
"419201"	"Frasson, Claude"	"Emotionally based intelligent tutoring systems"	"Emotions are part of human behaviour. Our previous work has highlighted the importance of emotions in learning particularly for memorizing information and problem solving. The general objectives of our research are:(1) to determine the range of emotions which can be played by a virtual tutor   (avatar for instance) with a behaviour similar to a human tutor(2) to analyze how emotional communications can have an influence on learning performances according the student profile(3) to determine the emotional conditions which allow for the best transfer of knowledge into subconscious memory.More precisely we wish to:- build an emotional agent (emotional tutor) able to detect learner's emotions and communicate with the learner through emotional reactions,- control and influence learner emotional state in order to optimize the transfer of knowledge,- adapt the tutoring discourse so that the learner be in optimal emotional conditions of learning,- continue our preliminary work on subliminal learning which allows the transfer of large quantities of knowledge, information and procedure, into subconscious memory,- understand and experiment the influence of emotions on subliminal learning.The impact of these works on learning should be significant : they should allow to- finally better understand how the brain functions which can have an impact on various applications(education, health, social works) in which the human emotional state can be controlled and optimized to increase memorization and problem solving.""443476,""Frate, Jonathan"
"419873"	"Funt, Brian"	"Computational colour vision"	"The overall objective of this research is to fully understand the computational processes underlying colour perception. While of intrinsic scientific interest, understanding colour perception is also essential for improving colour reproduction in the context of digital cameras, displays, and printers. In addition, it has important applications in computer vision in terms of colour-based object recognition, and the analysis of the colour changes created by shadows, interreflections, wetness and transparency.  Our approach to these issues utilizes the computational view of colour;  i.e., that human colour perception and the ways in which colour is used in understanding the visual world can be explored and explained as computational processes.  Specific short-term objectives include: (1) Working on full-spectrum printing to overcome the problem that arises with traditional 4-colour CMYK printing; namely, that printed colours that match one another under one light can, under a second light, change their appearance such that they no longer match.  (2) Investigating whether colour images with more than the standard 3 RGB bands might lead to improved colour constancy. The interesting and counterintuitive result we have so far is that the extra information in multispectral images does not yield significant improvement. (3) Improving picture rendering by identifying the manipulations that are predicted by colour appearance theory and compare those to the manipulations that have been practiced for decades in photography, video, and motion picture production.  The aim is to formalize some of the 'art' of film making techniques in terms of existing colour appearance models and simultaneously explore the limitations of these models. (4) Exploring uses of quaternions (4-tuple complex numbers) as a representation for RGB colours. The quaternion representation has the advantage that the 3 RGB components are treated as a single number.""443729,""Furgal, Christopher"
"435055"	"Gagné, Christian"	"Enabling autonomic computing with computational intelligence"	"An obstacle to future developments of computer software systems lies in their ever-increasing complexity. In a not-so-distant future, we will reach a state where the operation of these systems will be intractable for human experts. A solution to this has been proposed in the autonomic computing paradigm, which aims at developing computer software systems made of many interacting self-managing modules. This can be achieved in part through the use of open standards for service discovery and communication between modules. But autonomic computing also requires the injection of intelligence and robustness in the software systems, in order to free human operators from low-level details. Computational intelligence encompasses many techniques combining elements of learning, adaptation, evolution, and fuzziness. Using these techniques for the development of self-managing software systems looks promising, and will be explored within the context of the current research program.Three main projects will be executed within the program: 1) designing a multiagent-based software architecture for autonomic computing; 2) exploiting reinforcement learning for enabling effective self-adaptation of software modules; and 3) investigating cooperation methods between modules for an efficient decentralized processing. These projects will be executed within the specific applicative context of monitoring different types of environments with wireless sensor networks. Sensor networks involve the use of miniaturized sensing devices for a robust distributed processing of information, with a sensing close to the phenomenon observed. These resources-constrained devices can greatly benefit from autonomic-based methods, as judiciously designed policies at the sensor level can lead to an efficient perception at the network level, while maximizing energetic autonomy.""423794,""Gagné, Christina"
"425047"	"Gagnon, Langis"	"Content-based video summarization system"	"The evolution of video-related information technologies (network bandwidth, personal camera, compression tools, disk capacity, displays, etc.) has pushed video as the primary medium for the capture, storage and presentation of visual information. Video is now replacing still images in many application areas like, forinstance, video surveillance and on-line Web advertising. From small clips to full feature-length films, digital videos are everywhere and often part of a larger multimedia digital library. As a result, the need for tools that can quickly browse and retrieve video content has increased and led to two major topics in modern computer vision: content-based video retrieval and video summarization.The proposed research program targets video summarization. The objective is to develop and integrate multimodal analysis tools to segment a video into Logical Story Units (LSU) or scenes based on video elements like shots transitions, key-objects, key-faces, key-places as well as audio characteristics. This will be integrated into a LSU-based GUI, rather than the commun shot- or key-frame-based one, that will provide a graphical ToC to the user with the possibility to access parts of the video from it. The GUI will be adaptable to allow user to access video based on his preferences.""438009,""Gagnon, Langis"
"426976"	"Gaur, Daya"	"Linear programming based approximation algorithms for optimization problems"	"Approximation algorithms are an elegant formal framework for dealing with intrinsically hard optimization problems. An approximation algorithm for an optimization problem produces a feasible solution in polynomial time that is within a constant multiplicative factor of  the optimal solution  for every instance of the problem. Approximation ratio is only a worst-case bound and  pathological instances are usually hard to find. It has been experimentally observed for a variety of approximation algorithms that, on average, the performance is vastly superior than the stated bound. An approximation algorithm can always be refined and tuned to specific classes of instances arising in practice, thereby improving  the performance. I am studying approximation algorithm design based on linear and semi-definite programming techniques. Quality or the integrality gap of the relaxation, techniques used to round the fractional solutions  to the integer solutions, and the time needed to compute the optimal solution to the relaxations, all effect the quality of the approximation. My research focusses  on improvements on  these fronts and it will lead to improved techniques and approximation algorithms for optimization problems.""426465,""Gauthier, Clément"
"421781"	"Gauthier, Robert"	"Photonic quasi-crystal based integrated optic devices"	"Research activities in translationally symmetric photonic crystals have continuously increased since its conception in the late 1980s.  Significant advances in theoretical analysis and applications have been made with numerous device designs proven in the laboratory environment.  In the 1990s, the high rotationally symmetric photonic quasi-crystal made its debut and was shown to display the important properties of the translation symmetric counterparts.  However they have received less attention as the theoretical tools required to thoroughly analyze the structures are still lacking and structure fabrication is more difficult.  This research project is focused on advancing the photonic quasi-crystal know how through advancements in theoretical analysis, structure fabrication and device development.  Theoretical advancements will follow along the lines of exploring the rotational symmetry properties of photonic quasi-crystals with the objective of linking a polar spectral decomposition with a polar band structure.  Existing in-house developed simulation tools will be enhanced based on the theoretical advancements and chosen device configurations.  In the fabrication of structures, the plan is to explore the production and characterization of photonic quasi-crystals in SOI, LiNbO3 and glass.  These three substrates are selected for the following reasons, SOI provides a high dielectric contrast for photonic quasi-crystal structures and displays a high potential for next generation devices. LiNbO3 is a mature substrate technology showing several interesting optical properties such as birefringence and is electro-optic.  LiNbO3 also retains a high dielectric contrast for photonic quasi-crystal structures. Glass provides a dielectric value compatible with the optical fiber technology and as such fiber-waveguide coupling is simpler.  The high dielectric contrast is achieved by loading the glass waveguide with a thin high dielectric cladding.  Structures fabricated will be characterized in the existing optical measurement laboratory and designs showing promising results will be configured into optical devices. A large portion of the SOI research will be conducted in collaboration with a researcher at the University of Arizona's Optical Sciences center.""438408,""Gauthier, Sylvie"
"420771"	"Gburzynski, Pawel"	"Practical low-cost wireless ad-hoc networking for sensing and monitoring"	"My proposed research deals with software for small and inexpensive devices which can communicate wirelessly (via radio channels) and organize themselves into so-called wireless sensor networks. Such networks are intended to be large and inconspicuous at the same time (e.g., consisting of a large number of small and cheap nodes), durable (e.g., capable of operating for several years without replacing the batteries), robust (resilient to failures of its individual members), and maintenance-free (self-organizing and requiring no human attendance).  Application areas for such networks include ecological monitoring (temperature, humidity, soil condition), tracking movable objects (industrial assets, goods requiring special attention, animals including cattle, as well as people), detecting anomalies in the environment (diagnosing hazards, triggering alerts), assisting disaster response, and many others. In my work, I will study ways to program wireless sensor networks as to accomplish these goals: 1) maintaining small program size without sacrificing its structurability, readability, and reusability; 2) devising robust collaboration patterns (distributed programs), i.e., ones that do not break down in the face of inherently poor reliability of the individual nodes; 3) making it possible to authoritatively test the network programs in a simulated environment before deploying them into the real network; 4) modularizing network programs with the intention of recycling their components in other wireless sensing applications.  One of my objectives is to build real sensor networks which other researchers can use as tools in their work (e.g., the EcoNet project at the University of Alberta).""432591,""Ge, Bixia"
"428860"	"GhodsiBoushehri, Ali"	"Dimensionality reduction: methodology and applications"	"This research focuses on simple representations of complex data sets. Three specific problems of interest are data mining, finding a sequence of actions to achieve a particular outcome (planning), and maintaining a representation of one's location (localization). This work is applicable to numerous areas in which meaningful information must be extracted from extremely large and multivariate sequential data sets. In these domains, action taken at a data point (e.g. taking a treatment by a patient) has immediate effect on consequent observations. The goal is to find a representation in which the actions correspond to simple mathematical transformations such as translation and rotation.On the more theoretical side, I propose to explore and formalize non-linear dimensionality reduction techniques as probabilistic models. I will address the problem of how such models should be constructed, and how they should respond when data is missing. This will work toward addressing two important open problems in the area: determining how to objectively evaluate a dimensionality reduction algorithm, and predicting how well an algorithm will generalize.""432982,""Ghonaim, Nour"
"427026"	"Godbout, Nicolas"	"Building blocks for the Quantum network"	"Quantum information is distinct from classical information by the fact that it is stored or transmitted using physical systems exhibiting properties of quantum mechanics.  Quantum information can be efficiently transmitted as single photonsover optical fibres.  The science of quantum information enables communication tasks that are impossible to realize using a classical network such as the Internet.  An important example of such a task is quantum cryptography, a technique enabling absolutely secure communications, no matter the technical resources of a potential spy.The overall theme of the proposed research program is the advancement of technology, implementation and protocols enabling the deployment of a quantum network, or QuantumNet, the quantum analog of the Internet. The specific advances that are targeted, which constitute the objectives of the research program, are: the fabrication of stable sources of entangled photons, unidirectional quantum communication links, implementation of new quantum information protocols and quantum information processing.The applications of the QuantumNet include: absolutely secure communications, anonymous voting, fair online gambling, efficient distributed computing and others.""426105,""Godbout, Stéphane"
"425351"	"Godfrey, Parke"	"Database support for 4D virtual and mirror spaces"	"Virtual worlds within which multiple users can engage, meet, and explore have become commonplace.Some of these worlds offer rich environments with which users can interact, in which they cancreate, and which they can alter.  These changes persist, as in the real world.  With ever improvingperformance and capabilities of standard computer, graphics, and network technology, more of theseworlds are rendered in 3D in ever increasing realism, and provide a compelling, immersiveexperience.  (4D refers to 3D plus the aspect of time.)    While many multi-user virtual worlds are games, such as EverQuest, Eve Online, and Final Fantasy,arising from the gaming industry, others are no longer games, per se, but social networks, such asHabbo Hotel, Second Life, The Sims Online, and There.  Some offer rich environments in which userscan create, and even sell their virtual creations to others.    As the technologies behind virtual worlds continues to improve, these worlds will become richer, and become more than just entertainment.  They will become regular social meeting places, trainingcenters, educational hubs, shopping malls, and informational kiosks.  Virtual worlds will change howwe interact, work, and search on-line.    Many technologies underlie virtual worlds.  The state of a virtual world is essentially a database.In this research, we address key challenges that virtual worlds raise for database systems.  Weresearch how to adapt and improve database systems to support better virtual world platforms, tomake them operate more efficiently and more robustly, how to scale these worlds effectively forgreater numbers of concurrent users and greater content, and to support richer content and vistaswithin these new worlds.""421911,""Godfrey, Stephen"
"427245"	"Gokaraju, Ramakrishna"	"Coordination of generator protection with control"	"The techniques for generator protection and control are well established separately in technical publications and manufacturers' literature, but there are no techniques addressing the coordination of the two to date to the author's knowledge.  The purpose of the applicant's current research is to develop a unified concept for protection and control at the power system generation level. The specific objectives to achieve the above goal are listed below:1. To apply  artificial intelligence techniques and pattern recognition techniques such as support vector machines (SVMs), principal component analysis, phase plane methods to achieve improved coordination between the protection and control functions using the generator capability curve analysis. 2. To use Flexible Alternating Current Transmission Systems (FACTS) compensation schemes for damping turbine-generator torsional oscillations (subsynchronous resonance).3. To apply adaptive supplementary control techniques (radial basis function identifiers and pole-shift controller) to FACTS for effective damping of subsynchronous resonance, power oscillation damping, voltage control.4. Closed loop testing of the protection and control functions on a power system using a real-time digital simulator, which is able to mimic the actual behaviour of a real system.With the NSERC DG grant, the applicant intends to achieve a balance between theoretical and practical work for his graduate students. The findings from this research program will help in improving the standards for proper coordination of generator protection with control to enhance system security. He is optimistic that with his perseverance and zeal for innovative work, he will be able to contribute strongly in his research area and help in the advancement of Canadian and international research in power system protection and control.   ""436192,""Golas, Ewa"
"428856"	"Gondra, Iker"	"Machine learning methods for strong image segmentation"	"Over the last four decades, the development of image segmentation algorithms has been an area of considerable research activity. The reason for such significant attention to this problem lies in its practical importance. Image segmentation is a key step towards high-level tasks such as image understanding and serves in a wide range of applications including object recognition, scene analysis or content-based image/video retrieval. Many image segmentation algorithms have been elaborated. However, strong segmentation, which corresponds to a partitioning of an image's pixels into regions that are semantically meaningful to people, remains a difficult and as yet largely unsolved problem. Most image segmentation algorithms extract regions satisfying some uniformity (homogeneity) criterion, which is based on low-level data-driven visual features (e.g., color, texture). Unfortunately, homogeneous regions do not necessarily (and usually do not) correspond to semantically meaningful objects. This is mainly caused by the disconnection between low-level visual features and high-level semantics, which is commonly referred to as the semantic gap. This has motivated researchers to develop image segmentation methods that, by introducing high-level knowledge into the segmentation process, can break through the performance ceiling imposed by the semantic gap. The main disadvantage of those methods is their lack of flexibility due to the assumption that such knowledge is provided in advance, which heavily restricts the application domain.The main objective of the proposed research is to develop image segmentation strategies which, through the use of machine learning, generate and incorporate high-level knowledge into the segmentation process.""442669,""Gong, DaoBing(David)"
"431807"	"Goodyear, Bradley"	"Stockwell analysis of the brain's resting state"	"In this project, we will develop a new time/frequency analysis technique for functional magnetic resonance imaging (fMRI) that is interactive and operates in real time, in order to detect brain regions that are communicating while subjects are at rest.Although fMRI has emerged as a valuable tool to investigate brain function, results are dependent on the task the subject is asked to perform and the subject's behaviour during the task. This may be of major concern when investigating the effects of neurological disease or age on brain function. As a result, there is increasing interest in understanding brain activity during rest (that is, in the absence of a task or stimulus). It has been demonstrated that fluctuations of the resting-state fMRI signal over time are highly correlated between brain regions consistent with our current understanding of anatomical connections.     )The major concern for resting-state studies is reliably keeping the subject in a state of rest for the duration of the imaging session. If the resting state is disrupted even for a moment, such as during a brief movement or an active thought process, the estimate of the contribution of brain regions to the resting state may not be accurate. We propose that a time-frequency analysis of fMRI signals will successfully determine the signal components responsible for the resting-state, by detecting and removing temporally resolved frequency components responsible for the disruption of the resting state. As a result, our proposal has two main objectives. First, we will demonstrate that brief movements and active thought processes introduce frequency components into the fMRI signal that are detectable by the Stockwell transform. Second, we will develop and implement an interactive and real-time console that generates images of brain regions that are communicating during the resting-state, as a means to advance our technology towards improving healthcare and in the study of how changes in functional connections facilitate the learning of new skills.""441286,""Gooyers, Mark"
"427238"	"Gross, Warren"	"Design and implementation of coded signal processing systems"	"Signal processing systems rely on powerful error-correcting codes to protect data from corruption. Such codes are ubiquitous in digital communications where transmitted bits can be corrupted by noise and interference in the communications channel. The main challenge in realizing practical coded systems is the efficient hardware implementation of decoders. The proposed research focuses on a recently discovered algorithm called stochastic decoding. Stochastic decoders can be implemented with remarkably simple logic circuits. This proposal describes a research program that will investigate the design and implementation of high-throughput, low-area and low-power stochastic decoders.""443215,""Grosvenor, Andrew"
"425248"	"Groza, Voicu"	"Reconfigurable system-on-chip distributed instrumentation"	"Reconfigurable System-on-Chip Distributed InstrumentationDistributed instrumentation networks continue to be considered one of the most important technologies for the 21st century. Emergent information-technology Real World Web (as opposed to the virtual World Wide Web), based on Smart Sensor Mesh Networks, has been identified among the first three key areas for the period 2008-2012 by Gartner, Inc. The proposed research, a continuation of the applicant's previous work on distributed virtual instrumentation, aims to improve functionality, versatility, efficiency and robustness of smart sensor networks in remote monitoring applications, while increasing accuracy and adequacy of the acquired data. The focus is on intelligent sensors architectures, information harvesting techniques and objective interpretation of acquired data, rather than on communication over the networks. The project is build as a three layer hierarchical architecture encompassing smart SoC sensors at the lower level to performing signal acquisition and pre-processing functions, a middle layer for information processing, and an upper layer for knowledge extraction and complexity management. The reconfigurable sensing SoC will be capable to fusing data from orthogonal algorithms, adapting its quantization and sampling parameters to the dynamics of the monitored signals, HW reconfiguration to adapt algorithms with the acquired signals characteristics, self-testing, self-calibrating and validating acquired data. ""431316,""Grozic, Jocelyn"
"419962"	"Hafez, Roshdy"	"Resilient wireless communications using randomly deployed transceivers"	"There are many emerging wireless concepts and applications, where radio nodes of different sizes and capabilities are deployed under variable conditions. Also, the fast growth in the number of wireless systems deployed in a city or along a road will create an unpredictable interference environment. Since many of the new applications envisage using miniature radio devices with one or multiple antennas, it is important to develop new concepts that allow such devices to function with reasonable reliability under a wide range of conditions. The key is to build into the node enough flexibility in order to be able to adapt its communication parameters to the surrounding environment.  The proposed research focuses on four aspects of ad-hoc wireless communications. The first aspect is to model and characterize the pathloss and scattering properties of randomly deployed radios in a realistic way.  We plan to measure the signal strength and power delay profiles of the channels under different conditions.  The second aspect is to characterize the relation between the transmission bandwidth, the propagation environment. Distance and density of deployment, data transfer capability and the life time of the node. The third aspect is to design, simulate and test new concepts that allow a small node to adapt to its environment in order to maintain reliable communications.  The fourth aspect is to propose innovative methodologies to enhance the reliability of and health of the network.  Measurement-based propagation modelling will provide an insight into ways to create an adaptive structure for the wireless node. In particular, we are aiming at examining the role of antennas and adaptive filters as ways to build robustness. Finally, we aim to examine our models against real-life applications such as the conditions encountered in monitoring large areas for safety, security or for monitoring the quality of air and water.""420893,""Haffner, Douglas"
"435035"	"Harvey, Nicholas"	"Cominatorial optimization and communication networks"	"My current and future research has two primary goals. The first goal is applications of combinatorial mathematics to practical problems, particularly those involving communication networks. My coauthors and I have worked on several such projects, developing novel results in the areas of peer-to-peer sytems and network coding. The second goal is understanding fundamental problems in combinatorial optimization, particularly those relating to matchings and matroids. My research has developed the fastest known algorithm for several of these classical problems, and made progress on long-standing open problems.My future research continues to focus on intriguing questions in networking, especially the interaction between network coding and combinatorial optimization. I am also working on several challenging, fundamental optimization problems, including matroid intersection and the traveling salesman problem.""421253,""Harvey, Pierre"
"420471"	"Haslett, James"	"High speed serdes and ADC circuits"	"The objectives of the proposed research program are to exploit new time-based, rather than conventional voltage based, circuits that we have invented and patented, in a variety of applications. The main building block is a Time-to-Digital Converter (TDC) that provides a digital output each clock cycle with no delay between input and output. The initial TDC circuits that we have designed, tested and published are dramatically better than existing TDC's used, for example, in Positron Emission Tomography (PET) imaging for medical diagnosis. The TDC's have direct application in serializer-deserializer (SerDes) circuits for ""on chip"", chip-to-chip and board-to-board data links in high speed data communication networks. A completely new SerDes system based on the circuits will have many advantages over exisiting systems, including reduced crosstalk, better interference rejection, and reduced clock speed requirements for given data rates. When combined with a voltage-to-time front end (VTC), the TDC's can be used to build GigaSample-per-second (GS/s) ADC's whose performance will improve as semiconductor fabrication process feature sizes scale downward, unlike many circuits that suffer from scaling. Another major advantage is that circuit complexity increases only linearly with an increased number of bits n, unlike existing systems whose complexity increases exponentially with n. This saves silicon chip area, which is becoming very expensive, and also reduces dc power consumption, which is important for reducing heat dissipation and extending battery life in mobile applications. The ability to use these types of high speed low power ADC's in wireless receivers will enable ""software radios"", for applications such as multi-standard cell phones that are fully programmable and will work worldwide.. ""428451,""Hasni, Abdelkrim"
"428875"	"Hassan, Ahmed"	"Leveraging historical software repositories to understand and support software maintenance and change activities"	"Software repositories, such as source control and defect tracking systems, represent a detailed historical record of a system. These repositories are no longer used simply for record-keeping activities such as maintaining code versions or tracking defects. Practitioners often examine historical repositories, in an ad hoc fashion, to improve their understanding of a system, or to gauge the state of a project. For example, managers often examine historical changes to estimate the reliability of a project or to determine the need for testing resources. Few researchers investigate enriching their methods with the valuable information in historical repositories.             The long term goal of this research program is to systematically uncover the benefits of information in software repositories by empirical exploration. Over the next five years, we will conduct research along the following two dimensions: (1) the recovery and representation of historical information in a simple format to ease access to this information throughout software research and practice and, (2) the development of methodologies and approaches that explore and demonstrate the benefits of such information.             We will develop a formal model and language, as opposed to conventional ad hoc methods, to manipulate the historical information. We will use this model to understand and predict code change patterns, such as refactorings, in large projects over the short and long term. We will perform user studies to understand the side effects of adopting mining techniques, as their adoption is likely to affect the historical information on which they are based. We will study the effect of various development activities, such as code browsing and reuse on code expertise, instead of only using historical code change information as done by prior research.             This program will directly train three PhD and three MSc students. Students will gain an appreciation of the challenges facing practitioners on a daily basis and the potential of repository data in helping address those challenges. Students will also gain proficiency in validating their research hypotheses using statistical and empirical methods, knowledge valued by both academic and industrial sectors.""439306,""Hassan, Ahmed"
"425369"	"Hassanein, Hossam"	"Seamless service delivery in next generation wireless networks"	"Internet users are increasingly using interactive services such as multimedia messaging, social networks, sharing and collaborative applications. Emerging services like telemedicine or telepresence are also gaining popularity. Simultaneously, Internet users have an interest in maintaining full access to their services mix while mobile. Together, these developments are straining current architectures for service delivery in wireless and mobile networks.   Our long-term goal will be to establish a foundation that will integrate disparate access technologies, infrastructure modes and service management platforms in a seamless and transparent manner to enable seamless and ubiquitous quality of service (QoS) based communication. Generalized algorithms and techniques that oversee heterogeneous access and structural disparities will make the integration seamless.   From this research we expect to devise enhanced service delivery techniques that can provide the Canadian public and private telecommunications sectors with wireless and mobile networks that enable the realization of the mobile Internet of the future. The research will result in cost-efficient, profitable and capable architectures for service deployment, management and delivery that will ensure consumer satisfaction.    Training of highly qualified personnel involved in this program will include experience in broadband multimedia support, QoS support and resource management, software-defined radios, multi-hop networking and heterogeneous networks. I expect that seven PhD and seven Master's students will receive training through in this research program. There is a strong demand for these HQP in the information technology and telecommunications sectors and their future employment will accelerate disseminating next-generation communications technology to Canadian industry.""425677,""Hassanein, Khaled"
"420006"	"Hell, Pavol"	"Complexity of partition problems"	"Constraint satisfaction problems model many problems in scheduling, logistics, databases, and artificial intelligence. It has been observed that when the constraint language (or the constraint 'template') is fixed, the problems seem to fall into only two categories - polynomial time solvable or NP-complete. On the other hand, it is known that (unless P=NP) there must exist problems which are not solvable in polynomial time, but which are not quite NP-complete. Whether or not there are such 'intermediate' constraint satisfaction problems (with a fixed template) has become a very important open problem in theoretical computer science, driving much research effort based on algebra, analysis, logic, and combinatorics. Graphs seem to have an important role to play. Our results in graph theory have motivated the original problem, as well as several subsequent developments. It now appears that they will continue to motivate the research efforts. In this proposal, we propose research in the complexity of certain variants of graph homomorphism problems which would have implications for (variants of) general constraint satisfaction problems. These include list homomorphism, minimum cost homomorphism, and matrix partition problems.""430088,""Helle, Steve"
"428956"	"Hengartner, Urs"	"Security and privacy for urban sensing"	"In urban sensing, people, more specifically, their cell phones, become location-aware sensors. For example, a person traveling in a car has her cell phone transmit its current location and acceleration to a central server, which combines this data with data received from other cell phones to generate up-to-date traffic information, such as road congestion. There are many other possible types of applications that can be built on this kind of sensing model, considering that the sensing capabilities of cell phones have increased drastically.      For urban sensing to become a success, many privacy and security challenges need to be resolved first. In the example above, whereas sending location and acceleration data to the central server allows the server to provide traffic updates, it also enables the server to build a detailed location profile for the owner of the cell phone. In turn, this location profile will likely reveal sensitive information about the owner, which is not in the owner's interest. Therefore, the owner might refuse to let her cell phone participate in urban sensing, unless we guarantee to her that her privacy will not be violated. However, privacy guarantees alone are not sufficient, we also need security guarantees. Again, consider the traffic-update example. Here, a malicious cell phone owner could tamper with her device and have it report wrong location or acceleration data, in an effort to keep traffic away from the route that the owner is taking herself. Therefore, unless an application developer is guaranteed the integrity of the sensed data, he is unlikely to build urban-sensing applications.     The goals of the proposed research are to bring privacy and security guarantees to urban sensing. In terms of privacy, the research will lead to 1) a privacy-aware architecture for urban sensing where individuals maintain computers that process and forward information and 2) user interfaces for mobile devices that allow individuals to manage their privacy preferences. In terms of security, the research will lead to new integrity mechanisms for urban sensing, where some of these mechanisms will take advantage of protection mechanisms that people (implicitly) already take advantage of in their daily lives.""435105,""Heniche, Mourad"
"431675"	"Hinzer, Karin"	"Green optoelectronics: solar cells and lasers using nanostructured materials"	"Industrialized society is in the process of making a profound shift in its approach to the world as Earth's resources are no longer viewed as effectively infinite; neither in the Earth's ability to supply materials, nor to absorb waste.To allow this shift, devices that are efficient, low-cost, and have a low environmental impact are required. This research program proposes to advance knowledge in two families of devices that can benefit from the same nano-engineered materials while also delivering on those requirements:1- Low-cost, higher than 40% conversion efficiency solar cells for electricity generation,2- Low-power integrated semiconductor lasers for communications systems.This program is innovative because increased efficiency, combined with lowered production costs, of photovoltaic systems is required for widespread deployment of both solar farms and localised off-grid energy generation. Also, to decrease the total amount of green house gas emissions of the information and communication technology (ICT) sector significantly lower power devices, in low cost packaging must be developed.Canada will benefit socially, culturally and economically. The proposed program will contribute directly to improving the environment and reducing dependence on non-renewable resources, through clean, green, alternative energy technologies. It will contribute to positioning Canada as a world-leader in high efficiency devices. Equally important, the research will develop highly qualified and skilled personnel that will promote effective knowledge transfer and help fill the employment needs of an industry growing at more than 30% per year.""437916,""Hinzer, Karin"
"425940"	"Hossain, Ekram"	"Dynamic spectrum access and management in next generation heterogeneous wireless access networks"	"Next generation wireless/mobile communications systems are being developed with the vision of heterogeneity in which a mobile user/device will be able to connect to multiple wireless networks such as cellular wireless, WiFi, and WiMAX networks simultaneously. Heterogeneous wireless systems will achieve efficient wireless resource utilization, seamless handoff, and global mobility to enable mobile users to connect to the Internet in a seamless manner. Dynamic spectrum sharing (DSA) is a new design paradigm to enhance the radio spectrum usage efficiency in next generation heterogeneous wireless access networks. Different from traditional static spectrum allocation, in DSA, radio spectrum can be dynamically coordinated and allocated to radio access networks (RANs) and mobile users through the use of cognitive radio (CR) technique. Protocol engineering and architecture design for broadband heterogeneous wireless access systems based on DSA is an emerging research area. The major challenge here is to manage radio spectrum and other wireless resources efficiently from the service providers' point of view for maximum capacity and improved return on investment while satisfying the requirements for the different wireless services and applications. The objective of the proposed research is to address the problem of modeling, analysis, design, and optimization of the major architecture and protocol components to address these research issues in a dynamic spectrum sharing-based heterogeneous wireless access network. The results of the proposed research will have significant scientific as well as technological impact.The importance of the proposed research stems from: (i) the growing demand for low-cost ubiquitous wireless broadband services, (ii) the need to produce the expertise and the skilled graduates to transfer the relevant technologies to the Canadian telecom/IT industry for our economic advantage, and (iii) the goal to enhance Canada's position as a world leader in wireless/mobile networking research.""427228,""Hossain, KhandakerMuhammedAnwar"
"425374"	"Hossain, Shahadat"	"Computing with sparse and structured matrices: mathematical derivatives and beyond"	"One of the central themes of my current research is the synergy of combinatorics, graph theory, and numerical linear algebra to explore new and innovative ways of formulating and solving important  scientific  problems. With the technological advances in high performance computing systems a major challenge confronted by researchers is concerned with the design of effective algorithmic tools and frameworks that exploit the problem structure and the computing system architecture to solve increasingly complex and extremely large scientific problems. Accurate climate modeling code such as ``Community Climate System Model'', for example, belongs to the so called ``petascale simulation'' applications. One of the necessary computational components of  such a simulation model  is concerned with  the efficient calculation of ``sensitivities'' of a large number of output parameters with respect to a small subset of model inputs. Exploiting  a priori known information such as sparsity and structure of the underlying problem is crucial for designing effective  algorithms for such applications. Additionally, the architectural complexity of modern high performance computing systems  pose considerable difficulty in effective software implementation of such innovative algorithms.   ""434650,""Hosseini, Pedram"
"425360"	"Houghten, Sheridan"	"Computational techniques for combinatorial searches relating to coding theory and bioinformatics"	"When transmitting data, errors can occur due to noise. In binary data, these errors cause the value in a single bit to be changed from 0 to 1 or vice-versa. When using an error-correcting code, we store the data according to a specific pattern before transmitting it. After transmission, this pattern allows us to determine in which bits errors have occurred, and then to correct them. Traditional error-correcting codes, capable of correcting errors such as these, are used in many areas including for satellite transmissions and to store data on compact discs.Data is not always binary. In fact, data can also be stored in DNA, in which the logical counterpart to a ""bit"" is a ""base"" with four possible values. Also, other types of errors may occur: in biological applications, it is useful to correct insertions and deletions of symbols as well as substitutions. The related area of DNA computing uses the DNA itself to perform computations; using error-correcting codes improves reliability.Error-correcting codes do not exist for all possible sets of parameters. Thus if we wish to use a code of a certain type we may have to perform a search to determine if it exists and its structure. This is called a combinatorial search. Such searches typically have a huge search space, and thus the use of computers is generally a necessity. Combinatorial searches may be applied to many different types of structures, including weighing matrices and designs. Another example of their use is protein side-chain packing, which involves finding the best arrangement of a given protein; from a Computer Science point of view, this is often represented as another well-known combinatorial structure, a graph.The primary objectives of my research are to provide answers to the above types of problems, and furthermore to determine appropriate algorithms for use in different types of searches.""435668,""Houghton, Adrian"
"427295"	"Hranilovic, Steve"	"Indoor optical wireless communication systems for broadband access"	"Canada is among the leading nations in terms of access to broadband services.  However, a 2008 OECD study indicated that many international competitors were eroding this initial lead in broadband penetration.  An important factor in broadband penetration is the availability of suitable access methods.  This research program develops new communications theory, algorithms and prototypes to improve the rates and reliability of indoor broadband optical wireless links.  These links transmit data by modulating and detecting an optical intensity and provide high-rate, cost effective, interference-free indoor communications.  In particular, this research considers indoor optical channels at both visible and infrared wavelengths.  Visible light communication (VLC) channels employ white illumination LEDs whose energy efficiency is at least ten times greater than incandescent bulbs.  An often overlooked feature of white LEDs is there significant modulation bandwidth.  Inexpensive phosphor-based white LEDs have bandwidths in excess of 20MHz while RGB LEDs can have bandwidths in excess of 100MHz.  While others have considered using white LEDs for illumination and communications, there has been little development of rigorous communication theory, algorithms and modulation for these links.  This work will develop spectrally efficient signalling for indoor VLC channels to improve the available data rates focusing on three main approaches (i) optical MIMO techniques, (ii) bandwidth efficient modulation and (iii) equalization methods.  A unique feature is the integration of dimming and colour selection into the modem design problem. In addition, VLC links must be integrated with power-line modems that will deliver data to allow wireless access in a room.  At infrared wavelengths, high-speed dynamic spot diffusing (DSD) infrared communication systems will be investigated.  Estimates indicate that data rates in excess of 1Gbps are possible in such links when imaging receivers are employed.  Capacity maximizing path adaptation and short-length rateless code design will be developed.  This research program will leverage existing hardware in our lab to develop prototypes to validate the developed algorithms.""438281,""Hrenchuk, Claire"
"428770"	"Hussain, Sajid"	"Energy efficient, secure, and trusted middleware for wireless sensor networks"	"Wireless sensor networks (WSNs) are used in many applications, such as security, military, agriculture, automation, and habitat monitoring purposes. Although sensors have been used for decades, due to the latest developments in the electronics industry, it is possible to add some processing, storage, and communication resources with the sensors. However, due to limited battery power supply, it is necessary to design energy efficient communication techniques, which could extend battery life from a couple of days to a few years. Further, as sensors are deployed in harsh and hostile environments, it is necessary to incorporate security, trust management, and fault tolerant techniques; otherwise, the sensor data will not be reliable.I will use a database approach to reduce the burden of the application developer. The user will specify the query in a simple English-like language. However, the proposed middleware will address the following concerns: a) energy efficient communication, b) security and trust management and c) fault tolerance. I will use intelligent techniques such as genetic algorithms, fuzzy logic and simulated annealing to obtain energy efficient communication. Also, I will use endocrine communication system of the human body as a model to design energy efficient heterogeneous WSN. For security, I will develop energy efficient key management techniques to address the following issues: confidentiality, integrity and authentication, as well as several attacks such as man-in-the-middle, collusion and replay. The proposed middleware will be validated and verified for several deployment environments using a WSN testbed (e.g., WINTeR). The proposed middleware will be useful for several applications such as: a) industry process control automation, e.g., CNA Plant (training facility in St. John's, NL) and Hibernia (oil refinery near St. John's, NL) and b) intelligent applications for smart environments (nursing homes and tele-health).""427379,""Hussein, Abdulkadir"
"421364"	"Ivanov, André"	"Design for test and reliability of integrated nanoscale devices and systems"	"Electronics in the form of complex, highly integrated circuits and systems have now penetrated every aspects of our day to day lives.  We have seen more than five decades of constant growth and increasing complexity and performance. We are now rapidly entering the so-called late- and post-silicon era. Nanoscale types of devices are rapidly emerging, and a vast array of integrated microsystems geared for all kinds of applications, from personalized internal drug delivery to the monitoring of the environment, are being developed and deployed. The quality and reliability of such systems is very important and, in many cases, may be life-critical. This research focuses on the problem of making sure that these extremely complex systems can be tested efficiently such that only the devices and systems that meet all the intended  design and fabrication specifications are actually shipped to customers. Overall, this is a very challenging problem, because of  the very large number of possible failure mechanisms and the overall complexity of the microsystems. The work will yield robust methods to ensure the testability of systems, and thereby, their quality as well.  In addition to test, this research addresses issues of reliability. It is not only important that the systems be tested for outgoing quality, but many systems also need to perform reliably for many years, while deployed in the field. How such emerging microsystems ought to be designed and fabricated to ensure a reliable performance is a focus of this research as well.  This research will benefit Canadians in two ways. Highly qualified and versatile engineers will be trained.  The research will also yield important new test methodologies useful for other researchers in this field of integrated nano-/microsystems well as useful for current and future practitioners focused on making high quality and reliable novel microsystems rapidly available to the public at large.  ""424978,""Ivanov, Mikhail(Misha)"
"426610"	"Jatskevich, Juri"	"Modelling and analysis of power electronic and energy conversion systems"	"Sustainable energy systems are currently identified as the key strategic area in Canada, whereas the electrical energy is gaining particular importance. Developing powerful computational tools is vital due to the ever-increasing complexity of power systems, industrial automation, automotive industry, home appliances, etc. Accurate and fast simulations enable the design and operation of future electric grids and may prevent potential blackouts and system's failures. Electrical machines and power electronics are typically the modelling bottle-neck. The proposed research will continue the applicant's work on the following directions: We are developing computationally advanced models of synchronous and induction machines for widely used Electromagnetic Transient Program (EMTP) simulators. The objectives are to have the models with direct interface to the external circuit while including the non-linear characteristics and enabling the most efficient solution of the network equations. Approximate models that achieve the required direct interface with the external network but do not slow the EMTP solution will be investigated. Higher fidelity models will be considered for more accurate representation of the machine dynamics and switching transients.The detailed simulations of power electronic converters are computationally intensive due to switching and require long simulation times. We are developing dynamic average-value models that do not have switching and are particularly useful for the large-signal system-level transient studies as well as for the small-signal analysis. We are working on a pioneering computer-aided methodology for constructing such models for several classes of converters and machine-converter systems. Combining these thrusts under one common theme, we are well-positioned to make very significant contributions and enable the next generation of EMTP- and state-variable-based tools and programs so needed by many engineers and researches working on electrical energy systems in various industries in Canada and worldwide.       ""420970,""Jaumard, Brigitte"
"427104"	"Jedwab, Jonathan"	"Aperiodic autocorrelation properties of sequences and arrays."	"The study of the aperiodic autocorrelation properties of binary sequences began in the 1950s and continues to be centrally important in diverse areas, including radar information processing, power control for multicarrier wireless transmission, optical time domain reflectometry, medical ultrasound, and other digital transmission systems. Sequences and arrays whose aperiodic autocorrelations are collectively small in relation to the sequence or array size correspond to signals whose transmission wastes very little energy and which can be recovered efficiently at the receiver. Traditionally, the analysis of autocorrelation properties has concentrated on the periodic case, where the presence of rich mathematical structure admits many theoretical techniques. But the aperiodic case is much more significant in practice because it arises naturally in so many application contexts. Although conventional wisdom has long been that aperiodic autocorrelations display little structure, and that often the only realistic means of analysis is computer search, there have been several recent indications to the contrary.The short-term objectives of the program are to study particular classical problems in this area for which recent results have opened up new research directions. The long-term objective is to identify emerging methods, and develop new ones, for studying aperiodic autocorrelations of large sequences and arrays. This is becoming increasingly important, as modern digital communications systems are less likely to be restricted to the use of smaller sequences and arrays. The general scientific approach will be to combine tools and insights from discrete mathematics, computing science, and digital communications engineering. Where appropriate, experimental computation will be used to reveal patterns, identify key examples, improve intuition, and so point the way to new theoretical results.""439710,""Jee, Seohyun"
"435080"	"Jing, Yindi"	"Cooperative network designs-network configuration, multiple-user communications, and practical channel issues"	"With the increasing demands for wireless networks with high capacity, efficiency, and reliability, combatting signal fading in wireless channels is one of the most fundamental problems in wireless communications. Network cooperative diversity, a recently emerged concept that has drawn considerable attention during the past decade, has been shown to be a promising solution. Its basic idea is to mitigate fading by allowing multiple nodes in the network to help each other's communication tasks. The proposed research is the design and analysis of reliable, self-configuring cooperative networks and practical, adaptive cooperative schemes. More specifically, the applicant focuses on the following aspects.          1.) Optimization of cooperative network configurations. This includes the optimizations of relay clustering, relay signal processing functions, and power control.         2.) Signal processing in multiple-user cooperative networks. This includes finding and analyzing transmitter beamforming schemes, receiver combining and detection schemes, as well as the relay processing that lead to high reliability.         3.) The impact of practical channel issues. This includes analyzing the effects of training costs and channel errors on existing cooperative schemes and proposing efficient channel training, estimation, and feedback schemes for cooperative networks.    The proposed research improves the reliability of practical cooperative networks. The outcomes will provide guidance in the design, analysis, and implementation of future wireless networks. The applicant expects new research directions and ideas to be opened up through the program. Aiming at practical, efficient, highly-reliable, and self-configuring, this program would bring us closer to the ultimate goal of wireless communications -- to communicate reliably with anyone anywhere at anytime.""427919,""Jirasek, Andrew"
"428927"	"Joslin, Christopher"	"Adaptation methods for scalable graphics and animation on constrained devices"	"The proposed program of research entails the adaptation of scalable multimedia content based on dynamically variable contextual information. The scalable media in question is a single ""high-quality"" (i.e. a file which contains at least the best version of itself; e.g. 1080p video) media file that is able to scale down spatially, temporally, and in terms of its signal-to-noise ratio. This scaling can be achieved through an incremental scheme and/or the storage of redundant information; however the ultimate goal is to obtain a highly compressed file containing a very high degree of scalability (although this may not be ultimately achievable). Contextual information is application dependent, but generally comprises a range of information including the capabilities and condition of the terminal device (such as a mobile phone), the network connection, and even the user's restrictions and capabilities. In order to provide a proper service to a user the context is generally considered as a dynamic input, changing according to the environment, network conditions, other applications being executed on the terminal device, etc. The adaptation process essentially connects scalable media and context together often by making a fuzzy decision (as opposed to a direct binary decision) based on the contextual information provided and correctly scaling the media accordingly (and hopefully dynamically relating to the contextual changes). The two key parts of this process are (a) the interpretation of context in order to produce an efficient and effective adaptation process, i.e. to provide the user with the best experience under the given circumstances; and (b) the scalability and size of the media being adapted, efficient storage combined with fine grain control that will enable the best quality for a given situation. We focus these efforts on the constraints imposed by mobile devices and propose new methods to provide improved use of a range of applications - from Tourist Guides to 3D Collaborative Environments.""426297,""Josselyn, Sheena"
"422247"	"Kadoch, Michel"	"Mobile IPTV service over wireless mesh networks"	"In wireless mesh networks (WMNs), mobile IPTV service demands the support of multicast communication with quality-of-service (QoS) assurance. To achieve QoS guaranteed multicast, we consider to employ the following mechanisms: MAC Layer Enhancement, Prioritized Guaranteed Rate (GR) Packet Scheduling, Hybrid Error Control, and Traffic Engineering Enabled QoS Multicast Routing. MAC Layer Enhancement and Prioritized GR Packet Scheduling aim to reduce the delay and suppress the jitter for mobile IPTV service. Specifically, we will first improve the standardized IEEE 802.11 MAC layer, so that IPTV applications can have more media assess opportunity. We will then design a prioritized scheduling algorithm, which can give mobile IPTV top preference during the packet switching process in multi-service environment. Besides delay and jitter, bit error rate is another important QoS concern of mobile IPTV users. Accordingly, we will develop a new hybrid error control scheme that integrates interleaving, forward error correction (FEC), and automatic repeat request (ARQ) to mitigate the error and loss effects encountered in WMNs. All three mechanisms mentioned above will be provided to mobile IPTV applications through the QoS Multicast Routing process.  To address the efficiency of resource usage in WMN, we will develop a network graph preprocess approach, which enhances the multicast routing protocols with traffic engineering function. In addition to above considerations, it is also crucial to support mobile users roaming around the WMN without service interruption. This motivates us to develop an efficient fast handoff approach using distributed computing technology. Particularly, we will work on a mobile agent (MA) based handoff approach, which can significantly reduce handoff delay. In our mobile agent based handoff approach, it is very important to deploy call admission control (CAC) mechanism on the mesh router because it is an essential step for the provision of QoS guaranteed service and it will give handoff calls higher priority than new calls.""428364,""Kaern, Mads"
"419217"	"Keil, Mark"	"Algorithmic approaches to hard geometric and graph theoretic problems"	"Constructing and manipulating a computational model is central in many computer science applications. Unfortunately, often the problems that naturally arise turn out to be hard, in the sense that the best known algorithms cannot find exact solutions in a reasonable amount of time. Computational models involving graphs are pervasive in applications, and where physical objects are modeled, geometric models are important. The objective of this research program is to develop effective algorithmic approaches to some natural hard geometric and graph theoretic problems. The approaches will include the development of algorithms that yield approximate solutions as well as algorithms that yield exact solutions but only to restricted instances of the problems.The overall approach is to focus on the interplay between the areas of algorithmic graph theory and computational geometry. We will seek to exploit the natural relationships that exist among the problems, concepts and techniques of the two areas.In this research we focus on the following areas: polygon decomposition, triangulation, proximity problems and intersection graphs. In each of these areas there are many hard problems that arise in applications and in each there exists interplay between computational geometry and graph theory. In polygon decomposition, the goal is to represent a polygon as the union of a small number of simple components which are easier to subsequently process. Likewise a triangulation is used to represent a complex area as the union of triangles.This can be done in many different ways and sometimes the best way may be expensive to compute. Proximity problems involve computations that depend upon the distances between objects.   Intersection graphs can arise from modeling a geometric problem with graphs.""423020,""Keillor, Jeffrey"
"425268"	"Kim, Henry"	"Using ontologies and social network analysis for web 2.0 enabled knowledge sharing"	"I am interested in developing theories and applications for enabling new and powerful ways to share knowledge using information systems. The basis of my research is that knowledge sharing requires commonality in content AND context. Commonality in content refers to shared understanding of meanings of vocabulary, rules, and conventions. Commonality in context refers to common characteristics and environment of the sharers. Those that share similar contexts are much more likely to have shared understanding and hence apt to use a knowledge sharing system more effectively than those whose contexts are different. The underlying thesis of my research is that a knowledge sharing system constructed by combining models of content (ontologies) and context (from social network analysis) is more effective than a system built on models of content or context alone. With Web-based computing evolving to a paradigm popularly termed Web 2.0, I plan to explore the research opportunity that lies in applying Web 2.0 concepts and tools to my research thesis. Specifically, I plan to use corporate Wikis, blogs, social bookmarks, and social networking as sources to explicate and refine the content of ontologies, and analyze link structures to model the context of ontologies. Semantic Web technologies will be used to implement these ontologies.""426390,""Kim, Henry"
"431887"	"Kinshuk, K"	"Adaptive approaches for mobile learning technologies"	"There has been extremely rapid growth of wireless technology in recent years, along with increasing availability of high bandwidth network infrastructures (especially the SuperNet in Alberta). Advances in mobile technologies and the popularity of handheld devices have opened up new accessibility opportunities for citizens. The true potential of e-learning as ""anytime, anywhere"" has finally begun to be realized, particularly for those in remote communities.     The primary goal of this research program is to advance research on the innovative paradigms, architectures and implementations of wireless applications and systems for individualised and adaptive learning. With the use of personalized learning on mobile devices, the long terms goal is to facilitate education, at least as high a quality as available in a classroom environment, and even surpass that by incorporating situation-specific content and problem scenarios from learners' surrounding environments.     Within this long-term research program, current proposal aims to extend our understanding of mobile learning to provide rich learning experiences in higher education by exploiting the benefits of location, device and student modelling, and combining them with mobile technology to achieve personalized and situation-specific learning through the analysis of students' surroundings, situation-specific problem-solving by students through multiple forms of input (such as text typed by keypad or in digital ink, images taken by mobile devices' camera, audio recorded on mobile device and so on), for rich learning experiences.""441364,""Kinsman, Laura"
"419752"	"Kirkpatrick, David"	"Computational complexity of geometric and combinatorial problems"	"The objective of the proposed research is to further our understanding of the inherent computational complexity of a number of fundamental combinatorial and geometric problems. Our main goal is to provide answers to two complementary questions: (i) ""In what ways and to what extent do certain structural features (attributes of specific problem instances) contribute to the difficulty of solving problems in a particular family?"" and (ii) ""In what ways and to what extent can certain naturally occurring features or constraints be exploited to provide more efficient solutions for problem instances that exhibit these features?""The problems that we propose to address (like those addressed in my previous research) are distinguished by the fundamental role that they play in a variety of applications, including robotic motion planning and optimization, facility and sensor location, efficient schemes for broadcasting streamed information and optimization of log milling.  This, of course, provides significant practical motivation for progress on the second of the questions above. However, the problems are also chosen for their ability to serve as representatives of broader families of similarly structured problems. For this reason they motivate, from a more theoretical standpoint, progress on the first question.  Ultimately, our success should be measured in terms of (i) the development of new general techniques for the design and analysis of efficient algorithms and data structures, and (ii) the elucidation of inherent complexity limitations that apply in the broadest possible computational context.""434853,""Kirkwood, Andrea"
"425232"	"Knight, Andrew"	"Efficient energy conversion"	"The proposed research program will investigate methods to improve the efficiency of electrical energy conversion technologies and investigate novel approaches and applications of electrical energy conversion technology as a replacement for other, less efficient techniques. The focus of the research program in the short term will be to consider electro-mechanical energy conversion such as motors, generators and actuators. In the longer term, it is envisaged that the research scope will broaden to incorporate comparisons with other energy conversion technologies, especially as they relate to electrical energy storage and clean energy technology. Significant quantities of greenhouse gas emissions in Canada are related to losses in electrical energy conversion systems and inefficient non-electrical systems that could be replaced by new electrical technologies. Addressing these areas to improve energy efficiency at little or no incremental cost to the end user achieves two important goals: reduced total energy use and consequent greenhouse gas emission; improved competitiveness in the Canadian economy due to reductions in operating costs.This research program will address fundamental issues related to the mechanisms that causes power losses in electrical equipment. New models for losses will be developed and tested; their publication and use in industry will enable the minimisation of losses in future generations of equipment. In addition to broad-based fundamental research, specific application areas will be addressed. Two possible areas to be considered include the efficient use of electric drives for electric and hybrid vehicles and new techniques to harvest energy from small energy sources to enhance battery life. Through a combination of computer aided design and hands-on construction of prototypes,the research program will involve significant training of highly qualified engineers to carry out the next generation of research in industry.""441834,""Knight, Chelsea"
"427298"	"Knights, Andrew"	"Silicon-based materials and processing for optoelectronic applications"	"This research proposal is aimed towards the fabrication of novel, silicon-based devices incorporating high levels of optical and electrical functionality integrated on a single silicon chip. The choice of silicon as the base material for integrated optoelectronic circuits exploits the established global infrastructure for electronic device manufacture. The successful integration of both optical and electronic functionality will have significant ramifications for a number of industries and applications such as microelectronics, telecommunications, clean energy technologies and bio- and chemical sensing. The current work proposed here will focus specifically on the development of functionality such as: light emission from nano-structured silicon; fast optical-to-electrical-conversion for waveguide integrated photodetectors; and efficient broadband modulation without recourse to Mach-Zehnder geometries.  This functionality will be achieved using lattice scale modification via defect and dopant engineering of crystalline silicon and silicon-based dielectrics. The primary tool for this manipulation at the atomic scale is ion implantation in combination with thermal annealing, however an array of fabrication and characterization techniques will be employed throughout the proposed plan of work, all of which are available to the applicant at the home institution or via collaborators both within and external to Canada. The proposal builds upon knowledge previously obtained in the applicant's research group, while simultaneously setting challenging goals requiring methodology which will be world leading in its novelty. The training of students is a significant motivation. Knowledge of photonic and semiconductor design, fabrication and characterization offers new, highly qualified personnel a vast array of career opportunities. Students taking part in this research will be ready to take their place in Canada's hi-tech industries.""432984,""Knisely(Frey), William"
"428858"	"Kobti, Ziad"	"Evolutionary learning in complex social system"	"Complex systems and agent-based modeling are attracting a growing interest from experts from various disciplines to simulate and study real world applications. The implications are of significance because they broaden our understanding of little known complex social phenomena, including reasoning about the causes ofobserved system behaviour and predicting the outcomes of tested case scenarios or interventions. Exciting systems that this field encompasses include: terrorist networks, prehistoric civilizations, and vehicle safety. An agent for instance can represent an individual, a household, or a driver. System designers often build a statebased configuration for agents to react in a dynamic environment. Hence, the model provides a sandbox approach for the researchers to test various hypotheses. For example, an ancient civilization can be modeled using real field data where an  archaeologist can then examine the effects on the emerging population size andmovement under drought conditions compared to normal conditions in order to test a hypothesis suggesting that a drought caused the population to leave the study area. The research program proposes applying new techniques in agent learning when developing agent behaviour in a complex system. Beyond the basic finite state machine that an agent can use to react to its dynamic environment, a more realistic approach for agent intelligence proposes reactive agents that can accumulate knowledge at the individual and population levels, learn new plans and evolve new strategies on their own to fulfill their goals. Human behaviour however, is distinguished from other animal behaviours and consequently the conventional evolutionary frameworks need to be extended in order to build more realistic artificial agents. Through the proposed methods, the outcome of this research program will enhance the learning theory in artificial models and researchers working withagent-based simulations would be able to create more accurate and intelligent models using the algorithms from this work.""422906,""Kocabiyik, Serpil"
"420297"	"Kwasniewski, Tadeusz"	"Architechtural and circuit topology power consumption optimization for gigabit chip-to-chip interconnect"	"Our research is on power aware, energy aware and performace aware circuits.Over the last several years the CMOS technology facilitates very large - one billion devices designs while the CMOS nano-scale technologies introduce a static power dissipation even in classical digital circuits. There have been two main trends in dealing with the problem of rapidly increasing power dissipation. The first commonly used techniques concentrates on direct power reduction though reduction of the static - leakage current. In its simplest implementation a pass transistor in placed in series with the supply voltage and used to open a circuit during the no-activity periods. This proposal deals with the more recent techniques where the circuit itself or the signal processing block architecture is changed to allow for adjustment of critical parameters - and power dissipation on per need basis. These techniques were recently been termed ""power aware"" or ""energy aware"" circuits. Such ""awareness"" is obtained through ""digitally assisted"" mixed-signal or analog design. It is believed that such techniques would be widely adopted over the next decade uniquely addressing the nano-scale CMOS technology leakage and process parameter variation. Compared to currently uses ""worst corners"" design methodology significant power savings would be achieved.Extending our previously published work we would continue work in areas of wireline communications, frequency synthesis, oscillator design and reconfigurable digital designs.""441201,""Kwok, Bonnie"
"435099"	"Kyan, Matthew"	"Associative mining for intelligent organisation and analysis of multimedia information"	"The overwhelming amounts of multimedia information generated in today's media-centric society present a host of challenges to end-users, businesses and researchers alike: quite simply, technologies for efficient organization, navigation and analysis cannot keep pace. Part of the problem is that consumption of media (browsing/analysis) would be far better served by building a pre-conceived notion of what constitutes an ""interesting"" event, and forcing the media collection to be organized around that, thereby promoting fast and relevant access. The problem of restructuring large media collections for non-linear access is not new, having provided much impetus for both content- and concept-based retrieval: the two core research fronts underpinning current state-of-the-art in video search. Such successes, however, are highly dependent on annotation, and it is becoming increasingly evident that collections are fast outgrowing our abilities to reliably annotate.  Methodologies are desperately needed to automatically organize multimedia contents without the benefit of prior knowledge. This can be cast as a problem in unsupervised pattern or event ""discovery"", wherein an intrinsic representation of a media stream is sought as a basis for its reorganization.  In light of recent advances in visual attention modeling, which considers the natural queues and responses that direct human visual/aural attention, we ask: is it possible that there exist certain intrinsic patterns or irregularities in multimedia data, that can more appropriately reflect natural mappings to semantic events?  Specifically, the proposed research program will address event discovery by investigating the possible synergies between natural, self-organizing approaches to unsupervised learning and the visual attentive mechanisms that appear to significantly guide human interest. The generic scope of this research will result in the development of tools to serve a range of pressing deficiencies in the summarization, navigation and consumption of broadcast news, sports, TV and film production, meetings, personal lifelogs, eChronicles, media-centric management systems and PVR's,  security and unmanned surveillance, biomedical image informatics and biometrics.""428046,""Kyberd, Peter"
"421422"	"Lakshmanan, Laks"	"Query processing by, for, and of the masses: putting the user in the loop"	"Today's web search is confined to users submitting a keyword search query to a generic search engine and sifting through a large linearly ordered list of answers. While this is satisfactory for many purposes, it ignores the significant trends, spurred by the advent of Web 2.0, toward increased integration of content information that users search for and their social profiles (activities and connections), leading to social content sites. First, sites like del.icio.us and flickr started as social content sites: users can form social ties and tag content items and share them with their friends. Second, more and more, content-only sites like Amazon are now allowing users to tag/rate items and share them with their friends. Similarly, social sites like facebook are allowing users to share content with their friends. Third, there is a definite trend toward virtual social content sites: e.g., most news sites let users share news articles with their friends in, say MySpace. In all these social content sites -- virtual and real -- there is rich structured information on both content items and users as well on user-provided content. Treating social content sites the same way as any other site misses out on leveraging the significant content and social structure present in these sites. We envision a future where domain-specific social content sites will increasingly offer an attractive alternative for users compared to generic search engines. In this program, we will investigate the combination of techniques drawn from information retrieval style search, database style querying, recommender systems, social networks, graph modeling and querying, data analysis and data mining over rich social and content structure, and multi-dimensional querying and OLAP, to significantly enrich and advance user experience in interacting with collections of social content sites. Users will be able to seamlessly shift between heterogeneous groups of answers: these groups may consist of content items at various levels of granularity, or ""similar"" items which ""recommended"" those items, or ""similar"" or expert users who recommended the items and their tags/ratings, or other ""topics"" that may interest the user, leading to a powerful paradigm for information discovery. ""442493,""Lal, Nathaniel"
"428424"	"Lamontagne, Luc"	"Case-based reasoning for making decisions in dynamic game environments"	"In this research project, I will investigate how Case-Based Reasoning (CBR) techniques can contribute to the development of decision modules for dynamic game environments. My objectives are a) to determine the potential benefits of using CBR techniques for capturing and reusing game episodes, b) to study how CBR techniques can contribute to the development of interesting and efficient game AI decision schemes, and c) to address technical limitations of current CBR techniques to make them more suitable for dynamic and uncertain environments like real-time games.       Current technologies for implementing nonplayer characters (NPC) in games mostly rely on finite state machines (FSM), a static structure that can be quite extensive and difficult to edit for complex games. We believe that CBR, exploited as a dynamic decision scheme, can significantly contribute to game development by facilitating the design of elaborate behaviours. CBR consists of reusing past experiences to solve new problems. For this project, we foresee CBR as a mean to efficiently store and encode game episodes that can be reused for player modeling or behaviour cloning. However, to be exploited by a CBR system, episodes have to be properly encoded and stored in a case base. Due to the sequential nature of games, some issues have to be tackled for the structuring of the episodes. Technical issues to be addressed in this project are :   - how to conduct sequences of decisions using CBR;   - how to adapt the reasoning cycle to different decision timelines; and    - how to take into account implicit interactions with human players.      This proposal builds on a research effort we are conducting with some industrial partners and is an initiative to investigate longer term issues to be addressed in support of this collaboration.""419634,""Lamontagne, Lucie"
"427635"	"Lee, WonSook"	"Interactive fluid simulation and visualization"	"The main objective of in fluid animation in Computer Graphics is to reproduce plausible fluid behaviour. Simulating fluids is a computationally expensive task, and interactive computer applications largely simplify fluid behaviour at the cost of accuracy or limited flexibility; however newer computer applications require both high levels of realism and efficient computations. This motivates the research methods that achieve efficient fluid simulation with high accuracy in real-time. One research direction is the use of statistical analysis techniques for synthesizing fluid motion by forming parameterized database of fluid examples. The input data and parameterized fluid behaviour would then allow efficient generation of realistic fluid flows. The ability to reproduce fluid behaviour efficiently, based on heavy pre-processing step, will lead to the advancement of the field and various surrounding fields. We aim to contribute to the development of these other areas that make use of the simulation of fluids, focusing on two applications; art in virtual reality and medical application among many potential applications such as aeronautical engineering, mechanical engineering, education, and entertainment industry.""435371,""Lee, WoonYin(Paul)"
"427386"	"Lemieux, Guy"	"Low-cost, high-performance with FPGAs, structured ASICs, and processor arrays"	"Field-programmable gate arrays (FPGA) are a vital technology invented 25 years ago to build digital hardware. The key strengths of the technology are instant manufacturing, no up-front manufacturing costs, and the ability to to fix hardware bugs or add features at no cost. These strengths are obtained by prefabricating a generic logic circuit that is configured by the customer using computer-aided design tools. Despite their advantages, FPGAs are estimated to be 40x less dense, 4x slower, and use 12x more power than ASICs.Improvements and alternatives to FPGAs are needed to overcome these disadvantages. FPGA interconnect dominates the area, power, and delay of the device. FPGA tools cannot easily map parallel software into FPGAs. Structured ASICs are a promising alternative to FPGAs; they vastly improve density, power, and delay by premanufacturing most (but not all) of the semiconductor masks. Single-chip massively parallel processor arrays are a promising alternative to run parallel software, but they can also used to implement high-performance and high-density computaitonal circuits with much faster mapping runtimes and higher capacity limits. Together, these three devices provide very low-cost accessibility to the latest semiconductor technology for implementing digital hardware and highly parallel systems.The goals of this research are to realize advances in three areas: improved FPGA processing speed through interconnect optimization and vector processing, improved structured ASICs with easier design tools, and demonstrate the ability to efficiently map circuits to massively parallel processor arrays. Combining hardware performance with software cost and ease-of-use leads to dramatic improvements in cost/performance ratio. Achieving this convergence is a key enabler for new applications that will benefit society in important areas such as health care, communications, and scientific computing.""432283,""Lemieux, Hélène"
"419477"	"LeonGarcia, Alberto"	"Design of converged communications and computing infrastructure"	"Internet-based applications have had a profound impact on how we communicate (instant messaging, Skype), interact with our peers (Facebook, LinkedIn), find information (Google), make purchasing decisions (eBay, Amazon), are entertained (YouTube, iTunes), and travel (GPS, location).  All of these applications are supported by software operating on a largely invisible ""cloud"" of communications and computing equipment.  The innovation and creativity that led to these new applications will continue unabated and will place increasing demands on the future computing cloud.  Moreover, the continued improvement in the cost and capabilities of computing and communications provides an opportunity to marshal the next generation of cloud computing to support applications that address societal-scale problems.  Two important examples of such large scale applications are:  Real-time control of regional vehicular traffic and energy consumption to minimize carbon emissions;  Monitoring and control of health system resources to provide responsive and efficient services to a community.          This research project supports a core set of activities in a research program that addresses the following questions.  What architectures can enable the rapid creation of large numbers of diverse new applications, both small as well as global-scale, over the future computing cloud? What scalable infrastructure architecture for cloud computing should guide the organization of the next generation resources:  dense multi-core processors, memory, data storage and optical communications? How can the resources in the computing cloud be controlled to ensure that these applications meet specified ""Qualities of Experience?""  What mechanisms can flexibly redeploy resources to enable automatic deployment of applications?  How can these mechanisms be directed to optimize carbon emission and non-renewable resource usage?       This research benefits Canada by stimulating the creation of commercial products and services, and supporting the competitiveness of the Canadian economy through the application of ICT.""423712,""LeonGarcia, Alberto"
"428727"	"Lhotàk, Ondréj"	"Practical static analysis of object-oriented programs"	"Program analysis is a technique for proving properties about the possible behaviour of programs when they are executed.  It is used by software development tools such as compilers, verification and testing tools, and program visualization tools.The first goal of the proposed research is to devise program analyses that can be used in practical programming tools. Many program analyses have been determined to be efficient and precise in a theoretical setting under unrealistic assumptions. The proposed research will evaluate techniques that would enable these assumptions to be lifted, to make these program analyses applicable to mainstream programming languages and usable in real programming tools.The second goal of the proposed research is to develop new program analyses for reasoning about the behaviour of individual objects and interactions between objects. Object-oriented programs are structured as collections of objects that work together to achieve the goal of the program. Understanding the functioning of the program requires understanding the communication between individual objects, and many programming errors can be prevented if the possibility of unintended object interactions can be detected before the program executes. The proposed program analyses will guide the programmer to points in the program that may cause objects to interact in undesirable ways.""436777,""Li, Alan"
"428774"	"Li, Zongpeng"	"Internet multicast routing: algorithms, mechanisms and systems"	"The past decade has witnessed the proliferation of Internet applications. The most significant category of them, when measured in traffic volume, is data dissemination, accounting for over 70% of today's Internet data flows. These include applications such as the distribution of software patches, peer-to-peer transmission of movie files, the streaming of stored or live media, video conferencing and online group gaming. They share the common property that digital information, often in large volume, is requested by and delivered across the Internet to multiple users. Such applications are modelled as multicast in the networking community. Substantial research effort has been devoted to understanding the underlying structure of information multicast, creating mathematical models for it, and designing effective algorithms that optimize its performance. To date, most multicast algorithms assume a cooperative network environment with honest, cooperative and altruistic agents. However, such assumptions are not always safe in practice;  and when they are not, the network performance may deteriorate dramatically. We study how new multicast algorithms can be combined with economic measures to enforce desired behaviours on selfish agents in the Internet, by jointly investigating information coding, data flow routing, and selfish agent control. Latest developments on network coding will be incorporated for an amenable algorithmic structure of the multicast routing problem. Optimal routing algorithm models will then be designed, both to reduce the total cost shared by selfish users, and to provide a concrete ground in which cost sharing schemes may be developed. Incentive engineering mechanisms for noncooperative game design will be studied, to ensure the solution is stable even if agents in the network value their own utilities more than the overall well-being of the Internet. This research program represents one of the earliest attempts in optimizing data dissemination with selfish behaviours in different components of the Internet explicitly addressed, with a goal of contributing towards the ultimate reliability, robustness and efficiency of the entire Internet. ""429534,""Lian, Keryn"
"422864"	"Liao, Simon"	"Image analysis with moment descriptors"	"The general objective of this proposed research program is to discover the theorems, algorithms, and solutions for a number of issues related to the moment descriptors in the field of image analysis. Since the moments of an image can uniquely provide global characteristics and geometrical features of the image with some invariant properties, the moment methods have been utilized in many applications in image analysis, pattern recognition, object classification, and many other scientific fields in recent years. However, the theoretical supports on some fundamental issues have not attracted sufficient attention, and the lack of such supports has limited the role that moment descriptors are able to play. These issues include:(1)    )The need to discover new types of image moments to enrich the moment methods;(2)    )The prospect of applying moment methods to images stained by various types of noise;(3)    )The relationship between global moments and local moments of the same image. This proposed research intends to undertake the abovementioned three outstanding issues. To resolve these subjects, the new theorems, algorithms, and techniques are needed to be developed. The answers to these inquiries will undoubtedly have strong impacts on the moment-based applications in image analysis and other scientific fields.""439787,""Liao, SiYun"
"435070"	"LiboironLadouceur, Odile"	"Low-power photonic interconnects for data centres"	"The growth of video sharing over the Internet is forecast to represent 50 percent of all traffic by 2012 (Cisco Systems Inc.). High performance data centres such as high-speed routers and storage networks are expected to support this increase in bandwidth as the speed of processors increases. However, the full computational potential of processors is becoming difficult to achieve in data centres. Electrical data buses between interconnected processors and shared memory are bandwidth limited and exhibit high losses as the data rate increases. Consequently, the power dissipation of off-chip data communication is significant. In fact, today's data communication is responsible for 0.5% of carbon dioxide emissions globally (2007 Gartner Group estimation). In the future, this power dissipation will be limiting the performance scalability of data centres. To address this limitation, the investigator is proposing a research program in the area of low-power photonic interconnects for data centres.Photonic interconnects offers a high capacity platform with key features over copper, most notably scalable bandwidth by exploiting the parallelism and capacity of wavelength division multiplexing (WDM). However, significant power dissipation still resides in the signal conversion between the electrical and optical domains. In current approaches, the power dissipation is coupled to bandwidth utilisation and increases with the number of optical wavelengths. The investigator's research program is on the development of novel approaches built in Photonic Integrated Circuit (PIC) devices to decouple the power dissipation from bandwidth and enable better performance in data centres. The research program emphasis is on the validation of theoretical work by integrating PIC devices within a demonstrator system to perform complete data transfer in a computer cluster architecture replicating a data centre. The research program will lead to lower power dissipation in photonic interconnects in high performance data centres.""422054,""LiChan, Eunice"
"429783"	"Lilien, Ryan"	"Novel Algorithms to expedite experimental protein structure determination"	"Despite the importance of macromolecular structure in unraveling the biochemical basis of life, experimental structure determination remains a long, difficult, and expensive endeavour. The interest in protein structures is motivated by the general mantra that protein function is dictated by protein structure. This suggests that knowledge of protein structure is helpful not only in analysis of wildtype (or natural) proteins but also for mutant proteins such as those implicated in disease processes. Consequently, knowledge of protein structure has emerged as a critical resource for drug discovery and development. In the future, macromolecular structure determination will be an automated turn-key operation, and should be fast, inexpensive, and high-yield. My proposed research focuses on developing accurate and efficient algorithms to expedite the process of experimental protein structure determination. The proposed research program integrates knowledge from structural biology with algorithms in machine learning, machine vision, and computational search. The long-term goals of my interdisciplinary research program are to develop and apply techniques from Computer Science and Engineering to problems in Structural Biology (the study of protein structure and function). The research highlighted in this proposal represents part of a family of projects in my lab including algorithm development for experimental structure determination, protein design, structure-based drug design, and chemical synthesis planning. In this proposal, we describe several novel computational approaches for automating current experimental bottlenecks in X-Ray Crystallography and Electron CryoMicroscopy. As useful algorithmic milestones are achieved, we will package our software and make it freely available to the research community.""439846,""Lillico, Heather"
"435087"	"Lin, Xiaodong"	"Security and cooperation in vehicular delay tolerant network and its applications"	"Road safety and efficiency are major concerns worldwide. Therefore, it is crucial to explore new techniques to improve road safety and efficiency. The proposed research program aims to investigate the applicability of emerging vehicular communication networks, also known as vehicular ad-hoc networks (VANETs), as a promising approach to increasing road safety and efficiency. In VANETs, vehicles are equipped with wireless communication, positioning and computation devices with a variety of vehicle applications enabled by communication between vehicles such as emergency braking warning. Unfortunately, VANETs have faced various security threats and privacy concerns which would jeopardize public safety and become the main barrier to the acceptance of such a new technology. As a result, security is crucial to success of VANETs, and so is thereby a prerequisite for market-ready VANETs. In the past, insufficient attention has been paid to this area of research, in particular security in vehicular delay tolerant networks (DTNs), where end-to-end network connectivity between vehicles is not available. Traditional approaches may not be suitable in such a vehicular DTN due to its unique characteristics, such as intermittent connectivity due to high-speed mobility of the network node (or vehicle), resource scarcity, and the extremely large amount of network nodes. The proposed research is envisioned to solve the security and privacy issues in vehicular DTNs, which have not been taken into consideration in previously reported studies. It is expected that this research program will contribute to the discovering of new ideas and the development of innovative automotive products and services. As a result, new business models and revenue streams can be devised to foster growth and promote the troubled automobile industry in Canada. The research will also lead to HQP training in information security and vehicular communication techniques, which will greatly contribute to the Canadian communication, automobile and information security industries.""422601,""Lin, Xiaodong(Sheldon)"
"426930"	"Liscano, Ramiro"	"Autonomic computing in heterogeneous sensor networks"	"The objective of this research is to investigate new technologies that are targeted to support autonomic computing in multi-tier heterogeneous sensor networks. Autonomic computing systems are distributed computer systems that can manage themselves and therefore their complexities are invisible to end users. These systems rely heavily on feedback within the network to adapt their behaviour. Unlike sensor networks that simply collect and distribute sensory data to end users, an autonomic sensor network can aggregate and interpret sensory data and make decisions in the network that can affect the collection and routing of data within the network. A multi-tier sensor network is a sensor network that utilizes several different protocols; typically one layer will leverage Internet-based protocols (a SensorWeb) while the other layers use protocolscustomized for the particular network.With this high-level research objective in mind, several key research directions will be investigated. These are: the use of policy languages as a mechanism to facilitate customization and programming within sensor networks; the use of semantic knowledge to enable seamless inter operation between distributed sensor domains; support of sensory plug and play capabilities; and access control models for highly distributed sensor systems. A key challenge in all of these research directions is the interoperability between the sensor network layers, in particular the use of Internet-based protocols within resource constraint wireless sensor networks.""431966,""Liscombe, Michael"
"427313"	"Liu, Jiangchuan"	"Toward adaptive, scalable and socialized multimedia content distribution over ubiquitous networks"	"With rapid and widespread deployment of broadband and wireless communication infrastructures, audio/visual multimedia services are becoming increasingly popular among network users. Yet, given the limited network and server resources, and the best-effort nature of the IP network, the user experience with networked media services is still far from satisfactory, particularly during peak hours. Furthermore, the Internet has evolved into an ultra-large and heterogeneous network. Even the dominant client/server communication paradigm has shifted to a mix with peer-to-peer communications. We thus ask the following critical question: how can we distribute quality media content to a large population of highly heterogeneous clients seamlessly through diverse network infrastructures? To achieve the ultimate goals of ""anytime-anywhere"" media distribution, technology advances in various aspects need to be re-examined and jointly utilized under a coherent framework. In this proposed long-term research, we will address the above challenges, particularly on media over novel delivering architectures (e.g., peer-to-peer) and network infrastructures (e.g., wireless mesh networks). We will focus not only on the opti- mization of individual components, ranging from server optimization, flow and error control, to peer-to-peer overlay construction and handoff management, but also on their interactions that pose additional challenges to designers. A thorough understanding of the modules in this complex system as well as their interactions will facilitate the development of such popular applications as IPTV, movie-on-demand, video conferencing, and online games - all are essentials in the emerging digital entertainment/business world. Beyond the traditional media streaming, we will also examine the new generation of user-generated video sharing services, as represented by YouTube. We will conduct an in-depth study to understand and utilize their unique social networks for videos and users. This includes exploring the correlations among the videos/users to improve cache hit-ratio, designing smart pre-fetch strategies, and implementing novel inter-video overlays. ""432133,""Liu, Jie"
"424215"	"Liu, YanFei"	"High efficiency high power density voltage regulator module for next generation CPU"	"Computers, data centers, and servers have become the backbone of our society. The heart of a computer is the Central Processing Unit (CPU). Power conversion in computer, data center, and server typically occurs in two stages: 120VAC to a 12VDC AC-DC conversion followed by a Voltage Regulator Module (VRM) which regulate the CPU voltages at 1.2V (varying from 0.8V to 1.6V) from 12VDC. The next generation CPU will require the VRM to achieve high efficiency, fast dynamic response and lower cost.The proposed research program will develop new technologies to increase the VRM power conversion efficiency. Research work will be done in three distinct, but closely related areas, new MOSFET driver technologies to reduce switching loss, new control strategies to improve dynamic response, and new non-isolated DC-DC converter topologies for improved efficiency. It is expected that with the new technology, the VRM conversion efficiency can be improved by 5% (from 85% to 90%), which can reduce the energy consumption by $3 billion worldwide based on an analysis by Dell Computers. This will reduce green house gas emission. In addition, with the new technology, the cost of VRM can be reduced by around 20% and the VRM board area can be reduced by around 30%. The new computers will become more energy efficient, reliable and economic.It is expected that during the research program, six graduate students will be trained, 20 journal papers will be published, and several patents will be generated. The technologies developed in this research program will be transferred to industry improving Canadian industry's competitiveness and creating more jobs in Canada.""443386,""Liu, Yang"
"425359"	"MacGregor, Michael"	"Algorithms and architectures for data networking"	"This research program consists of two main thrusts within an overall context of data networking. The first is the invention and exploration of new algorithms to be used by the routers, firewalls, etc. internal to the network. The second is the development of models and controls for these devices based on knowledge of their system-level organization.AlgorithmsMost of the tasks performed by routers, switches and other network devices can be gathered under the title of ""classification"". For example, the main purpose of a router is to direct traffic towards its destination. The algorithms component of this program consists of the exploration of the effects and implications of approximation and imprecision in classification. Approximation and imprecision are desirable degrees of freedom in the design of many types of algorithms. In my view their application to classification has so far only been flirted with.ArchitecturesOne of the fundamental difficulties in conducting research in data networking is that both the hardware platforms (routers, switches, etc.) and their operating systems (Cisco IOS, Juniper JunOS, etc.) are closed, proprietary artifacts. Commodity, off-the-shelf (COTS) computers and switches can be combined with public domain software to create open systems for routing and switching. However the performance of the resulting systems is difficult to anticipate and to control. The architectures component of this program will consist of the modeling of COTS network devices based on black-box models of the individual components. These models will be used to derive effective control techniques for these devices, and tested in a prototype system.""421080,""Machel, Hans"
"420300"	"Maciejko, Romain"	"Improvements and applications of optical coherence tomography for biomedical imaging"	"Optical Coherence Tomography (OCT) and especially Doppler OCT (DOCT) are new technologies that raise much interest and just begin to reveal their full potential. OCT provides depth-resolved three-dimensional and (quasi-)real-time  imaging of living tissue and other semi-transparent samples in a non-invasive way using low-coherence interferometry. DOCT provides high resolution and high dynamic range measurements of fluid velocities in a variety of geometries relevant to microfluidics and vascular conditions such as stenoses and branching arteries. The proposed research program will capitalize on previous work done in this area at the Biophotonics Laboratory at Ecole Polytechnique. It will introduce several improvements in the existing system in terms of higher resolution (approaching 1 µm) and wider dynamic range (circa 80 dB), a faster response (a few images per second) and vectorial velocity maps of fluid flow thus opening the door to a) high resolution fluid dynamic studies in labs-on-a-chip by monitoring microflows in cell cultures and b) flow and wall shear stress studies in arterial phantoms, a topic of prime importance for the assessment of the effect of flow on plaque formation in atherosclerosis. Improvements will cover: a) more efficient hardware; b) faster data acquisition methods; c) better data processing approaches; d) display interfaces; e) multimodality; f) efficient and precise vectorial maps of velocities. Coupled with other imaging modalities (fluorescence, confocal, two-photon) on a single platform, Doppler OCT could become a truly powerful diagnostics tool with unprecedented resolution and functionality.""423023,""Maciel, Yvan"
"421990"	"MacKenzie, Scott"	"Motor and perceptual optimization of user interfaces"	"A program of research is proposed that will focus on ""motor and perceptual optimization of user interfaces"".  User interfaces are broadly interpreted as any computing device used by humans, for example, conventional desktop or tabletop systems, notebook or laptop systems, tablet systems, mobile devices, or PDAs,  The motor and perceptual aspects of the research pertain to the human output (motor) and input (perceptual) channels that are engaged as humans interact with computing technology. Optimization is a reference to the use of empirical research methods to test alternative interfaces.  Design options are tested and compared in user studies, with better performing options advancing for further optimizing and testing, and poorer performing options discarded.     The human side will research the use of hands, arms, fingers, and even the movement of head and eyes. (Speech input to computers is not part of this program of research.) There is considerable on going research in the way users interact with computing devices with their hands and fingers, and this will continue.  The PI will research touch-based mobile devices that support a variety of touch gestures (e.g., flicks) as well as multi-touch interaction (e.g., for zooming in and out).  Auditory and tactile feedback will be researched as a means to optimize interaction and to reduce the visual load.  The latter is important for mobile devices since they are typically used in dual-task settings where the primary task (e.g., walking or driving) requires the use of the eyes.""436868,""Mackenzie, Stuart"
"427678"	"Magierowski, Sebastian"	"Communicators and controllers for autonomous wireless sensor networks"	"The long-term objective of the research proposed in this grant is the realization of wireless sensing and computing devices (""motes"") less than 100 cubic-mm in volume.  The scale considered for these devices suggests that they could form ultra dense networks consisting on the order of 100,000 motes.  Networks of this size are envisioned as distributed computing devices directly interfaced to our environment and capable of influencing it to our needs.  Reducing mote size increases the size of the network they can form and also increases the diversity of spaces to which this network can be introduced.  Today, motes displace roughly 100,000 cubic-mm and form networks of around 100 elements.  At present this technology has already been demonstrated as a useful tool for environmental monitoring, surveillance, and health care.In the short-term, this research aims at two hardware innovations very important to the realization of useful motes.  Firstly, an ultra-low-power microelectronic wireless communicator will be designed.  Radios are fundamental to the formation of a mote network but today they constitute their most power hungry function (compared to sensing and computation).  Reducing the power consumption of the radio can lead to improvements in other domains such as size, computational complexity, etc.The second short-term objective targets a mote facility of emerging interest to designers: the ability of the device to physically propel itself.  Specifically, this research seeks to realize a specialized computer capable of guiding a mobile mote through space.  This ""path planner"" is essentially a novel nonlinear dynamical controller readily amenable to implementation using analog circuits.  This path-planner will be implemented as a reconfigurable low-power analog computer on a single chip.""438930,""Magnan, François"
"421572"	"Maldague, Xavier"	"Innovative developments of infrared thermography for non destructive evaluation"	"In Thermal Non Destructive Evaluation (TNDE) of materials, an external thermal stimulation is brought to the component to be inspected and response to this stimulus, recorded with an infrared camera, allows to determine the subsurface structure of this component revealing the presence of potential subsurface defects such as voids, delaminations, porosities. Many problems arise in TNDE either at the stimulation stage or at the signal recording and interpretation stage. Objectives of the proposed research program are oriented towards solution of these problems and they are in continuity with long term goals of our research activities. The present proposed research program offers good potential for novel and significant research activities: * Stimulation/recording stages: The objectives are to study in depth, through thermal modeling and experiments the effects of various inspection stimulation procedures (optical heating, inductive - eddy current - heating, ultrasonic heating) while developing adaptive heating by modifying the spectral content of the launched thermal waves in the component to maximize defect visibility. * Interpretation stage: The objectives consist to pursue development of the pulsed phase thermography (PPT), in which a frequency analysis enables to compute phase images less affected by noise and allowing deeper probing. A proposed enhancement to PPT is the use of the Wavelet transform maintaining access to the time information necessary to extract quantitative information of subsurface artifacts of interest. * Automatic/aided diagnosis: to improve reliability and to facilitate industrial implementation, fusion of eddy current data and thermoinductive heating and the use of ""skewness"" in infrared image processing combined to automatic detection are proposed. Graduate students will be strongly involved in the work presented here which is promising since it will bring fresh ideas to the TNDE community and industrial users. The significance of the proposed activities is two-fold. The basis of the work has a strong theoretical foundation, while results of the proposed program are well applied for instance in industries in order to ensure valuable transfer.""423501,""Maldague, Xavier"
"419773"	"Malik, Om"	"Electric power systems - simulation, operational behaviour control and protection"	"The primary thrust of the reserach proposal is the development of real-time digital control and protection schemes for improved performance and stability of electric power systems. It includes the development of improved techniques for machine parameter estimation, study of specific problems relating to machine behaviour and the analysis, design and development of control and protection schemes. A common cause of system performance degradation using fixed parameter controllers is that the power system characteristics change with time and the original contoller parameters are no longer suitable. A solution to this is to monitor the dynamic behaviour of the system in real-time and automatically adjust the controller parameteres on-line. Adaptive control algorithms, particularly suitable for application to electric generating units, that adjust controller parameters automatically to suit actual system conditions are being developed. Adaptive power system stabilzers based on a self-tuning algorithm using a newly developed self-adjusting variable pole-shift control technique have been installed in hydro and nuclear generating stations after extensive laboratory and field tests. Use of optimal control theory, fuzzy logic and neural networks to design adaptive self-tuning controllers that are robust, effective and universal is being investigated for application in renewable, such as wind power, generation schemes.Use of correct prameters is essential for all system studies. Unobtrusive techniques to determine synchronous machine parameters using measurements taken during normal operation is being investigated. A novel hybrid travelling wave and boundary protection scheme for HVDC lines is being investigated. Application of the impedance algorothim, commonly used in protecting transmission lines, is being extended to the protection of transformers. Performance of transmission line differential protection using Wi-Fi protocol for wirless data transmission is being explored. This will obviate the need for land connection between the two ends of the line for transmission of data.""440524,""Malik, Robin"
"425242"	"Mandal, Mrinal"	"Intelligent image analysis for multimedia and medical applications"	"The fields of visual Multimedia and Medical Imaging are continually evolving. During the past 1-2 decades, there has been a significant increase in the level of interest in designing efficient image analysis systems for multimedia and medical applications. Many new application areas, such as the Computer aided diagnostic systems, Image Visualization, Content-based Image and Video Retrieval, Multimedia Communications, Image and Video Databases, World Wide Web, are feasible with the current technology. However, as new applications are coming up, new requirements are being specified, and a substantial amount of work remains to be accomplished in this area.The proposed research focuses on the development of novel image analysis systems for multimedia and medical applications. The main objectives of the proposed research are to explore superior techniques related to the following two complementary areas:1. Super Resolution Imaging and Visualization: We will focus on designing efficient low complexity techniques for generating high resolution videos by combining information from several low resolution videos. The developed techniques will be useful in areas such as Medical Data Visualization, Sports Training, Event Modeling, and High Definition Video Generation.2. Image Analysis and Feature Detection using Stochastic Resonance: We will develop efficient image analysis techniques using the recently discovered stochastic resonance phenomenon. Both theoretical development of stochastic resonance and its application in signal and image processing will be considered. These techniques will be very useful for image analysis in a noisy environment. Three applications will be considered: The fault detection in a Gearbox (industrial testing and monitoring), Detection of lesions/tumors in MRI brain images (health application), and Digital watermarking (consumer imaging application).""426811,""Mandal, Saumendranath"
"422243"	"Mann, Stephen"	"Design intent in surface modeling"	"In a parametric solid modeling package such as SolidWorks, the idea of design intent is that the design should reflect the user's intent. Part of this is a requirement on the design process. As a simple example, if the desired object has mirror symmetry such as a car body, the user should design one half the object and then reflect this half object to obtain the other half. The benefit is that if the user later changes details on the object (such as changing the style of headlights in this car example), the change only needs to be made to one headlight rather than to two. But a modeling system should also try to infer what the user is trying to design. For example, if the user draws a straight line and extrudes it, the user most likely desires the extrusion to be a plane.In this proposal, I will investigate how design intent applies to surfaces. Surfaces are often designed by the user sketching boundary curves and then requesting that the Computer-Aided Design (CAD) package fill in the interior of a surface patch.  While this is an under-constrained problem, additional constraints can be inferred from design intent: if opposing boundary curves each have a single peak, then an interior slice of the surface should also have a single peak.  The goal of this project is to identify surface design intent, find corresponding geometric constraints, and construct surfaces that obey these constraints.The significance of this work is that it will ease the design task for users of CAD systems. Currently, there is sufficient flexibility in tensor product surfaces that designers can design the surfaces they want. However, after making an initial design with curves and having the CAD system fill in the interior, designers need to spend time adjusting the surface to achieve the shape they wanted when sketching the initial curves. The proposed method attempts to infer what the designer desires when constructing and thus reduce the time required for the second adjustment currently required.""424234,""Mann, Steve"
"435090"	"Marble, Andrew"	"Sensor design for open, low cost magnetic resonance"	"MRI scanners and NMR spectrometers are used by doctors, physicists, chemists and industry scientists and technicians to non-invasively examine a wide range of patients, chemicals, materials and products. These magnetic resonance (MR) instruments are designed and built by electrical engineers. Scanners are usually geometrically closed in order to generate the well controlled magnetic fields necessary for the MR phenomenon to be observed. This limits both the range of samples that can be investigated, and the convenience of MR measurements. My research program develops open MR scanners, using electromagnetic design techniques to generate the same high-quality magnetic fields with open access instrumentation. I focus on low cost instrumentation, which can bring this powerful technology to places where costs were previously prohibitive. Examples are routine industrial quality control measurements, food inspection, and remote medical facilities.""432810,""Marceau, Claude"
"421381"	"Marti, Jose"	"Real time operation of large integrated power systems and multiple critical infrastructures"	"This proposal addresses two broad themes of large systems integration: a) Real-time operation of extended electric power grids and b) Coordination of interdependent multiple infrastructures during large emergencies. Large systems integration is of critical current importance as systems become more distributed and interdependent. Energy expansion (e.g. gas, wind, solar) will come from diverse ownerships and will be interdependent on other infrastructures (e.g., transportation, telecommunications, financial). Transmission capacity limitations will require operation under tighter, more coordinated operational limits, while at the same time allowing for more local intelligent actions to prevent cascading effects.  Maintaining the integrity of the national fabric of infrastructures (power grid, telecommunications system, water system, health system, transportation system, etc.) both during normal and emergency times is becoming increasingly complex as infrastructures become more interdependent on each other and problems in one infrastructure are likely to cascade into the others. Large disasters (earthquakes, tsunamis, hurricanes, terrorist attacks, etc.) threaten lives and the national fabric of infrastructures. Similarly, production and financial interdependencies can lead to unforeseen economic disasters.  Four research areas are discussed in the proposal:1) Real-time power systems operational limits assessment at power system control centres.2) Online local assessment and control at power system substations.3) Online condition monitoring of power system equipment.4) Real-time critical infrastructures coordination.""436357,""Martin, Amanda"
"431895"	"McGregor, Carolyn"	"Distributed temporal multi-dimensional data mining"	"The process of temporal abstraction (TA) takes either raw or pre-processed stream data as input and produces context sensitive and qualitative interval based representations. Data Mining provides a mechanism to perform knowledge discovery on ever increasing volumes of data. That is, to transform it from data to information and ultimately to knowledge. Research has begun to apply TA concepts to the pre- processing steps of data mining. A number of limitations of existing temporal abstraction systems in particular relate to (1) their focus on one particular clinical event/condition, rather than creating an environment to explore multiple clinical events/conditions; (2) an inability to support complex abstracts that for example represent abstractions of behaviour across multiple streams and (3) an inability to support abstractions within a multidimensional context. These limitations are impacted by limitations within the supporting data mining principles for pre-processing temporally abstracted time series data.There is enormous potential to data mine historical de-identified patient datasets to detect common trends and patterns, occurring prior to the onset of disease or illness, that have common contextual behaviours rather than the same exact numerical values. To enable this, the proposed research program will build on established collaborative research projects to establish new approaches to data mining that incorporate mechanisms to detect behaviours rather than actual values are required. These approaches will provide a highly flexible environment to support a range of clinical research projects. In addition, these new approaches need to function where the de-identified datasets are located in many different locations. The research foci proposed are innovative and emerging with significant potential, supporting many national, regional and UOIT research priorities and goals. The research program proposed will support the training of masters and PhD students thereby creating the next generation of Health Informatics high quality personnel.Healthcare represents a significant portion (> 30%) of the Canadian National Budget. Health Informatics has the potential to reduce mortality and morbidity rates, improve quality of life and reduce health care costs. Intensive care units (ICUs) and in particular, Neonatal ICUs generate vast amounts of physiological patient data. Recent clinical research has shown the potential for common patterns in seemingly unrelated physiological data to potentially support earlier diagnosis of disease and the onset of illness in neonatal babies.""424018,""McGregor, Carolyn"
"427077"	"Michelson, David"	"Propagation and channel modelling for vehicular environments"	"Intelligent transportation systems (ITS) are a suite of emerging technologies that will be used to make operation of land vehicles in urban centres or along transportation corridors safer and more efficient. Because much of the information that will be delivered and exchanged in ITS applications is time-sensitive and location-dependent, short-range vehicular networks that consist of wireless links between a vehicle and either roadside units or other vehicles in the immediate vicinity and that operate between 4.9 and 5.9 GHz have attracted particular interest in recent years. Anticipated applications of such networks include: (1) enhancing traffic safety by providing warnings and alerts in real time, (2) easing traffic congestion by adaptively changing traffic rules, (3) providing location-dependent information to drivers, (4) aiding traffic regulation enforcement, (5) enabling electronic payments and toll collection, (6) assisting in direction and route optimization, (7) providing information concerning services for travelers and (8) enabling automated highways.  Because vehicle-to-vehicle and vehicle-to-roadside links involve both terminals and obstructions that are in motion, they experience relatively severe fading that is both time and frequency selective. In order to analyze, design and simulate short-range vehicular networks, designers and developers require channel models that capture the propagation impairments that degrade wireless signals in such environments. During the course of this project, we will contribute to the worldwide effort to measure and model short-range wireless channels in vehicular environments by: (1) developing usage models that describe both typical and extreme operating scenarios in such environments, (2) developing channel measurement equipment, also known as channel sounders, that are capable of characterizing time and frequency selective channels of the type encountered in such environments, and (3) capturing the results in the form of channel models useful to designers and developers.""419557,""Michener, Gail"
"435065"	"Mirhassani, Mitra"	"Real-time continuous signal processing systems for advanced technologies"	"I have concentrated my efforts on developing alternative number theories and implementations of intelligent systems, and consequently alternative forms of signal processing. The objectives of the proposed research are:(a)Design and implementation of fully analog neural networks based on the Continuous Valued Number System (CVNS) Adaline model for intelligent signal processing. High performance implementation of neural networks using integrated circuits remains an active research area, since such systems can be applied in system-on-chip applications where computational tasks have to be performed in real time. The eventual goal is to surpass the biological models in compactness, power efficiency, and real time performances. (b)Developing equivalent functions of Finite-Impulse-Response (FIR) filter, and Discrete Fourier Transform (DFT) based on the CVNS theory, and implementation of these functions with application in hard-disk based audio players based on analog signal processing application. (c)Design and implementation of low-power area efficient digital multipliers with application in low-power filters and real-time signal processing. In the last three decades with the significant advancements in digital technology almost all of the computational operations and signal processing tasks are based on the binary signals.  However, digital technology faces problems with the increasing demand on higher speed and lower area, lower power and lower system noise, and cross-talk noise, specially in handheld and portable media devices. Extending the battery life of portable devices has become the main design criteria as more functionality is integrated into these devices. The CVNS can offer an alternative method of computation to overcome some of these design limitations. The resultant designs will be implemented as a hardware proof-of-concept system.The proposed research seeks to develop special processing modules and circuit techniques and optimize arithmetic structures based on this approach for data representation to meet the future needs of the audio processing, and intelligent signal processing.""426002,""Miri, Ali"
"419863"	"Mitiche, Amar"	"Computer vision for enhanced communications"	"COMPUTER VISION FOR ENHANCED COMMUNICATIONS: Multimedia communication systems in applications such as internet, database management, broadcasting, mobile telephony, multisensor systems, robotics, and surveillance, continually produce, transmit, and receive massive  amounts of visual data. Unless processing systems are effective at extracting  useful information from this data, applications will perform poorly, resulting in degraded quality of service and loss of economic opportunity. In this context, communication content analysis and interpretation are vital to productive applications. This research in visual communications is part of the multimedia research programme at INRS-EMT. Its purpose is to develop efficient computer vision methods to expedite and enhance  visual communications. It investigates two broad subjects in analysis and interpretation of visual data: image segmentation and motion analysis in image sequences. The anticipated solution of these subjects key problems promises to enable a rich array of efficient new image interpretation techniques. These techniques will serve visual communications in critical such applications as medical diagnostic and intervention, robotics, and security.""443331,""Mitlin, David"
"425204"	"Moallem, Mehrdad"	"System design for advanced motion/force control"	"This proposal is concerned with developing innovative solutions for motion and force control problems in mechatronic systems by utilizing new and cost-effective technologies. Advanced motion and force controllers have a wide range of applications such as medical mechatronic devices, haptic systems, tele-operation, factory automation, energy converters, and micro-electromechanical systems (MEMS). For example, in the field of medical technology, future endoscopic devices equipped with grasping fingers, or procedures involving robot-assisted insertion of a flexible catheter into a body organ, would require mechanisms equipped with novel sensors and actuators that are integrated with proper control and signal processing algorithms. In future drive-by wire vehicles, the bulky steering wheel could be replaced by a lightweight haptic device to improve energy efficiency while preserving safety. As all these systems continue to mature, requiring higher precision and greater performance levels, the challenge for developing novel technologies for force and motion control would grow. Advanced force/motion control systems are normally comprised of electronic, mechanical, and signal processing components such as sensors, actuators, robotic mechanisms, signal processing and control algorithms, and embedded computers. A main focus of the proposed research would be to develop innovative solutions that create a synergy between mechanical design, sensing and actuation mechanism, digital information processing, and feedback control algorithms. This research would require the blending together of concepts from disciplines in mechanical engineering, electronics, sensors and actuators, embedded computers, control and signal processing, and software engineering. This research would provide opportunities for graduate students and the researcher to work with and develop enabling technologies requiring multidisciplinary skills. These skills would be beneficial to Canadian industries in various strategic areas including health-care, energy, and advanced automation technologies.""437241,""Moats, Kenneth"
"423022"	"Molloy, Michael"	"Probabilistic graph theory and theoretical computer science"	"Graph theory is a fascinating and important pure mathematical field which, with the advent of the computer age, gained a new importance because of its applied aspects. Many of the problems that arise in computer science are best modelled by graphs.  For example, massive networks are  massive graphs and the problem of finding good schedules is what is known as a graph colouring problem.  Thus, over the past few decades, the use of graph theory to study computing has grown immensely.One of the most important modern trends in graph theory is the use of tools and concepts from probability theory.  The probabilistic method is a powerful and elegant tool for proving theorems.  Also, the use of random choices in algorithms has led to the development of much simpler and more efficient methods for many important problems.  When studying the behaviour of an algorithm, we often ask how it performs on an average input, which amounts to analyzing its behaviour on a random input.  This has led to a whole new need for the study of random graphs - a mathematical field that was introduced by Erdos and Renyi in the 1950's, long before its importance to computer science was realized.  More recently, random structures have been recognized as a vast source for difficult inputs that can be used for the testing and refinement of algorithms.My research program encompasses many aspects of graph theory and related fields, and their role in theoretical computer science.  Much of my work concentrates on the areas which involve probability.""438794,""Molot, Lewis"
"424226"	"Moschopoulos, Gerasimos(Gerry)"	"New Ac-Dc power converters with power factor correction"	"The use of electrical equipment powered from the utility grid is rising dramatically and will continue to do so, mainly due to the demands of an information age. Action must be taken to meet the accompanying demand for energy to avoid dire economic consequences, but there is little desire to build expensive hydro-electric projects or polluting coal plants that contribute to global warning, and more research is needed before ""green"" sources like wind become common. Campaigns such as ""FLICK-OFF"" in Ontario and global events such as Earth Hour have stressed the need to conserve energy and to process it more efficiently. Power electronic converters made up of semiconductor devices (switches) and passive elements arranged in electrical circuit structures (topologies) are needed as the interface between source and end application in almost all electrical equipment. Ac-dc power converters are the utility/application interface in most electrical products and equipment. New and improved ac-dc converters of low to medium power (< 6 kW) will be investigated for the proposed research. The novelty will be in the originality and improvement in cost and/or efficiency of the proposed alternatives to existing converters. Cost and complexity will be reduced by using topologies that require fewer active switches, and efficiency will be increased using techniques that reduce the power lost in the converter switches when they are turned on and off with high frequency. The results of the research can be used in applications such as power converters for computer, telecom, uninterrupted power supplies, and alternative energy systems. It can be extended to applications where a dc input source is used such as for low power fuel-cell and solar power converters. The proposed research will help strengthen Canadian capabilities in this area and train highly qualified personnel, including 3 Ph.D and 4 Master's students.""437707,""Moscicki, Michele"
"419371"	"Mouftah, Hussein"	"Design issues in wireless heterogeneous sensor networks"	"In recent years, advances in miniaturization; low-power circuit design; simple, low power, yet reasonably efficient wireless communication equipment; and improved small-scale energy supplies have combined with reduced manufacturing costs to make a new technological vision possible: Wireless sensor networks. A sensor network is composed of a large number of sensor/actuator nodes, which are densely deployed either inside the phenomenon or very close to it. The position of nodes need not be engineered or pre-determined. This allows random deployment in inaccessible terrains or disaster relief operation. The aim is to develop and facilitate the use of scalable and modular wireless heterogeneous sensor/actuator networks in the e-Society. The objective of this project is to develop network architectures along with supporting protocols and algorithms and network control mechanisms for supporting sensors/actuators networking via different types of wireless networks. To support these new architectures, self organized algorithms, new power efficient protocols, novel gateway designs, robust mobility management techniques, and new radio resource management frameworks, are required. Depending on the application, one or more wireless network technologies, such as WiMax, WiFi, ad hoc networks, along with wireless mesh, Internet access and cellular systems would be deployed. The research will focus on four issues: i) Architecture and system design; ii) Gateways and internetworking; iii) Mobility and radio resource management; iv) Self organized and distributed systems. Performance evaluation methods for various architectures, protocols, algorithms, strategies, frameworks, and novel techniques will be carried out using analytical and simulation approaches as well as actual emulation and lab measurements.     ""423391,""Mouftah, Hussein"
"420071"	"Moulin, Bernard"	"Holonic multi-agent geo-simulation for decision support"	"In this research program I consider Multi-Actor Dynamic Spatial Situations (MADSS) that involve a large number of actors acting in geographic spaces of various extents. Decision makers from governmental and private sectors need to monitor the evolution of MADDSs in order to make informed decisions in order to insure human security, equipment preservation and the respect of public order. Although there exist GIS-based data collection and data fusion systems, most available civilian MADDS management systems lack simulation tools that can be used by emergency teams and managers for rehearsal purposes and training. During the past 7 years I showed that multi-agent geo-simulation (MAGS) may provide such tools. I developed with my team a MAGS approach and a simulation framework to simulate MADSSs, as well as tools to analyse and compare simulation outputs. We applied this approach and tools in various domains: 1) crowd simulations; 2) customers' shopping behaviours in malls; 3) the spread of the West Nile Virus; to name a few. However, there is still a lack of tools to help decision makers to qualitatively specify scenarios and assess simulation outputs at the levels of detail they need. Fundamentally, this research program, articulated around 3 main goals, aims at addressing two main requirements: 1) the need to explicitly introduce models of individuals/groups/populations and to simulate their interactions in MAGS; 2) the need to represent/describe/analyse situations at different levels of detail and in a qualitative way that suits decision makers' assessment/reasoning/planning activities using incomplete and approximate information. To address the first requirement, I propose to enhance our MAGS method/tools using a holonic approach that provides appropriate mechanisms for the explicit representation and flexible organization of individuals and groups (Goal1). To address the 2nd requirement, I propose to adapt Zadeh's Computational Theory of Perceptions (CTP) and Granular Computing (GC) to MAGS (Goal 2). The third goal is to exploit/refine the Holonic MAGS+CTP+GC framework and associated tools and to develop new decision support systems for MAGS in selected application domains.""443601,""Moulin, Diane"
"435079"	"Muhaidat, Sami"	"Cooperative diversity for vehicular networks"	"Recent advances in information and wireless technologies have led to growing interests in the development of the Intelligent Transportation Systems (ITS), which promise enormous improvement in  road safety, and elimination of the excessive cost of traffic collisions.As a key component of ITS, Inter-Vehicular Communications (IVC) has recently attracted great attention in both academic and industrial communities. The most important feature of IVC is its ability to extend the horizon of drivers, thus improve road safety and traffic flow by alerting drivers to possible traffic collisions or traffic jams. In addition to these navigation safety functionalities, other applications have recently emerged, such as audio/video streaming, high-speed internet access, and cooperative downloading.The recently proposed cooperative diversity provides an ideal physical layer solution for inter-vehicular ad-hoc networks, an issue largely avoided in the literature. However, the main challenge is that the available research on cooperative diversity is not directly applicable to IVC. Therefore, the main objective of this research is to investigate cooperative diversity for inter-vehicular communications in order to satisfy the stringent performance requirements for vehicular safety applications. Specifically, we will develop a general framework for the design and development of cooperative communication paradigms for vehicular networks, which are compatible with IEEE 802.11p standard for Wireless Access in Vehicular Environment (WAVE) also known as Dedicated Short Range Communications (DSRC).""434619,""Muhly, Tyler"
"421257"	"Myrvold, Wendy"	"Finding torus obstructions/graph theory and algorithms for chemistry"	"Project 1: Finding the Torus Obstructions- A torus is a surface shaped like a doughnut. A topological obstruction for the torus is a graph G with minimum degree three that is not embeddable on the torus but for all edges e, G-e embeds on the torus. A minor order obstruction has the additional property that for all edges e, G contract e embeds on the torus. To date, we have found 249,227 topological obstructions and 17,354 minor order obstructions. The aims of our future research are to check the previous results using independent computations, to develop better computational tools so that the search can be extended, to search for patterns in the known obstructions so that they can expressed more succinctly, to generate and prove conjectures regarding the complete set of obstructions, and to use the insights gained for developing practical algorithms for torus embedding.Project 2: Graph Algorithms for Fullerene Chemistry- Fullerenes are all-carbon molecules whose structures correspond to 3-regular planar graphs with face sizes five or six. This discrete molecular form of carbon was unknown before 1985, but now fullerenes and their relatives the carbon nanotubes have a chemistry and physics that is described in thousands of papers in the literature, with potential applications from materials science to medicine all being driven by their unusual combination of properties. There are various graph theoretical properties which are of direct interest to chemists. Working in collaboration with a theoretical chemist, a system called FuiGui (Fullerene Interactive Graphical User Interface) has been developed to faciliate research on the graphs which represent fullerenes. The goals of this research are to continue development of the FuiGui research tool and to apply it to provide further insight to fullerene chemistry.""421158,""Mysak, Lawrence"
"422770"	"Nasiopoulos, Panos"	"Correcting methods for efficient multiview video coding and improved display quality"	"Multiview video involves capturing 3D information from a scene, allowing exciting applications such as 3D TV and free viewpoint TV, while adding another dimension to surveillance and inspection by greatly increasing the accuracy of object recognition and tracking. Calibrating a large number of cameras is difficult, resulting in significant inconsistencies in brightness, colour, contrast and focus between the different views. These inconsistencies pose a number of problems in multiview systems, ranging from unpleasant visible mismatches for the viewers as they switch between different viewpoints to decreased efficiency in compression, rendering and tracking. The objective of our research is to develop digital pre-processing methods for correcting inconsistencies in multiview video sets, thus improving video coding performance and 3D display quality. Specifics include:Colour Correction: Modifying the colour of multiview video sets to be consistent across all cameras.  Matching colour views will result in increased coding efficiency and better 3D picture quality. Sharpness Correction: Modifying the sharpness/focus of multiview video sets to be consistent across all cameras. We will investigate segmenting frames into objects of different depth, so that different correction parameters can be applied at different depth levels.  Time Varying Correction:  For long multiview sequences, conditions may change over time.  Multiview video may be captured with moving cameras, which will require different correction parameters over time.  A major challenge will be to maintain consistency between views and between successive frames within each view.Real Time Correction:  Developing correcting multiview algorithms for video that is being captured and broadcast in real time.  The proposed research falls in newly emerging multimedia and entertainment sectors, which are among the most dynamic and fastest growing industry sectors in the Canadian economy. ""440804,""Naslund, Eric"
"428740"	"Nasser, Nidal"	"QoS resource management for future broadband wireless access systems"	"Future Broadband Wireless Cellular-based Access Systems (BWCASs) such as High Speed Packet Access (HSPA), UMTS Long Term Evolution (LTE) and the Worldwide Interoperability for Microwave Access (WiMAX), pose a myriad of new opportunities for leveraging the support of a wide range of ""content rich"" mobile multimedia services (e.g., video conferencing, audio and video streaming, broadcast TV and online gaming) with diverse Quality of Service (QoS) requirements. This is due to the remarkably high bandwidth that is supported by these systems, which was previously only available to wireline connections. Despite the support for such high bandwidth, satisfying the diverse QoS of users while maximizing the revenues of network operators is still one of the major issues in these systems. Without efficient resource management, network operators may not be able to meet the increasing demand of users for multimedia services, and hence they may suffer great revenue loss.  The objective of this research is to distinguish and devise effective resource management techniques for BWCAS networks. To this end, I will consider designing and developing efficient bandwidth management frameworks, which aim at supporting multiple classes of traffic with different users having different QoS requirements, supporting inter- and intra-class fairness, preventing network congestion and maximizing the network operator's revenues. Moreover, I will propose architectures for guaranteeing end-to-end QoS, and evaluate alternative technological or methodological approaches for delivering services with QoS guarantees. I shall also devise congestion pricing solutions for BWCASs, which includes defining and deriving pricing functions that consider the QoS of different wireless services in addition to the current load in the network. The research findings of this project represent a definite step towards meeting the ""anywhere, anytime and any form"" communication concept. More importantly, the findings will benefit the Canadian telecommunications industry and business with means to plan, implement and deploy BWCAS networks in a manner that is both time- and cost-efficient.""441124,""Nassrallah, Georges"
"427243"	"Nerguizian, Chahé"	"Development of ad hoc and wireless sensor networks using ultra wide band (UWB) radio technology"	"Various surveys show that Canadian consumers and businesses have a strong interest in adopting new wireless products to improve their standard of living and increase their productivity. The evolution of wireless access towards data-oriented connections, has led to the coexistence of multiple-access technologies providing new products and services, and exhibiting improved system characteristics (lower power consumption and higher data rates). Over the last seven years, novel architectures like ad hoc and wireless sensor networks have generated new challenges regarding the way radio communication infrastructures must be addressed. Unlike traditional approaches, these architectures require complete self-organization of their nodes as well as support for mobility. The main objective of the proposed research project is to develop efficient and cooperative ad hoc and wireless sensor networks (WSN) based on ultra-wideband (UWB) radio technology to meet the requirements of this new market. The first goal of the research program is to apply UWB with innovative front-end solutions to enhance the system performances in different applications. The second goal is to deploy cooperative algorithms applied to WSN in order to find high efficiency solutions with low complexity implementations for these applications. To achieve these research objectives, the project will be divided into two different directions, which will be investigated in parallel. First, ad hoc UWB wireless sensors will be designed for cooperative localization/tracking in two different indoor environments (in-building and mining). Second, UWB radar sensors will be designed for vehicule collision avoidance applications such as mobile robots and outdoor ground vehicles. The originality of this hardware and algorithmic project consists in combining ad hoc network with wireless sensor devices using a low complexity UWB transceiver front-end in conjunction with novel cooperative algorithms to give application flexibilities, low-cost products and high performance results. Consequently, improved system performance and network cost represent the advantages of the proposed approach compared to the existing ones.""441854,""Néron, Alex"
"422212"	"Ng, Raymond"	"A data mining framework for genomics biomarker and signature identification"	"Biomarkers are indicators used to objectively measure and evaluate normal biological processes, pathogenic processes or pharmacological responses to therapeutic interventions. In medical practice, biomarkers facilitate diagnosis and prognosis of disease, as well as monitoring clinical response to therapy. In environmental studies, biomarkers can be used to assess the health states of animals and plants, with respect to, for instance, the amounts of toxic materials they are exposed. Advancements in the fields of genomics and proteomics have revolutionized novel biomarker discovery with new tools for high throughput strategies. In this proposal, we present a comprehensive framework for using large amounts of genomics and proteomics data for identifying biomarkers and signatures. The proposal has three key focuses: (A) data cleansing and pre-processing; (B) biomarker and signature identification; and (C) validation with real data sets. For data cleansing and pre-processing, we consider a range of topics:  (A1) Quality control for microarray data; (A2) Proteomics platform calibration; and (A3) Formation of protein groups for proteomics data. We emphasize that before data can be mined, they need to be properly cleansed and transformed.  For biomarker and signature identification, we explore a number of key topics: (B1) Gene set enrichment analysis; (B2) Ensemble methods for combinatorial analysis; and (B3) Embedded clustering methods during biomarker discovery. Last but not least, for the objective of validation with real data sets, we discuss how the aforementioned tasks will be applied to two large-scale genomics and proteomics data sets funded by Genome Canada and NSERC. ""431324,""Ng, Sandy"
"435056"	"Nielsen, Christopher"	"Nonlinear set stabilization: application to coordinated path following"	"The set stabilization problem is an important problem in control systems because there are many emerging applications in which set stabilization is the natural way to formulate control objectives. Some examples are the control of a formation to make three wheeled robots arrange themselves in a triangle on the plane; the control of the walking motion for a biped robot; the path following problem for a car, ship or airplane (make the position of the mobile vehicle approach and follow a path); the creation of virtual constraints in a robot for haptic applications; the synchronization of coupled oscillators. Theoretically, many existing concepts from nonlinear control that deal with equilibria stabilization have analogous counterparts in the set stabilization realm which would represent a fundamental generalization and contribution to control theory.This proposal focuses on (a) computational aspects of the set stabilization problem, (b) application to coordinated path following, and (c) implementation of coordinated path following algorithms. Part (a) focuses on making set stabilization a more feasible and useful approach to control design. This entails creating a software toolbox for calculating the control laws and working on set stabilization using output feedback. Part (b) deals with the problem of getting a group of agents to each follow a path in a coordinated way while part (c) implements the theoretical development from (a) and (b).It is becoming increasingly clear that the Canadian labour market is undergoing a major restructuring in light of the emergence of new economic powers. The pressure for increased productivity leads to a more automated society in which robotics and advanced control theory will play a major role.  The requirements that this will place on the engineering profession are vast and varied. Control theory and specifically advanced techniques from nonlinear control is one of the disciplines that will be relied upon to lead these changes.""440819,""Nielsen, Dana"
"421969"	"Nishimura, Naomi"	"Tractability in structured problems"	"The idea of structuring information is at the heart of computational problems in diverse application areas such as social sciences, artificial intelligence, biology, Web computing, and many other areas.  There may be restrictions in the way that information can be structured, either due to the types of inputs to the problems (the types of information on which the problem is being solved) or due to the type of solution being sought.  The goal of my research program is to determine ways of characterizing structure that allows for efficient solutions to fundamental problems; these in turn can be applied to specific problems in target areas.  My interests range from extracting structure from problems to developing characterizations and associated solutions, especially for fundamental problems of broad applicability across areas.     One type of problem is to reconstruct information given one or more collections of possibly faulty measurements.  It is also possible to determine the similarity between two data sets by determining if one appears as a substructure of another, either once or multiple times, exactly or otherwise.  These problems can be approached in different ways depending on what similarity measures are used.  In some cases, it is also interesting to be able to determine the number of solutions possible.     Whereas many problems are stated for one fixed point in time, it may be the case that there are slight changes over time, necessitating modifications to the solution.  I am interested in situations in which the new solution is faster to find by using the previous solution than by starting the computation all over again.  In particular, I am using as a parameter the amount of drift possible, and looking at problems for which it is difficult, in general, to find solutions that can be computed in time polynomial in the size of the input.   I hope to find solutions that are polynomial in the size of the input but possibly a larger function with respect to the parameter; this would yield practical results for situations in which the amount of drift (and hence the parameter) might be small.""441418,""Nissan, Ramen"
"431740"	"Noble, Scott"	"Spectral imaging for crop protection"	"The objective of the proposed research program is to develop methods for automated interpretation of spectral image data from plants and scenes of plants.  This includes determining the structure, species composition, and health of a plant canopy.  Spectral images of the scenes are collected using spectral imaging equipment.  These instruments provide detailed spectral and spatial information, allowing subtle differences and relationships in pigmentation, structure, and surface characteristics to be observed and used.  It is also possible to use light outside of the range visible to people.  Based on prior spectral work, this project proposes to use the ultraviolet spectrum in addition to the visible and very-near infrared.  This will be a unique contribution to the state of the art in this field, and imaging more generally.Two particular aspects of interpreting complex biological scenes will be studied: species identification, and using the directional dependencies of reflectance to reconstruct canopy structure.  Species identification will be accomplished by developing techniques to quantify definable characteristics about species in ways that do not require direct comparison to other species.  Directional dependency effects will be studied using customised equipment that will enable spectral image data to be acquired from multiple angles around a plant while simultaneously scanning a profile of the plant.  These data will assist in scene segmentation tasks by recognizing areas with similar orientation, and allowing for some canopy geometry information to be inferred.The ability to automatically interpret visual information about complex biological scenes has been identified as a priority for specialty crop production. Application examples include vision systems for greenhouse automation, crop scouting (for disease, weeds, insects, and plant health), and reducing agricultural chemical inputs.""438651,""Nobles, Andrew"
"420792"	"Oldford, Wayne"	"Methods for statistical data visualization and cluster analysis"	"The research to be carried out will bring computational, mathematical, and statistical tools to bear on the development of new data visualization and clustering methods for the exploration of high dimensional data.  The methods will be applied and adapted to data of varying complexity -- access to several novel data sources (recently made available to us from various government, industry and medical researchers) will provide meaningful and valuable test beds.Two distinct but related research areas will be addressed. The first is the methodological study and development of data visualization methods. The second is the development of a methodological and computational framework for the study of existing clustering methods as well as the development of new methods. In both cases, software will be developed and distributed as part of the R project.""420219,""Oldham, Keith"
"428756"	"Osgood, Nathaniel"	"Hygeia's toolbox: computation and mathematics in support of public health decision making and insight"	"Despite great past achievements, Canada faces pressing threats to public health.  While designing appropriate public health policies requires judicious choice of intervention strategies, such choice is complicated by the high complexity associated with the effects of such interventions -- raising the risk that even well-intended policies may prove ineffective or cause worse problems than they solve.  To enable them to better grapple with these challenges, public health decision makers increasingly turn to simulation models for insight.  By letting decision makers simulate the effects of policies, such models can help inform the design of health interventions that are high leverage, robust, and cost-effective; models also facilitate quicker reaction to an outbreak of infectious disease when it occurs, aid in understanding of health trends, help prioritize data collection, and assist in communication with diverse stakeholders.  Unfortunately, such models are frequently needlessly challenging to build, hard to understand, computationally expensive, and difficult to share and collaboratively explore in a team environment.  There are also growing modeling challenges associated with tackling the many unexplored problems.    We propose here to design and apply tools to inform understanding of population health trends and health policy tradeoffs.  On the application side, we seek to continue our collaboration with cross-disciplinary teams to create simulation models to inform the design of health interventions.  In this area, we will specifically focus on investigating novel approaches to modeling challenging and important health concerns, such as obesity and diabetes, and the implications of those conditions for infectious disease spread.  Our work will continue to place a special focus on the Aboriginal Peoples' health. Our methodological work seeks to enhance model performance, thereby opening up more time to carefully investigate policy impacts. We also seek to aid the construction of models that are better designed and more widely shared and understood by building software to improve team access to models and model results, and by supporting improved modeling languages.""419342,""OShaughnessy, Douglas"
"435200"	"Pal, Christopher"	"Large scare semi-supervised pattern recognition and data mining from images and text"	"Large scale pattern recognition and data mining techniques have resulted in broad and tangible impacts on applications ranging from improved general purpose internet search and semi-automated internet portal construction to accelerated discovery in bioinformatics.  Many problems in pattern recognition and data mining arise in a setting where it is easy to obtain a small amount of labeled training data relative to the amount of available unlabeled data.   Semi-supervised methods for pattern recognition allow labeled data to be combined with unlabeled data to improve the results of a recognition system.  Information extraction is a type of data mining that seeks to extract structured database records from unstructured data. Once information is extracted it is possible to use the underlying structured representation for a variety of tasks such as enhancing search or subsequent data analysis.  The research proposed here will develop new general purpose semi-supervised information extraction techniques.  There is also a growing interest in addressing problems in pattern recognition and data mining can benefit from a tighter coupling of computer vision and text processing techniques.  Concrete examples include: internet scale image search, object recognition with thousands of potential categories and the extraction of information from both images and text descriptions of biological experiments; however, many other emerging problems have similar properties.  The research in this proposal thus also aims to develop principled and efficient methods for semi-supervised pattern recognition, data mining and information extraction with an emphasis on problems that involve the simultaneous processing of images and text.  While the emphasis of this work will be on general purpose algorithms and techniques, to develop and evaluate techniques we propose to use the concrete scenarios of creating an extremely large scale visual encyclopedia by mining the web and creating algorithms also tailored to the more specialized settings of biological image and text analysis.  Research will result in training highly qualified personnel in the development of techniques appropriate for internet scale and genome scale processing - skills in high demand.""421918,""Pal, Rajinder"
"425285"	"Paquet, Joey"	"GIPSY - General intensional programing system"	"The GIPSY project aims at the practical investigation on the intensional programming language paradigm as implemented by the Lucid family of programming languages. Lucid variants range from the famous real-time programming language Lustre, to the C-Lucid hybrid programming system GLU. The Lucid family of programming languages being very eclectic and in constant evolution, the GIPSY project aims at providing a common platform for the compilation, execution and development of Lucid variants. With flexibility and generality in mind, our ongoing research program has allowed us to establish a framework approach to compiler and run-time system construction for the execution of programs written in all variants of Lucid. This approach provides a flexible infrastructure for the semi-automated generation of many components of the GIPSY. Using syntactic and semantic language description and translation rules, components like parsers and semantic analyzers and translators can be automatically generated for all Lucid variants. The compiled programs can be further analyzed to allow optimization of their distributed run-time behavior by implementing various solutions for task scheduling using our semantically sound demand-driven execution model. We have recently designed a promising multi-tier approach to our run-time system, yielding more flexible and scalable solution to the execution of Lucid programs. We are also in the process of designing and implementing a novel object-oriented version of Lucid that shall make the language more in-line with current industrial trends. Additionally, independent research on intensional programming at the University of New-Brunswick and the University of New South Wales have proven to be compatible with our approach. Part of our future goals is to incorporate results from these teams into our theory and implementation. That shall lead to a unification of current research in the area, and establish the GIPSY as the norm in intensional programming research and development.""429809,""Paquet, Marc"
"422523"	"Parsons, Jeffrey"	"Information integration using principles of classification"	"The problem of semantics is one of the pressing issues both in effectively representing the semantics of a domain in information systems artifacts (data and software), and in supporting large-scale sharing of information across multiple independent sources, such as those available over the Internet. The Addressing the problem of semantics effectively requires overcoming two challenges: (1) specifying semantics for a single information source; and (2) providing mechanisms for reconciling the semantic specifications provided by independent sources.  The proposed research will develop new approaches to providing semantic specifications to information sources and to reconciling independent sources, and will evaluate the effectiveness of these approaches. We will build on earlier work by focusing on three areas. First, we will develop methods to enhance the semantics of information models by exploiting the inference capabilities afforded through the classification of instances. Second, we will apply the concept of classification-based inference to provide a semantic layer to tagging mechanisms that have recently emerged as popular tools for annotating content in social networking contexts (e.g., del.icio.us and Flickr.com). Third, we will apply and evaluate the methods developed in the first two phases of research to business contexts. In particular, we will use the classification principles to develop domain ontologies for the online travel industry in conjunction with an industry partner based in Asia.As the availability and scope of networked resources grow, the need for effective methods and tools to locate and manage information is becoming more critical to both individuals and organizations. This research aims to contribute to information engineering by supporting the effective classification of information resources in order to improve the ability to search, filter, and use these resources.""429786,""Pasek, Zbigniew"
"434828"	"Pearce, Joshua"	"Effects of nanostructure and defect states in solar photovoltaic materials"	"The world requires inexpensive, reliable, and sustainable energy sources. Solar photovoltaic technology, which converts sunlight directly into electricity, provides a promising source. This research program aims to enhance characterization and analysis techniques to improve the performance of solar photovoltaic cells. In this research program solar cell efficiencies will be increased using a novel and systematic approach. First, this program will quantify the effects of deposition conditions on the quality, defect state density, type and distribution in thin semiconductor films of indium gallium nitride (InGaN) films and mixed phase amorphous and nanocrystalline silicon (a-Si:H and nc-Si:H).  In the short-term the objectives are to i) create deposition phase diagrams for the InGaN, ii) couple the microstructure of the materials to defect state densities and types, and iii) complete characterization of all layers of a protocrystalline Si solar cell fabricated with mixed phase materials. Then this new information about the material properties will be used in the long-term to develop optimized photovoltaic devices that overcome established obstacles in the technical development of each of these material families. This research program will produce higher efficiency and more stable mixed phase Si:H solar cells and will also generate ultra-high efficiency InGaN solar cells using systematic incremental improvements following well-tested successful paths of previous research. These technical improvements all correspond to decreases in economic costs of solar cells, and have the potential to have an enormous positive effect on the Canadian energy supply and economy. As Canadian society transforms itself into a solar powered economy, the HQP trained in this program will be ready to begin working immediately in the growing Canadian solar industry. These students will be better trained to work on the energy solutions for this century in Canada. ""427170,""Pearce, Trevor"
"429037"	"Perkins, Theodore"	"Systems biology and biological information processing"	"My research is targeted towards computational problems that arise in the study of complex, dynamic biological networks.  Much of my research is motivated by ""high-content"" technologies, such as in vivo expression imaging or flow cytometry, in which the activities of a relatively small number of types of molecules are measured quantitatively with high spatial and/or temporal resolution in single cells or in tissues.   My research focuses on three areas: (1) model formulation and fitting, particularly for spatial/temporal gene expression data, (2) experiment design, especially for time-series experiments and drug screening problems, and (3) theoretical analysis of the information processing capabilities of biomolecular systems.  In the first two foci, I develop techniques that allow us to maximize the amount of information obtained from molecular biological data, and the efficiency with which that information is extracted.  In the final focus, the goal is to understand how biomolecular systems themselves make decisions, or how they can be engineered (via ""synthetic biology"") to carry out computational tasks.""436728,""Perley, Danielle"
"431690"	"Pichevar, Ramin"	"Anthropomorphic coding and recognition of mujltimedia signals through sparse representations and auditory objects"	"- Anthropomorphic signal processing has emerged as a new way of processing signal at the ""image of man"". It is designed to solve a problem in signal processing by imitation of the processes that accomplish the same task inhumans. In the area of audio an""422313,""PichoraFuller, Margaret"
"424863"	"Pizzi, Nicolino"	"Classification methodology for mitigating the effects of unreliable class assignments in high dimensional biomedical data"	"Classifying biomedical data involves finding a mapping (relationship) from patterns (e.g., data relating to some type of tissue) to a set of classes (e.g., disease states). Patterns are represented by features (e.g., concentrations of biological compounds) and class labels are assigned using a reference test (RT) (e.g., a medical expert's analysis of tissue being ""normal"" or ""abnormal""). This process often suffers from three significant challenges: the RT may be unreliable; the number of patterns is low; and the number of features in a pattern is high. While RTs may be well-established benchmarks, they are seldom perfectly accurate and sometimes improperly applied. Nevertheless, any strategy that compensates for this imprecision must ensure that the mapping is correctly validated against the benchmark. The last two challenges, known collectively as the ""curse of dimensionality"", cause an inability to find robust, general solutions. This is often addressed by transformations that select the most discriminatory subset of features while not impeding the medical expert's ability to make informed judgments about the mapping's predictive power. Techniques using Computational Intelligence (CI) - adaptive, learning, or evolutionary algorithms - have demonstrated their effectiveness in data analysis. This proposal involves the development of a comprehensive CI based Classification Methodology to mitigate the effects relating to many features, few patterns, and an unreliable RT. Its novelty and significance is in its tightly integrated approach, where the enterprises - accurate classification, effective feature reduction, unreliable class assignment mitigation - incrementally inform and drive each other in a unified manner. This methodology will be instrumental in the analysis of complex biomedical data and will lead to more reliable classification systems. Further, while this investigation involves methodological and foundational concepts, it will simultaneously deal with ""real-world"" data. Finally, by collaborating with biomedical experts, the fundamental knowledge created by this investigation will generate tangible practical outcomes for the biomedical research community.""432981,""Pjontek, Dominic"
"419343"	"Plamondon, Réjean"	"Modélisation des mouvements humains rapides pour la conception de systèmes biométriques débiés à l' analyse de la  détection de troubles neuromusculaires"	"L'OBJECTIF GÉNÉRAL de mon programme de recherche vise A) le développement d'une théorie globale de génération de mouvements et de perception de trajectoires visant à mieux comprendre les processus neuromoteurs et cognitifs impliqués dans le contrôle et l'exécution de tâches complexes chez l'humain.  B) l'exploitation des connaissances dérivées de cette théorie pour la conception de systèmes informatiques intelligents dans différentes applications biométriques. C) La généralisation et l'intégration de cette théorie dans un modèle neurocognitif unificateur dans le but ultime de comprendre les fondements psychophysiques et physiologiques qui sont à l'origine de la biogénèse et de l'universalité des phénomènes log-normaux dans la nature.QUATRE OBJECTIFS SPÉCIFIQUES sont visés :1)Intégrer dans nos modèles de génération de mouvements des mécanismes de feedback et des contrôleurs intelligents pour générer des tracés complexes pour des applications de calibrage de classificateurs et d'apprentissage en reconnaissance de mots. 2)Élaborer un modèle de perception exploitant les saccades oculaires comme mécanisme fondamental, pour des applications en reconnaissance et indexage de documents historiques.3)Concevoir des algorithmes permettant d'étudier et caractériser la neuromusculature de l'être humain à partir de l'acquisition et de l'analyse des tracés manuscrits et des signatures, pour des applications en biométrie et en traitement de biosignaux (EMG/EEG).4)Développer des systèmes informatiques intelligents basés sur l'étude des mouvements humains en deux et trois dimensions, afin de reconnaître les signes avant-coureurs de certains problèmes de santé, pour des applications en génie biomédical. Ma DÉMARCHE SCIENTIFIQUE repose sur le développement et l'exploitation d'une Théorie Cinématique des mouvements humains que j'ai élaborée au cours de la dernière décennie. L'IMPACT ÉCONOMIQUE ET SOCIAL de nos travaux est très important, par exemple, la détection de troubles neuromusculaires précurseurs de problèmes cérébrovasculaires permettrait d'économiser des dizaines de milliards de dollars au Canada seulement.""442697,""Plamondon, Robert"
"427369"	"Prodic, Aleksandar"	"Reconfigurable digitally-controlled low power SMPS"	"Switch-mode power supplies (SMPS) are highly efficient power processing devices providing regulated voltage or current for virtually all electronic applications today. In low-power (LP) devices analog-controlled SMPS operating at switching frequencies between several hundreds of kHz and several MHz have been preferred to more flexible and intelligent digital-controlled solutions. Applications include portable and consumer electronics, computers, lighting system, and numerous other devices consuming power from a fraction to several hundreds of watts. The prevalence of the analog designs comes from the requirements for a fast controller action and simple implementation that, at the high switching frequencies, are challenging to achieve with conventional digital systems. Through recent research, enabling technologies for digital control of LP SMPS have been developed. In addition to that, novel features, such as parameter extraction and consequent auto-tuning, have been introduced, giving significant advantage to digital over analog solutions. The proposed research goes beyond the sole controller implementation and addresses the operation of the entire digitally-controlled LP SMPS, i.e. power stage and its controller, as well as its interaction with supplied digital loads. The ultimate goal is to fully utilize advantages of digital control and create a new generation of low-power reconfigurable SMPS topologies that, compared to existing solutions, will have higher power processing efficiency, smaller dimensions, higher flexibility, and improved reliability. To provide those benefits, the reconfigurable SMPS will dynamically change their own topology, depending on the power source conditions and estimated load needs, or through communication with the same.    ""440782,""Profitt, Maxine"
"421451"	"Qiao, Sanzheng"	"Integer least squares and sphere decoding"	"In digital communications, the source signal is often a vector of integer codes. After travelling through a communication channel, the source signal, added by a noise, is received. The problem is to recover the source signal from the receiced signal and the knowledge of the channel. In mobile communications in particular, hight speed and low error rate is essential. Sphere decoding is a widely used technique. We propose to investigate accurate, robust, and efficient sphere decoding methods and more general integer least squares methods. Our project will strengthen the competitiveness of Canadian digital communication industry. Students in this project will receive training in digital communication, scientific computing, numerical software engineering, and numerical analysis. Although our project focuses on the application of communications, integer least squares also has applications in cryptography and computer security.""439030,""Qin, Lei"
"434961"	"Qureshi, Faisal"	"Virtual vision and smart camera networks"	"Camera networks are an emerging enabling technology for a broad range of applications, including video surveillance, environment and traffic monitoring, elderly care, intelligent environments, and urban and participatory sensing. Most researchers who are motivated by the challenging open problems in camera network research, however, simply do not have the resources to work with a large-scale physical camera network, given the cost and other serious impediments of deploying and experimenting with a camera network of appropriate complexity in an extended public space. A compelling alternative is ``Virtual Vision'', a novel camera network research paradigm that prescribes using visually and behaviorally realistic virtual environments for developing and evaluating camera networks. Virtual vision enables researchers to deploy virtual camera networks in visually and behaviorally realistic 3D computer simulated environments and to experiment with these networks under realistic conditions.In prior work, I have implemented a prototype virtual vision simulator and exploited it to develop two novel camera networks that are capable of carrying out observation and sensing tasks with minimal human intervention. I now propose to start a new research program at the UOIT, whose objective will be to take this promising initial work to the next level. The proposed research program will have the following four objectives: 1) to implement richer 3D simulations for virtual vision by developing new, more powerful behavioral and cognitive virtual human models, 2) to develop scalable 3D simulations of this kind on high performance computing systems, 3) to develop next-generation ad hoc networks of smart cameras by tackling fundamental problems in multi-camera control and coordination, and 4) to validate the virtual vision paradigm by using a real, physical multi-camera system.""429874,""Raahemi, Bijan"
"419759"	"Radhakrishnan, Thiruvengadam"	"Human centered computing and medical data visualization"	"Computing, communication, and networking have gone through a phenomenal evolution in recent years. Mobile or cell phones embedded with computing power are finding their in roads even in rural population in developing nations. As a result there has been an increased interest in ""HCC or Human Centered Computing"". HCC is not a new field but a new focus in the analysis, design, development, and deployment of application oriented software systems. Human computer interface (HCI) and user centered design (UCD) focus more at the interface level whereas the scope of HCC is expanded to the software life cycle level with due emphasis to human aspects and it is relatively more interdisciplinary in nature. In the use of computing as a tool, humans work on their own interacting with a computer system or interacting with other humans assisted by the computing infrastructure. As computing becomes ubiquitous, researchers in technology have a social obligation not to ignore the efforts required to bridge the ""digital divide"" that the computing technology is introducing. Content creation, dynamics of the content, trust on the information, local relevance and sustainability, all these in the context of social development are non-trivial issues. The proposed research in this grant application is a small  attempt towards a larger goal of addressing the above issues. We focus on three sample problems that are well defined in scope for an individual researcher. The problems are selected to match, social relevance, feasibility of the experimental approach proposed and the availability of test grounds. The problems addressed are to seek answers to: How to incorporate socio-technical models in software requirements elicitation and engineering? What is the best way to support interacting clinical researchers in their quest to use multiple sources of data in formulating their hypotheses that might lead to discovery? How to empower chronically ill patients by providing right information at the right time in a right manner? Creating appropriate models, integration of models, developing model based design and prototyping, experimental evaluation with real end users are the common themes to these three selected sub problems in the proposed research.""435734,""Radich, Allison"
"420777"	"Rappaport, David"	"Computational geometry and data analysis"	"I study the geometry in problems with the intent of producing correct and efficient computer programs. Applications such as computer graphics, computer vision, optical character recognition, robotics, computer-aided design, and computer-aided manufacturing, network design, and data analysis all have some geometric component. My work focuses on the geometric structure or geometric patterns that can be used to solve these types of problems. I study the geometry in problems with the intent of producing correct and efficient computer programs. Applications such as computer graphics, computer vision, optical character recognition, robotics, computer-aided design, and computer-aided manufacturing, network design, and data analysis all have some geometric component. My work focuses on the geometric structure or geometric patterns that can be used to solve these types of problems. The nature of my research is primarily theoretical in the sense that general methods and techniques are given. These results may not immediately apply to a particular problem. However, with the appropriate modifications, these results may be used for a variety of applications.      The problems I work on are motivated by a particular class of applications where interactions between components in a system are often displayed as a graph, that is, the relationships between elements of the system are represented in a mathematically rigorous way. Examples of the use of this type of representation are subway maps, project time lines, communication networks, or the hierarchical management structure of large organizations.      I will strive to maintain a cadre of three graduate students, both at the Master's and Doctoral level. These students will pursue research that is either entirely theoretical, an application of theoretical results, or both. The training these students will receive will enable them to access, master, and create results in the field of computational and combinatorial geometry.""423196,""Raptis, Leda"
"434975"	"Riecke, Bernhard"	"Enabling effective human spatial orientation and behaviour in virtual environments"	"GOAL: To investigate what constitutes effective, robust, and intuitive human spatial orientation and behaviour. This fundamental knowledge will be applied to design novel, more effective human-computer interfaces and interaction paradigms that enable similar processes in computer-mediated environments like virtual reality (VR) and multi-media. MOTIVATION: While modern VR simulations can have stunning photorealism, they are typically unable to provide a life-like and compelling sensation of moving through the simulated world, thus limiting perceived realism, behavioural effectiveness, user acceptance, and commercial success. APPROACH: I propose that investigating and exploiting self-motion illusions might be a lean and elegant way to overcome such shortcomings and provide a truly ""moving experience"" in computer-mediated environments. Multi-modal, naturalistic and immersive VR provides a unique opportunity to study human perception and behaviour in reproducible, clearly defined and controllable experimental conditions in a closed action-perception loop.  SHORT/MID-TERM GOALS: To perform experiments with naïve human participants in VR to investigate and optimize multi-modal and higher-level contributions and interactions for spatial orientation and self-motion perception, while minimizing reference frame conflicts. LONG-TERM GOALS: To investigate how we can best employ self-motion illusion in VR to enable life-like, robust and effortless spatial orientation and behaviour in VR. SIGNIFICANCE: This research will lead to a deeper understanding of human perception and behavior that enables us to design more effective human-computer interfaces and interaction paradigms. In particular, applied research will identify the essential parameters of perception/action and pin-point the ""blind spots"" that will enable us to trick the brain when simulating VR.  This will enable the creation of better, cost-effective, virtual solutions for numerous applications, such as driving/flight simulation, space exploration, education, gaming, engineering, recreation, emergency training, video conferencing, minimally invasive surgery, and telemedicine.""430081,""Riedel, Michael"
"434956"	"Rogan, Peter"	"Modeling mRNA splicing by exon definition in silico"	"Interpretation of the effects of variation in genes is among the most challenging and important problems to be addressed in deciphering the sequences of complete genomes. My laboratory has developed bioinformatic theory, software, and applications to understand and predict the consequences of DNA sequence changes. This framework uses information theory to relate differences between the strengths of interactions between proteins and DNA or RNA to their effects on gene expression. Information theory can be used to detect and quantify signals in DNA or RNA that are recognized and bound by proteins in the cell. This project will develop and evaluate mathematical models for the natural cellular process to identify elemental protein-coding units of genes, termed exons, from the sequences of unprocessed RNA transcripts.  Correct processing of these transcripts is called mRNA splicing. We will use information theory to define these signals, and then combine multiple signals within individual exons to distinguish and quantify correctly processed mRNA transcripts from potential decoys. The predictions will be compared with published sequence databases of expressed genes to determine the accuracy of these models.  The models, methods, and software developed  in this will be useful for predicting gene expression patterns in humans and other species. These resources will eventually be used in studies of common sequence variants that are predicted to affect normal mRNA splicing.""438694,""Rogawski, Anna"
"422520"	"Roy, Langis"	"MM-wave active antennas for communications applications"	"The wireless industry continues to push for higher speed transmission, lower product size and cost, and greater product functionality.  At the same time, increasing demands are placed on wireless devices, which are expected to be aware of their radio-wave environment and adapt to it in terms of operating frequencies, power levels and radiation characteristics.  Such requirements, coupled with the realities of high-volume commercial applications, translate into several component-level technological challenges that today are barely being met:  frequency-agile or multiband RF circuits and antennas, increased integration levels of RF transceivers and antennas at limited cost/performance degradations, acceptable DC/RF conversion and operating efficiencies, to name only a few. These difficulties are severely compounded when operating frequencies move toward the mm-wave band, as they eventually must to avoid spectrum crowding and to permit emerging applications such as short-range multi-Gbps wireless links (i.e. 60 GHz and above).  A number of exciting new materials and RF technologies appear promising, at least individually: nanometric (65 nm, 45 nm) CMOS for low-power active circuitry, GaN HEMT for high-efficiency power stages, liquid-crystal polymer (LCP) and low-temperature co-fired ceramic (LTCC) substrates for passives, antennas and packaging.  This work focuses on active antennas that intimately combine these and other active, passive and packaging components with novel radiating structures.  The proposed research addresses in several ways the major challenges associated with the development of next generation mm-wave wireless communicators.Successful realization of high-performance mm-wave active antennas using advanced system-on-chip and system-in-package approaches will have a tremendous impact on the wireless communications industry: terminals may be produced more competitively than ever before, as they will benefit from the advantages of reduced size and weight, better reliability, increased bandwidth and functionality, and lower cost (by incorporating functionality ""for free"" in the package).""440912,""Roy, Manon"
"426565"	"Rueda, Luis"	"Efficient linear dimensionality reduction and optimal multi-dimensional clustering"	"Pattern recognition (PR) is a field of artificial intelligence, which involves the process of taking raw data and making a decision to assign a sample to a category or class. PR systems have been extensively studied by researchers for many years, in both theoretical and application aspects - the latter includes a variety of disciplines such as medicine, molecular biology, bioinformatics, genomics, proteomics, biometrics, security and manufacturing, just to mention a few. The essence of a PR system is to devise and evaluate an efficient classifier which is able to accurately classify unknown objects. Whether or not the classes are known, the recognition process is called supervised or unsupervised. In any case, devising a classifier requires the underlying objects be represented by means of features, typically, as multi-dimensional vectors, and the underlying classifiers are derived in terms of functions or algorithms on the corresponding multi-dimensional space. The main goals of this research proposal are to devise an efficient scheme for optimal clustering, which is capable of dealing with multi-dimensional data, and develop a robust linear dimensionality reduction scheme based on statistical properties of the classes, which is capable of achieving high accurate classification for a wide range of problems, while selecting the best subset of features.Applications of the newly proposed methods will be pursued on various problems in bioinformatics, proteomics, DNA microarrays and microbial biofilms. One of these is to efficiently cluster time-series DNA microarray data using unsupervised techniques. Another problem that will be tackled using the proposed clustering methods is DNA microarray image gridding and segmentation. Protein-protein interactions will be studied in terms of finding important residues and energetic factors in interfaces, and which are correlated to classification of protein complexes and amino acids.""434846,""Ruediger, Andreas"
"419705"	"Ruskey, Frank"	"Efficient algorithms for sequencing, counting, manipulating and visualizating discrete structures"	"The purpose of this research is to investigate essential algorithmic and mathematical properties of certain fundamental discrete structures, including Venn/Euler diagrams, Gray codes, nested recurrence relations, and polynomials over finite fields.  Of these structures, Venn/Euler diagrams will be most familiar to the general public.  Formally, a Venn diagram consists of n simple closed curves drawn in the plane with the property that each of the 2^n intersections of the interiors or exteriors of the curves is non-empty and connected.  If regions are allowed to be empty, then it is an Euler diagram.  There is a demand for software that will render ""Venn"" diagrams of populations in such a way that the area of each region is in proportion to the size of the underlying population.  Basic questions about when this is possible, and if possible, how to best draw the diagram from the users point of view, are still unresolved, although some progress has been made.  One goal of this research is to resolve these questions.Other problems that will be investigated first arose in Hofstadter's famous book: Godel, Escher and Bach; namely determining properties of the recurrence relation Q(n) = Q(n-Q(n-1))+Q(n-Q(n-2)), with Q(1) = Q(2) = 1, and a plethora of related similar recurrence relations.  As an example problem, it is not known whether Q(n) is defined for all natural numbers n.We will continue our work on discovering and efficiently implementing combinatorial Gray codes, which are lists of discrete objects in which successive objects are ""close"" in some natural sense.""419939,""Russell, Alan"
"424867"	"Sahraoui, Houari"	"Addressing issues in software engineering automation using visualization and search-based methods"	"The use of software is now part of our everyday life and the process of developing it has gone through several technological changes. Of the various research directions that have been studied, automation of Development and Maintenance Activities (DMAs) is the one that has gathered the most support. Automation helps reduce the complexity of such activities; for example, Model-Driven Engineering (MDE) approaches concentrate the development and maintenance effort on models and use automated transformation mechanisms to generate/modify code. Many factors limit the automation of DMAs. One of the most important ones is the lack of reliable and widely applicable knowledge for many activities. The lack-of-knowledge problem is amplified by the complexity of the DMAs. Starting from these facts, our research objectives are to propose alternatives for DMA automation that address/circumvent the lack-of-knowledge and complexity problems. More specifically, we will investigate the use of metaheuristic techniques that consider some DMAs as optimization problems, we will develop interactive visualization techniques that allow experts to compensate for the lack of knowledge by providing contextual knowledge in the context of complex tasks, and we will propose techniques that manage the uncertainty inherent to the lack of knowledge.""420968,""Saif, Mehrdad"
"431945"	"Salvail, Louis"	"Two-party quantum cryptography"	"In 1984, Bennett and Brassard proposed a scheme allowing two parties to agree on a secret key provided they have access to a channel capable of transmitting quantum bits. This scheme, called quantum key distribution, is remarkable since it achieves something impossible through classical means alone. Until the mid 90's  quantum information appeared to be cryptographers' best friend. Expectations were that all  modern cryptographic applications could be resolved  unconditionally using quantum communication whereas classical cryptography could only hope for security relying upon unproven computational assumptions out of which the majority do not hold against quantum adversaries. One essential class of cryptographic applications are the ones involving two parties willing to collaborate but otherwise not trusting each other. Many daily electronic transactions are of that kind, making two-party cryptography a critical tool for the information age. An example is our frequent interactions with ATMs asking us to identify ourselves before being granted access to our bank account. In 1995, Mayers and Lo & Chau shattered this dream when they prove that the most important cryptographic primitives for two-party cryptography cannot be implemented securely using quantum communication. On the other hand, quantum protocols have been found for weak two-party primitives otherwise impossible using classical communication alone. Unlike for key distribution, the capabilities of quantum cryptography in the two-party setting  are not well understood. The project's main goal is to provide a deeper understanding for the true power of quantum cryptography in two-party applications. Improving our insight on what it can achieve in this setting could have important consequences for the design of systems providing security in electronic transactions. New systems inspired by this research could offer privacy more efficiently and under weaker assumptions. Even entirely classical protocols could benefit from this work by extending the realm of their security to adversaries in possession of quantum computers. Any satisfactory advance toward our main objective would improve our comprehension of the differences between classical and quantum information.""432302,""Salvante, Katrina"
"421372"	"Sebak, Abdel"	"Ultra wideband and electromagnetic band pass enhanced antennas"	"The telecommunications sector has been one of the fastest growing sectors in the past decade. This trend has been fueled by an unprecedented growth in the number of new applications in wireless communications and the expansion of existing ones. With rapid growth, managing the capacity and quality of the wireless air interface becomes a challenge. Ultra-wideband (UWB) radio is emerging as an innovative solution for the increase demand for short-range low cost reduced-power wireless devices with video transfer capability within home and office environments. The primary objective of the proposed research program is to address the above requirements and playing a leading role in the development of innovative solutions with focus on two innovations. First, to study, design and implement novel UWB low profile antennas that satisfy UWB technology requirement. Research in this area will focus on theoretical and experimental studies to overcome many challenges for discovering new design rules, new innovative antennas, and new application areas. To meet wireless industry requirements for small size antennas in handheld devices, our efforts also include miniaturization process that compromises between the size and performance of the antenna. Second, we investigate the basis for the design of new Electromagnetic Band Pass (EBP) dielectric waveguide-based antennas and microwave devices. EBP periodic structures allow energy to propagate with certain properties to form low cost, high performance and fully integrated circuits, antennas, and systems for emerging wireless telecommunications and microwave imaging applications. We will investigate EBP pass band characteristics as opposed to their stop band or band gap characteristics and develop EBP-based novel devices for a broad range of emerging applications. Trainees involved in this program will gain advanced knowledge area expertise and technical competency in wireless systems related areas. The developed technologies integrated with intelligent networks have the potential to offer businesses and consumers greater control over communications environment resulting in increasing productivity and improving the efficiency and quality of services.""433127,""Sebak, Safaa"
"425969"	"Shakshuki, Elhadi"	"Cooperative intelligent distributed systems for context aware and calendaring systems"	"1) Enhanced context aware with agents, WSN and RFIDEntities, ranging from any kind of objects to users, are equipped with WSN and RFID tags as the means for gathering their context information. The integration of agents, WSN technology and RFID that provides unique identifiers explores a new technical horizon leveraging the simultaneous ID and sensor data manipulation intelligently. Our aim is to investigate how to simplify and enable on-demand access to RFID stored information and integrate it with the information sensed about the environment to enable identity and context aware applications, also to enhance WSN efficient algorithms with RFID. Our research will lead to efficient, intelligent and cost-effective context aware services, which would involve integrating agents, WSN and RFID. The outcome of this research will improve the quality of health care such as medication error reduction and also provide merit to applications such as accurate location tracking and condition monitoring of products. 2) Intelligent calendaring management system Despite the widespread adoption of calendar or schedule management software applications, they still require significant user input, especially when users are attempting to schedule group meetings. One of the main challenges associated with this application relates to conflict resolution. The aim of this research is to significantly enhance the calendaring experience by developing an agent-based software application that will automatically and autonomously (with built-in user control) book meetings on behalf of meeting originators and participants. The ultimate goal of this research is to bridge the gap between all calendaring applications. Agents do not override user preferences or book meetings before the user has authorized the agent's decision.""424310,""Shalaby, Ahmed"
"427499"	"Shatkay, Hagit"	"Translating data into knowledge: From biomedical sequences and text to disease prediction and prevention"	"The tremendous amount of data produced since the sequencing of the human genome raise hopes for understanding biological processes underlying common complex diseases and for curing them. To effectively use the data and realize these hopes, computational analysis tools - carefully tailored for specific biomedical needs - must be developed. In broad terms, the goals of the proposed research are to develop computational, intelligent, statistical data analysis tools for biomedical data, and to model complex biological systems in a way that supports explanation and prediction of disease.In the past few years, I and my group, in collaboration with prominent researchers throughout the world, have developed several highly effective tools that address eminent current problems in biomedicine. For instance, we developed SherLoc, the most comprehensive and accurate system reported to-date for predicting proteins' location within the cell, by utilizing text and sequence data. We introduced F-SNP - an integrated system for assessing and predicting the functional impact of small mutations on the genome, known as Single Nucleotide Polymorphisms (SNPs). We also proposed a new way to use image data within published documents as a step toward well-targeted biomedical text mining.Building on our proven success in effectively using and integrating text and sequence data, I propose to develop methods for identifying biologically informative areas in text and in images within scientific publications, as well as highly informative functional mutations on the genome. I then plan to specialize and apply the resulting tools to a wealth of disease-related data (specifically Autism and Cancer), supporting early prediction, intervention and effective drug-design.""431891,""Shaver, Michael"
"435086"	"Sherif, Sherif"	"Design and implemenataion of novel biophotonic computational imaging systems"	"We will design and implement novel biophotonic computational imaging technologies that will go beyond the current capabilities for non-invasive structural and spectroscopic imaging of biological samples.We will develop advanced statistical models for the interaction between light and tissue. These models will be used by the biomedical industry to improve the accuracy of killing brain tumors using lasers. We will also use these models to design and implement digital algorithms to increase the penetration depth and improve the extraction of structural and spectroscopic information in optical coherence tomography (OCT). We will design and implement next generation OCT technology: Integrated computational OCT with considerably higher penetration depth than the current systems.  We will develop an integrated computational method to extend the depth-of-field, without loss of resolution, in very high magnification optical microscopes, as well as a method to significantly increase optical efficiency in wide-field OCT. We will collaborate with researchers from the University of Manitoba to image: 1) Axolotol embryos to understand embryo development, 2) changes and movement of nuclei in plant cells in response to fungal infection, and 3) muscle fibers growth to understand degenerative muscle diseases.This program will create unique opportunities to train students and researchers in both theoretical & practical aspects of biomedical optics, image processing and spectroscopy.""442878,""Sherk, Douglas"
"435046"	"Shi, Wei"	"Distributed computing with mobile agents in unsafe networks"	"The wide acceptance of Internet-based technologies, the pervasiveness of transaction-based techniques in a multitude of systems (such as telecommunication ones), as well as the emergence of Grid systems, all strongly emphasize the paramount importance of distributed algorithms in today's computing world. Also, the use of mobile agents is becoming increasingly popular when computing in networked environments. But the presence of security threats greatly complicates solutions for all but non-trivial tasks. That is, in systems supporting mobile agents, security is the most pressing concern, and possibly the most difficult to address. The overall objective of this research consists in developing efficient and testable distributed algorithms that use mobile agents to solve non-trivial tasks in unsafe networks, that is, in the presence of security threats. The proposed research is organized into three broad goals: Goal 1: Investigate distributed algorithms in the presence of harmful hosts;Goal 2: Investigate distributed algorithms in an unsafe network using the mobile faults model.;Goal 3: Develop a testable model for the validation of distributed algorithms in an unsafe network. Goal 1 specifically focuses on networks with harmful hosts. We are particularly interested in solutions that constrain as much as possible the requirements imposed on agents and network (in order to maximize the feasibility of the realization of such solutions on actual networks supporting agents). More specifically, we aim for algorithms that are as efficient as possible while not requiring loosely constrained (possibly unrealistic) mechanisms. All work pertaining to Goal 1 is rooted in the use of the processor failure model, that is, a model in which faults are localized in nodes of the network. In contrast, Goal 2 adopts the more general mobile faults model. Goal 2 is two-fold: a) investigate how to adapt distributed algorithms that use the processor failure model in an unsafe network to instead use the mobile faults model and b) develop new efficient distributed algorithms in unsafe networks that adopt the mobile faults model. Finally, Goal 3 addresses the testing of distributed algorithms in unsafe networks.""435698,""Shi, Xiaomeng(Shirley)"
"421607"	"Shpak, Dale"	"Digital signal processing and filter design"	"Digital signal processing (DSP) enables many technologies that are in widespread use including cellular telephones, wireless data networks, digital audio, digital video, speech coding, voice and video compression, etc.  In data networking applications, DSP is used to reduce the effects of noise, interference, and signal distortion so that higher data rates can be achieved along with lower error rates.  In compression applications, DSP is used to analyze and modify signals and thereby reduce the number of bytes required to store or transmit the signal.  In audio and video equipment, DSP is used to enhance the perceived quality of the signal.This proposal requests support for an ongoing research program that includes the optimal design of the digital filters that are used in these applications.  This research program has resulted in several widely-used filter design algorithms.  A second aspect of this proposal is for applied digital signal processing, with the current application being the frequency analysis, modification, and resynthesis of music signals for both studio applications and live performance.""432335,""Shramchenko, Vasilisa"
"419483"	"SidAhmed, Maher"	"Image processing and pattern recognition technique"	"The extraordinary discoveries reported in the areas of Image Processing, Pattern Recognition, Machine Vision, Multimedia Systems and the Universal Adaption of the Internet, have created a heavy demand for economical, fast, adaptable, accurate and efficient Image Processing, and Pattern Recognition Systems. The design, hardware implementation and applications of image processing and pattern recognition systems have received considerable attention in recent years. Yet as technology advances and microelectronics hardware becomes faster, more versatile, and more sophisticated, new problems, new solutions, and new opportunities emerge. Furthermore, many of the difficult to solve problems of the past are beginning to find efficiency and viable solutions now.The aim of this research is to develop:i)    )Improved definition television (IDTV) techniques and their hardware implementation.ii)    )Motion estimation technique and their hardware implementation for the application of IDTV.iii)    )New Image processing techniques for thresholding application using Neural Networks and Hidden Markov Model (HMM) for document processing application and Inspection of Manufactured parts.iv)    )New face recognition algorithms for different illuminations conditions using modified HMM and Neural Network.This work is considered significant as further progress in a number of diverse applications beneficial to Canada contingent upon developing efficient and high accuracy software and low cost, high speed hardware image processing and pattern recognition systems with near real time processing capabilities.""434273,""Siddiqi, Kaleem"
"425142"	"Silver, Daniel"	"Inductive transfer and machine life-long learning"	"Machine learning research has focused on inducing a model of a function or classification task strictly from a set of training examples.  Consequently, machine learning systems do not take advantage of previously acquired task knowledge when learning a new and potentially related task.  This is unfortunate since prior task knowledge offers the potential to produce more accurate models in less time and using fewer training examples.  My research investigates machine life-long learning (ML3) systems (particularly, architectures of artificial neural networks) that are capable of retaining and selectively transferring task knowledge when learning a new task. Data mining and adaptive software systems can benefit from the use of inductive transfer and ML3 methods.  For example, more accurate medical diagnostic models can be developed from small samples of patients by using previously learned and related diagnostic models.  Adaptive systems can learn more efficiently the interaction model (profile) of a new user by selectively transferring knowledge from similar user models.    This research program will explore several questions raised by my most recent research into the use of multiple task learning (MTL) methods: What characteristics must a machine learning method have to affect inductive transfer using contest sensitive MTL (csMTL) encoded examples?  Under what conditions will the csMTL approach fail? What is the best use of available training examples?  Does the csMTL method mitigate the need for a measure of task relatedness?  Can the approach be used for domains with real-valued outputs and multiple outputs per task?  Can csMTL be used to accurately consolidate task knowledge?     The return on investment in this research will be advancements in machine learning theory, the creation of a valuable machine learning research system that will be shared with the community, the training of several high quality researchers and the application of next generation machine learning to data mining and adaptive software systems.""437604,""Silver, Dylan"
"427301"	"Sirouspour, Shahin"	"Hardware-ased parallel computing tools for realtime haptic and deformation rendering of soft objects"	"Interactive virtual reality simulators are becoming increasingly realistic by incorporating sensing modalities such as vision, kinesthesia and force feedback, also known as haptics. Interactions with non-rigid deformable objects often occur in medical applications involving biological soft-tissue. While a great deal of progress has been made in physics-based mathematical modeling of these interactions, practical applications of such models in real-time simulations have remained limited mainly due to their computational complexity. In particular, applications with haptic feedback would require very high simulation update rates to maintain the system stability, imposing strict timing constraints beyond the capabilities of existing single-processor computers. Building upon our previous work in haptics, this research explores a new paradigm in parallel computing for real-time high-fidelity simulation of soft-object interaction involving visual and haptic feedback. Customized computing tools will be developed that concurrently employ thousands of processing units to solve a large sparse system of equations arising from the finite element models of soft-object deformation. The solution will be obtained at sufficiently high update rates for simulation stability and fidelity. These computing tools will be essentially parallel implementations of recursive equation solvers on synthesized hardware architectures using the Field-Programmable Gate Array (FPGA) technology. Massive parallelization of the computations on multiple interconnected FPGA chips will provide enormous computing power in an inexpensive and compact package enabling the simulation of high-fidelity soft-tissue deformation models. The results of this research will be instrumental in the development of the next generation of computer-assisted surgical systems and medical training simulators. The integration of the resulting core computational technologies into such systems will create improved and new operational capabilities in computer-assisted medical training, diagnosis, and planning and execution of medical interventions. Other scientific applications requiring fast solutions to sparse linear systems of equations will also benefit from this research.""420227,""Sislian, Jean"
"425403"	"Sivoththaman, Siva"	"Development and practical implementation of spectral engineering and nanotechnology concepts for high efficiency photovoltaic devices"	"This research program will develop novel and practically implementable technologies involving advanced device architectures and nanotechnology concepts and combine them with silicon (Si) wafer-based photovoltaic (PV) technologies to demonstrate high efficiency (>25%) PV devices. The research aims at overcoming some fundamental performance limits of Si solar cells by broadening the harvestable range of solar spectrum via implementation of spectral engineering concepts. Several advanced device configurations including thin films, nanostructures, and quantum dots, in combination with wafer-based technologies, will be developed and demonstrated.The major components of this far-reaching 5-year research program are; (1) Development of low temperature Si thin films with high crystallinity and controlled doping, (2) Simplified, mask-less fabrication method for Si nanowires (NW) in a top-down process resulting in size-controllable, removable, and manipulable crystalline NWs; (3) Development of Si quantum dot arrays in a relatively simple technology that is easily implementable for PV cells, (4) Development of techniques for polymeric layer embedment of optically tuned CdSe quantum dots and Si NWs for device level deployment; (5) Design and fabrication of a new 3-terminal PV device involving epitaxial thin films combined with Si substrates; (6) Development of a simple methodology to form arrays of Si quantum dots for bandgap modification; and (7) Demonstration of new high efficiency PV cell architectures on Si wafer platforms with photon shifting layers of nanowires and quantum dots.  The proposed work will result PV devices with demonstrated efficiencies exceeding 25% in Si wafer devices, and, more importantly, through techniques that are simple and up-scalable. The research outcome is expected to enable transfer of the new technologies to manufacturing levels in the short-to-medium term. ""428702,""Siwick, Bradley"
"427651"	"Skorobogatiy, Maksim"	"Guided micro- and nano-photonics for unsolved fundamental problems of today and industrial applicaiton of tomorrow"	"Design and fabrication of Photonic Crystal Fibers (PCFs) is currently booming due to a number of fundamental, yet practical discoveries that were made in this field including: supercontinuum generation, hollow core guidance of light at virtually any wavelength, bi-stable switching, fiber lasers at exotic wavelengths, compact and ultra-sensitive integrated plasmonic sensors, etc. Moreover, it is widely recognized that photonic crystal fiber fabrication technology is the most mature compared to the fabrication technologies of 2D and 3D photonic crystals. Therefore, PCFs present a unique platform for making groundbreaking fundamental discoveries, while also bringing such discoveries to commercialization in short-to-medium term.        Our global objective is to exploit the key advantages offered by the PCFs and Photonic Crystals (PCs), in general, to enable novel fundamental and applied applications related to the flexible light delivery and its use in compact and ultra-sensitive sensors anywhere form the visible to THz frequency range. A strong focus in my group is on the development of industrial strength fabrication techniques for potential commercialization of the discovered PCFs and PCs. As a direct result of the last Discovery Grant we have established the following promising applied research objectives which we would like to continue exploring: I. Short term. Design and fabrication of the plastic Photonic Band Gap fibers for applications in (a) short-range data communications, (b) label-free sensing in the visible and near-IR. II. Medium term. Design and fabrication of (a) photonic crystal waveguide-based Surface Plasmon Resonance sensors for portable bio-chemical sensing in the visible and near IR, (b) highly porous, sub-wavelength, and hollow core PCFs for low loss guidance of the mid-IR and THz light. III. Long term. Investigation of the use of ferroelectric polymers and nano ceramic impregnated polymers for the design of mid-IR and THz (a) PCFs and PCs, (b) pseudo-plasmonic sensors.""424575,""Skorobogatiy, Maksim"
"435092"	"Smith, James"	"Robotic orthoses for gait augmentation"	"Human orthosis design is generally restricted to stiff and passive mechanisms.  Given the presence of biarticular muscle-tendon complexes which are important in energy transfer and joint function coordination, improvements to these orthoses may be achieved through mechanical compliance, damping, motors and generators which mimic and augment these characteristics.  Robot surrogates can be used to provide insights into normal and pathological locomotion and can be used as robust and deterministic design and test platforms for these next generation orthoses. Advances in powered prostheses or orthoses, such as the BLEEX exoskeleton, have relied on power-hungry technologies, requiring the use of batteries or other power sources.  Additionally, generative knee orthoses  have undergone human trials and been shown to be capable of moderate conversion of human subject kinetic energy to electrical energy. However, in neither of these cases has electrical energy transfer been implemented at an inter-joint  level. The long term objective is the development of robotic orthoses for gait correction and augmentation which are suitable for daily use in home and out-patient contexts. In the short term, a robotic test platform will be developed which is capable of simulating healthy and pathological gait. As well,  orthosis motoring and regenerative robotic subsystems must be developed which can modulate energy usage at each joint and which can transfer energy between joints, mimicking and augmenting existing neuromechanic subsystems. By applying regenerated joint power at specific points in the gait cycle it will be possible to extend the usable life of prostheses and orthoses, making them more effective. ""424784,""Smith, James"
"434960"	"Smucker, Mark"	"Utilizing interaction to improve information retrieval"	"Information retrieval (IR) has become an important part of life with people regularly using IR systems to search the web, their email, and other collections of documents.  Even with the success of commercial web search companies, people still fail to find the information that they need to answer their questions.  We propose to better utilize the power of user interaction to improve the quality of IR systems.  It is well-known that user interaction has the potential to significantly improve retrieval quality, but many user interaction mechanisms fail to be adopted by either commercial search providers or by the users of search systems.  To create interaction mechanisms that are both powerful and adopted, we propose to create a collection of simple mechanisms that can be easily added to existing search systems.  These simple interaction mechanisms, when taken as a whole, should speed the rate at which users can find answers to their questions.  The proposed research program will build on our past work where we have shown that a widely adopted, simple interaction mechanism, ""find-similar"", has the potential to boost retrieval quality by over 20% compared to a state-of-the-art non-interactive retrieval system.  We first plan to validate via user studies our earlier simulation based results for the find-similar interaction mechanism.  Next we intend to develop new interaction mechanisms that will speed the rate that users can answer complex questions with an IR system.    Finally, we will create new interactive IR evaluation techniques by further merging automated usability testing methods (e.g. user simulations) with traditional IR evaluation methods.  The research program will follow a cycle of formative design of interaction mechanisms followed by summative user testing.  Each part of the cycle will feedback into the other.  Applying this process to the problem of complex question answering, we expect to achieve a 50% or greater improvement over non-interactive retrieval.""421434,""Smy, Tom"
"427628"	"Somayaji, Anil"	"Securing software with automated functional diversity"	"Computer security remains a ongoing challenge for most computer users.  Existing technologies such as network firewalls, virus scanners, and automated software updates defend against many attacks.  Despite this, virtually every computer connected to the Internet has the potential to be compromised at any time due to administrative error, user mistakes, and unpatched software vulnerabilities.  As there is no ""silver bullet"" for computer security, this situation will continue for the foreseeable future. Thus, if we wish to secure the Internet, we need to develop defense strategies that can tolerate imperfections in our computer systems and in the humans who work with them.     One strategy for addressing this problem is software diversity. The idea is if different computers run different software, then attacks that work against one won't necessarily work against others.  Many researchers have recognized the problems of the ""software monoculture,"" and there has been some work on automatically adding diversity to computer systems.  Past diversification approaches, however, only addressed a few classes of software vulnerabilities because they only changed the low-level details of programs: to a regular user, everything appeared exactly the same as before.  As an alternative, we plan to develop techniques for functional diversity, i.e., ways to make software behave differently on different computers.     We plan to study two approaches to functional diversity.  One approach is based on anomaly detection: a computer should detect unusual behavior (by users or programs) and then react to those changes in a useful but highly variable way.  The other is to recombine different versions of a program together to produce new, slightly different versions, much as living things create new organisms through sexual recombination.  Through a combination of these processes we believe we can create computers that are specialized to their specific uses and thus are resistant to generic, mass attacks.""425822,""Somé, StéphaneSotèg"
"419911"	"Sood, Vijay"	"FACTS applications in electric power systems"	"This proposal concerns primarily the modelling and simulation of power electronic voltage source converters (VSC) using intelligent or advanced control techniques to meet the increasingly complex and integrated utility demands. Such converters are mostly employed in the flexible AC transmission (FACTS) systems.The use of FACTS based devices is needed in the next generation of integrated power system to control power flow over specified rights of way. Although this is an area of active research in academia, little is being done at the utility level to provide suitable models and tools to evaluate the performance and impact of such devices.Since hardware implementation of these controllers is likely to be very expensive, the software modelling approach is being relied upon. The use of software tools, such as EMTP RV and other power system packages, which are standard tools within the industry (i.e. PSSE) are being used to (a) verify the models, (b) provide valuable guidance to the industry, and (c) to train the next generation of utility engineers. Strong links to local industry have been established so that the proposed research work will have a practical element.""433241,""Soontiens, Nancy"
"426973"	"Soutchanski, Mikhail"	"Modular and taxonomic representations for tractable reasoning about actions"	"A long standing and fundamental problem of AI research is this: the development of a comprehensive and general theory for representation and efficient reasoning about actions of agents operating in uncertain and incompletely known large-scale domains. The proposed research will contribute towards this long-term goal by eliciting structured logical representations and  subsequently elaborating them to guarantee efficient reasoning in structured domains. The proposed research focuses on representations of actions formulated in order-sorted first order logic and in description logic-based languages. The primary objective of the proposed research program is the theoretical development of those representations of actions and their effects that can be characterized as  modular, taxonomic or structural and the experimental assessment of these representations on solving the common reasoning tasks. This research is original and innovative for several reasons. First, it emphasizes the development of logical theories of actions that exploit the declarative structure of application domains: taxonomies of actions and objects.  Second, it proposes to integrate taxonomic reasoning about actions and about objects with order-sorted logics and modular description logic ontologies. Finally, it proposes to use these representational leverages to solving challenging and important problems in AI, including decision-theoretic planning and learning. While the proposed research program will help to advance our knowledge, it also has a significant promise of developing new logical frameworks, information technologies (and subsequently, software products) that will be relevant for a broad range of applications including health-care, Semantic Web services and industry.""431101,""Southam, Gordon"
"421248"	"Stacey, Deborah"	"Semantic web framework for software system design, testing and maintenance for syndromic surveillance applications"	"Syndromic surveillance is the monitoring of the health of a population using datastreams such as emergency department visits, calls to telehealth, over-the-counter drug sales, laboratory reports, demographic information, etc. One major activity in this surveillance is the use of aberration detection algorithms to alert public health officials to a possible disease outbreak situation. While these systems have been successfully used in the field, they have not proven to be easy to extend and thus many new and interesting aberration detection algorithms are not being tested and deployed. This research program is developing a framework to aid in the development of aberration detection software systems using the concepts and tools of the semantic web.  The research will conceptualize the entire process of information system development in the form of an ontology for algorithm selection, composition, deployment, testing and evaluation. The resulting ontology and related reasoning systems will facilitate the abstraction of the information system development process into a form of description that is accessible to the public health domain expert (or in fact, any domain expert who does not have software expertise).  A proof of concept will be developed around the case of stochastic algorithms such as artificial neural networks (ANNs). ANNs are often perceived as complex, mysterious and too difficult to handle for the end user. There are many stages in the design, deployment, testing and evaluation of ANNs that are distinct from other types of algorithms  and an ontological description of this could be used by an information system development system to correctly integrate ANN methods into an aberration detection system. A further ontological description of the implementation phase of an algorithm could also be developed that would facilitate the use of distributed computing resources by a surveillance system in a method that is transparent to the user. Once again, this is another area (Grid or Cloud Computing) that is overly complex for the end user but has great potential to extend the computational capability of many applications (particularly in the life sciences) that often do not take advantage of available computational resources.""432401,""Stacey, Melissa"
"423895"	"Stroulia, Eleni"	"Systematic, managed evolution of service systems"	"The objective of this project is to develop a framework for understanding, planning and managing the evolution of Service-Oriented Applications (SOAs). The term ""service"" is an interestingly ambiguous abstraction, representing (in Computing Science) well-defined, network-accessible hierarchically composable software modules, and (in Business) value co-production between organizations and customers, frequently supported (or even driven) by software. I strongly believe that this ambiguity signifies a ""phase transition"" in the business of software engineering, where strategic business decisions on ""what new service processes to deliver"" imply the need for (and are enabled by) the evolution of the SOA driving the current processes. The proposed work will develop a framework for managing and systematically supporting the evolution of service systems. I take a broad view of ""service systems"" including software (designed in an SOA style), hardware and interaction mechanisms with external people and systems, in order to address two fundamental questions: (a)how should service systems be designed, managed and evolved to support business strategy? and, (b)how should business strategy be informed by the evolution potential of the service system  supporting the business? To that end, the three project activities focus on developing a strong foundation for analyzing and understanding change in SOA systems, developing a method for systematically planning for change, and analyzing the costs and benefits of potential changes in both technical and business terms.""442919,""Stroulia, Eleni"
"428929"	"Swamy, Chaitanya"	"Algorithm design in game-theoretic environments and settings with uncertainty"	"As information and communication systems become increasingly complex, there is a growing need for the development of efficient algorithmic tools for the design, maintenance and management of such systems, especially decentralized networks like the Internet and high-speed telecommunication networks. This research program will investigate the algorithmic questions that arise in such settings, focusing on two main problems: (a) The design of protocols that need to function in the presence of self-interested agents (e.g., autonomous subnetworks on the Internet). In such settings, the task of algorithm-design is complicated by the fact that the data relevant to the problem is often controlled by self-interested agents. Consequently, one needs to elicit this data by cleverly incentivizing the users so that self-interested behavior leads to desirable outcomes. We shall focus on the development of techniques for the design and analysis of such efficient ""manipulation-resistant'' mechanisms. A particular emphasis will be to identify ways for ""exporting'' algorithms into manipulation-resistant mechanisms, and thus contribute to the field of algorithmic mechanism design.(b) Decision-making under uncertainty. Uncertainty is a facet of many decision environments, and for effective decision making, one needs to consider models that explicitly incorporate the uncertainty. One of the broad objectives of our research is to explore models that abstract such settings, and devise methods for solving the algorithmic problems that arise in such models. We shall study key algorithmic problems that capture the underlying difficulties in such settings, and develop techniques that will find use in a variety of settings.  In many cases, the algorithmic problems encountered are computationally intractable, and our approach will be to devise an approximation algorithm for the problem, that is, a polynomial-time algorithm that always delivers a provably near-optimal feasible solution. Approximation algorithms are a common thread in our research, and our research will also further the development of techniques for the design and analysis of these algorithms.""420464,""Swamy, Srikanta"
"421996"	"Szymanski, Ted"	"Near-perfect quality of service in wired and wireless IP networks"	"This discovery grant will explore a new scheduling algorithm which can achieve near-perfect Quality of Service (QoS) over general packet-switched networks and infrastructure-based Wireless Mesh Networks (WMNs). The Internet has had an unprecedented impact on society over the last two decades. Compared to other disruptive technologies such as radio, telegraph, telephone, television, and the personal computer, the Internet has had one of the fastest adaptation times to reach the majority of the North American households. The outlook for new services over the Internet is tremendous, and these include telemedicine, telerobotic systems and Intelligent Highway System. However, there are significant unsolved technical problems to be addressed. In 2007, the US National Science Foundation (NSF) has embarked in a new program called 'Global Environment for Network Innovations' (GENI), with the goal of addressing the problems that have 'plagued the Internet' since its inception. These problems include Quality of Service, security, reliability, and the seemless integration of wireless technologies. These problems are so complex that the GENI program is open to a 'clean-slate' approach in which the entire Internet may be re-designed from ground up. This applicant has developed a new scheduling algorithm. The algorithm can achieve near-perfect Quality of Service for statically provisioned multimedia traffic flows in general packet-switched networks such as the Internet, including the wired IP networks and infrastructure-based Wireless Mesh Networks. Funds are requested to explore this algorithm in more depth.""437098,""Szyszka, Beata"
"424891"	"Tadj, Chakib"	"Design of a pervasive self-managing computing system"	"The human means of communication have a significant characteristic: it is possible to make cooperate (i.e. to use simultaneously and in a complementary way), several modes of communication both in perception and in production. The use of this possibility should bring more speed, effectiveness and comfort to the user-machine interaction, especially for the people suffering from a specific handicap or sensory deficiency. Therefore, the use of multimodal applications would be an appropriate and powerful solution.By multimodal we mean systems combining natural input modes, such as speech, touch, gestures, etc. A comprehensive command or a meta-message is generated and sent to multimedia output devices.In a pervasive multimodal computing system (PMCS), the system is expected to provide an infrastructure that allows the user to continue working on a computing task whenever and wherever he wants, using the appropriate modality and media devices. The system is expected to adapt seamlessly to the evolution of user's interaction context, the variations and constraints in computing resources, and the possible failures of system components and/or connectors.In the other hand, autonomic computing (AC) is the perceived solution in solving complexity in large computing systems. In the proposed research project we intend to (1) propose a new model for measuring quality of context (QoC) in a PMCS, (2) model the dynamism in a PMCS and its architectural software representation, (3) design a fault-tolerant system that satisfies the functional and quality attribute requirements, and 4) a service ontology for context modeling and a context-aware service adaptation.It is our goal that over a long period of research, we would have developed mechanisms and principles leading to the application of AC in pervasive multimodality.""434004,""Tadjalli, Arash"
"429080"	"Tardos, Gabor"	"Complexity issues in geometric and combinatorial arrangements"	"One major research direction of this proposal is the study of arrangements of simple geometric objects, such as lines, segments, disks, triangles, etc. and find out what makes them simple or complex, when can they avoid certain patterns and answer many similar questions.A simple example is the study of the complexity of the union of geometric regions. The union of n line segments can partition the plane into a quadratic number of disconnected regions, but the complement of the union of n discs is consist of just a linear number of regions and the boundary can be described in linear time (linear complexity). Long and narrow triangles behave like segments, while ""fat"" triangles behave like circular disks with regard to the complexity of their union. I propose to conduct research in three and higher dimensions where there exist some partial results on the subject but the situation is much less understood than in the planar case.Another problem I propose to study is the conflict-free coloring problem. Assume a few radio stations (with known reception ranges) are used to broadcast a station and one wants to assign frequencies to these stations. A user can only receive a station if he is within the reception range of that station but outside the range of all other stations that broadcast on the same frequency. What is the minimum number of frequencies that must be used if we want each user within the range of some station to be able to receive some station? What is the worst case number of frequencies needed as a function of the broadcast stations? The answer depends on the geometric shape of the broadcast regions. It is well understood for circular (disk) regions and somewhat well for axis-parallel rectangles, but a lot of further study is needed to tackle the problem for other types of regions.""423740,""Tardos, Gabor"
"428780"	"Terry, Michael"	"Open source usability"	"In the past ten years, open source software (OSS) has grown from a grassroots community of hobbyists to an integral part of business, government, education, and research. However, the successes of OSS have largely been in the area of systems software, software such as operating systems, databases, and web servers. My research interest is in making open source software more usable by improving the open source software design process.My long-term research goal is to develop self-sustaining usability infrastructures for the open source community that require little cost to set up and maintain, yet which demonstrably improve usability practices. This goal encompasses two high-level research problems not previously investigated by the human-computer interaction community: 1) How to transform existing usability tools and techniques to function effectively within the resource-constrained, distributed, and public development environments of the OSS community, and 2) How to leverage the unique features of the OSS community to yield new usability tools and methods not possible in closed-source software development. Importantly, the open source community presents a unique opportunity for developing new human-centric software design processes that are not limited by the constraints of the marketplace or the need to sell the software. My research directly capitalizes on these opportunities.""430628,""Tersigni, Andrew"
"419917"	"Therien, Denis"	"An algebraic approach to computational complexity"	"The formalization of the notion of computation represents one of the great intellectual achievements of the last century, and this advance is indissociable of the technological revolution that has so profoundly transformed our world.A most useful approach to refine that notion is to classify computations according to the amount of resources that are required to produce their solutions (for example time and memory space). My work has been concentrating on the study of the relationships between complexity classes induced by this classification, using sophisticated mathematical tools, most notably from algebra.""435650,""TherouxRancourt, Guillaume"
"420798"	"Toussaint, Godfried"	"Computational geometry for polygonal reconfiguration, pattern analysis and recognition and music information retrieval"	"The proposed research aims to develop efficient algorithms for solving geometric problems that arise in the following areas. (1) The exploration of algorithms for the reconfiguration of different types of linkages in 2 and 3 dimensional space, under various restrictions on the types of motions allowed. Another goal is to explore under what conditions linkages are ""stuck"" in the sense that they cannot be reconfigured to a flat convex configuration. These results are relevant, not only to robotics, but to protein folding in molecular biology. (2) One of the most promising approaches to pattern recognition is the nearest-neighbor decision rule (also referred to as instance-based learning). It is proposed to improve the space and time efficiency of state-of-the-art nearest neighbor rule algorithms, by incorporating proximity graphs. A second objective is to develop new measures of string similarity and polygonal chain similarity for pattern recognition problems. (3) It is proposed to explore the application of computational geometric tools to problems that arise in music information retrieval and music theory. These problems range from measuring the similarity of rhythms and melodies, to performing cluster and phylogenetic analyses of families of rhythms, with the goal of obtaining a deeper understanding of both, music theory and practical applications to music information retrieval. (4) Traditionally the scientific analysis of textiles has been carried out on several physical structural levels of the fabrics. However, a scientific analysis of the geometric structure of the patterns that appear on the textiles has been largely ignored. We propose a new approach to the scientific study of textiles that breaks with this tradition: the phylogenetic analysis of geometric patterns that decorate the textiles. At the heart of our approach is the design of a measure of dissimilarity between two textile patterns, that mimics the way in which biologists measure the dissimilarity between two DNA molecular sequences, i.e., the minimum number of mutations (simple local and global transformations) required to transform one textile pattern into the other. This research will have an impact of image-based search on the internet, as well as textile retrieval systems for use in museums and libraries.""443461,""Toussaint, Kristopher"
"419921"	"Tsotsos, John"	"Active vision, visual attention, and their application"	"My research has 3 major thrusts in a lab of 14 people (10/2008): visual attention, mobile robotics, and computer vision. The overall objectives are two: 1. To further understanding of visual attention in humans and show how that understanding leads to better machine vision systems, and, 2. To develop the technology needed to help people with mobility impairments. For the first objective, there is the promise that a better understanding of human visual attention may lead to better understanding of visual and attentional mechanisms and thus, the failures that lead to serious problems such as neglect or degraded visual performance in the elderly or stroke victims. There is also the promise of more robust computer vision systems that may truly match performance of human vision in real world tasks. For the second objective, we hope to contribute to the solution of how to best allow people to age gracefully, in their homes or institutions, with mobility assistance and proper monitoring. To this end, the bulk of our work focuses on the development of an autonomous, intelligent, visually guided and visually monitored, wheelchair and robotic manipulator.""423468,""Tsotsos, John"
"424828"	"Ulieru, Mihaela"	"Emergent engineering: design of evolvable architectures for large scale enetworked systems"	"How to craft the nervous system of our eNetworked world to enable seamless evolution through challenging times and adaptation not only to meet the unexpected and survive it, but thrive on it? Recognizing the fundamental limitations of engineering in dealing with highly complex systems, we frontally approach the radical shift of paradigm, brought by the ubiquity of computing and communication environments that link systems and people in unprecedented ways via eNetworks. One major characteristic of these new technological developments, termed here Cyber-Physical Ecosystems (CPE) - is that, given their very nature, they cannot be a-priori defined - but rather emerge from the interactions between individual components, interactions facilitated by the eNetworks. This challenges the traditional mainstream engineering school of thought in its very core in disruptive ways. In essence, the traditional active role of the designer in crafting the organizational, systems and software blueprint 'with the end in mind', namely by a-priory defining the system and its performance requirements following a hierarchical, 'top-down' linear thinking is being fundamentally challenged by the emergent behavior of eNetworked CPE which demand the designer to rather take the passive role of observer in the process of 'self-design' by which the system emerges its organizational structure resulted from the interactions between a myriad of elementary components. We aim to unravel an 'eNetwork DNA' and put it to work similarly to how DNA molds the fundamental cells in natural systems such that they can evolve to accommodate gradual or abrupt change in the environment or internal operating conditions. The aimed at new science of emergent engineering opens the perspective towards a wide range of applications critical today, including: disaster response through deployment of holistic security ecosystems, green electricity generation and distribution, pandemic mitigation, networked transportation and manufacturing, environmental monitoring and sustainability assessment.""423558,""Ulieru, Mihaela"
"422580"	"Villemaire, Roger"	"Logics, structures and verification"	"This research program addresses applications of logic to computer science. Logic is the formal study of properties; how they can be stated and verified. In computer science, logic allows to formally state properties of computer systems and gives algorithmic methods to efficiently find errors in systems' design. This research program will consider both new verification tasks and new ways of reducing verification time.In networks such as the Internet, equipments such as routers, switches, firewall etc. must be properly configured in order to exchange information in a secure and efficient way. Maintaining a properly configured network is an expensive and complex task, which takes a great part of network engineers' time. This research program extends logical verification methods to network equipments' configuration. Automating as many configuration tasks as possible, this will allow simplifying and speeding up network configuration tasks.Propositional logic has been introduced in the 19th century as a calculus for thoughts and has lead to fruitful applications such as in electronic circuits design. Recently fast algorithms for solving propositional formulas have been effectively applied to find errors in computational systems such as network protocols and processors. While being effective, these methods are limited in many cases by the formula's size. This research program considers methods for breaking down these formulas in smaller parts, in order to solve them more rapidly. This will allow error detection in bigger and more realistic computer systems.""438819,""Villemur, Richard"
"435238"	"Vu, Mai"	"Distributed communications in cognitive wireless networks"	"Cognitive wireless networks is an emerging research field. A cognitive network contains heterogeneous users with different needs and constraints, but sharing the same resources (space, time and frequency). These users often consist of the primary users, who have priority access to the resources, and the cognitive users, who usually have restricted access, subject to a constrained effect on the primary users' performance. These more constrained users, however, may use some cognition in their schemes to satisfy the constraints and achieve better performance. Cognitive networks have potentially wide applications, including cognitive ad hoc networks (infrastructure-less) and cognitive cellular networks (infrastructure-based). More generally, they can either be integrated into an existing, under-utilized network, or be operated as a new network application.This exciting area of cognitive wireless networks poses new challenges above the traditional point-to-point communications and a step beyond homogeneous networks. The simple question of how different groups of users can coexist in an wireless network has no simple answers. Involved are the issues of how to mitigate the interference among these users, how to cooperate, how to use side information, all in order to understand fundamental limits and to process information efficiently. Furthermore, because cognitive networks are often distributed, solving these issues usually requires distributed algorithms. In tackling these challenges, I plan to leverage tools from statistical signal processing, information theory, convex optimization, and communication theory. My goal is to understand and contribute to the design of efficient, emerging wireless networks. I envision building a dynamic group and creating both fundamental information theoretic results and efficient, distributed communication signal processing algorithms.""437666,""Vujanovic, Gojko"
"434963"	"Wang, Chun"	"Decentralized scheduling for multi-agent systems through mechanism design"	"This research program is concerned with the design of scheduling mechanisms for multi-agent systems. Multi-agent systems scheduling can be seen as an abstraction of the decentralized scheduling problems arising in the context of Internet-based supply chain management, manufacturing, information processing and various service industries.  It is an important research field of great practical interests.Our long term objective is to advance the design of scheduling mechanisms for multi-agent systems. In the period of the next five years, our main research activities include: (1) developing domain specific bidding languages which naturally and concisely represent scheduling objectives and constraints, (2) designing winner determination algorithms to deal with computational complexities of computing the output of a scheduling mechanism from agents' bids in auction-based scheduling, and (3) studying approximation scheduling mechanisms for the supply chain scheduling models.Attempting to tackle scheduling problems in a strategic setting, we address algorithmic and strategic challenges at the interface of computer science, mechanism design, and scheduling theory. This research program will enrich the existing understanding of complexity issues in applying economic models to the domain of scheduling. It also advances scheduling research in terms of extending classic centralized models to decentralized models. In terms of practical significances, the outcomes could provide theoretical and technical foundations for designing automated scheduling systems for those inherently decentralized application domains. We anticipate this research to contribute to the development of scheduling components in large scale internet-based trading systems and strengthen the time-based competitive advantage of Canadian industries.""424311,""Wang, ChunyanCW"
"425370"	"Watrous, John"	"Interaction and entanglement in quantum information and computation"	"Present-day computers store and process information electronically, using binary-valued components and logic gates, in a classical manner: each of the components in such a computer is in some definite logical state at each instant, and the evolution of these components from one state to the next can be described without any reference to quantum physics.  Quantum computers, on the other hand, are not restricted in this way.  Using quantum mechanics to their advantage, they may store and process information in ""superpositions"" of logical states, and can compute along many paths through these logical states simultaneously. My research focuses on theoretical aspects of quantum information and computation.  In particular, I am interested in understanding the mathematical properties of quantum information, and its potential uses in computation and cryptography.  This particular research project focuses on properties of quantum entanglement, interactions among multiple quantum computers that arise in computational and cryptographic settings, and complexity classes of computational problems that represent the power of quantum computers.Quantum information processing has the potential to bring a transformative change to the way we build and use computers, communicate privately and implement cryptographic protocols, and study the nature of quantum physical systems.  This research project will lead to a better theoretical understanding of quantum information, to new computational methods that are useful in its study, and possibly to new ways that it can be used.""422275,""Watson, Andrew"
"435061"	"West, Brian"	"Applied nonlinear plasmonics"	"All-optical data processing as an alternative to electronic data processing is critical to the telecommunications and computing fields, due to the theoretically orders-of-magnitude increase in operating speed. To realize this technology in a compact platform, nonlinear (NL) waveguide photonics are used. Here, an optical ""pump"" beam alters the physical environment of an optical data pulse, causing it to be passed, blocked, re-directed, etc.; the degree of NL response is proportional to the power density of the pump. Unfortunately, photonic waveguides have a minimum size that is limited by physical considerations, requiring unacceptably high pump powers in order to produce a useful NL response. Plasmonic waveguides, however, can confine optical fields into a cross-section that is one to two orders of magnitude smaller, facilitating NL effects at correspondingly reduced powers. These unique structures manipulate surface plasmon polaritons (SPPs) - rather than photons - at the nanoscale (a SPP results from the coupling of the electromagnetic field with oscillating electrons at the surface of a metal.)      The goal of my research is to design nonlinear plasmonic waveguide (NLPW) devices for applications in low-power all-optical logic and data routing. This program aims to address numerous fundamental and applied aspects of NLPW technology. Particular issues that must be resolved include: how can we ensure the existence of guided NL plasmonic modes in the frequency/intensity regime of interest? What materials and SPP frequencies are best suited for NLPWs, both in terms of device functionality and technological feasibility? What figures of merit can best describe NLPW devices, and how are they optimized in the waveguide design stage? What assumptions of photonics modeling are no longer valid in the plasmonic realm, and how can these be properly modified? Is there a fundamental limit to the confinement of SPPs? As these and other questions are answered, the realization of NLPW devices is certain to revolutionize the fields of telecommunications and computing.""442788,""West, Frederick"
"434971"	"Witte, René"	"Engineering of semantic software systems and applications of semantic technologies in solftware engineering"	"Dealing with the overwhelming amount of information readily available today is one of the biggest challenges for individuals, as well as organizations.  Current database and information retrieval systems enable users to quickly obtain vast amounts of information in textual form -- but a serious bottleneck remains by reading, interpreting, and using the collected information.    Clearly, users are in dire need of more advanced, semantically-oriented systems that help them analyzing, transforming, and creating knowledge from large amounts of (textual) data. Foundations for this kind of support have been developed for many years in fields like text mining, ontology, and the Semantic Web. Yet widespread support has not yet materialized in the tools and desktop environments available in current systems. This is due to a gap between current knowledge analysis research, which aims at algorithmic foundations and is rarely concerned with software engineering questions, and software engineering research that likewise barely targets the systematic engineering of knowledge-intensive systems and their particular requirements.    This research aims to close this gap. It is one of the first to address both software engineering of semantic systems, as well as knowledge management foundations using ontology and text mining approaches. Foundations and applications are developed in close collaboration with domain experts. One particular domain is software engineering itself, which thus becomes both foundation and target for semantic system development. The research will progress from short-term objectives, analyzing requirements and developing foundations for fields like biomedical research, cultural heritage data management, and business intelligence, to long-term foundations in semantic system engineering. A central focus and testbed of our research targets the conceptual and technical integration of semantic technologies with common desktop environments, including word processors, email clients, Web browsers and software development environments, which will fundamentally change the way users interact with their systems.""438194,""Wittenberg, Katherina(Karin)"
"422684"	"Woit, Denise"	"Improving reliability quantification and magnitude through development process mutations for iteratively developed software"	"When computer software products are being developed, the process managers and software designers routinely make decisions that trade-off development budget, software features, and software quality.  These decisions ultimately impact tremendously on the quality of the software, the company reputation and its profitability. We will develop procedures and guidelines that will help managers and developers  in their decision-making process, by generating ongoing statistical data that can provide them with information to help them make these trade-offs in a systematic fashion, and to aid in producing software with measured, desirable quality versus cost. To develop these guidelines and procedures we will use some formal, mathematical models and proofs of concept. We will also obtain some results using experimental evidence. Other results will be extrapolated from data we have accumulated from previous real-world projects. Feedback from our preliminary work (from individuals, both academic and industrially-based, contacting us after reading our publications) indicates our work is significant and desirable to both business and development, and we will continue to work on expansion of our results and reports, with graduate student assistance.""439205,""Wojciak, Maciej"
"419762"	"Wong, Johnny"	"Performance evaluation of distributed systems"	"In recent years, considerable attention has been given to organizing the computing resources in a data centre as a common infrastructure to be shared by multiple applications. Each application has its own workload and performance goal. The workload is time-varying because the demand from users may change over time. In managing the resources in a data centre, an important consideration is to achieve efficient resource utilization and high probability of meeting the performance goal of individual applications. Virtualization technology can be used to improve flexibility in resource management. Applications are deployed on virtual machines which are implemented on physical resources. By dynamically allocating these resources to virtual machines, the system can adapt to changing workloads and provide good performance to individual applications.  The proposed research is concerned with the design and evaluation of dynamic resource allocation algorithms for data centres. The system performance seen by each application is affected by the amount of resources allocated to the various virtual machines. Utility functions are used to provide a basis for performance optimization. These functions can be used to represent user satisfaction or revenue generated. The objective is to maintain a desired balance between maximizing a utility function based on workload and response time performance and minimizing the cost of resource usage. The latter is relevant to power management because using fewer resources means lower power consumption. It is expected that effective algorithms and tools for dynamic resource provisioning and self optimization in large data centres that support a diverse set of applications will be developed.""425365,""Wong, Kenny"
"419901"	"Wong, Max"	"Advanced signal processing for digital communications"	"At present, modern communications are faced with two important challenges: a) the ever increasing demand in transmission capacity, and b) the demand of higher quality and lower costs in transmission and reception. MIMO wireless systems are important due to their enormous potential in meeting these challenges. The use of multiple antennas at both the transmitter and the receiver in a MIMO system increases both the reliability of communication as well as the transmission rate (or capacity) for the same bandwidth and with no additional power expenditure. However, current practical MIMO implementations are still far from predicted theoretical performance due to limitation of the present hardware technology to meet the complexity of sophisticated MIMO transceiver techniques. The goal of the present proposal is to devise MIMO transceiver algorithms by applying advanced DSP techniques so that the advantages offered by the MIMO concept can be realized and implemented in practice. Specifically, research work will be carried out in the following areas with the immediate goals of:a)  To develop novel convex optimization algorithms, and to fully exploit and develop our knowledge in matrix transformations for facilitating the use of simple and implementable receivers, resulting in optimum designs of transceivers for single-user and multi-user MIMO communications under imperfect channel state information.b)  To fully develop simple optimum space-time block codes for MIMO systems equipped with simple low-cost receivers, and to exploit the time diversity of these codes to overcome the performance disadvantage of the simple receivers.c)  To fully develop our knowledge in the area of robust adaptive beamforming applied under the circumstances of incomplete or imperfect channel state information in MIMO communications.""433835,""Wong, Michael"
"427198"	"YadidPecht, Orly"	"Advances CMOS image sensors"	"The objective of the proposed research is to investigate, design and implement new advanced CMOS image sensors for biomedical and security applications, consisting of highly parallel, mixed analog, and digital very large scale of integration (VLSI) circuitry. In the past, solid state imagers were based on charged coupled devices (CCD). Recently, however, complementary metal oxide semiconductor (CMOS) based active pixel sensors (APS) with outstanding advantages in integration, power, reliability and cost have been developed and implemented in leading edge microelectronics and VLSI circuitry. This has led to the development of a whole new generation of cameras capable of being used inside the body for many hours, such as the pill camera taking images within the gastro-intestinal tract.This proposal will advance the development of ""smart"" CMOS imagers that support high performance, compact, low-cost sensors with real time output. The focus of the research will be on developing core sensors and on-chip peripherals including digital and analog blocks that can incorporate multiple functions in real time while minimizing their on-chip area. Particular emphasis will be placed on developing a High and Low Light Level Imager (HaLLLI) based on our work in CMOS APS with wide dynamic range and our work on Low Light Level Imaging. This work is of particular importance to biomedical applications where there might be much more than 10 bits in brightness ratio between the darkest and brightest parts of the image (which CCD or CMOS sensors usually can accommodate). Further research and development emphasis will be placed on the creation of an imager with an inherent watermark for enhanced security. One anticipated result of this work is a semi-robust watermarking algorithm suitable for VLSI implementation in a system-on-a-chip. This work is instrumental in the development of integrated imagers and sensors that can be used in applications where, for example, data authenticity is crucial, or where distributed image sensor systems are prone to attack. -""433016,""Yaghoobi, Parham"
"427373"	"Yang, HongChuan"	"Adaptive relay transmission for ubiquitous gigabit wireless communications"	"While the wireless communication industry has witnessed an unpreceded era of exciting developments over the last three decades, the ultimate goal of delivering ubiquitous multimedia services with the same high quality as wire-line systems still requires significant more research efforts. A major design objective of beyond fourth-generation (B4G) wireless systems is the delivery of extremely high data rate services, in the order of multiple gigabits per second, to every eligible users in the system. The proposed research program will develop important cutting-edge enabling techniques for ubiquitous gigabit wireless access. We will devise effective transmission schemes by optimally integrating advanced point-to-point transmission techniques, such as adaptive transmission and reception technique, multiple antenna transmission and reception (MIMO) technique and cross-layer scheduling technique, with cooperative/multihop relaying techniques. Specifically, the following three broad research directions will be actively pursued. First, as multiuser MIMO transmission can effectively explore the spatial degree of freedom for capacity gain, we address the design and analysis of efficient scheduling and signaling schemes for multiuser MIMO systems to maximally benefit from collaborative relay transmission. Secondly, the time varying nature of the wireless propagation channels and user traffic patterns entail an adaptive implementation for high system efficiency. As such, we will develop innovative adaptive signaling schemes for relay/multihop transmission links. Finally, the efficient utilization of the limited resource of wireless systems mandates the operations of different layers in the protocol stack be properly coordinated. We will also investigate the optimal joint design of various adaptive mechanisms in different layers to cost-effectively provide the desired high quality transmission service over multihop wireless links. The resulting multihop multi-antenna adaptive transmission involving multiple layers will provide an attractive novel solution for extremely high rate wireless transmission.""441173,""Yang, Howard"
"428962"	"Ye, Qiang"	"Multi-layer resilience for real-time applications in high-speed networks"	"The Internet is a revolutionary technology that changes our life dramatically. Business and individuals have benefited much from traditional Internet applications, including email, www, etc. The Internet has evolved over the past decades. Nowadays, it is moving forward in several aspects. In terms of access technology, relatively low-rate asymmetric access (such as the 1.5 Mbps ADSL) is migrating to high-rate symmetric technologies (such as the 50 Mbps VDSL2). In terms of Internet applications, more and more real-time applications, such as large-scale online gaming and online interactive television, are emerging. Despite the popularity of the Internet, network malfunctions are not uncommon. Physical failures are relatively frequent in the fiber-optic networks that form the core of the Internet: a top-tier carrier will, on average, experience one fiber failure every three hours. Of course, simple congestion events are even more common than failures. Thus, network resilience, the ability to recover from network malfunctions, has been a very important issue in the Internet. Most of the existing resilience mechanisms were designed for non-real-time applications with low-rate access. With the migration from low-rate access to high-rate symmetric technologies and the appearance of more and more real-time applications, the resilience mechanisms in the Internet need to evolve accordingly. Our research is focused on efficient resilience mechanisms for real-time applications in the transitioning Internet. Our vision is that the mechanism to be proposed would be a ""multi-layer"" approach.""434372,""Ye, Sha"
"426696"	"Yu, Ming"	"Miniturization of microwave devices using lossy networks"	"The objective of this research program is to develop an innovative class of microwave devices for next generation ground and space based wireless communication systems. The research is expected to result in miniaturized high performance RF/Microwave filters and multiplexers.It will truly be a technological breakthrough in the field in the fact that it utilizes the existing dissipation in the device to act favorably  toward communication system performance enhancement. This is a major paradigm shift in the field of filter design, which enables the engineer to design very high quality RF/Microwave filters and multiplexers using very low cost and low volume resonators. The proposed method will have a significant impact in the low power section of the communication system. In this project, we also propose to embed the RF/Microwave filter and multiplexer assembly in a small chip, together with the already existing LNA assembly. This becomes possible by using advanced lossy filter synthesis techniques, combined with tiny amplifiers exploiting RF/Microwave transistors. The outcome will be one integrated circuit chip including both LNA and filters, with tremendous savings in size, mass and cost. The next part of the proposed research deals with  tunable devices. Having manufactured the device on a  single small chip, it then readily becomes an excellent candidate for electronically tunable RF/Microwave filters and multiplexers, which are already of a great interest to the industry. We also propose the use of Micro Electro Mechanical Systems (MEMS) technology to provide further quality and tunability to these devices. MEMS technology has already shown its potential for tunable RF/Microwave components but suffers from low-Q electrical components. Our proposed techniques in the design of lossy filters/multiplexers can deliberately overcome these issues, while taking advantage of the electronic tunability of MEMS. This program will definitely promote more academic and industrial collaborations between university and industry.""435094,""Yu, NamYul"
"425221"	"Yuan, Fei"	"Passive wireless microsensor and transponder systems for emerging applications"	"Passive wireless microsensors including transponders are microsystems that harvest their operation power from radio-frequency waves sent by a base station known as interrogator. The absence of bulky batteries significantly not only reduces the physical dimension and implementation cost of these microsystems, but also removes the need for maintenance. As a result, these microsystems can be embedded in products permanently or implanted in living bodies to either provide a unique identification that distinguishes from others or provide the precision detection of the parameters of products that otherwise cannot be attained. The emerging applications of passive wireless microsensors include implantable bio-MEMS pressure sensors, implantable retinal prosthetic devices, swallowable capsule endoscopy, multisite pressure sensors for wireless arterial flow characterization, embedded micro-strain sensors for product performance and safety monitoring, wireless temperature sensors for human body and environmental monitoring. Passive wireless transponders have been used as radio-frequency identification (RFID) tags for object tracking in logistics automation and replacing bar codes in retailing, warehouse inventory automation, e-tickets, e-passports, to name a few. The intrinsic attributes of passive wireless transponders including small size, battery-less operation, programmability, low-cost, and wireless accessibility also make them a viable choice for low-cost high-security product authentication keys to avoid the difficulties and high cost of conventional product authentication means such as holograms, watermarks, invisible barcodes, security threads, chemical, and DNA markers. The objective of the research program is to develop low-cost high-reliable high-security passive wireless microsensor systems including passive wireless transponder systems for emerging applications in healthcare, product quality control, product authentication and counterfeiting, object tracking and assess management, security, logistics and warehouse automation, file management, etc. and capitalizing research findings in the form of technology transfer to Canadian industry.""431521,""Yuan, Liang"
"421290"	"Yuan, LiYan"	"Integration of XML data with relational database systems"	"Integration of XML Data with Relational Database SystemsThe proposed research is to develop an efficient schema for integrating XML data and relational database systems, and to evaluate the performance of various XML database systems.Various schemas have been proposed to store and query XML data in a relational database by converting XML data into a set of tuples stored in relational tables. However, the querying performances of these existing approaches are still far from satisfactory, especially for large XML documents. We have developed a new integrating schema, called Xregion, and an effective concurrency control schema based on formula locks. We believe that combining the Xregion and formula lock provides an efficient integrating schema for XML database systems. In the proposed study, we will first develop an experimental database management system using Xregion for threading XML data and formula lock for concurrency control. We will then evaluate the performances of various XML database systems under multiple user environment.""437501,""Yuan, Qiuyan"
"425350"	"Zemel, Richard"	"Machine learning for image understanding and user modelling"	"Machine learning research aims to build computer systems that extract useful information from data.Considerable progress has been made in the two main areas of learning.  Supervised methods, which relyon having target labels for each input, have proved to be very powerful and widely applicable,particularly on classification problems, in which each input example is assigned to one of a small setof classes.  These methods generally scale poorly with the number of training examples and classes.Unsupervised methods utilize unlabeled data, where no targets are provided, and attempt to constructrepresentations useful for many input-output mappings.  These methods scale readily but are not aspowerful, because without labels considerable structure must be imposed to make the algorithms work.This proposal focuses on the development of learning methods that utilize both labeled and unlabeledexamples, addressing two fundamental problems.  The first is scene analysis, which entails picking outmultiple items in visual input.  For example, a video of a busy street recorded digitally as a stream ofcolor pixel-maps may be re-represented in terms of the cars and pedestrians and their motions. This isan essential and difficult problem, involving a combination of low-level image properties withhigh-level, object-specific knowledge.  The second problem is {it rapid learning}.  The human abilityto acquire a visual concept from a few examples, and to recognize instances in a complex scene, poses acentral challenge to the fields of computer vision, cognitive science, and machine learning.  Improvingmachines' ability to acquire new concepts readily has applications not only to vision, enabling analysisof scenes containing relatively novel objects, but also to the domain of online interaction, as rapidlearning system can build an accurate model of a user with just a few questions, or limited knowledge ofthe person's web-surfing history or product preferences.""431635,""Zemp, Roger"
"426500"	"Zhang, Xiupu(John)"	"Investigation of enabling technologies for multi-band OFDM ultra wide-band and millimeter-wave wireless signals over fiber transmission systems and architectures"	"Future high capacity wireless access networks require new technologies for distribution. Orthogonal frequency division multiplexing (OFDM) modulation has been considered to provide many technical benefits over the current or conventional modulation technologies. To obtain high capacity wireless access, there are typically two technologies: ultra wideband (UWB) OFDM wireless communications and millimeter-wave OFDM wireless communications. In the both cases, radio over fiber technology is required to distribute wireless signals to remote sites from a central site. For this project we will explore enabling technologies for multiband OFDM UWB and millimeter-wave wireless communications using radio over fiber technology for distribution.  There are 14 channels in UWB and 4 channels in millimeter-wave wireless. Each channel can deliver data of up to 480 Mb/s in UWB, and 1.5 Gb/s for millimeter-wave. Therefore, with the help of the radio over fiber system both UWB and millimeter-wave wireless communications can provide high capacity service over a large coverage, such as an office building, a hospital, and an airport etc.       It is believed that such a research project is unique in Canada that it will significantly contributes to IT technologies and Canadian industries.""437526,""Zhang, Xuekui"
"427374"	"Zhao, Lian"	"Radio resource managemnt for cooperative wireless communication networks"	"The world is demanding more from wireless communication services now than ever before. Many new wireless applications, such as internet access, mobile office, and adult entertainment are emerging. With the increasing demand for wireless communications, the issues of how to improve system performance, such as increased system capacity, enhanced system throughput, lowered transmit power, and improved Quality of Service (QoS), have attracted great research interests in recent years.Diversity technique has been an important approach to combat fading in wireless communications. Recently, there is great research attention on multiuser diversity, which can be realized by exploring cooperative transmission. The basic idea of cooperative transmission is to create multiple channels for each user, while each user is equipped with only one transmit antenna, to deliver his/her information to the destination. As a result, spatial diversity gain can be achieved in the presence of channel fading.For a wireless network operating in a fading environment, power and bandwidth are precious radio resources which need careful planning. With the introduction of user cooperation, the radio resource management problem faces increased degree of freedom. In this proposal, we aim to work on the design of the advanced radio resource management strategies for cooperative communication protocols. The proposed cooperative protocol should maintain diversity gain, compensate the rate loss due to relaying transmission, and justify the hardware complexity. The following three aspects of a wireless network design issues will be investigated: (1) Joint rate and power allocation. (2) Joint power/rate allocation and partner selection in a network environment. (3) Cooperation application in wireless sensor network (WSN).""433082,""Zhao, Linlu"
"427430"	"Ziou, Djemel"	"Gestion par le contenu de larges collections de documents visuels"	"De nombreuses collections d'images et de vidéos disponibles sont exploitées dans divers secteurs d'activités comme la sécurité publique, les médias, la santé, l'industrie du tissu, les loisirs, et le e-commerce. La taille de ces collections augmente d'environ 30 % par année. De nombreuses méthodologies et technologies ont été développées pour la gestion des images et des vidéos. Parmi celles-ci, il y a celles qui exploitent uniquement les métadonnées (e.g. Google, Yahoo, Facebook), celles qui exploitent le contenu visuel des images et des vidéos (e.g. QBIC) et enfin celles qui exploitent simultanément les métadonnées et le contenu visuel (e.g. Atlas développé à Sherbrooke). Les deux dernières sont confrontées aux problèmes d'échelle dans la mesure où il est difficile de gérer de grandes collections. À ce jour, les méthodologies  existantes pour la gestion par le contenu ont été souvent  validées sur des collections de quelques centaines de milliers d'images.  Notre projet vise à développer des méthodologies et des technologies adaptées aux collections de plusieurs millions de documents visuels. Ici, le traitement d'images et de vidéos, la représentation de très grandes collections, leur organisation,  leur mise à jour, la collaboration entre usagers et l'adaptation de documents visuels doivent être étudiés en profondeur. Des prototypes de systèmes informatiques seront développés et s'exécuteront  sur une grappe d'ordinateurs.""430216,""Ziou, Djemel"
"427045"	"Zulkernine, Mohammad"	"Methods and tools for intrusion-aware software systems"	"Software systems must be engineered with reliable protection mechanisms, while still delivering the expected functionalities. The principal obstacle in achieving these two different but interdependent objectives is that current software engineering processes do not provide adequate methods and tools to achieve security goals. Security loopholes often leave the system vulnerable to malicious attacks and abuses. In this research, various methods and tools will be developed with a view to building and monitoring intrusion-aware software systems.    Available software specification languages are not completely suitable for specifying security related aspects of software, while attack languages are not useful for software development. The use of two different languages for software specification and security specification involves a number of unwanted and complicated issues. This research will focus on the factors related to the limitations of the existing intrusion scenario description mechanisms and the corresponding software monitors and design ways to overcome them.  A specification method will be proposed which can be used to specify both normal operational behavior and the behavior of the target when it is under known attacks. Tools will be built for intrusion scenario description, automatic signature generation from the scenarios, and automatic detection of intrusions by comparing the run-time behavior of the software with the signatures. Intrusions exploit vulnerabilities which often exist in multiple modules of software. To address this cross-cutting issue, we will extend our aspect-oriented intrusion-aware software development framework for component-based systems. An aspect-oriented intrusion monitoring and response technique will be proposed considering components as black boxes. The resulting components will be intrusion-aware with the ability to monitor intrusions automatically.     One of the unique aspects of this research proposal is to provide a balanced training facility for five graduate students which will include both software engineering and security engineering principles for building secure software products.""427888,""Zuluaga, Luis"
"419353"	"Özsu, Tamer"	"Studies in advanced distributed data management"	"This proposal covers distributed data management issues in two increasingly important domains: XML query processing and data stream systems.- Distributed XML query processing: XML has emerged as the de-facto standard for publishing and exchanging data of many different types. While extensive research has been conducted on efficiently querying XML data in a wide range of settings, these have primarily focused on data that reside on a single machine (i.e., centralized). This has two potential problems. On the one hand the solutions may not be scalable to very large XML databases, while on the other hand, these approaches do not address the necessity to query and retrieve existing XML data that may already be distributed. In this research we will study distributed XML query processing in two main environments: (1) when there is some control over where the XML data can be placed (i.e., tightly controlled distribution), and (2) when data are already distributed (i.e., the data integration scenario). - Mining data streams: A data stream is a real-time, continuous, ordered (implicitly by arrival time or explicitly by timestamp) sequence of items. The particular problem that will be studied is the mining of data streams with distribution changes (i.e., ""dynamic data streams""). Traditional data mining assumes that data are entirely stored on disk, and almost all traditional mining algorithms require scanning the data multiple times. Neither of these assumptions hold in the case of streaming data. ""Distribution change"" refers to changes over time in the underlying statistical distribution of the data. For dynamic data streams, mining results generated in the past may not be relevant as time progresses. However, since data can be looked at only once, historical data are no longer available, and, thus, it is even hard to just determine the change point of the underlying distribution, let alone updating mining results to fit the new data. We will perform a systematic study of the problem focusing on effective (and statistically meaningful) detection of distribution changes and online mining algorithms to cope with these changes.""442260,""Ozubko, Jason"
