"id"	"researcher_name"	"application_title"	"application_summary"
"454428"	"AissaJaidi, Sonia"	"Cognitive radio networks for agile ubiquitous communications: fundamental analysis, enabling technologies, and service provisioning"	"Enabled by regulatory changes and technology advances, cognitive radio (CR) is emerging as the promising solution for utilizing the radio spectrum in an efficient way to satisfy the increasing demands for spectrum access, and provide ubiquitous communications in the future generation of wireless systems. CR offers new perspectives for spectrum sharing opportunities. However, this can only be achieved by implementing intelligent, flexible and dynamic spectrum-sharing control and management between the different involved parties, e.g., service/content providers and users. This trend poses significant challenges, which range from defining the fundamental performance limits of this radio technology to designing communication strategies and protocols that ensure efficient deployment and operation of future cognitive radio networks (CRNs). In this context, the main goal of this research program is to advance fundamental contributions to enable essential functionalities in future CRNs. Our focus is on developing communication strategies and networking protocols that make the best use of the available resources and deliver ubiquitous communications and quality of service guarantee. To that end, we propose solutions built upon an agile architecture that provides the CRN with the most useful control capabilities and adaptation, and demonstrating an awareness of the environment and an ability to automatically react to changes and requirements. Our research program couples research and educational development closely, and will integrate communication theory and networking in a unique and natural fashion. We will introduce a quantitative, systematic and fundamental approach to CR networked communications. Our research will be in a leading position for providing smart and reliable communication schemes for these emerging environments. As such, significant technical and industrial impacts are expected from this research program. The new technologies to be developed will not only result in high-technology transfer, but will also contribute to placing Canada at a cutting-edge in the development of new wireless technologies. The research program will also provide leading-edge training for graduate students.""453341,""Aissaoui, Rachid"
"449680"	"Akyel, Cevdet"	"Calculation of mutual inductance, magnetic force and torque"	"SUMMARYCalculating the magnetic forces, torques, mutual inductances and the magnetic fields of permanent magnets are essential and required for many industrial and commercialised applications (i.e., implantable medical devices, levitation systems, induction coil launchers, sensors, actuators, transducers, coil models for eddy current inspections, design of the superconductor magnet). In this study,  An analytical-numerical model is proposed to calculate the magnetic force, the magnetic torque and the mutual inductance for different coil configurations as and the calculation of the magnetic field of permanent magnets with radial and axial polarizations. Our approach is based on either Amperian or Coulumbian approach to obtain these important electrical parameters in semi-analytical form. Obtaining these semi-analytical forms are very essential for the industrial and commercialised applications requiring fast and accurate solutions because of time cost of the powerful numerical methods such as FEM and BEM""452602,""Al, Tom"
"449554"	"Alagar, Vangalur"	"A framework for the development of a family of trustworthy systems"	"Software plays a predominant role in our society, often not just guiding us but forcing us to respond to specific actions, bringing thus both rewards and risks. Software for smart medical devices and on-line health care systems are prime examples. They bring enormous benefits, but when they malfunction or fail the patients are in peril. Safety, security, and privacy are the attributes that determine trust in such applications. In the sector of on-line finance and E-commerce, software can be trusted only if it is secure, provides timely service, enforces obligations, and affords privacy. In transportation domain, aircrafts have autopilots installed in them, which once initialized will take away the control of a pilot, resulting in cruising pleasure and occasionally resulting in a severe accident. Safety, security, reliability, and availability are all essential attributes to ensure trust in such software. In the energy sector, large power grids and nuclear power plants should be safe-guarded and protected without fail. Without a direct evidence of safety and security software that monitors and manages these systems cannot be trusted. These examples illustrate the types and severity of risks that vary from one sector to another sector. The risk in interacting with software must be made explicit in order that clients may decide how well they want to trust the system before using it. In particular, for software in privacy and safety-critical domains the dependability argument must be in the form of direct evidence that can be audited by a third party who need not be an expert. This argument should be an expression that includes the global context information, the critical properties of the software, and assumptions on its embedding, its environmental constraints, and direct evidence that the specified properties are satisfied. The goal of this research is the investigation of theory, methods, and a framework for a rigorous development of a family of trustworthy systems. The development framework should be general enough to enable the development of a trustworthy system in any domain for which a contract is specified. Formally constructing a chain of direct evidence and validating it at all stages of system development is an essential activity.""464310,""AlAidroos, Naseem"
"455149"	"Allison, Robert"	"Stereoscopic surface perception in real and virtual environments"	"Display technology is becoming increasingly sophisticated and seamlessly integrated into a broad range of activities and situations in modern life. One such technology is augmented reality systems that seek to combine two or more views of a scene into a single view that looks perceptually 'fused'. Typically one is a direct or camera view of the real world and the other is a transparent overlay that adds graphical elements to augment the scene. Advances require both continued progress in display technology and an understanding of basic human perceptual mechanisms that permit or limit the use of the devices. Designers of such displays face challenges in effective presentation of information on these displays without interfering with the user's ability to extract important information from the scene. Often one wishes to superimpose virtual markings on real surfaces to have them form a single augmented object; other times the goal is to have perceptually distinct virtual and real features and transparency is potentially useful. This research program will provide essential data about surface cohesion, segregation and transparency for imagery typical of many augmented reality and heads-up display applications. We will also look at the role of dynamic focus cues in defining and segregating surfaces and the utility of these cues in information display. These advances will allow me to propose design rules, heuristics and tolerances that will aid the specification, design and calibration of stereoscopic displays for demanding applications such as aircraft simulation, augmented reality and remote operation of robotic vehicles. In terms of basic vision science, the significance of this work is to understand the mechanisms for stereoscopic surface perception, which is of practical importance in computer graphics and simulation and may suggest solutions for analogous problems in computer vision. ""461430,""Allison, WilliamTed"
"454618"	"Amari, Smain"	"Advanced theory and design of microwave components with arbitrary bandwidth"	"The drive towards miniature components and systems in information technology, biomedical engineering and bio-engineering continues at fast paste. However, by reducing the size of these components, strong, and often unwanted interaction, between the constitutive parts can not be ignored. It then becomes practically impossible to accurately model the system as a set of localized physical entities. An equivalent circuit of these compact systems must take into account the strong correlation between its elements and carefully handle their physical meaning and interpretation.The theory of representation has been pivotal in some of the most spectacular successes in fundamental physics such as quantum mechanics and mathematics such as group theory. Unfortunately, it has found little or no applications in engineering except in some areas of control theory and automation. The applicant has recently shown that representation theory sheds new light on how sophisticated components in microwave, wireless and satellite communications work and can be modeled. The goal of the proposed research is the development of novel models and advanced design techniques which are applicable to passive components of arbitrary bandwidth for microwave, optical and nano-technology applications. Emphasis will be on those systems with direct applications in information technology and biomedical engineering. ""472212,""Amelard, Robert"
"457315"	"Aravind, AlexAlagarsamy"	"Development and performance study of synchronization algorithms"	"As technology advances, computing with multicore processors is becoming ubiquitous, and in the near future almost all computers and computing devices will have multicore processors sharing common memory. Also, on the application side, automation is slowly infiltrating into everyday life. Concurrency is inherent in both modern computing technologies and most applications that involve interactions with human and other systems. As a result, future systems are becoming inevitably concurrent and we are entering into concurrent programming era. This paradigm shift, although it faces many challenges, seems unavoidable. My research focuses on the fundamental issues and applied aspects of programming for concurrent systems. In particular, I am interested in designing novel synchronization algorithms, proving their correctness, building software tools for verification and visualization of synchronization algorithms, and studying a select set of algorithms for their suitability in practical use. As a result, new synchronization algorithms suitable for different applications will be proposed, and software tools will be developed to identify and implement suitable algorithms to solve various synchronization problems encountered during the design and development of software systems. Concurrently, I will work  systematically towards developing a theory of synchronization algorithms.""466401,""Arbanas, Carolyn"
"455056"	"Arbel, Tal"	"Probabilistic inference in computer vision and medical imaging"	"Advances in medical imaging have permitted a large amount of information to be made available to medical experts.  However, in many cases, the information is difficult to use for guidance and diagnosis without further interpretation. Image-guided neurosurgery systems (IGNS), for example, have provided a wealth of patient-specific pre-operative images (e.g. MRI, fMRI) to the surgeon during the procedure to use for planning and guidance. However complex brain movements during open-skull operations reduce the effectiveness of using these images for guidance. The objectives of the proposed research program are to develop new probabilistic frameworks in computer vision for a wide range of medical contexts, where they have the potential to lead to concrete improvements in medical diagnosis and patient care. Specifically, we will begin by developing probabilistic medical image registration tools that will permit us to quantify the uncertainties in the resulting interpretation. In the context of neurosurgery, this will permit us to match tracked intra-operative images (e.g. ultrasound) to pre-operative images in order to quantify and correct for brain deformations, and to communicate to the surgeon where the system has confidence in the interpretation.  This leads to the first active image-guided neurosurgery system (AIGNS) where visual feedback will permit a surgeon to optimize placement and use of intra-operative imaging systems to reduce ambiguities in the interpretation. Clinical benefits include faster and more accurate surgical procedures, and reductions in patient trauma and hospital costs. We will also apply the framework to breast cancer radiotherapy, with the goal of attaining more accurate radiation treatment of lumpectomy (resected tumour) sites. Finally, this proposal aims to develop new probabilistic frameworks to automatically learn the local statistical variability of image patterns from large datasets of brain images, potentially leading to breakthroughs in our understanding of the natural variability of healthy human brains and to discoveries of new biomarkers of neurological diseases (e.g. Alzheimer's).""466255,""Arblaster, Jennifer"
"450212"	"Atkins, Stella"	"Medical image display and analysis for diagnosis and treatment"	"Medical images such as magnetic resonance images (MRI) and colour skin images can be used for research, for diagnosis, and for surgery planning. Our planned research will focus on Diffusion Tensor MR images (DT-MR), a  new medical imaging modality that measures the random motion of water molecules in biological tissues (mainly the brain and heart) non-invasively and in vivo. It results in a 3D image where at each voxel the direction of water diffusion is modeled as a diffusion tensor. Processing and analysing DT-MR images such as performing noise reduction, segmentation, registration, etc. requires working with diffusion tensor distance metrics. Several DT distance metrics have been proposed; we are working on new metrics which separate the shape of the tensor from the angle. This approach has promise in studying brain disorders such as Parkinson's Disease, and in locating regions of the brain where the fiber tracts exhibit anisotropy. Another area of research is in dermoscopy, where using either polarized light or oil applied to the skin, the stratum corneum becomes translucent and otherwise undetectable morphological structures, known as dermoscopic structures, become visible. Proper interpretation of these dermoscopic structures leads to increased diagnostic accuracy. We also are trying to improve the design of medical workstations used by radiologists and surgeons when viewing medical images and performing surgery, using light-weight head-mounted eyegaze trackers being developed by a new Canadian company.All our research is highly interdisciplinary, performed in collaboration with MDs and radiologists as well as other scientists.Our interests in medical image display and analysis also include other medical areas where computers are used, such as in medical workstation design, computer-assisted diagnosis and in telehealth, including specifically the use of telepathology and teledermatology in remote diagnoses.""453166,""Atkinson, Gail"
"457950"	"AuclairFortier, MarieFlavie"	"Vision panoramique pour la modélisation d'environments virtuels réalistes"	"Une des problématiques des environnements interactifs est la modélisation 3D de décors dans divers contextes (ex. pré-production cinématographique, visites virtuelles). Présentement, cette modélisation se fait par des artistes infographistes de façon manuelle. Le programme de recherche proposé se situant dans ce contexte a pour objectif de permettre de construire une modélisation informatique 3D d'un décors réel existant, en utilisant les possibilités offertes par la technologie de la vision panoramique qui comprend l'ensemble des capteurs et algorithmes permettant de construire des images ayant un champ visuel très large (allant jusqu'à 360°). Les images produites par les capteurs pluridirectionnels sont déformées géométriquement et photométriquement et donc nécessitent un traitement avant leur utilisation dans un contexte plus large.  Les capteurs traditionnels sont beaucoup moins onéreux et offrent une résolution beaucoup plus élevée que les capteurs pluridirectionnels mais ont besoin d'artifices algorithmiques de recoupement d'images (mosaïques) pour créer les images panoramiques. Le programme proposé vise à mettre au point des méthodes et techniques permettant de passer de une ou plusieurs images panoramiques à un modèle 3D représentant la scène voulue. Ces méthodes et techniques incluront la représentation de la scène en image panoramique à 360° par correction géométriques, photométriques ou par mosaïques, l'extraction des indices de profondeur à partir d'une ou plusieurs de ces images et la modélisation 3D comme telle. La modélisation 3D par la vision panoramique est un domaine relativement peu exploité et jeune. Les résultats de ce projet de recherche seront par le fait même très innovants. En plus de répondre à un besoin réel et criant du milieu cinématographique, ils pourront être transposés à d'autres applications comme les visites virtuelles de musées ou autre.""459421,""Aucoin, Marc"
"455640"	"Aycock, John"	"Proactive identification of computer security threats"	"The sheer number of threats posed to computers by malicious software like computer viruses is staggering. A number of anti-virus products defend against well over a million known threats, and that number is rising fast. The overwhelming sense within the anti-virus industry is that defenses that simply react to new threats are inadequate; proactive defenses are needed.    While most anti-virus software is proactive to a very limited extent, it does not protect our computers well against entirely new types of threat. Given that many ""bad guys"" are profit-driven now, and there are entire underground economies fueled by lapses in computer security, it is reasonable to expect that the bad guys have a lot of incentive to innovate and develop new types of threat to make even more money. Furthermore, there are any number of unfortunate examples demonstrating that motivated bad guys are extremely good at thinking outside the box and developing new attacks. Despite this, there has been relatively little academic research into new types of threats.    This research - proactive identification of computer security threats - will help fill this gap. I will research new, as yet unseen threats and discover ways to defend against them, before the bad guys discover the threats. I also examine the complicated ethical issues involved in this work. In the long term, I will be moving beyond individual threats to look at how the bad guys' future business models can be disrupted, methodical ways to discover threats proactively, and how to lay a lasting ethical foundation for proactive threat research.    Because our society in Canada and around the world relies on computers, this work has potential to positively contribute far beyond the research community, into homes and businesses. The proactive threat research I propose will allow us to jump ahead of the bad guys and have defenses in place before new threats appear.""463864,""Ayele, BelayTeshome"
"450213"	"Baecker, Ronald"	"Assistive technology for cognition and communication"	"Global advances in medicine, health, and nutrition are leading to a dramatically aging society.  The U.N. reported in 2001 that 10% of the world's population is over 60, and projected this to increase to 20% by 2050, and 33% by 2150.  The good news - we are living longer; the bad news - as we age, we must cope with serious health issues. Some if these issues are termed ""special needs.""   Recent years has seen much research on technology for individuals with special needs, often known as assistive technology. Most work has focused on visual impairments, speech and hearing impairments, and motor disabilities. There has been much less work on assistive technology to combat cognitive impairments, to support declining memory, executive functions, and communication, and to help senior citizens remain mentally fit and to maintain their social relationships.   We therefore propose research to develop and evaluate the effectiveness of novel digital media to aid individuals with cognitive and communication impairments of three specific kinds:-    )Assistive technology to stem cognitive decline. We aim to develop innovative gaming environments that will be used by senior citizens worried about Alzheimer's disease (AD), and to provide a platform for rigorous evaluation of proposed ""mental fitness"" interventions claimed to delay mental aging.-    )Assistive communications technology to aid those who are physically and socially isolated to maintain social ties to family and friends and to enable them to reach out to new friends. This is particularly relevant to seniors living alone and to individuals in long-term hospitalization.-    )Assistive context-aware cognition and communications technology to aid retrieval and articulation of words and sentences including proper names.  This is particularly relevant to stroke survivors suffering from anomic aphasia and to some afflicted with AD.""453332,""Baenziger, John"
"463714"	"Bagheri, Ebrahim"	"Quality engineering for software product lines"	"The product line approach to software development is based on systematic and large-scale reuse of design and implementation artifacts such as design models or software components in order to facilitate the production process of a set of functionally similar software systems. Through reuse engineering, software product lines attempt to shorten the development time and lower software maintenance costs. Due to the nature of reuse-based application engineering technology such as software product lines, the lifespan of their artifacts is much longer in comparison to that of a single software system, and have a longer lasting effect on the software production process. This entails that inaccuracy in the product line approach can have ripple effect on the validity or usefulness of the systems developed based on its artifacts; therefore, quality engineering becomes extremely important in software product lines, and its assurance will guarantee higher degree of quality in its spawning products. However, the evaluation of all product variants of a product line is very expensive and time consuming; hence, early-stage quality assessment, measurement, and validation techniques are more desirable, which would avoid costly problems in the late stages of the software development lifecycle.The primary goal of this research proposal is to focus on feature modeling techniques, as a widely used approach in software product lines, in order to cater methods, techniques and decision support tools that would ensure early-stage quality engineering from syntactic (structural validation of feature models), and semantic (verification and validation of feature models) perspectives.  At least 1 PhD, and 2 MSc students, 9 undergraduate research students and staff at collaborating companies and institutes will be trained in cutting edge quality engineering technology, which is highly relevant to Canadian industry.""468796,""Baglole, Courtney"
"464721"	"Bahreyni, Behraad"	"Facility for characterisation of MEMS, NEMS, and thin films under controlled conditions"	"The goal of this proposal is to obtain funding to acquire a vacuum probe station and a network/spectrum analyser in order to establish a facility for characterisation of micro- and nano-transducers and materials at Simon Fraser University. This facility will immediately serve at least ten research teams from four departments at SFU. The requested vacuum probe station and the network/spectrum analyser will be used jointly for the majority of the intended research. At a reasonable cost, this facility provides the means for experimental research on characterisation of new devices and materials, verification of the theories, and insight into the fundamentals of operations of micro- and nano-devices.There is a significant amount of collaboration among the ten applicants and their teams. We are demonstrating that the requested equipment will be extensively used by a large number of users from Microelectronics and Mechatronics programs in the School of Engineering Science as well as the Departments of Physics and Chemistry at SFU. More than 20 research programs, encompassing the work of more than 50 researchers, will directly benefit from the establishment of this facility. It is anticipated that numerous multidisciplinary research programs will benefit from existence of this facility in future. Other academic researchers will be able to use the facility on a reservation-based arrangement without a user-fee. To serve the current research programs of the applicants and to enable future research, it is necessary to purchase and install this facility within our laboratory.""461473,""Bahreyni, Behraad"
"461509"	"Barlage, Douglas"	"Nano-scaled interface and material control for III-N electronics"	"The work proposed in this discovery grant is to be performed at the University of Alberta, Electrical and Computer Engineering. The proposed discovery research is an effort to extend semiconductor device capabilities through the continued development of Gallium Nitride (GN) and related column III-N materials asa device material for digital, microwave and high power applications. The common limiting problem to all of the potential opportunities is the formation of low resistivity source-drain regions with high injection efficiency and an effective dielectric for passivation and charge control. GaN is becoming an increasingly important material in optical devices. It is currently being implemented in RF systems as the amplifier of choice for high linearity high power considerations. The problems that remains to be solved for full implementation is reduced defects in substrates, a reliable and suitable low trap density dielectric material and a dramatically improved way to make low resistivity contacts. This discovery proposal will address these challenges at the nano-scale.  To fully examine the structural requirements for the full development of GaN devices new material techniques need to be explored with techniques such as Atomic Layer Deposition (ALD). 1.) Selective Growth of Source-Drain to lower contact and access resistance. Indium incorporation, gradation to highly strained Silicon and Germanium will be explored with this system.2.) Rare earth oxides will be explored using ALD, to determine best possible gate dielectric.  In addition to the multi-disciplinary activity, students trained in this program will also be required to learn basic electronic device design techniques and using system requirements to design devices at the nano-scale. The successful graduate students will understand the key material requirements for achieving a particular system function.""463739,""Barlatt, Ada"
"451852"	"Bergevin, Robert"	"Fundamental issues in cognitive computer vision"	"This research program is in the field of cognitive computer vision. Researchers in cognitive computer vision develop mathematical models, algorithms, and computer programs to automatically determine what types of objects and scenes appear in digital images and videos. Even though a completely general solution to this difficult problem is still years away, regular progress is made on different aspects that have an immediate impact on real-world applications of the technology. This research proposal intends to contribute further by addressing fundamental issues related to key questions facing the researchers, namely the possibility to detect and recognize objects over a large range of distances and viewpoints, the improvement of the algorithms to cover appropriately general types of objects, and the reduction of the time it takes to run the programs. Progress is to be made on a number of important issues leading to robust, efficient, and effective automatic cognitive computer vision methods. These are required whenever an application necessitates an understanding of the image contents within a context that allows only limited specific expectations. As such, this research program is to help actualize a large number of different applications in security, transport, media, health care, and education fields, all with high socioeconomic impact. Image understanding methods usually attempt to reproduce human performance without modeling and estimating parameters of the underlying brain processes. For this reason, a proper and distinct methodology is required and must be followed in developing and testing cognitive computer vision methods. Recently, we defined such a methodology as part of our effort to improve the way cognitive computer vision methods are developed, documented, and evaluated. Seven graduate students are to work on the research program. Past students are holding positions in universities, research laboratories, and high-tech industries.""451890,""Bergler, Sabine"
"450031"	"Bhat, Ashoka"	"Soft-switched high-frequency power converters for energy systems"	"With increased energy demand and environmental concerns, alternate energy sources (fuel cell, wind, photovoltaic and ocean) are becoming popular. The outputs of these energy sources vary widely including some operational limits. Some examples are: (1) Fuel cells generate widely varying low voltage dc (e.g., 22 to 41 V) having slow transient response for load changes and has degraded performance with ripple currents. (2) Permanent magnet synchronous generators used with wind turbines generate varying voltage and varying frequency ac output demanding power factor (pf) corrected ac/dc converters to get a regulated dc bus. For all these sources, an additional dc/ac converter is required for stand alone or utility interface to augment power generation. In addition, when different energy sources are connected to a dc bus, a bidirectional dc-to-dc converter is needed for battery charging as well as to supply the dc bus and an electrolyser used to generate hydrogen for fuel cells requires a dc/dc converter.  Therefore, all these energy sources require low cost, highly efficient dc/dc, dc/ac and ac/dc power electronic converters while satisfying the constraints imposed by the energy sources. This is possible using soft-switching (including resonant) converters operating with low losses and high switching frequencies. These converters have a number of advantages, e.g., high efficiency, small size, light weight, reduced EMI, low component stresses, etc. Therefore, main objectives of this work are: (1) to propose, analyze and design high frequency transformer isolated single-phase and 3-phase soft-switching converters (unidirectional and bidirectional dc/dc, dc/ac for stand alone and utility interface, and ac-to-dc with high pf); (2) to analyze and design new topologies of soft-switching power converters, in particular, zero-voltage transition and zero-voltage switching converters; (3) consider interleaved operation for high power applications; and (4) to simulate and build prototype experimental converters to verify the theory and performance in actual energy systems. The proposed converters are expected to have heavy impact on increasing the efficiency and performance of energy systems.""470425,""Bhatt, Neil"
"457171"	"Bilodeau, GuillaumeAlexandre"	"Contributions to intelligent visual surveillance"	"A recent analysis of the needs and challenges of intelligent video surveillance has shown that current commercial systems lack many useful video processing features because the works developed in research are still not producing results that are reliable enough. Thus, actual commercial systems only rely on basic image processing, do not assist well security agents, and as such cannot be considered as being intelligent. Among the video processing features needed for intelligent video surveillance, we can find 1) human tracking, 2) tracking with Pan Tilt Zoom (PTZ) cameras, 3) video fusion from different types of cameras, 4) the use of thermal sensors, 5) crowd analysis, and 6) understanding the activities in the scene. This research program aims at studying the first four required video processing features.The long-term objective of this research program is to improve the performance (reliability, coverage, and commercial applicability) of visual surveillance using cameras from different modalities and PTZ cameras. The short-term research program objectives are to:- Combine information from an infrared camera and a visible camera to improve object detection and tracking. This means using the information from an infrared camera to improve detection and tracking in the visible spectrum. We have to devise ways to combine the information and integrate them in tracking.-Track human and detect carried objects using infrared and visible image fusion. Carried objects are often at a temperature different from the human.-Model infrared image regions and measure the temperature of moving targets (human and animals). Temperature can be used to detect pathologies, muscular efforts, and to distinguish humans from each other and from other objects. -Detect and track an object using a PTZ camera. A PTZ camera allows monitoring a larger area with a single camera. Here, we will investigate tracking with this type of camera.""469489,""Bilodeau, Julie"
"450106"	"Blake, Ian"	"Cryptographic primitives and error control coding"	"The first part of the proposal is concerned with the investigation of the fundamental primitives of cryptography of homomorphic encryption and hash functions. Homomorphic encryption systems find wide application in numerous important cryptographic protocols. New homomorphic encryption systems will be developed with a view to making their application to certain secure multiparty computation protocols more efficient. Hash functions are an essential part of many commonly used cryptographic protocols such as digital signatures. Most currently used such functions are fast but lack a security proof. The few currently known hash functions that have some form of security proof tend to rely on sophisticated mathematics and are very slow. The proposed research will investigate possible techniques to formulate a new function that has some form of security proof and improves on the speed.For the second part of the proposal, further properties of random coding techniques for the erasure channel will be considered. Such systems have proved very useful for multicast internet downloads where a symbol erasure corresponds to a packet loss. Systems that have linear complexity (in the number of information symbols) encoding and decoding techniques, for the binary case, are available although have limitations in terms of implementation. Nonbinary systems will be investigated that have the potential to improve performance and efficiency in terms of speeds of convergence and simplicity of implementation.""453360,""Blake, John"
"451452"	"Booth, Kellogg"	"Collaboration technology and multi-user interfaces"	"Interaction techniques for shared displays in collocated and distributed environments present a number of challenges. The proposed research will examine the affordances of tabletop, large wall-mounted, and hand-held displays to determine how each best fits with specific tasks during face-to-face collaboration, and how each can support the other. Included in this will be development of novel interaction techniques for use at a distance from the display, especially when used in collaborative environments where mutual awareness must be traded off against interference and distraction by one user's interaction with another's workflow. One component will look at the special case of collaborative authoring, which is often performed asynchronously.Classroom and meeting room presentation support using shared displays is a special case of the more general work that is of particular interest. As with the more general topic, this continues recent work. In this case, studies in actual classrooms using special-purpose software designed to support multiple projectors to extend standard PowerPoint presentations to a much larger screen area more like traditional multi-blackboard ""chalk talk"" lectures is the topic of a just-finishing doctoral disseratation. Further work will assess specific pedagogical hypotheses and ways to engage students as active participants, rather than simply passive receptors.Augmented, mixed, and hybrid reality offer another avenue for extending the research on shared displays. In this case the displays will be super-imposed or embedded in physical objects, and interaction techniques that flow seamlessly back and forth between the real and virtual representations of the objects or the information underlying them will be studied. Current work has looked at a variety of multi-projector augmented reality techniques for architectural plans.Document authoring and annotation tools are fundamental tools for collaboration. They are among the earliest examples of computer-based productivity applications, yet a number of problems still remain to be solved. Commercial desktop software, such as the Microsoft Office suite, offers many solutions, but does not yet provide adequate support for shared authoring of the type that is undertaken almost daily by academic researchers. Google provides an interesting counter-point with its documents, which are essentially collaboration tools first and document authoring tools second (though admittedly quite good at both). Nevertheless, neither these nor other commercial tools fully capture the workflow typical of collaborativewriting, especially during the editing and annotation phase. The proposed research will build on previous research by the applicant and colleagues to examine how structured documents and annotations can naturally reflect the distributed nature of annotations and revisions in realistic settings. This will also examine other types of documents such as archives of webcasts, again building on recent work by the applicant and colleagues.""464723,""Booth, Kellogg"
"454204"	"Boulet, Benoit"	"Advanced control for medical applications and renewable energy systems"	"Two major concerns of Canadians are health and the environment. The main goal of the proposed engineering research program is to develop robust control systems for: (a) medical applications, more specifically for real-time glucose control in type 1 diabetes, blood osmolality control in central diabetes insipidus, and balloon deployment in angioplasty procedures, and (b) renewable energy systems. Successful research on such control applications will result in very significant benefits for society and the environment. Objective (a): The objective of the glucose control system research for diabetic patients is to design and implement a closed-loop device that would regulate glucose levels for people with Type 1 diabetes. The closed-loop device (so-called artificial pancreas) measures blood glucose levels and uses these measurements via a control algorithm to properly dose insulin using the subcutaneous insulin pump. The second medical application is to control blood and urine osmolality (concentration of electrolytes) in patients with central diabetes insipidus using sensor feedback to automatically dose a synthesized anti-diuretic hormone. Patients with central diabetes insipidus have an excessive thirst and an excessive production of very dilute urin, which makes them exposed to dangerous dehydration. The third medical application is the control of balloon expansion during coronary angioplasty. This procedure is one of the most practiced operations today. Currently, the surgeon inflates the balloon manually with a bio-compatible fluid, but the lack of stress and strain feedback or sensing sometimes causes tissue damage which can lead to restenosis. We propose to study the sensing and real-time control of stress and strain of the stenosed region during balloon deployment using pressure and optical coherence tomography (OCT) sensors. Objective (b): Plug-in hybrid electric vehicles (PHEVs) produce little carbon dioxide and are seen as the future of sustainable transportation. We propose to study the control of PHEVs from the point of view of dynamic energy optimization. Secondly, we propose to investigate the control of micro power grids incorporating renewable energy systems, notably wind generators. ""462703,""BouletAudet, Maxime"
"454743"	"Bremner, David"	"Geometric aspects of optimization"	"Many computational problems are either inherently geometric (e.g. CAD/CAM), or can usefully be analysedin a geometric framework (e.g. the configuration space of robot).  Of the latter type, applications ine.g. pattern classification and non-parametric statistics are inherently ""high dimensional"", or concernedwith many more variables than our familar 3 or 4 coordinates.  Computational geometry brings a richtoolkit of algorithms and data-structures to these problems, but for the most part these are optimizedfor the case of an enormous number of input objects in small dimensions.  Techniques from optimization,although typically slower for many objects in small dimensions, are usually more effective in the face ofmany variables/dimensions.  One theme from my research is thus the developement of better algorithms forhigh dimensional geometric problems using techniques from optimization.  Current projects in this themeinclude training of maximum margin classifiers (support vector machines), representation transformationfor systems of linear inequalities, and computing the multivariate rank of a point in a data cloud. A second theme in my work is the analysis of geometric structure as a way of understanding the behaviourof algorithms, particularly algorithms in optimization.  This analysis has two related aspects.Convexity underlies almost all efficient optimization methods and has a rich geometric theory going backto the 19th century.  Combinatorial Geometry provides a way of understanding the structure of solutionsets, particularly important for discrete optimization algorithms.  With several collaborators and usinga combination of theoretical tools and massive computation, I am currently analysing the structure of thefeasible regions of linear programs, particularly as it relates to the worst-case performance of thesimplex method.""451942,""Bremner, Murray"
"450989"	"Brett, Michael"	"Nanostructure engineering by glancing angle deposition"	"Dr. Michael Brett has developed a process known as Glancing Angle Deposition or GLAD that enables control over the nanoscale architecture of thin porous materials and which has been adopted worldwide by other researchers.  Brett's group has studied applications of this process in the fields of photonics, sensing, and energy, has published in leading journals such as Science, and has initiated commercialization with industrial sponsor Micralyne.  Fundamental to all applications of this materials nanostructuring process is continued development of the GLAD process.  With the proposed research, the Brett group will retain its leadership in innovation and improvements to GLAD, and generate a full nanostructuring toolset suited for commercial use.  ""454108,""Brett, Michael"
"451734"	"Brown, Stephen"	"FPGA design flows for improved productivity, performance, and energy"	"Digital technology plays a key role in modern products, from consumer items to broadcast and medical equipment, to industrial control.  When developing a product using digital technology, two approaches are prevalent: software and hardware. In the software approach the design includes a microprocessor, and the unique features needed are implemented by writing software programs that are executed on the processor. This approach is commonly-used in practice, because it is straightforward and software personnel are widely available; they outnumber hardware developers by about 10x. But processors have two big drawbacks: they use a lot of energy, and do not provide high performance in comparison to hardware. In this project we intend to ameliorate these issues, by making the benefits of hardware design available to software developers.   Field-programmable gate arrays (FPGAs) are a key technology for hardware design. FPGAs are integrated circuits that can be programmed by an end user to implement any desired digital hardware circuit. An FPGA chip comprises a large collection of programmable hardware resources, which are configured by using CAD tools. Implementing a hardware design in an FPGA is much for convenient and less expensive than the alternative of creating a custom integrated circuit from scratch at a silicon foundry.   The key idea in this project is to develop a novel type of processor that can be programmed into an FPGA, which has the ability to automatically profile the code that it executes, in terms of performance and energy used. The results of this profiling identify the critical parts of the program that can provide the most benefit if converted into hardware circuits. FPGA CAD tools would then be used to automatically perform these conversions, and to add the generated hardware to the processor. By utilizing this hardware on future executions of the program, the processor gives the benefits of hardware design to software developers.   In a related thrust, we plan to work on three aspects of FPGA CAD tools: the speed of FPGA CAD algorithms, debugging of hardware/software designs, and  incremental design.""461169,""Brown, Steven"
"457561"	"Bulatov, Andrei"	"Algorithms and complexity of the constraint satisfaction problem"	"The aim in a constraint satisfaction problem (CSP) is to find an assignment of values to a given set of variables, subject to constraints on the values which can be assigned simultaneously to certain specified subsets of variables. In its counting version (#CSP) the goal is to find or approximate the number of such assignments. CSP and #CSP can be used to model a wide variety of computational problems in computer science, discrete mathematics, artificial intelligence, and elsewhere, they have found numerous applications in those areas.     As the general CSP is NP-complete (and #CSP is NP-hard), and therefore it is unlikely that there is an efficient general algorithm for these problems, a major research direction in these problems is to determine the complexity of certain restricted problems of this type. Several very successful approaches to tackle this goal have been developed over the past 5-7 years. One of the most fruitful ones has been the algebraic one.      The overall objectives of the present proposal include: (1) Determine the exact complexity of restricted CSPs. (2) Study the complexity and approximation complexity of the weighted #CSP.     Progress in (1) will lead to designing new efficient algorithms for the CSP and better understanding of such problems. It also has assisted and will assist in related fields such as graph colorings and homomorphisms, universal algebra, and certain areas of logic.     In (2) the weighted #CSP generalizes the standard #CSP and includes, in particular, such problems as computing partition functions that appear in statistical physics and elsewhere. Recent results show that weighted CSPs with rational weights are equivalent to the standard #CSP, and therefore the algebraic approach can be used in this case. Since efficient exact algorithms for #CSP exist only for a very restricted type of constraints, approximation algorithms become especially important as a more practical tool for solving such problems. ""451053,""Bulgak, Akif"
"463704"	"Bunt, Andrea"	"Facilitating user involvement in intelligent interactive systems"	"Intelligent interactive systems embed techniques from the field of artificial intelligence (AI) to tailor their human-computer interactions to individual user needs.  Given the right underlying reasoning mechanisms, intelligent interactive systems have great potential to improve users' experience as they interact with complex applications and information spaces both qualitatively (e.g., user satisfaction) and quantitatively (e.g., task efficiency). Their success, however, relies not only on their ability to model and reason about relevant user and contextual factors, but also on the manner in which the intelligence is delivered, or in other words, the interaction design.  Of particular importance in the interaction design is providing sufficient opportunities for user involvement.  The proposed research will examine issues such as how to make the system's intelligent behaviour understandable, how to divide the labour between the user and system so that their respective strengths are leveraged, and under what circumstances an intelligent interactive system should provide proactive advice.  These issues will be examined through two complementary approaches:  i) using qualitative methods, such as semi-structured interviews, to understand how users perceive and use deployed intelligent interaction systems; and ii) creating and systematically evaluating research prototypes to assess the impact of aspects of their design.   A better understanding of how to design intelligent interactive systems will benefit both industrial and research communities.  There are millions users in Canada and worldwide interacting with technology that could be made substantially more efficient and effective from an individual user's perspective given the right intelligent support, including search engines, e-commerce websites and productivity applications (e.g., word processors).  Within the research community, this work will help those designing underlying AI reasoning mechanisms to ensure that poor usability is not a confounding factor in any validation work involving human participants. A second contribution of this work will be a strong methodological basis with which to build and study intelligent interactive systems.""454029,""Buono, PietroLuciano"
"457847"	"Cai, Lin"	"Configurable protocol design and optimization for ubiquitous networks"	"We have witnessed the explosive growth of the Internet, which becomes a ubiquitous information transport infrastructure with hybrid wired and wireless links. However, the current network protocols encounter fundamental challenges from both new applications and emerging communication technologies. For example, many media-rich applications such as Internet Protocol Television have stringent quality of service (QoS) requirements and are very demanding in network resources; future vehicular networks need to deliver emergency messages which are both time and location critical; underwater, underground, and deep-space networks rely on communication channels significantly different from the terrestrial ones. We propose to develop network protocols that can be flexibly configured to support heterogeneous applications over hybrid networks with emerging communication technologies. Specifically, we will develop a) physical layer technologies configurable by software to support differentiated services; b) medium access control and routing protocols that adapt to changing network environments and applications; and c) transport layer protocols with configurable control parameters. The innovation and expected contributions of the proposed research lie in three aspects. First, without abandoning the layered network architecture, the proposed configurable protocols can be scalable, robust, and incrementally deployed. Second, with the inherent adaptiveness of the proposed configurable protocols, we can optimize protocol parameters to not only improve system performance, but also enable new services that are not possible with the traditional protocols. Third, the work on a configurable physical layer supporting service differentiation will shed new light on traditional information theory. The proposed protocols will be implemented and tested in the wireless testbed with software defined radio peripherals. The success of our research will contribute to ubiquitous network services that will underpin advances and reforms in many industry sectors such as commerce, manufacturing, education, health care, and facilitate weather forecasting, natural resource exploration, disaster warning, and environment safeguarding.""461742,""Cai, Lin"
"457478"	"Caloz, Christophe"	"Multiple-scale metamaterials for enhance radiofrequency devices and systems"	"The proposed program is a large-scale research program aiming at developing a next generation of metamaterials (MMs), which will systematically exploit multiple material structuring scales in order to reach unprecedented properties and performances, for enhanced and novel radiofrequency (RF) applications.    The multiple scale MMs which will be investigated are classified in four categories: 1) micro-scale (uS) MMs [artificial plasma, resonant particle and transmission line structures], 2) atomic-scale (aS) MMs [ferromagnetic (FM), ferroelectric (FE) and multiferroic (MF) materials], 3) nano-scale (nS) MMs [FM nanowires (FMNW), carbon nanotubes (CNT) and graphene nanoribbons (GNR) composites], and 4) multi-scale (MS) MMs [MMs incorporating at least two of the three scales].    A host of novel RF structures, devices and systems are proposed in these different categories, including: 1) uS: enhanced components and smart antennas; dispersion-engineered analog signal processors; quasi-optical bi-anisotropic systems;2) aS: FM smart antenna systems, lenses and artificial electromagnetic conductors; FE reconfigurable devices, surface acoustic wave analog processors and nonlinear components; MF negative-refractive index (NRI) and bi-isotropic structures and components;3) nS: FMNW self-biased, integrated and mm-wave non-reciprocal and anisotropy-engineered components, magnetostatic and spin-torque based devices; CNT array traveling wave amplifiers, antennas, interconnects, meta-substrates and detectors; GNR waveguides and antennas, active devices, super-sensitive THz sensors and surface plasmon detectors;4) mS: an example is an NRI antenna radome made of a uS Rotman plasma for the negative permittivity embedded in a nS FMNW structure with natural aS FM spin controlled anisotropy.""453970,""Caloz, Christophe"
"449392"	"Cameron, Robert"	"Wirespeed text processing with parallel bit stream technology"	"The proposed research is into tools and technology for high-performance (wirespeed) text processing using parallel bit stream technology on commodity multicore processors. Current research results have established that parallel bit stream technology can provide order of magnitude performance improvements over traditional byte-at-a-time character processing on modern pipelined processors in applications such as string search, character set transcoding, lexical analysis, regular expression matching and XML parsing. In effect, the parallel bit stream technology makes substantial use of intraregister parallelism in order to process up to 128 characters at a time using 128 bit registers. However, this involves new forms of programming for which high-level tool support (e.g., high-level programming languages) are not yet available. Furthermore, current trends incommodity processor design involving multicore processors are creating demands to find new forms of parallel programming to take advantage of intrachip parallelism. By developing new techniques to leverage intraregister parallelism in support of intrachip parallelism, further performance advantages of these parallel text processing techniques over sequential byte-at-a-time techniques can be expected. By developing high-level programming tools that map to implementations based on parallel bit stream technology, these performance benefits can be made broadly available to programmers without the need for tricky and error-prone low-level programming.""472435,""Cameron, Robin"
"456496"	"Cardinal, Christian"	"Low complexity error control coding using graphs for high throughput cooperative wireless networks"	"Cooperative communication technique enables the single-antenna mobiles to share their antennas to create a virtual Multiple-input/Multiple-output (MIMO) communication system. Using error control coding techniques based on graphs, in conjunction with cooperative communication systems, the network capacity can be approached. Although efficient approaches for cooperative coding networks have been proposed in the literature, these issues and their related problems still remain to be solved especially when code-on-graph and iterative decoding procedures are used. The main objectives of this proposal are to explore, analyse and evaluate the overall performances of wireless communication networks that use efficient codes-on-graphs and iterative decoding procedures.  The first general objective is to elaborate much more accurate models for dense wireless coded cooperative networks and propose related analysis tools that will be used to predict the overall performances of wireless coded cooperative networks.  The second general objective consists in examining and determining the design criteria for codes using graphs that optimize the performances of the cooperative networks. The design criteria must take into account all constraints imposed by the desired Quality of Service (QoS), error performances, decoding latency and hardware complexity. The third objective is to elaborate theories and methods for finding the best match between graphs representing the codes and those representing the fixed and/or mobile wireless networks. These codes should be adaptive to time variant conditions of wireless channels and desired QoS. Furthermore, this research aims at exploring solutions to the challenges posed by the efficient exploitation of code-on-graph and iterative decoding procedures for fixed and mobile wireless coded cooperative ad hoc networks. Finally, the fourth objective is to explore related hardware and software implementations, which will allow evaluating and validating the real performances of coded cooperative networks.""472033,""Cardinal, Mikaël"
"456136"	"Charette, Paul"	"Sub-micron resolution surface plasmon resonance microscopy of binding kinetics under active microfluidic mixing"	"Research into miniaturized biosensing, whether in microarray or lab-on-chip formats, has seen explosive growth in recent years. Surface plasmon resonance (SPR), a powerful method for biosensing, is well established in both cases. Most SPR systems are based either on propagating plasmons in ""macroscopic"" (mm2 or cm2) thin-films, or on non-propagating localized surface plasmons (LSRP) in nanostructures. Though systems at both ends of the spectrum have been extensively studied, there exists a knowledge gap between the two where surface structure and its impact on chemical reactivity and optical properties of SPR-active metals are not fully understood. This gap is important to explore because, in the push to ""go nano"", scientists and engineers may not have all the elements in hand to make informed decisions about optimal levels of scale in biosensor design. This research aims to fill this gap by developing instrumentation & methods for observing SPR phenomena at the sub-µm scale linked to real-time measurements of surface chemistry kinetics and microfluidic mixing. The program will address these challenges with a novel imaging platform capable of producing high-resolution tomographic SPR microscopy images co-registered with that from a scanning electron microscope (SEM), and incorporating active on-chip microfluidic mixing. This system will enable the study of surface functionalization and assay kinetics in relation to metal thin-film properties, at the sub-µm level of scale, and allow direct measurement of Langmuir isotherms. The work will have wide-ranging repercussions in next-generation miniaturized biosensors in applications where SPR is prevalent such as rapid medical diagnostics, environmental monitoring, and the pharmaceutical and agri-food industries. The long term objective of this research is to enable microarray- and lab-on-chip-based SPR systems to achieve levels of density, miniaturization, and reliability of measurement not currently possible.""471721,""Charette, Stéphanie"
"452683"	"Chechik, Marsha"	"Abstraction and automation for model management"	"Improving the dependability of hardware and software systems is widely recognized as one of the most important challenges in information technology today.  Interest in reliability and security issues has grown tremendously as a consequence of relatively frequent and expensive failures.  These can be far in space (i.e., failure of the Mars probe) close to home (TTC failure, problems with CIBC's and Royal Bank's transaction processing systems) or affecting half the continent (the 2003 East Coast blackout).  Formal verification has long been advocated as the best solution for improving the reliability of hardware and software systems, but the inherent difficulty in applying formal methods in practice has historically limited their success outside of academia.  I believe that the key to solving this problem is abstraction and automation.  The proposal builds on two major industrial developments:  the success of automated model checking techniques, routinely applied in hardware companies, such as Intel and AMD. and the increased usage of model-driven development which promises automated generation of correct-by-construction code, in software industry.  The goal of the proposal is two-fold:  (1) to build fundamental techniques to enable the application of model-checking to software systems; specifically, code; and (2) to create support for all aspects of formal reasoning about higher-level artifacts:  models.  This entails giving models formal semantics, synthesizing preliminary odels from behavioural properties and usage scenarios, dealing with inconsistency which inevitably occurs when multiple people attempt to describe the same systems, and many others.  Whether we lift models from code or create them from requirements, we expect results of this proposal to be vital to the task of constructing higher-quality software systems.""449295,""Checkel, David"
"451723"	"Chériti, Ahmed"	"Commande à modulation de densité d'impulsions duale appliquée aux convertisseurs de puissance"	"Actuellement, nous assistons à une prise en conscience planétaire de la nécessité urgente d'un développement durable. Ce concept de développement exige entre autres une gestion du secteur énergétique, au sein duquel l'énergie électrique occupe une position de premier choix en tant que forme d'énergie tampon permettant d'interfacer des sources d'énergie et des charges de nature très diversifiées, qui vise la pérennisation des ressources. L'électronique de puissance, ensemble des techniques associées à l'énergie électrique depuis le stade de la production jusqu'à celui de la consommation, peut contribuer activement à atteindre cet objectif, si la conception des convertisseurs de l'électronique de puissance est faite sur une base multicritères : une meilleure adaptation aux éléments connectés en aval et en amont des convertisseurs, un meilleur rendement de conversion et une réduction du nombre des composants servant à leur fabrication. Ainsi, on pourra concevoir des convertisseurs de puissance écologiques, performants et peu coûteux. C'est dans cette optique que le projet de recherche 2004-2009, axé sur la commande à modulation de la densité d'impulsions, a été soumis et réalisé. Les résultats obtenus ont rencontré les barèmes scientifiques et universitaires qui étaient escomptés. Aussi, ces résultats ont montré que la marge d'amélioration simultanée des performances techniques et des coûts (réduction du nombre de composants utilisés) plafonnait.Nos récentes investigations montrent qu'il est possible d'accroître la possibilité d'élargir cette marge par la définition d'une nouvelle forme de commande dérivée de la modulation de densité d'impulsions : la modulation de densité d'impulsions duale. C'est dans cette logique de continuité et d'évolution que s'inscrit le présent projet de recherche. L'objectif visé est donc le développement de nouvelles familles de convertisseurs basés sur la modulation de densité d'impulsions duale et qui cadreraient encore mieux avec l'esprit du développement durable.""451830,""Cheriyan, Joseph"
"450956"	"Chouinard, JeanYves"	"Méthodes de transmission robustes et sécuritaires pour les réseaux coopératifs sans fil"	"Les récents développements en télécommunications permettent de concevoir des réseaux sans fil coopératifs fixes et mobiles. Ils ont conduit à la définition des nouvelles infrastructures de communication sans fil à très hauts débits, dont les normes de quatrième génération WiMAX et LTE. Le déploiement de tels réseaux constituent cependant de nouveaux défis de conception aux manufacturiers : en raison de la forte densité géographique d'utilisateurs, ces réseaux subissent des niveaux d'interférence mutuelle élevés, ce qui limite la distribution en temps réel d'information multimédia. Les objectifs du programme de recherche visent à déterminer la capacité des réseaux coopératifs, à concevoir des méthodes de codage coopératif avec diversité, efficaces et robustes, et sécuritaires. Le programme de recherche permettra de mieux comprendre les processus de transmission dans les réseaux sans fil affectés par des évanouissements sélectifs en fréquence. Le premier objectif consiste à déterminer la capacité des réseaux coopératifs sans fil afin d'établir les meilleures stratégies de coopération entre les relais. Le second objectif s'adresse à la diversité de coopération dans les réseaux tout en tenant compte des imperfections des canaux sans fil : on déterminera la corrélation entre les liens constituant un système à antennes multiple virtuel dans un réseau à relais multiples. La sécurité dans un réseau coopératif fait l'objet du troisième objectif : on développera des techniques efficaces de protection de l'information dans les réseaux sans fil coopératifs, robustes face aux attaques mais de faible complexité de calcul. Les méthodes de coopération en réseau les plus prometteuses, développées dans le cadre de ce programme de recherche, contribueront au développement de technologies répondant aux critères des infrastructures sans fil de quatrième génération : elles seront utiles aux manufacturiers et à l'industrie canadienne des télécommunications.""451822,""Chouinard, LucEugene"
"456663"	"Chowdhury, Sazzadur"	"A MEMS multi-spectral transducer array"	"The objective of the proposed research proposal is to design and fabricate a MEMS based multi-spectral transducer array that is capable of generating and gathering information concurrently in the acoustical and microwave portions of the spectrum. The state-of-the-art in planar and non-planar capacitive micromachined ultrasonic transducer (CMUT) array and MEMS based ultrawideband (UWB) microwave radar will be extended to develop a multi-spectral transducer array that integrates the capabilities of both the non-planar CMUT array and the UWB MEMS radar. While the acoustical imaging using the beamforming and beamsteering capability of the planar or non-planar CMUT array will identify an object or feature in the target area, the microwave imaging using the UWB radar will enable to look inside the object or the feature by generating an image with a much higher resolution and optimum depth compared to that obtained if only the ultrasonic imaging technique is used. A microscale fusion of two different transduction technologies will advance the state-of-the-art in high resolution 3-D imaging that will result in lower cost faster smaller equipment with a higher imaging capability. The transducer array can be used for non-intrusive or non-destructive imaging in diverse applications such as medical diagnostic imaging, automotive collision avoidance, material characterization, and for biometric based identification and security purposes. The transducer's simultaneous acoustical and microwave imaging capability will offer many advantages in medical diagnostic applications that will lead to the earlier diagnosis of medical conditions when they are more readily curable and less costly to treat. The conduct of the research will also contribute to the development of a number of highly qualified persons (HQP). The planned technology transfer initiative should enhance the global competitiveness of Canadian firms participating in collaborative research and innovation partnership.""452748,""ChowFraser, Patricia"
"451160"	"Christara, Christina"	"Numerical methods for partial differential equations: algorithms and software on innovative computer architectures"	"Our world is comprised of physical and technological phenomena, objects and situations. In order to study the world, mathematical models are often used. These are usually equations, for example, ordinary or partial differential equations, linear or non-linear equations, integral equations,etc. The mathematical models are often insolvable by analytic mathematical techniques. Numerical methods are applied to solve or approximate the solution to mathematical models. These methods are implemented and run on computers. Numerical methods as well as computations may introduce errors. The computer results are studied and possible sources of errors are investigated. If the results are not satisfactory, either the mathematical model, or the numerical method to solve the model, or the computer program implementing the method have to be adjusted and the whole procedure repeated. Mathematical modelling, the development of numerical methods to solve mathematical models and the implementation of the methods on computers are important and wide areas of research, spanning computer science and mathematics, as well as physics and engineering.Partial Differential Equations (PDEs) are the basis of many mathematical models of important physical and technological phenomena. For example, PDEs are essential mathematical elements for the prediction of weather and climate or the construction of high-rise buildings. This research involves the development and analysis of numerical methods for PDEs, and the development, testing and evaluation of mathematical software for the solution of PDEs on a variety of computer architectures. Questions that this research tries to answer include:- How accurate is a numerical method for computing tomorrow's temperature, atmospheric pressure and wind velocity? How can we reduce the error?- To what extent can we use a number of computers or processors working in parallel to reduce the calculation time?""458703,""Christen, Andreas"
"456811"	"Coady, Yvonne"	"STORM: an environment for cloud computing"	"As we face a dramatic increase in clouds/clusters/cores, we need to rethink the engineering of scalable software. The challenge is not only to meet the expectations of demanding application environments, but also to make efficient use of available resources. Applications will increasingly rely on distributed, collaborativeand virtual environments that require scalable and robust deployment and maintenance strategies. This requires innovation for new interfaces and new infrastructure. Whether a developer is composing services between virtual nodes in clouds, or coordinating computation between physical cores on the same machine, the question of how to support sustainable engineering practices for software that must scale remains largely unanswered.Recent efforts to address some of these issues in the parallel programming community have resulted in new domain specific design patterns, languages, libraries and frameworks to assist mainstream developers now working on many-core systems. The primary research objective of STORM (Scalable Techniques forOverseeing Resource Management) is to build upon these efforts and to establish support for sustainable software engineering practices within cloud environments. Specific objectives for the research include: determining requirements in these environments and identifying current challenges in deployment and maintenance tasks in terms of interfaces and infrastructure, determining efficacy of current tools and identifying current processes developers follow for constructing applications in these environments, discovering potential to better support composition and coordination activities and designing tools and infrastructure accordingly, and implementing prototypes and evaluating efficacy relative to current approaches for both experts and newcomers to the domain.""455972,""Coates, Mark"
"463854"	"Collins, Christopher"	"Information visualization for large-scale text and mixed data"	"Textual data is at the forefront of information management problems. Despite the improvements in the ways computers manipulate language data, our most common interfaces provide insufficient support, presenting information as long sequences of text for sequential reading.  Information visualization has been successfully employed to provide interactive tools for summarizing and analyzing many types of data.  However, existing visualization techniques do not make use of available computational methods for text analysis, relying instead of simple word counts.  The lack of easy-to-use methods to access, explore, comprehend, use, share, and annotate large volumes of text data is likely one of the reasons for growing feelings of information overload.  In order to solve this problem, we need to create new interfaces which present information in ways that facilitate reading, searching, analyzing, and annotating ever larger volumes of text data.  Specifically, the proposed research will develop new techniques which combine linguistically-sophisticated algorithms with custom-designed interactive visual representations grounded in human perceptual capabilities.  We will contribute new tools for analyzing large-scale online and streaming text datasets, from blogs to the proceedings of Parliament.The research will move beyond text databases, incorporating non-textual data such as illustrations from digital books, statistics from scientific experiments, and data related to text such as author names and dates.  Combining both textual and non-textual data, we will present contributions which leverage complementary information in multiple types of data to present powerful new visual analysis methods for large mixed datasets.  We expect up to 9 students, including undergraduate, master's, and doctoral to receive research training through participating in this research program.""456153,""Collins, David"
"463575"	"Constantin, Nicolas"	"High performance integrated circuit module for multiple-input multiple-output wireless transceivers"	"Mobile wireless communication equipment must be energy efficient to preserve battery life; enable increasingly high speed data transmission; and be small. The Radio Frequency (RF) transmitter section in a wireless transceiver has to meet all these requirements, as it plays a critical role in energy efficiency performance and is subject to the technological challenges related to transmission speed and size.This research program will contribute to the advancement of Radio Frequency Integrated Circuit (RFIC) technologies, mixed-mode Analog-Digital circuit techniques and Digital Signal Processing (DSP) techniques intended for the RF transmitter multi-IC modules that will be used in 4th generation (4G) wireless communication equipment. Our research work will emphasize the development of new methods to improve the energy efficiency and miniaturization of these modules. The general objectives are: (i) to propose and validate innovative wireless transmitter architectures employing Digital, Analog, and RF IC and DSP techniques for 4G Long Term Evolution (LTE) communication equipment that employ a Multiple-Input Multiple-Output (MIMO) architecture; (ii) to find new solutions for high energy efficiency and miniaturized transmitter multi-IC modules for mobile LTE/MIMO equipment; and (iii) to establish a permanent pool of highly qualified researchers in mixed-signal and RF ICs at the École de technologie supérieure (ÉTS), who will master the skills required in terms of research, design and prototyping for the development of new semiconductor and RF micro-system-based devices for wireless communication equipment.This research program will also contribute to the training of highly qualified personnel in Canadian universities, and prepare Canadian engineers and researchers for lucrative employment in the high tech sector. Once the program's objectives have been achieved, an effort will be made to transfer its outcomes to Canadian companies involved in microelectronics for wireless communications. Therefore, it will contribute to improving the high tech employment prospects for Canadians and help strengthen the Canadian economy.""458035,""Constantinescu, Daniela"
"450294"	"Corinthios, Michael"	"Generalised distributions and digital signal processing applications"	"1.    )A first objective is to apply the new extended Bilateral Laplace, z, and Fourier-related transforms recently introduced by the candidate, thanks to his generalisation of distribution theory, to digital signal processing of physical, chemical, biomedical signals, as well as to the harmonic analysis of music signal. A book by the candidate, just published June 2009, ""Signals, Systems, Transforms and DSP..."",  1346 pages, Taylor and Francis (CRC), is announced by the publisher as the new Reference Book of the domain. The book, which introduces the generalised distributions and the new transforms, should be instrumental in guiding graduate students through new areas of research. Two Ph.D. and four Masters students are pursuing projects on the applications of the new transforms.  Writing a second book on the generalization of distribution theory and the extension of transforms is proposed. Moreover, one Ph.D, candidate has applied to be admitted to the team. 2.    )A second objective is to extend the candidate's recently proposed alternative z-domain approach to Prony's Method to cases where the signal component frequencies are not 'well behaved'. Initial results surpass Pade Approximation, Prony's method, least squares estimation, linear predictive coding and the Steiglitz-McBride approach used by Matlab, in the case where the signal component frequencies are  'well behaved' multiples of the fundamental frequency. 3.    )A third objective is to implement parallel cellular architectures for digital signal processing using Field programmable Gate Arrays FPGAs. Thus far a deposition for a patent of invention of one Masters student, with the candidate as co-inventor, has been applied for. Parallel processing using FPGA-based complex-variable floating point arithmetic cellular arrays is proposed. Transformations are to be effected on a general-radix hypercube, configuring the machine architecture by employing a general number of such general-radix parallel arrays.""468125,""Corkery, Catherine"
"450604"	"Corless, Robert"	"Algorithms for structured nonlinear models"	"Many scientific and engineering applications are studied in part by constructing computational models and simulations.  These models, which often use advanced mathematics to describe real situations of practical interest, then need to be 'solved' in order to find design parameters that create the desired behaviour in the model or simulation.Examples of particular interest to me include computational models of the dynamics of various diseases; some caused by parasites, like malaria, and others caused by hereditary defects, like Stargardt's disease (which is a rare condition causing blindness, starting at about the age of puberty, and affecting about ten thousand Canadians).  Other applications I am interested in include problems in computer-aided geometric design, which is used for example in robotics and in manufacturing.The nonlinear differential equations that underlie many of these models give rise to nonlinear algebraic equations whose solutions describe the location and stability (that is, sensitivity or lack thereof to small changes in the data or model equations) of the design situation.This project examines new, numerically stable ways of looking at a wide variety of these kinds of problems, and proposes computationally robust methods for solving many of them. These solutions are intended to be useful in addressing the application problems discussed above.""465926,""Corlett, Eric"
"452557"	"Cros, Jérôme"	"Optimisation de l'efficacité énergétique des machines électriques fonctionnant à grande vitesse"	"Le domaine des machines électriques et des convertisseurs électroniques de puissance connaît un essor important en raison du développement des énergies renouvelables et de la multiplication des applications de transformation et de stockage d'énergie. Les machines doivent maintenant satisfaire à des exigences de plus en plus sévères en termes d'efficacité, d'intégration, de compacité, de cycle de vie et d'impact environnemental. Ces dernières années, nous avons pu constater des progrès significatifs dans les matériaux magnétiques qui offrent de meilleures performances à haute fréquence. Il en est de même dans l'électronique de puissance avec de nouvelles structures de convertisseurs et de nouveaux composants plus puissants et pouvant fonctionner à plus haute fréquence. La commande numérique a aussi connu une révolution technologique avec de nouveaux microprocesseurs et DSP, à faible coût, qui sont plus rapides et qui permettent l'implantation d'algorithmes complexes. Tous ces progrès ont des répercussions importantes en électrotechnique pour augmenter la compacité des systèmes de puissance en agissant sur la fréquence de fonctionnement et la vitesse de rotation des machines électriques. Cette tendance s'observe sur une très large gamme de puissance (50W à 5MW) pour des applications très variées. En effet de nombreux domaines requièrent le développement de motorisations électriques légères et performantes qui fonctionnent à vitesse élevée (Systèmes embarqués pour les véhicules hybrides, le transport ferroviaire et l'aéronautique). Cependant, un fonctionnement à vitesse élevée comporte de nombreux défis technologiques qui doivent être surmontés. Il faut rechercher des structures, des géométries et des matériaux qui offrent le meilleur compromis possible entre la compacité, les performances, les contraintes mécaniques, la qualité du fonctionnement et le coût. Il faut aussi définir de nouvelles techniques de fabrication pour les bobinages et l'assemblage du moteur. Ce projet va traiter cette problématique pour favoriser le développement d'entrainements compacts et efficaces dans des applications à grande vitesse.""467311,""Crosby, Alex"
"456538"	"Damen, MohamedOussama"	"Enabling techniques for wireless networks"	"This proposal addresses the construction of enabling techniques in wireless networks and consists of two research projects. First, we tackle the development of efficient techniques for synchronous networks. Second, we develop novel and efficient protocols that take advantage of the synchronization errors between the different nodes in a wireless network. We address the fundamental problem of finding optimal MIMO coding schemes for a given complexity constraint. We then extend our approach to joint channel and network coding in wireless networks. In the second part, we will focus on exploiting synchronization errors that occur naturally in wireless networks in order to increase the available degrees of freedom in the system. The performance enhancement that we obtain via the asynchronous relaying protocols is due to the fact that the resulting channel model in the asynchronous scenarios is similar to a parallel channels model with as many independent links as the number of transmitting nodes, each with independent transmitted signals. We propose to use this insight in some wireless communications setups that are of major importance for future standards. In particular, performance analysis in the finite signal-to-noise ratio regime will be carried out for artificially asynchronous co-located MIMO channels with strictly bandlimited waveforms. We will also tackle the problem of interference alignment using artificial asynchronous transmission combined with oversampling techniques at the receiver. We expect to show that in this case there is no need to have a time/frequency/space varying fading coefficients in order to align the interference. The new method of interference alignment, obtained by projecting the interfering signals onto the additional degrees of freedom available form the asynchronous transmission, is more practical than conventional interference alignment because it does not assume perfect channel knowledge at the transmitter.""451710,""Damha, Masad"
"455714"	"Damian, Daniela"	"Requirements-driven collaboration"	"This project studies the collaboration of cross-functional teams that are involved in the activities of requirements definition, negotiation, implementation and management throughout a project lifetime. This collaboration, which we term requirements-driven collaboration, involves communication and coordination activities of diverse project members (customers, business analysts, architects, developers, testers). Research shows that effective ongoing coordination among these team members is critical to successful implementation of requirements and hence project success. It also shows that it is challenged by the multidisciplinary nature of these team members' work activities as well as professional, organizational and cultural (in global teams) backgrounds. Our main research objectives include (1) the development of models of communication, coordination, and knowledge management in requirements-driven collaboration, (2) the development of techniques to study the alignment between the technical and social dimensions of work in these cross-functional teams, and (3) the design and evaluation of tools that implement these models and techniques and support effective coordination of cross-functional teams. To conduct and evaluate our research, we will conduct field studies that leverage our already established industrial collaborations with IBM and Dell. Our data analysis includes methods of social network analysis on social networks of cross-functional teams in requirements-driven collaboration, other methods such as content and conversation analysis of communication for the qualitative data about collaboration.The innovative aspect of this research is the concept of role is what differentiates the study of cross-functional teams from the traditional studies of developer teams. This research will bring significant empirical evidence and empirically developed models and techniques to complement the scarce knowledge we have about how large distributed teams communicate and coordinate their work across functions to implement requirements.""472507,""Damjanovski, Sashko"
"455237"	"Datta, Suprakash"	"Algorithms for communication networks and bioinformatics"	"My research draws from my background in Computer Science and Electrical Engineering and aims to propose solutions to problems in Computer Networks and Bioinformatics. I work on various aspects of wireless sensor networks, which are an exciting new development in Computer Science and Engineering. Sensor networksare a wireless network formed by tiny battery-powered nodes that are capable of doing computation and communication. Typically these nodes are deployed in some random manner and then they self-organize into a coherant network that can be queried by a remote user. These networks have applications in many scenarios including regions dangerous for humans or regions where human activity is not desirable. I am especially interested in the problem of localization or determination of geographical locations of sensor nodes in a distributed manner. My previous work (supported by NSERC) resulted in better algorithms for sensor network localization. I plan to work on localization in more general scenarios as well as other problems in this area. I would also like to improve performance by making use of better battery models.I will also work on designing efficient algorithms for computational problems in Bioinformatics. Recent advances in Biology have resulted in the generation of huge volumes of data that cannot be analyzed by hand. For example, many new genomes are being sequenced and automated analysis is essential for understanding them. I collaborate with Biologists at York University as well as the University of Rochester to solve computational problems and design efficient algorithms that result in tools that Biologists can use in their research. Much of my work is in the area of genomic signal processing which is a new field that involves application of ideas developed for signal processing (including image processing and speech processing) to Biological data. ""464613,""Dattani, Nikesh"
"455249"	"Day, Nancy"	"Abstraction in model-driven engineering"	"Formal methods (FM) can reveal critical logical errors in computer-based systems.  Companies, such as Intel, Microsoft, and Rockwell Collins, and producers of safety-critical systems, such as NASA, are investigating FM to find bugs prior to product deployment.  FM evaluate system behaviour symbolically and exhaustively.  With these improved capabilities to evaluate software quality, we can (1) build systems with increased design complexity for exciting applications, such as pilotless aircraft; (2) rely on software for critical tasks in mission-critical systems, such as air traffic control and medical equipment; and (3) reduce development and maintenence costs because errors discovered earlier in the design cycle are cheaper to correct.  Model-driven engineering (MDE) is an emerging and popular software engineering process in which downstream development artifacts (such as code) are derived directly from a model of a software-based system.  FM give a precise meaning to these models, which in turn enables the analysis of these models for purposes beyond just derivation of code. A major challenge in the application of FM for MDE is the state space explosion problem -- the fact that most systems remain too big to explore exhaustively even a finite set of behaviours despite the rapid increase in computing capacity. MDE would benefit from the ability to create more abstract models to enable its use from the beginning of the system development process. The goal of this research is to investigate whether three types of abstractions users create during modelling can be exploited to extend the size of the state space we can explore in formal analysis of abstract models. The three types of abstractions considered are: language abstractions (those provided by high-level modelling languages), structural abstractions (such as definitions), and evolutionary abstractions (placeholders for incomplete information). As a result of this research, it will be possible for formal abstract models to be used earlier in the MDE development process, and play an active, integrated role throughout the evolution process.""454293,""Day, Troy"
"449601"	"Deen, Jamal"	"High-performance optical detectors and imaging systems for emerging applications"	"Over the next 5 years, we will pursue an aggressive research program on advanced, high-sensitivity and high-speed photodetectors and optical detection systems targeted to emerging applications in medicine, biology, biomolecular sciences and chemistry. This will represent a major forward leap in current research. It will significantly leverage existing research expertise and leading edge results, to model, design, fabricate and characterize novel photodetectors and imaging systems using optimized device designs, circuits and systems, as well as emerging semiconductor detector structures and materials such as quantum dots and antimonides, respectively. We will also leverage existing research expertise in ultra-low-power CMOS-based integrated circuits to create fully integrated silicon-based and hybrid compound semiconductor-based optical detection systems for  low-cost, miniaturized analytical systems. These systems will be used for diagnostic testing in medicine, for fluorescence-based detection, and for detecting various gaseous pollutants such as methane, ammonia, carbon monoxide, carbon dioxide and sulphur dioxide. These hybrid or fully integrated systems will utilize photodetectors based on silicon for new single-photon and pixel configurations, or on antimony for quantum-dot infrared photodetectors. The proposed research work will be divided along two major material types - silicon and compound semiconductors.     The novelty of the proposed research is several-fold - design optimization using solid physics-based modeling and successful, first-time manufacture of novel silicon-based optical detectors and imagers; computationally efficient and accurate modeling and fabrication novel compound semiconductor photodetectors; and high-performance optical detection systems for ultra-low level light imaging. The research is also highly significant because it targets novel and emerging applications of high national interest and needs in health and environmental sciences, and can also be applied in industrial and security monitoring.""453756,""Deen, MJamal"
"450432"	"Dehne, Frank"	"Parallel algorithms for multi-core & many-core processor clusters and applications in online analytical processing (OLAP) and computational biology"	"The aim of parallel computing research is to create enabling technology for solving data intensive and/or computationally hard problems. Processor clusters, including clusters of multi-core and/or many-core (GPU) processors, have emerged as architectures of choice because they are built from commodity hardware and have a good price performance ratio. However, for many problems there exist no efficient parallel algorithms and parallel software that achieve satisfactory speedup and are scalable. This research program is focused on the design and implementation of efficient parallel algorithms, the interrelationship between the theoretical analysis of parallel algorithms and the performance observed on current parallel architectures, and the use of efficient parallel algorithms for parallel online analytical processing (OLAP) and parallel computational biology & bioinformatics.    The following is a list of proposed research topics: (1) Parallel algorithm performance and performance portability on the multitude of current multi-core and many-core processor clusters, with emphasis on combinatorial and graph problems. (2) Parallel real-time online analytical processing (OALP) algorithms and software prototypes, providing efficient real-time OLAP updates that allow business intelligence applications to analyze up-to-date business data. (3) Parallel risk modeling on multi-core and many-core processor clusters, providing improved accuracy for risk management decision support systems and enabling insurers to better analyze accumulations of risk. (4) A large scale, parallel protein interaction prediction engine (PIPE) that is at least two orders of magnitude faster than the current PIPE for yeast proteins and enables us to predict the entire human proteome. (5) New parallel algorithms for the efficient design of RNA/DNA pools for in vitro selection of more complex nucleic acid molecules, called aptamers, enabling e.g. biosensors that bind to target molecules such as antibiotics or viruses with improved affinity and specificity.""460965,""Deibel, Donald"
"455051"	"Denzinger, Jörg"	"Testing multi-agent systems for unwanted behavior"	"Computer systems that adapt themselves or interact with other systems provide new challenges to software testing methods. While such systems offer the possibility for more autonomy (and less need for human administration) and the possibility for emergent behaviors that provide abilities that are not possible for a single system, these same possibilities can also result in negative and unexpected effects. A system might adapt in the wrong way, or we might have emergent misbehavior when several systems try to interact with each other. This research proposal is about new testing methods that are able to find such unwanted behavior of a system or a group of systems (a multi-agent system). This will help to ensure that we can make use of the advantages of multi-agent systems without special safety concerns.In contrast to ""conventional"" testing methods that test if a system shows the right response in given scenarios, our testing method is able to find scenarios in which a system is showing an unwanted behavior. Using our approach, we were already successful in finding unwanted behavior in the FIFA-99 soccer game (namely user actions that always resulted in scoring a goal against the computer player), in a group of rescue agents written by students in a multi-agent system class (namely creating behaviors of agents interacting with the students' agents in such a manner that the students' agents froze), and in harbour patrol and interception policies (namely behaviors for terrorist boats that resulted in them achieving a given objective despite the defenders correctly following the policies). With the proposed research program, we investigate extensions to our basic method that expand our basic process to allow for the introductions of subgoals, that allow the integration of various knowledge about testing, the application area and the tested system, and that create the possibility to have previous test runs influence the next test run. This research will enable us to develop an extended general framework for testing multi-agent systems for multi-agent specific problems together with guidelines on how to instantiate this framework for concrete multi-agent systems.""455485,""DePassillé, AnneMarie"
"463934"	"Descoteaux, Maxime"	"Computational diffusion magnetic resonance imaging: acquisition, modeling, processing and visualization to study brain connectivity and tissue microstructure"	"Alzheimer's disease, Parkinson's disease, Huntington's disease, multiple sclerosis, and other neurodegenerative diseases currently impact on a large proportion of the aging Canadian population. It remains unknown how fiber connections in the brain white matter are degenerated and damaged by these diseases. Diffusion magnetic resonance imaging (MRI) is the only non-invasive technique able to obtain information about the neural architecture of the white matter. Diffusion MRI has proven of utmost importance in the past 10 years in a wide range of neuroscience studies; for neurodegerative diseases as aformentioned, but also for brain development studies and neuro-surgical planning. Hence, diffusion MRI provides a unique tool to study anatomical connectivity and better understand how the brain is wired. The main objective of my research program is to develop state-of-the-art diffusion MRI acquisition, modeling, processing and visualization tools to study anatomical connectivity and tissue microstructure in brain development studies and neurodegenerative diseases. Another important objective is to transfer my diffusion MRI methods, algorithms and systems to clinical applications.In the current proposal, I will develop diffusion MRI acquisition, modeling and reconstruction algorithms to study brain connectivity and tissue microstructure. I will also develop algorithms to merge information from different imaging modalities, such as positron emission tomogramy (PET), MRI, diffusion MRI, and functional MRI, in order to take advantage of the molecular, anatomical, structural, and functional information available. As a long-term objective, I will propose new techniques to validate fiber tracking methods using ex vivo phantoms, animal models and contrast agents dedicated to diffusion in the white matter tissue.""466073,""Descoteaux, Raphaelle"
"464030"	"Desrosiers, Christian"	"Forage de données dans les réseaux dynamiques complexes"	"Les réseaux sont reconnus comme étant de puissants modèles pour représenter des entités et leurs relations dans diverses applications. L'existence de méthodes efficaces pour analyser et extraire de l'information utile dans ce type de données est essentielle à plusieurs domaines importants dont les télécommunications, la bioinformatique, la sociologie et l'Internet. Cependant, l'émergence récente de nouvelles applications utilisant des réseaux dont les entités et leurs relations évoluent dans le temps a mis en évidence la nécessité de développer des outils permettant de modéliser et d'analyser efficacement les propriétés de réseaux dynamiques. L'absence de tels outils représente un problème sérieux car elle limite la pleine croissance des technologies utilisant ce type de données et empêche le développement de nouvelles applications prometteuses. L'objectif de ce projet est de développer de nouvelles approches pour analyser et prédire l'évolution des entités de réseaux complexes et de leurs relations. Pour atteindre cet objectif, deux problèmes importants seront étudiés: (i) la découverte de patrons relationnels complexes temporellement corrélés et de patrons récurrents, et (ii) la prédiction de l'état global d'un réseau dynamique complexe à l'aide d'une représentation dans un espace latent à faible dimension. Le développement de méthodes pour identifier des corrélations temporelles bénéficiera à plusieurs applications clés dont la détection d'événements importants, pouvant correspondre à des activités malicieuses dans le réseau, et l'identification de faiblesses structurelles dans le réseau. Ces méthodes permettront, entre autres, de prévoir l'apparition de certains patrons dangereux à partir de l'observation de patrons leur étant corrélés, rendant ainsi possible l'instauration de mécanismes efficaces de prévention. De même, prédire l'état futur d'un réseau évoluant dans le temps permettra d'anticiper l'émergence de tendances structurelles ou sociales importantes, telles que la formation de nouveaux groupes au sein d'une population.""450034,""Desrosiers, Jacques"
"455156"	"Deters, Ralph"	"Software infrastructures for mobile cloud computing"	"""Cloud Computing"" is a broad term that encompasses applications/functionality delivered as services over the internet and the software/hardware infrastructure to host them. This ""utility computing model"" (John McCarthy, 1961), allows individuals and organizations to purchase virtualized hardware (Infrastructure as a Service, IaaS), software platforms (Platform as a Service, PaaS) or applications/functionality (Software as a Service, SaaS) in a pay-as-you-go manner, comparable to the metered purchase of electricity, gas or water. While seen traditionally as a means for organizations to outsource IT infrastructure, cloud computing can also be used to provide the billions of mobile handsets (mobile/smart phones) with additional resources. While mobile handsets are nowadays heavily used for data communication most notably SMS and basic internet services like email and browsing, their limited computational resources restrict their access to applications/ functionality delivered as web services. To overcome this challenge Mark Baccue proposed the idea of Mobile Cloud Computing, which is based on the concept of using cloud-hosted software components (e.g. proxies) to aid mobile handsets in accessing web services.This research has 4 objectives. First, it studies the design and use of proxies as a means of supporting mobile/smart phones in accessing web services. Second, it investigates ways to minimize the latency for the mobile devices by using caching and pre-fetching. Third, it investigates ways to enable the large scale deployment of proxies by focussing on scalability and dependability issues. Forth, it evaluates the energy footprint of users accessing applications/functionality via mobile/smart phones as opposed to using PCs. ""453142,""Deugo, Dwight"
"451848"	"Dew, Steven"	"Nanofabrication and nanointegration processes"	"This proposal is for three sub-projects related to nanofabrication. The first is to develop an integrated understanding of the sputter deposition process employed extensively to create thin films used in micro- and nanoelectronics, magnetic storage, optics, and other industries. The project will provide a better understanding and control of the relationship between film properties and process parameters. As a result, higher quality and newer applications of films can be developed. The second project is to study the ability of electron beam lithography to form nanoscale patterns. This work will lead to better electron beam processes to allow for higher resolution and greater repeatability of lithographic structures. The final project deals with the use of biomolecules to assemble micro- and nanocomponents into functioning nanosystems. Antibodies and DNA will be used to allow the highly specific and parallel assembly of various different submicron components from solution onto a common substrate.  In particular, this work will focus on positioning, orienting and attaching nanochips onto a silicon wafer and then wiring them up. This will provide an architecture for the commercial realization of functioning nanosystems.  These projects are proposed to be in collaboration with industry and the NRC National Institute for Nanotechnology (NINT).""464233,""Dewar, Michael"
"454631"	"Dickinson, Sven"	"Image abstraction"	"One of the most challenging problems in computer vision is object categorization -- the problem of recognizing previously unseen objects belonging to known categories.  While early object modeling paradigms were 3-D and captured the prototypical shape of an object, the representational gap between such high-level shape models and the low-level image features that could be reliably extracted was wide, and such systems were restricted to highly constrained scenes.  Over the past 20 years, the recognition community moved away from modeling the 3-D prototypical shape of an object to modeling the specific 2-D appearance of the object, supporting the recognition of highly complex, highly textured objects.  Unfortunately, appearance-based features such as texture and colour are seldom generic, and while different members of a category may have similar prototypical shape, they may have dissimilar appearance.  As a result, an appearance-based categorical object model does a poor job of modeling previously unseen members of the same category that have significantly different appearance.  Shape is now making a comeback in the object categorization community, but is typically used to model the specific, as opposed to prototypical, shape of an object.  The wide representational gap still remains, and can only be bridged through the process of image abstraction: perceptually grouping together image features that belong to the same object or object part, and simplifying each such group using an appropriate low-dimensional, or reduced, model.  This research program addresses the problem of image abstraction on multiple fronts, including the extraction and grouping of symmetric parts from a 2-D scene, the recovery of 3-D surfaces and volumetric parts from a 2-D scene, and learning the semantics of objects and actions in images and videos through language-vision integration.  I believe that the problem of image abstraction is the most challenging and most important problem facing the object categorization community.  Only when we've defined an appropriate set of 2-D and 3-D abstractions and can recover them from complex, cluttered scenes, can we accomplish true object categorization.""463291,""Dickinson, Timothy"
"456626"	"Dubé, Danny"	"Improving the implementation of high-level languages"	"The primary objective of my research consists in improving the implementation of high-level programming languages, and that of functional languages in particular.  Programming has slowly been moving towards high-level languages, that is, towards simpler, more expressive, more powerful, more intuitive languages.  Most of today's software projects would simply not be feasible if assembly languages were still the only languages available.  Even if languages such as C and C++ are considered high-level compared to the assembly languages, they remain low-level compared to languages such as Haskell, ML, Scheme, and Java.  While my primary objective relates to the implementation on high-level languages, I have other main objectives.  I am interested in improving the implementation of languages, even those that are not high-level languages.  I am also interested in increasing the reliability of software in general, not just the language-related tools.  Even if I have a strong preference for high-level languages, lower-level languages are widely used nowadays and might continue to be used in certain applications for a very long time.  Consequently, improving the implementation of these languages is important too.  Regarding the reliability of software, it is obvious that it is not as high as it should.  If cars, airplanes, buildings, and bridges were as (un)reliable as software, it would be catastrophic.  Software production is clearly not as well established and as systematic as other production processes, e.g. in most engineering disciplines.  Given my objectives, I intend to continue to conduct research on compiler technology, especially on analysis and optimization techniques, whether it is related to high-level languages or not.  I also intend to contribute to the development of techniques that directly aim at improving the reliability and the security of software---like programming with contracts and static and dynamic formal software verification.  Projects that I intend to realize include the static profiling of functional programs, the extension and improvement of adaptive static analysis, the development of a quick control-flow analysis, the application of our technique called Extended PCC to oracle-based PCC, the pursuit of the development of bit recycling.""469417,""Dubé, Dany"
"457280"	"Dubois, Maxime"	"Conversion électromécanique pour éoliennes"	"La demande en électricité ne cesse de croître et l'énergie éolienne est considérée comme une forme d'énergie clé pour réduire l'impact environnemental de notre consommation tout en assurant cette croissance. Bien que le kWh éolien soit de plus en plus compétitif, il reste beaucoup à faire pour réduire ses coûts et c'est dans cette perspective que s'inscrit le projet de recherche proposé ici. Une éolienne est une technologie complexe qui fait intervenir de nombreux composants mécaniques et électriques. Étant donné qu'une éolienne tourne à très basse vitesse, il est assez fréquent de voir les constructeurs utiliser des engrenages mécaniques pour adapter ce régime à celui de l'alternateur, unité qui convertie l'énergie mécanique en énergie électrique. Cependant, ces composants mécaniques réduisent le rendement global de l'éolienne et doivent être entretenus. D'autres constructeurs ont plutôt adoptés la philosophie de concevoir l'alternateur de manière à se qu'il soit compatible avec les faibles vitesses de révolutions des pales. On qualifie ce type d'éolienne à attaque directe. On évite ainsi l'utilisation de la plupart des composants mécaniques ce qui améliore le rendement et simplifie la structure de l'éolienne donc de l'entretien. Or, pour parvenir à des puissances de 2 MW, l'alternateur d'une éolienne sans engrenage devrait avoir un diamètre de plus de 5 mètres. En conséquence, l'objectif à long terme de la recherche est la réduction des coûts de l'énergie éolienne et plus spécifiquement des coûts, masse et volume des alternateurs/convertisseurs pour éoliennes à attaque directe par l'utilisation de topologies de machines à aimants à flux transverse et par l'utilisation de nouvelles approches de convertisseurs électroniques. Pour y arriver l'équipe entend : 1) construire 2 prototypes d'alternateur à flux transverse à stator hybride, les installer dans deux éoliennes et les mettre à l'essai en conditions réelles; 2) simplifier la fabrication par le brasage de pièces magnétiques; 3) développer un modèle raffiné de pertes magnétiques; 4) construire un redresseur à diode assisté en commutation et le mettre à l'essai dans une éolienne et 5) réduire le couple de détente des alternateurs à flux transverse pour améliorer le démarrage des éoliennes par vents faibles.""469419,""DuboisLachance, Rémy"
"451692"	"Dymond, Patrick"	"Complexity and parallel algorithms"	"Complexity and parallel algorithmsParallel complexity theory studies the algorithms, machine structures, networks, and resources needed for parallel computing (including parallel time, communication costs and number of processors.) Many computational tasks remain beyond the reach of even the fastest sequential processors.The long-term objective of this research is the development of a general theory of methods to solve computational tasks efficiently on massively parallel and/or widely distributed processors. To make progress on this objective, many significant problems must be addressed, including learning how algorithms can be designed to make effective use of parallelism (i.e. efficiently make use of many processors). In addition, we study application areas of parallel and distributed computing,  including multi-agent robotics and path planning, with applications to robot search and to accessibility assessment.""451314,""Dywan, Jane"
"456825"	"Eck, Douglas"	"New machine learning methods for music: learning at multiple timescales"	"Great advances have been made in the design of machine learning algorithms for music.  However, most approaches ignore global temporal structure, chopping an audio file into segments of five seconds or less and treating each segment individually.  By ignoring long-timescale structure, these algorithms are unable to learn important high-level indicators of musical meaning such as melody and chord progressions.  This gap defines the core challenge addressed in this research proposal: how can we approach music at multiple timescales so as to in parallel learn important local and global structure from audio?  The objective of this research program is to develop new machine learning algorithms for music recommendation, human-like music performance and adaptive context-aware generation of music for video games. Our goal is to design hierarchical multi-timescale algorithms that incorporate evidence at many levels. We believe that musical meter is a particularly informative prior for a machine learning model trying to learn how music is organized in time. A model that can track meter in a human-like manner knows when important events will occur in the music signal, thus avoiding an exponentially-long search for correlation among all possible time lags in the music.  The music related research addresses several difficult and relevant tasks related to music information retrieval and to music performance. Specific tasks include predicting the similarity between two audio sequences, predicting the artist, genre or style of audio, performing music in a human-realistic fashion, generating music (e.g. for video games) that is stylistically appropriate and that fits a particular context (e.g. scary or calm). In addition to contributing industry-relevant applications, the fundamental research carried out here will advance the state-of-the-art in machine learning algorithms for time series data and will further our understanding of how music is produced by musicians and perceived by listeners.""453917,""Eck, Peter"
"450433"	"Ellen, Faith"	"The complexity of fundamental problems in distributed computing"	"Distributed computing is concerned with processors that communicate with one another. They may run at different speeds and may be competing for resources, such as the use of a shared printer. It is difficult to program the processors to ensure that all of them make progress on their own tasks, especially if some of the processors can fail.My research studies fundamental problems that frequently arise in distributed systems. One example is how to maintain a queue of tasks that any processor can take from or add to. Good solutions to these problems are used as building blocks to solve other, more complicated problems.My goal is to find the provably most efficient solutions to these problems. This involves understanding what aspects of these problems are difficult, to develop better solutions or prove limitations on what can be achieved. I also investigate what features in the hardware make these problems easier or harder to solve.""453510,""Ellery, Alex"
"450462"	"Elmasry, Mohamed"	"Logic-in-memory architectures for variations-aware integrated circuits"	"Next Generation Digital Microchip Design for 2020The usage of digital microchips in a wide range of applications, from smart phones to cars and from PCs to aircrafts has been exponential.This exponential increase in microchip usage was made only possible because researchers, including UW's Prof Mohamed Elmasry and his team, for the last 35 years were able for every new chip generation not only to double chip performance and functionality but also to do it at half the cost.The cost of a microchip is directly related to yield; the number of good chips within a fabricated silicon wafer. Yield is a complex function of different parameters including those related to fabrication. In the design phase these parameters must be taken into consideration.To design next generation microchips, for the year 2020 and beyond, the design process must include a more complex yield function with higher number of parameters, and new design methodology, computer-aided design tools and architectures must be developed. Today variations in fabrication parameters, on-chip voltage and on-chip temperature are all posing major challenges to the design of future high-performance digital microchip design using deep-sub micron (less than 90 nm) silicon technology. The objective of this research is to build on the research results of Prof Elmasry's UW team to explore novel chip architectures to mitigate the impact of these variations resulting in fabrication of high-performance digital ICs at higher yields i.e. lower cost.""457063,""ElMekkawy, Tarek"
"454208"	"Englehart, Kevin"	"Myoelectric control of powered upper limb prostheses"	"The loss of one or both arms is a major disability that profoundly limits the everyday capabilities and interactions of individuals with upper-limb amputation. There are approximately 10,000 patients in Canada with an upper limb amputation.  A suitable prosthetic replacement of the amputated limb(s) would enhance their ability to care for themselves, enable a more diverse range of employment, and provide more opportunities to enjoy an active lifestyle. Myoelectric prostheses use the electromyogram (EMG) signals (the electrical signals generated during muscle contraction) from residual limb muscles to control motorized arm joints.  The use of EMG offers a non-invasive means of establishing a natural interface to the neuromuscular system to control the lost functions of the limb.    Expectations for control of upper limb prostheses are very high however, because of the standard established by able-bodied dexterity.  Although significant advances have been made in building lighter, stronger and more versatile prostheses for clinical use over the last 20 years, little progress made in clinically viable control of these prostheses. Patients who have used myoelectric prostheses clearly indicate that it is the reliability and dexterity of control that is the most significant factor in acceptance of these devices.    The objective of this research is to deliver clinically robust, dexterous control to myoelectric prostheses.  Although great success has been demonstrated (by the applicant, and others) in myoelectric control using pattern recognition methods in controlled laboratory settings, the ability to have these methods succeed clinically (in user's homes and work environments) requires advances in robustness and dexterity that will translate into a meaningful improvement in user experience. This will be accomplished by introducing new training and signal processing paradigms, including novel methods in pattern recognition, adaptive learning, and signal segmentation.    ""472386,""Englezos, Peter"
"457433"	"Evoy, Stéphane"	"Nanoelectromechanical resonator arrays for the analysis of biomolecular mixtures"	"Identification and analysis of biological molecules are vital in many fundamental problems in molecular biology. Micromechanical devices have been proposed as highly-sensitive transducers for the rapid detection and assaying of molecular systems. More specifically, resonant mechanical sensors operate by monitoring shifts of resonance frequencies associated to the binding. The adsorption sensitivity per unit area of mechanical resonators scales favorably as their dimensions are reduced, offering a compelling path for the development large arrays of molecular sensors with exquisite mass-sensitivities. These arrays would enable the realization of multiplexed binding assays that could quantify complex mixtures of molecular systems. We have recently developed a fabrication process allowing the realization of nanomechanical resonators as narrow as 20 nm and with a yield approaching 100%. We have also demonstrated the specific detection of biomolecules such as proteins using individual resonators. This device platform is therefore ready to be applied to multiplexed assays involving millions of resonators per square centimetre. To that end, this project will focus on the four following goals: i) Initial proof of concept through analysis of DNA binding affinity using moderately large arrays, ii) A second proof of concept through analysis of peptide-metabolite affinity on moderately large arrays, iii) Realization of ultra-large arrays using nano-imprint lithography, and iv) Development of all-optical automated read-out system. This cross-disciplinary program will train a group of graduate students in key areas of device research such as silicon surface machining and the fundamentals of MEMS/NEMS technology. It will also expose them to other important areas such as the chemical and the biological sciences. The project will also include a significant amount of synergy with respect to the inter-provincial collaborative activities. Finally, it will bring to fruition intellectual property related to the novel nanomachining technique that was developed during the last five years under support of the Discovery Program.""462882,""Ewanchuk, Andrea"
"454674"	"Farmer, William"	"MathScheme: Integrating axiomatic and algorithmic mathematics"	"The mission of mechanized mathematics is to develop software systems that support the process people use to create, explore, connect, and apply mathematics.  New applications of mathematics in science and technology, especially in software development, often require the use of computers to perform large and complex computations and to check long series of logical deductions.  As a result, there is a strong need for mechanized mathematics systems that can better support and manage the process of doing mathematics.In mathematical practice there is a powerful synergy between computation and deduction.  The division between algorithmic computer algebra systems and axiomatic theorem proving systems has broken this synergy.  To significantly advance mechanized mathematics, this synergy needs to be captured within a single mechanized mathematics system.  That is, algorithmic mathematics and axiomatic mathematics need to be integrated into a single framework.The short-term objective of this research is to develop an experimental mechanized mathematics system in which symbolic computation and formal deduction are integrated without sacrificing the computation power of a computer algebra system or the soundness of a theorem proving system.  The long-range objective is to produce mathematical software for engineers, scientists, mathematicians, and students that is more powerful but easier to use and supports a wider range of mathematical activities than existing systems.""473211,""Farnood, Ramin"
"451256"	"Fleet, David"	"Video-based analysis of human motion"	"We spend a lot of time looking at people.  Arguably, future computer systems will do so too, enabling new man-machine interfaces, perceptive consumer devices, smart surveillance systems, and myriad systems we have yet to envision.  A key enabling technology for such purposes is video-based human pose tracking.  The fidelity with which one needs to estimate 3D pose varies from task to task, but for most interesting applications current techniques are insufficient, especially when compared to the subtle movements with which humans communicate.  The problems stem from the sheer number of joints in the body, which entails a high-dimensional estimation problem.  Soft tissue and clothing obscure the skeleton, producing great variability in appearance, and further ambiguities are due to limited image resolution, the inability to distinguish which parts of an image correspond to which person, and because some body parts are occluded.  To resolve ambiguities and reduce uncertainty caused by noisy (or missing) measurements, prior models play a key role in pose tracking, resolving an otherwise ambiguous estimation problem with a bias toward plausible human motions.This proposal concerns the formulation of computer vision techniques for pose estimation, tracking and the analysis of human motion from digital video.  It also concerns the formulation of techniques for learning and optimizing models of human motion and pose.  This includes the learning of kinematic models from motion capture data, the design and optimization of physics-based models to ensure that estimation motions are physically plausible, the formulation of models that facilitate the inference of interactions between a person and other objects in the environment, and models to support the inference of various physical attributes and mood from the way in which people move.  The long-term goal is the formulation of models of how people move, and the estimation of human motion from digital video.  Many potential applications exist beyond computer vision, including computer animation, biomechanics, rehabilitation and elder care. ""449074,""Fleming, Alison"
"457282"	"Fofana, Issouf"	"Étude de la dégradation et évaluation des performance de l'isolation des transformateurs de puissance"	"Les transformateurs représentent des investissements capitaux dans l'infrastructure de chaque pays. Ils constituent le coeur des réseaux de transport et de distribution d'énergie électrique. Il est alors essentiel qu'ils fonctionnent correctement. L'âge des transformateurs, installés la plupart du temps pendant les années 60 et 70, associé au fait que la demande en électricité n'a cessé de croître au cours de ces dernières années, augmente de jour en jour le risque de voir les plus vieux équipements tomber définitivement en panne et entraîner d'importantes coupures d'électricité. Ce qui soulève des inquiétudes tant au niveau de l'approvisionnement énergétique, de la sûreté du public, de l'environnement qu'au niveau des investissements.L'objectif du projet de recherche proposé est non seulement d'améliorer nos connaissances actuelles du vieillissement des isolations par une étude systématique des mécanismes fondamentaux qui y sont impliqués, mais surtout d'améliorer la fiabilité des transformateurs de puissance. Des solutions permettant de réduire les effets des sous-produits de vieillissement et de l'humidité seront identifiés et testés. Des outils de diagnostic innovateurs et méthodes d'évaluation de condition seront développés pour déterminer le niveau de dégradation et la durée de vie restante des transformateurs et des composants vieillissants. Ce travail de recherche fournira aux ingénieurs et planificateurs d'entretien, des outils d'analyse pratiques à la prise de décision et planification à long terme sur une base solide scientifique. L'application de la recherche proposée à la prolongation de la durée de vie offrira non seulement des avantages économiques importants, mais a également comme conséquence le potentiel significatif de garantir la distribution d'électricité aux utilisateurs. La formation avancée des ingénieurs et chercheurs canadiens dans ces techniques sera un des résultats significatifs du programme proposé.""453935,""Fofana, Issouf"
"455287"	"Fontaine, Réjean"	"Scanners de tomographie d'émission par positrons (TEP), tomodensitométrie (TDM) et bimodaux TEP/TDM haute performance"	"La tomographie d'émission par positrons (TEP) est une modalité d'imagerie mesurant la distribution d'un traceur radioactif injecté dans un sujet. La résolution spatiale de la TEP a largement progressé depuis 10 ans afin de répondre aux besoins de l'imagerie moléculaire. De 2 mm il y a 10 ans, il est maintenant possible de localiser des anomalies, comme des cancers, aussi petits que 1,3 mm. La résolution spatiale devrait plafonner autour de 0,7 mm dans la prochaine décennie due à des phénomènes purement physiques. Afin d'augmenter le taux de bons diagnostics, on fusionne à la TEP une image anatomique comme la tomodensitométrie (TDM) qui consiste à mesurer l'absorption de rayons-X dans un sujet. Plusieurs études mettent en relief le besoin de réduire la dose injectée dans le patient afin de ne pas l'exposer inutilement au rayonnement potentiellement nocif. L'Université de Sherbrooke travaille sur un tel programme de recherche ambitieux où l'on fusionne les appareils TEP et TDM pour utiliser un seul système de détection avec la même électronique.    )Atteindre les performances maximales de la prochaine décennie en TEP et TDM exigera le développement de nouveaux détecteurs et d'une électronique intégrée associée. Étant donné la densité anticipée de détecteurs et la nécessité d'intégrer des systèmes intelligents capables d'extraire l'information pertinente, l'électronique conventionnelle ne pourra plus répondre aux besoins. Pour ces raisons, je propose d'utiliser un nouveau détecteur à base de photodiodes à avalanche (PDA) d'environ 25 µm opérées en mode Geiger et comportant un circuit d'étranglement actif analogique. Chaque PDA sera lue en parallèle par une électronique numérique laminée dans un empilement 3D de circuits intégrés. Je développerai la couche numérique incluant les systèmes intelligents capables d'extraire l'information TEP/TDM. Les travaux formeront des PHQ en l'électronique 3D et en de l'imagerie médicale et supporteront des recherches de pointe en biologie, génomique, protéomique et oncologie. À terme du programme, la santé des Canadiennes et Canadiens sera améliorée grâce, aux découvertes sur les processus des maladies et sur le développement de nouveaux médicaments.""469668,""Foo, Brian"
"456286"	"Franks, Greg"	"Discovery grants - Individual reverse engineering performance models from trace data"	"Distributed computing systems are becoming increasingly important for information access and for administration as organizations in education, health, government, and business move into networks of interacting computers.  Constructing performance models of these systems has shown to be beneficial during all stages of a project. For example, a model can be used to set performance budgets for components of a system.  The model can also be used to find feasible designs and locate potential bottlenecks.  Finally, the model can be used to plan the resources required for installed systems.  In all of these case cases, the use of models can lead to substantial cost savings for a performance-sensitive project. The major problem impeding the use of performance models is the actual generation of the models themselves.  The architecture of the system being studied must be extracted and the resource requirements of the components in the system must be specified.  Getting this information can be time consuming and difficult. The purpose of this project is to simplify the entire model building, solution and analysis process.  Performance models will be created automatically based on the automatic instrumentation and tracing of pre-existing software.  The user need only to execute her software, possibly as part of normal system testing, to generate the performance model.  The instrumentation will be added at the operating system level where events of interest will be logged for further analysis.  The event stream will be converted into a UML sequence diagram which will, in turn, be the basis of a performance model.  Performance modeling will then be moved out of the realm of experts and into everyday use.""449104,""Franks, Ian"
"453061"	"Frize, Monique"	"Development of a distributed intelligent decision support system"	"Today, no technique does well in predicting the occurrence of a premature birth. We intend to integrate various data mining techniques to increase the accuracy of the prediction of the following outcomes: premature birth, delivery type, and status of the infant at birth. We plan to study three clinical databases to assess which factors best predict these outcomes. The proposed work includes the development of an electronic patient record (EPR) capable of handling the multiple types of information (data, images, etc..) and  define a multimedia model adapted to current medical practice in obstetrics and perinatal care. The EPR will be integrated to an improved and expanded decision-support system (DSS). We will implement standards-driven, open and extensible plug-in system architecture; this will seamlessly analyse patient-specific data and information to deliver personalised healthcare. Results stored in the EPR, along with response to treatments and patient prognosis will offer a unique clinical practice tool not available in current research or commercial systems. The final stage is to develop a web services-based infrastructure to enable easy, but secure access to the DSS and to the databases from local and remote locations and by several users at points in time when the information is needed. The prototype will undergo a short pilot test in a clinical setting at the Ottawa Hospital and at the Isaac Walton Killam Hospital in Halifax. Physicians will be able to intervene earlier and improve patient outcomes. Using our system for screening, the costly invasive test (fibronectin) could now be done mainly for patients whose risk estimation is high. Several students will be trained in an area of high demand, in the multidisciplinary environment of engineering, computer science, and medicine.       ""450638,""Froda, Sorana"
"450435"	"Frost, Richard"	"Executable specifications of natural-language speech applications"	"The functionality of computers and the world wide web would be significantly improved if there were more theories and open-source tools that could be used by non-experts to build and deploy natural-language speech applications. Not only would this benefit people with visual, cognitive, or motor disabilities, it would also benefit users who require hands-free access. The objective of this research is 1) to investigate various semantic theories of natural-language and to develop a compositional semantics that will facilitate the construction of efficient user-friendly natural-language interfaces to databases and other sources of knowledge on the web, 2) to develop theory and tools which will allow natural-language processors to be created by programmers with little knowledge of parsing technology, as executable specifications of attribute grammars which define the syntax and the semantics of the language to be processed, and 3) to create tools that will facilitate the deployment of hyperlinked natural-language speech applications that can be accessed by the public using freely-available speech browsers and commonly-used communication protocols.""460269,""Frunza, MagdalenaOana"
"452536"	"Funnell, Robert"	"Three-dimensional modelling of complex natural structures"	"The objectives of our research are to develop better methods for processing 3-D image data representing complex natural structures; for generating 3-D models from the image data; and for manipulating those models.Complex natural structures are often studied using 3-D imagery, but building complex 3-D models from such image data is hard, especially for systems that change with time, like embryos and beating hearts. Image segmentation (identifying structures within the images) is the core of building models but is difficult and time-consuming. And if complex 3-D models are hard to create, they are also hard to understand and manipulate once they have been created.The techniques developed here will be applicable to data analysis and model building in a vast range of fields - mineralogy, paleontology, biology, anatomy and many others. In all of those fields it will also lead to improved methods for teaching and learning at many levels, from Continuing Education all the way down to high school and perhaps even elementary school. We shall particularly address the teaching of anatomy and training for diagnostic and surgical procedures.Our graduate students will be well prepared for jobs in the fields of 3-D image processing, graphics, modelling and virtual reality, which are growing especially fast in the area of medical applications.""449654,""Funt, Brian"
"451607"	"Gagnon, François"	"Computationally efficient wireless reception algorithms under a long term low energy constraint"	"Multi-user wireless communication with active mobility is computationally challenging. Additionally, the complexity typically squares when the Multiple Input Multiple Output, MIMO, architectures are taken into account. A rapid appearance of several long echoes combined with high data rates favor the use of Orthogonal Frequency Division Multiplexing, OFDM. OFDM-based communications are particularly appealing because of the very low complexity frequency equalizing scheme which is used. However, new strategies such as channel shortening and sparse Recursive Least-Squares, SPARLS, equalization are emerging. These render single carrier techniques more computationally competitive by selecting only significant paths for channel equalization. All these issues have a direct impact on power consumption, thereby on battery life. This project aims at reducing the long term average energy consumption related to signal processing. In this context, long term averaging refers either to a multiplicity of successive connections, or calls, for mobile energy efficiency, or equivalently to short-term power consumption for hundreds of simultaneous users detection at a base station.   The objective of the proposed project is to obtain and evaluate new algorithms for signal reception by realistically benchmarking them through average complexity as opposed to peak computational load. The methodology consists in leveraging the latest communications techniques by implementing them from this new perspective. The project will involve 4 graduate students working on leading edge technology, requiring of a mixture of algorithmic and microelectronic skills which have always been eagerly desired by ETS' industrial partners.""463988,""Gagnon, François"
"463626"	"Gagnon, Ghyslain"	"New circuits and architectures maximizing the resolution of high-speed ADCs using rigorous probabilistic analysis of component imperfections"	"Historically, analog-to-digital converters (ADCs) have not kept up the pace set by digital technology in terms of speed and resolution. Consequently, the ADC has become a central pivot device, being the performance-limiting component in numerous applications. For example, in wireless communication systems, improved high-speed ADCs would translate into enhanced flexibility and better services through advanced techniques such as dynamic resource allocation and collaborative networking.One of the main obstacles to the realization of high-speed high-resolution ADCs is the limited precision of the integrated-circuit fabrication process. The variance of the characteristics of basic circuit elements, across the chip area and from one chip to another, poses a limit on the maximum achievable resolution of ADCs. Moreover, for economic reasons, ADCs are implemented using the same manufacturing processes as purely digital circuits such as computer processors, for which the needs are mainly computation speed, density and power consumption. Unfortunately, improvements in these areas are not naturally translated into increased performance of analog circuits and thus call for new design solutions.This research project is aimed at developing accurate mathematical models of ADC circuits, including the effects of the deviation in the circuit component values due to the manufacturing process. Based on these mathematical models, new design methodologies will be proposed, giving rise to new ADC architectures circumventing the circuit errors and thus improving robustness and performance.""463950,""Gagnon, Graham"
"454734"	"Gazor, Saeed"	"Multidimensional-cognitive and statistical signal processing"	"The ever-increasing demand for broadband spectrum has highlighted the critical need for smart technologies that can make use of scarce spectral resources as efficiently as possible. In this research program, we will invent multidimensional statistical-signal-processing algorithms for applications in cognitive wireless communication networks, radar networks, medical imaging systems and voice communication systems. The subsystems interact with and monitor the environment, collaborating and altering their own parameters and behavior in order to enhance their performance. For example, they scan and detect the availability of signal free channels, and opportunistically use them. Intelligent and practical algorithms will be invented for applications such as in multimedia and in enhanced medical imaging devices. To this end, many critical problems are studied including monitoring of the environment, channel identification, spectral information acquisition, collaboration, and information sharing among distributed users, and competitive distributed resource allocation/management. At present, there is an international push to produce and deploy such intelligent technology. Many industries in Canada and around the globe are investing in this research area as the anticipated market and economic/social impacts are immense, for example, Statistics Canada indicates a revenue of $12.8 billion in 2006 for the wireless communications that depend on signal-processing algorithms. Timely research advancements achieved by the proposed research will have a direct economic benefit for Canada.The scope of the research will ensure that trainees will be well-qualified for employment in Canadian companies with R&D activities in this area which is an important economic sector for Canada. Each year, it is planned that four PhD, one MSc and one BSc student will receive research and development training.""457824,""Gazzarrini, Sonia"
"455228"	"Gemino, Andrew"	"User-centered design of requirements engineering techniques"	"Organizations experience difficulties in information systems projects for a variety of reasons, chief among which are unclear and volatile requirements specifications and the lack of business user involvement during projects. While the field has long recognized the importance of user engagement in information technology projects, the involvement of these users in developing requirements for these projects remains disturbingly low. This research proposes to take an innovative user centered design approach to system requirements presentation methods and will contribute to theory and empirical research in the area of requirements engineering. Its aim is to mitigate low levels of user involvement by improving methods for communicating information system requirements to business users.Cognitive Load Theory and Cognitive Theory of Multimedia Learning provide the theoretical foundation for this cross-disciplinary research. Design principles established from theoretical considerations will be used to develop improved presentation methods for system requirements Redesigned and new methods will be demonstrated using a prototype application. Their effectiveness will then be tested through a series of experiments. An experimental testing platform, constructed during the tenure of my previous NSERC grant, will be expanded and improved allowing me to increase the scale and scope of experiments. This platform will also support collaborative researchers across Canada. The long-term goal of this research is to improve the communication of system requirements in order to increase the success rates in information technology projects. Increasing project success rates is of critical importance to the growth of the Information and Communication Technology industry in Canada and, consequently, Canada's ability to innovate through information technology. A user centered approach to requirements engineering that engages users in the design of systems, has the potential to increase the value realized from investments in information technology, which are significant in this country.""451602,""Gencay, Ramazan"
"451091"	"Ghannouchi, Fadhel"	"Intelligent and green radio transmitters for broadband communication systems"	"Future communication networks will need to be more ubiquitous and agile than they are today. They will also be required to support the large demand for mobility and high throughput requirements within the environment of multi-standard communications. This adds up to the stringent power and spectrum efficiency needs of wireless and satellite communications equipment. Accordingly, future radio systems have to be designed to meet all the above mentioned critical capabilities. Although multi-band receivers and low-power handheld terminals are commercially available, their high-power counterparts for base stations and repeaters for wireless and satellite communication applications are difficult to design within the required performance, in terms of bandwidth, linearity and power efficiency.The research program targets the investigation of the scientific and technical problems related to software reconfigurable radio technology suitable for broadband communications, multi-standard and multi-mode handsets, and base stations. This will lead to well thought out design methodologies of fully digital single-input, single-output (SISO) and multiple-input, multiple-output (MIMO) SDR systems. The optimization of the system from the signal encoding perspective is intended to match and adapt the signal with the transmitter type and architecture, as well as to optimize the partition of the tasks required between different digital processor units (FPGA and DSP). System architecture selection and optimization, design of power-efficient switching-mode RF front-ends, and adaptive signal processing algorithms will constitute the main focus of the research proposal.Given the potential of SDR systems, it is highly anticipated that they will become, in the near future, the preferred solution in the design of broadband and multi-standard wireless terminals. Pursuant to this leading-edge research program, it is anticipated that a better understanding of SDR and MIMO radio systems will be gained, leading to a disruptive SDR-based radio technology.""452895,""Ghannouchi, Fadhel"
"456546"	"Gokaraju, Ramakrishna"	"Application of intelligent system techniques to power system protection & control"	"Smart grids are becoming important to the power industry today. Demand has increased for higher reliability, security, control, and protection of power systems. The long-term vision of my research program is in developing intelligent approaches to achieve superior protection and control in power systems, so that system components can optimally self-regulate in the event of faults and disturbances.  To meet this overall goal, my  research program consists of the following projects: (a) Development of model-reference and self-tuning control schemes for FACTS. (b) Development of detailed transient simulation models of power system components suitable for protection and control studies. (c) Application of AI and modern digital techniques such as support vector machine techniques and state-plane graphical methods to achieve dynamic coordination between the generator protection and the generator control functions. (d) Hardware implementation of the proposed relays and controllers on digital signal processing (DSP) boards and closed loop real-time testing to achieve practical prototypes.The applicant is optimistic that with his perseverance and zeal for innovative work, he will be able to contribute strongly in his research area and help in the advancement of Canadian and international research in power system protection and control.""464264,""Goksel, Orcun"
"451795"	"Gruver, William"	"Development and integration of distributed intelligent systems"	"The worldwide proliferation of wireless local area networks has resulted in a rapid increase of wirelessly enabled products for industrial, commercial, and military applications. Although network infrastructures are being implemented by many service providers to offer wireless connectivity, they fail to provide services that enable devices to use the network efficiently and intelligently.  In present day implementations of wireless applications, the ability to support communications, information storage, and decision making usually resides in servers and controllers. Thus, many devices continue to be relatively unintelligent because they can only function properly when they are connected to these servers and controllers. Dependence on centralization for systems integration and decision making adds an extra layer of complexity and administration that negatively impacts the efficiency, robustness, and cost of the overall system. The remedy to this difficulty is available in a paradigm based on distributed intelligent systems, and, in particular, the concept of Holonic Intelligence. With these principles, the proposed project will develop and integrate systems that utilize Holonic Intelligence together with recent advances in reconfigurable field programmable gate arrays to develop a control and coordination infrastructure for distributed intelligent systems. We expect the proposed research to result in a more flexible, scalable, robust, and cost effective infrastructure to support the development and integration of distributed intelligent systems and provide a migration path from current client/server technologies to fully distributed intelligent systems for which dependence on centralized servers is unnecessary.""461108,""Gruyer, Nicolas"
"449752"	"Haccoun, David"	"Codage correcteur d'erreurs par codes convolutionnels et décodage itératif et applications aux systèmes de communications sans fil"	"Depuis quelques années les nouvelles technologies des communications et de l'information connaissent un essor considérable, permettant aujourd'hui des communications personnelles fiables afin de transmettre et recevoir  de façon quasi instantanée la voix, les données, les images fixes et mobiles avec n'importe qui,  n'importe quand et n'importe où. Quelque soit le canal utilisé, terrestre ou par satellite, un des problèmes sous jacents importants est celui de concevoir et de développer des techniques de communications qui soient puissantes, efficaces et pratiques, afin d'assurer et de protéger la qualité de l'information transmise en dépit des bruits, interférences et perturbations nuisibles présents dans ces canaux. Cette protection de l'information transmise requiert l'utilisation de techniques puissantes de codage correcteurs d'erreurs pour assurer la fiabilité des transmissions dans ces canaux peu fiables, en particulier les canaux terrestres de sorte qu'aujourd'hui le contrôle et la correction des erreurs par la technique de codage de canal fait partie intégrante de tout système de communication numérique performant. Ce projet de recherche porte sur l'analyse, le développement et l'évaluation de nouvelles techniques de codage correcteur d'erreur qui sont puissantes et efficaces ainsi que sur leurs applications pratiques aux systèmes de communications sans fil modernes. De façon plus spécifique le projet concerne la nouvelle technique de contrôle des erreurs basée sur les codes appelés ``Convolutionnels Doublement Orthogonaux`` (CDO) avec décodage à seuil itératif et en particulier sur les codes CDO récursifs dénotés CDO-R. Ces nouveaux codes et leur décodage s'inscrivent dans la technique générale du décodage itératif qui permet d'atteindre des performances d'erreur nettement supérieures à celles existant jusque là, s'approchant plus que toute autre technique des limites théoriques de la capacité du canal. Un aspect important concerne l'analyse et le développement d'algorithmes efficaces pour la recherche de nouveaux codes CDO-R, le développement de décodeurs efficaces, l'évaluation de leurs performance, ainsi que leurs applications aux systèmes de communication sans fil des 3e et 4e générations.""453647,""Hache, Alain"
"456823"	"Hamarneh, Ghassan"	"Novel optimization strategies for medical image analysis"	"Anatomical and functional medical imaging modalities, e.g. MRI and PET, are allowing physicians to peer inside the human body and observe a wealth of data crucial for understanding, diagnosing and treating diseases. The volume of medical data acquired is growing rapidly. The dimensionality of images has increased from 2D scalar images to dynamic 3D multi-valued fields. This is resulting in image data that cannot be effectively processed with traditional visual inspection. Therefore, automated computational tools for medical image analysis (MIA) are becoming indispensable in modern healthcare systems. The three most important and ubiquitous MIA tasks are image segmentation, registration, and shape analysis, which constitute the crux of image interpretation and quantification tasks. Segmentation is the process of identifying regions of interest in an image (e.g. to measure wall thickness of the myocardium), whereas registration is the process of finding meaningful correspondence between images (e.g. to compare across subjects or time). Shape analysis captures geometric and topological properties and reveals crucial information about disease stages, treatment progress, or growth rates. Despite numerous advances in these areas in the past few decades, accurate and automatic MIA algorithms continue to defy solution. The vast majority of these algorithms rely on solving optimization problems. However, very little work has been devoted to evaluating the appropriateness of the objective functions being optimized or to integrating high-level domain knowledge into the process. This proposal will focus on studying formal approaches for evaluating objective functions and designing them from the outset using rigorous mathematical and computational techniques. This research will also complement low-level optimization techniques with high-level, knowledge-driven MIA strategies using a novel artificial life framework. The goal is to ensure higher accuracy of automated MIA algorithms, in order to advance computer-aided diagnosis, computer-assisted interventions, and the many other aspects of healthcare that rely on medical imaging.""470797,""Hamat, Shaunna"
"463922"	"Han, Jie"	"Toward innovative, robust, energy-efficient and probabilistic circuit and system architectures based on nanoscale devices"	"In the past decades, computers have been getting cheaper and at the same time more powerful. This is due to the continuous shrinking of electronic devices, so that more and more components can be packed into a single chip with a relatively low cost. However, this scaling trend is not endless and is predicted to be soon hitting a brick wall. In the nanometer (one billionth of a meter) regime, where the current silicon technology has reached, both silicon and other nanometer-scale (or nanotechnology) devices start to have problems such as poor reliabilities, high error rates and high power consumption. These new characteristics of nanoscale devices challenge the traditional thinking about how computers are made. To cope with the problems at the device level, we propose to use innovative computational approaches that are fault-tolerant and energy-efficient at the system level. Specifically, these approaches will be based on probabilistic and stochastic principles rather than the traditional deterministic ones. At the end of the program, working circuits employing the new approaches are expected to be demonstrated to show their advantages and limitations. Innovative and revolutionary circuit and system architectures are expected to be developed for future computer and sensor systems using nanotechnology devices. Since it is important to train a new generation of engineers and equip them with the fresh way of thinking, the proposed research will provide training and educational programs to our undergraduate and graduate students, who will be tomorrow's circuit designers or system architects.With the booming of nanotechnology research in Canada, this proposed program will help in the process of bridging the gap between fundamental research and the applications of nanotechnology in information processing systems. It will also facilitate the dissemination and transfer of knowledge, as well as the training of highly qualified personnel, which will be of great social and economic importance to Canada.""470203,""Han, Minyang"
"454654"	"Harutyunyan, Hovhannes"	"Optimization problems in graphs for message dissemination"	"The proposed research program will investigate optimal or near optimal message dissemination problems in computer systems for parallel and distributed computing. Different parallel and distributed primitives such as routing, broadcasting, multicasting and gossiping will be considered. Various information dissemination models are considered by placing constraints on the amount of information available for each processor, number of senders and receivers, length of the message, transmission or processing delay, the number of faulty links, the number of messages, etc. These primitives, especially broadcasting and multicasting, play an important role in parallel processes, in data migration, in cache coherence and in data sharing in communication networks. ""455225,""Harvel, Glenn"
"455040"	"Heidrich, Wolfgang"	"Computational imaging for graphics and vision"	"Realistic computer graphics has become a core technology for many applications that are of importance to both industry and society at large. Some examples include design applications (e.g. interior- and industrial design), medical applications (e.g. surgical training simulators), education (e.g. learning software), heritage projects (virtual museums), as well as entertainment (e.g. special effects for movies, computer games).Although much progress has been made on realistic computer graphics in the context of entertainment, the methods developed for this purpose emphasize manual interaction and artistic control to fine-tune the resulting images. This is undesirable in most other applications, where the computer generated images need to quantifiably represent existing real-world objects. To tackle this issue, several subproblems need to be addressed. First, physical models of real-world objects have to be acquired through imaging and other measurement techniques. Second, captured real world objects and phenomena need to be analyzed, and computational models need to be extracted for their behavior. Finally, algorithms need to be developed that are capable of using realistic, physically accurate models in rendering processes.Following these long-term goals, the goals for this application are novel developments in the following areas:- Automatic methods for measuring and processing objects with complex optical properties, such as translucent or highly specular objects- Imaging methods for dynamic phenomena such as fluid flow and fire- Systematic analysis of dynamic phenomena based on measurements, and the development of computational models that can be used in simulations.- Efficient, high-quality rendering algorithms for the acquired models.""449440,""Heikkila, John"
"463677"	"Helaoui, Mohamed"	"Ultra-wideband transceiver architectures for millimetre-wave applications"	"As a result of the increasing number of high-speed wireless applications, there is a need for additional spectral resources, which is driving the exploration of unused frequencies. Special interest has been given to the millimetre-wave (mm-wave) bands, which are suited for high data rate applications.Operating in mm-wave frequencies, however, is challenging since it imposes stringent constraints on the design of the transceiver's front-end components. The increase in the parasitic effects and circuits imperfection introduces complex gain imbalance and linear and nonlinear distortions in the transceiver circuits. These distortions lead to unacceptable signal quality deterioration. For ultra-wideband applications, these linear and nonlinear distortions are frequency dependent and exhibit severe memory effects in the system.The requirement of a GHz processing speed is another challenge in designing ultra-wideband transceivers. Implementation of complex signal processing algorithms at GHz sampling frequencies not only requires tremendous processing resources, but is also highly power consuming.In order to offer an effective solution for designing an ultra-wideband mm-wave transceiver, two research topics are explored in this project:The first research topic addresses the design of a low cost ultra-wideband and linear homodyne down-converter. New signal processing algorithms for estimating and compensating for the composite effect of branch imbalance and memory effects in conventional direct-conversion receiver architectures is proposed. To further improve the signal quality performance and to lower the design costs, new homodyne receiver architecture is also proposed.The second research axis aims at implementing the signal processing algorithms for these ultra-wideband receivers at GHz sampling speed, while maintaining acceptable power consumption levels. Using parallelized architectures, dedicated digital circuits are considered and implemented in FPGA.""471681,""Helbing, Caren"
"456685"	"Helmy, Amr"	"A novel platform technology for enabling the next generation photonic integrated circuits"	"Light is immensely useful because of the insight it provides when utilized to visualize chemical, physical and biological processes. Laser-generated light, on the other hand, enables much more than vision. It can be used as a tool to probe the innermost workings of matter, to manipulate it and to even generate new states of matter. Laser-generated light also possesses properties by which its quantum mechanical properties can be utilized to enable fundamentally secure cryptography (quantum cryptography) and more powerful imaging techniques, only to name a few.Semiconductor chip-based lasers are the most attractive laser sources available. This is due to their relative ruggedness, efficiency, affordable cost and small size. Semiconductor lasers are employed in a vast spectrum of applications with an astounding economic impact from consumer electronics to photodynamic therapy. The color of radiation emitted by lasers is limited by the bandgap (and hence color) of the material from which they are made. There is a limited set of suitable materials which can be used in generating infrared laser radiation in semiconductors. Infrared radiation is pivotal for numerous biomedical, sensing, environmental, telecommunications, scientific and industrial applications. With the aide of a technology developed in my group, where new world records of efficiency have been broken in recent months, a novel type of laser; namely monolithically integrated optical parametric oscillators will be feasible for the first time. This laser does not exist in chip-based form to date. These oscillators produce laser-like light with vastly more versatile properties than conventional semiconductor lasers.When developed, these sources have the potential to revolutionize the aforementioned applications.""470055,""Helou, Bassam"
"455038"	"Heywood, Malcolm"	"Continuous symbiotic program evolution"	"Genetic Programming (GP) is a subfield of Evolutionary Computation (EC) in which the general goal is to evolve programs from a population of candidate programs. Currently the applicant's research enables multiple programs from the same trial to learn to interact to establish a non-overlapping cooperative behavior during evolution. Post evolution, some subset of programs would have learnt to act under a unique subset of circumstances (a parallel framework of deployment). This project undertakes a substantial generalization in which program evolution is (1) a continuous process and (2) the interaction between programs can be hierarchical as well as parallel. Under (1) support for a continuous process of evolution assumes a competitive coevolutionary model of evolution; thus, training scenarios are coevolved with the learners (e.g., as in video game difficulty increasing as player performance improves). This also provides the basis for evolving different programs to solve different subsets of tasks, but introduces the requirement for continuous diversity maintenance in both the learner population and the population representing training scenarios. Under (2), limiting learners to a parallel form of deployment (of programs), as in ensemble style `voting' frameworks, will naturally place constraints on the types of problems against which solutions can be evolved i.e., you cannot build a new behavior from one previously learnt. Conversely, under a biological context, symbiosis has been widely acknowledged as a very important mechanism for building more complex organisms by subsuming simpler organisms in their entirety. This project pursues a symbiotic framework for hierarchical model building under GP. As additional layers of symbiosis are added, solutions evolved in earlier training scenarios are subsumed in their entirety and potentially deployed in contexts dissimilar from that in which they were originally evolved. Scalability still holds care of competitive coevolution; thus, as learners become more capable, training scenarios are sought which promote the generalization of multiple behaviours from the current population, promoting a continuous process of model building.""469125,""Hfuda, Abdulhafid"
"459463"	"Hoge, Richard"	"Multi-modal calibration techniques for quantitative brain imaging"	"This research program aims to develop new imaging methods that allow more detailed and accurate measurements of different physiological processes in the brain.  These techniques will involve a combination of magnetic resonance imaging and new optical imaging methods, where highly penetrating low energy light is projected through the head.  The data provided by these two techniques can be integrated using new physical models of the magnetic and light propagation properties of brain tissue to produce more accurate measures of the brain's ability to deliver and use oxygen.  Such measurements will provide important information on changes in brain function during aging and diseases such as Alzheimer's and other neurodegenerative conditions.""468412,""Hoggarth, David"
"463720"	"Hogue, Andrew"	"Variational map representation for robotics"	"Imagine going for a walk in an unknown city without a map. As you walk, you memorize the path: street names, buildings, turns that you made and distances between them, creating a mental map of your route. At the same time you use this dynamic representation to estimate your location by recognizing, for example, church steeples from a distance or by remembering how far you have walked. You can use this ""map"" to explore the city even further on the next day. This approach is known in mobile robotics as Simultaneous Localization and Mapping (SLAM) and is used to create representations of unknown environments. These maps are concurrently (and later) used to estimate the robot's location without relying on external systems such as the Global Positioning System. Basic mathematical foundations of SLAM are now known: extraction and recognition of features from sensor data, dealing with uncertain measurements, fusion of noisy data from multiple sensors, and the reduction of accumulated errors upon recognition of previously visited sites. Numerous successful SLAM demonstrations have been carried out under assumptions of planar worlds (3 degree of freedom estimation).  But many open issues related to SLAM remain, including the estimation of unconstrained sensor motion and maps within large-scale, dynamic, 3D environments. The proposed research program investigates the development of sophisticated environment representation strategies--and thus more effective algorithms in general--for the SLAM problem. In particular, I plan to scientifically evaluate the efficiency and effectiveness of existing map representations and to develop a novel method to incorporate variational techniques to model the environment (i.e. energy-based implicit surfaces) within a probabilistic SLAM estimation framework.""459389,""Hohlweg, Christophe"
"455036"	"Hoos, Holger"	"Programming by optimisation: Computer-aided design of high-performance algorithms for hard combinatorial problems"	"High-performance algorithms for solving hard computational problems are found at the core of many complex software systems used in real-world applications. The overall goal of my research is to develop and establish a paradigm for the development of such algorithms that I call ""programming by optimisation (PBO)"". The key idea behind this paradigm is for human experts to specify not an algorithm, but a potentially large design space of algorithms, and to use automated methods to find performance-optimised algorithms within this space. This approach allows human experts to focus on the creative task of thinking about possible mechanisms for solving given problems; at the same time, once a rich and potentially large design space has been specified, the automated search for a performance-optimised algorithm within it can be specific to a particular type of input, such that custom-optimised designs for different application situations can be obtained with minimal human effort.The research proposed here is aimed at developing the ""programming by optimisation""  paradigm to the point where it can be established as a standard way of designing algorithms for solving the hard combinatorial problems routinely arising in various areas of computer science (in particular, artificial intelligence), operations research and many application areas (such as bioinformatics). Applications of this paradigm directly studied in the context of this research include software verification, which plays an important role in ensuring correct and reliable behaviour of computer software, and prediction of RNA structures, which play an important role in many fundamental biological processes, including ones known or believed to be associated with human diseases, as well as in designing RNA molecules that could be used in molecular therapy approaches.""463967,""Hooton, Douglas"
"452634"	"Hu, Alan"	"Extending the success of formal hardware verification to system software"	"Computer chips, computer systems, and computer software are by far the most complex things ever designed by humans.  For example, even the very simple programs written by our students in their first computer science course have far more possible behaviors than there are photons in the entire universe!  Not surprisingly, it's hard to get all the details right.  In fact, the most effort is spent not on design, but on validation and verification -- the task of determining if the system behaves as intended.Formal verification is a revolutionary approach that promises much higher quality with less verification effort.  Research breakthroughs over the past twenty years have made formal verification indispensible for verifying computer hardware -- all major computer companies, use formal verification on their computer chips.  This has been a tremendous success story of academic research creating enormous value for society.Formal verification of computer software, however, is fundamentally more challenging than computer hardware.  This proposal focuses on developing formal verification techniques for a specific kind of software -- system software, which is all the low-level software that underlies all the other software in a computer, e.g., the operating system, device drivers, shared libraries, embedded firmware, etc.  Like building on a foundation, all other software is built on top of, and depends upon, the system software, and if there are cracks in the foundation, it undermines everything on top of it.  Hence, it is some of the most important software to verify correct.  On the positive side, I believe that system software has certain characteristics that make it a particularly promising target for formal verification.  If successful, this research will generate a major quality and productivity boost for software companies and a quality-of-life boost for all of us who have to deal with computers in our daily lives (i.e., all of us).""461624,""Hu, Baoxin"
"456777"	"Huang, Xiangji(Jimmy)"	"Context-sensitive and task-oriented adaptive information retrieval"	"Using Google is easy; finding information using Google is another matter. Over the past decades, significant progress has been made in Information Retrieval (IR). However, many challenges remain. First, most Web search engines take a short text query as input and output a ranked list of documents. The retrieval decision ismade primarily based on the current query and document collection. For example, a person may use ""IRIX"" to mean Information Retrieval in Context at one time, but the IRIX operating systems at another time. It is impossible for the current search engines to distinguish between these two cases because the user's search history and context are not considered. Second, IR is, in general, an interactive process. With the current document-centered retrieval paradigm, interactive retrieval is treated as a sequence of independent simple retrieval decision-making steps. However, it has been brought into attention that analysis of task-oriented user sessions provides useful insight into the query behavior of the users. Third, most of present IR systems including general search engines (e.g. Google) and scientific literature search engines (e.g. PubMed) use keywords to query and index documents, which provides little semantic context for the understanding of the user's information needs. Thus, the integration of semantic context according to the user's information need and the user's understanding of the documents in the collection into IR systems will improve the IR performance. However, how to encode and use semantic context for the benefit of retrieval is a difficult problem. Our objective is to overcome the limitations of the existing retrieval methods and formally develop a new retrieval paradigm, in which semantic context and geometric context are both exploited in the retrieval decision process. In particular, (1) we will develop a personalized theoretical retrieval model for capturing user information and search context; (2) we will develop a novel task-oriented retrieval model that optimizes the long-term retrieval utility over an entire retrieval session.""462099,""Huang, Xiao"
"457330"	"Hung, Patrick"	"Services-oriented architecture for motion sensing services"	"A motion sensing (mo-sensing) service is defined as an autonomous unit of handheld console enhanced by motion sensing capability for supporting different motion sensing data exchange patterns in sports science and physiology. This proposed research program defines mo-sensing services as a type of rich presence. Rich presence is described as an enhanced form of presence awareness in which a service or user can observe other online service or user attributes, such as personal identifiable information, location, time, behaviour, movement, type-of-software, type-of-device, and type-of-network. In the context of a long term research program, this research program focuses on the theoretical model for supporting interoperability and scalability between different motion sensing data schemas from different mo-sensing services with eXtensible Markup Language (XML) and Services-Oriented Architecture (SOA). On the other hand, the major short-term objectives of this proposed research program are to: (1) design a XML-based Domain Specification Language (DSL) for motion sensing data; (2) build a workflow model based on SOA for supporting mo-sensing services; and (3) demonstrate the completed model in an illustrative sports physiology system. In particular, the illustrative sports physiology system can be derived from the workflow model that allows: (1) provide an interactive measurement system that monitors an athlete's position and analyzes the individual's movements with analyses that match the movements to idealized patterns for the athlete; (2) is derived from evidence-based frameworks for physiotherapy of athletes; (3) provides cognitive support for coaches as a decision support when examining sport usage patterns of the tool while also being linkable to wellness-based educational interventions for athletes using the system; and (4) protects privacy and security of healthcare information collected from athletes using the system, including any data transferred to clinical settings for analysis. The benefits from this research program to research and develop a generic workflow model to support the emerging motion sensing applications in sports science and physiology.""463139,""Hungr, Oldrich"
"454628"	"Ibnkahla, Mohamed"	"Adaptive signal processing for wireless sensor networks"	"Wireless sensor networks (WSNs) are gaining an increased role in our society due to their easy deployment, low cost and scalability. In the last few years I have been applying this technology in a number of strategically important areas to Canada such as environment monitoring, precision agriculture, wildlife tracking, highway safety, food safety, and health care. Built upon our previous expertise in this area, the main goal of this research program is to investigate adaptive signal processing algorithms in order to enable reliable, energy and bandwidth-efficient wireless sensor networks. The adaptive and cognitive concept is important to make a breakthrough in wireless sensor networks because of their distributed toplogy in addition to the time-varying nature of the channel and network conditions. We will investigate innovative techniques such as adaptive channel modeling and identification, adaptive multi-hop equalization, adaptive modulation and coding over constrained nodes, as well as distributed cognitive signal estimation and data processing.From this research we expect to significantly contribute to the global effort of developing techniques and tools for the commercially-viable WSN systems of the future targeting key Canadian sectors. This research program will result in novel algorithms and techniques and will lead to patents and publications. Training of HQP involved in this program will include experience in wireless communications, adaptive signal processing, cognitive communications, channel modeling and identification, adaptive equalization, adaptive modulation and coding, and cross-layer adaptation. I expect that four PhD, four MSc and five undergraduate students will receive training through this research program. There is a strong demand for these HQP in the information technology and telecommunications sectors. Their future employment will accelerate disseminating next-generation communication technology to Canadian industry and will help maintaining Canada's leadership role in this sector.""468195,""Ibnkahla, Mohamed"
"455035"	"Ilie, Lucian"	"Combinatorial algorithms for approximate string searching and DNA sequencing"	"We live in the Information Age. The amount of information available nowadays is not only astounding but also growing at an exponential rate. The size of the deep Web was estimated in 2002 at 90,000 terabytes. A large part of this data consists of text, such as natural language, multimedia streams, programming code, signals, biological sequences, etc. The recent boom in XML advocates text as the favorite format for information storage. This textual information needs to be processed, which is done by text indexing. Constructing efficient indexes for approximate string search is the most important problem in Information Retrieval.The ability to process textual information has tremendous impact in many areas, one of which is Bioinformatics. Obtaining the sequence of the human genome was a historical achievement but the revolution towards individualized medicine will continue with the sequencing of each individual genome. The $1000 genome project is looking for the right technology to obtain high quality sequence of an entire human genome with less than $1000. New sequencing technologies produce gigabytes of data (short reads) in a single run but the goal cannot be achieved without smart algorithms to process that data.These are the topics that my research will focus on. It is difficult to overestimate their importance. The impact of good solutions is far reaching. As mathematical problems, they are combinatorial in nature and therefore quite suitable for my expertise.""457497,""Ilie, Monica"
"457092"	"Ilyas, Ihab"	"Probabilistic retrieval and cleaning of large uncertain and inconsistent databases"	"Data generated by modern applications such as object tracking, sensor networks, and Web data integration involve uncertainty, and various anomalies such as missing values and duplication. While many research efforts have been focusing on data cleaning and dealing with inconsistent databases, very limited research has focused on enabling information retrieval queries and data exploration in probabilistic and uncertain databases.This proposal aims primarily at allowing users to effectively query and explore large volumes of inconsistent and uncertain data. I will focus in this proposal on two main research directions: first, allowing information retrieval style queries such as ranking, preference handling, and aggregation in the context of uncertain databases; and second, developing declarative data cleaning approaches based on probabilistic modeling of erroneous and inconsistent data. The proposed techniques will be tested in QualityDB, a probabilistic database engine prototype we are currently building, based on an open-source Database Management Systems. The goal is to build a generic framework that adapts to different probabilistic data models, and encapsulates efficient query processing algorithms to allow for pipelined execution guided by data quality and cleaning measures. Towards this goal, the proposed research plan has a set of short-term and long-term tasks. Short-term tasks include the following: (1) revising current query semantics to integrate ranking and aggregation queries along with probabilistic semantics; (2) extending the capabilities of current relational database engines to support storage and efficient retrieval of probabilistic data; and (3) developing declarative query language for specifying user-defined quality guarantees that guide the retrieval, processing and cleaning of dirty raw data. The long-term research tasks address integrating this framework into broader data management environments including Web search and document retrieval in XML and text databases.""461863,""Ima, Chukwunonyerem(Samuel)"
"450011"	"Ionescu, Dan"	"Applications of computers in industrial environments"	"The objectives of the proposed research are to investigate selected theoretical and experimental topics related to the application of computers in an industrial milieu as a continuation and a development on a new level of the applicant's on-going research works. The research area related to applications of computers in industrial environment witnessed dramatic changes and evolutions in the last few years on both the hardware and software sides. Complex applications for the enterprise supervision and control, such as CRM, ERP, BI, etc pushed and push the computer technologies towards new boundaries. Recent evolutions in the distributed computing paradigm fundamentally changed the way that enterprise supervision and control architectures were designed. Growing adoption of cloud based services by government departments and enterprises requires new architectural models, while new dimensions such as user mobility and collaboration have stretched the research in this field at its extremes. Industry analysts (Gartner Quadrants) report that there is a strong need for converged, collaborative, unified and self-managed products or platforms capable to seamlessly unify the computer supervision and control applications across the enterprise. Consequently, for the present phase of our research, we propose to investigate new distributed computing models, which fundamentally disrupt the traditional way of offering services and applications at the enterprise level. The usage of mixed mobile and fixed computing devices in enterprise operations and management processes requires new communications and distributed computing principles, algorithms, and protocols.  We are aiming therefore, at investigating fundamental and experimental aspects of new models for autonomic computing, peer-to-peer, web-services and cloud computing technologies in the design of enterprise supervision and control architectures. We will also investigate the design of generic converged, unified, and collaborative environments for the above. A part of our research efforts will also be dedicated to the investigation of reconfigurable architectures as applied to the supervision and control of enterprises as a continuation of present research efforts.""472793,""Ionita, Sebastian"
"450581"	"Iravani, Reza"	"Cognitive microgrids"	"Deregulation of the electric utility industry, environmental concerns associated with the central power plants, volatility of energy cost, and rapid technological developments of Distributed Energy Resource (DER) units, e.g., solar PV, have resulted in a considerable level of integration of DER units in distribution systems under the microgrid and the active distribution system concepts. We propose research, investigation and development of the novel concept of cognitive microgrid to (i) increase throughput and efficiency, optimize power flow, enhance stability, provide reliable protection, maximize use of low cost DER units, harmonize load serving functions, improve utilization of existing assets, and enable widespread grid integration of electric vehicles. The cognitive capability is (i) provided by advanced information processing, communications infrastructure, controls and management systems, and (ii) achievable based on the availability of substantial computational power at various locations in the cognitive microgrid.The vision of cognitive microgrid is to break down barriers between (i) power transmission and distribution and (ii) information and communication technologies.  This will provide diagnosis and resolution of potential problems that may arise, and enable real-time exchange of information among all stakeholders, in particular the utility and its customers. In our vision, the cognitive microgrid will be a building block of the more general concept of ""smart grid"". The objectives of this project are research of robust control methods, adaptive protection, operational strategies, and business models. These will be based on pervasive use of (i) information and communication technologies, (ii) sensing and monitoring techniques/technologies, (iii) large-scale data processing, and (iv) advanced computing, to realize the cognitive microgrid.""460304,""IravaniTabrizipour, Mehrdad"
"463924"	"Iyer, Ashwin"	"Advanced metamaterial devices and applications"	"'Metamaterials' are artificial materials designed to possess exotic electromagnetic properties not available in nature, like a negative refractive index (NRI). The explosive growth of metamaterials research over the last decade was largely motivated by several profound theories, the most intriguing of which suggested that a metamaterial 'lens' with a NRI could theoretically produce an infinitely fine, focused image of an object -- a 'superlens' capable of outperforming conventional lenses. One variety of metamaterial, known as the negative-refractive-index transmission-line (NRI-TL) metamaterial, invented in Canada, was responsible for the design of the first metamaterial superlens.The ultimate goal of the proposed research program is to carry NRI-TL metamaterial technologies from current proof-of-principle efforts to the realization of practical devices and components for application in biomedicine, defense, microelectronics, and telecommunications. For example, if designed to operate in heterogenous environments like human tissue, a NRI-TL metamaterial superlens could aid in the detection and treatment of tumours; in soil, NRI-TL metamaterial scanners could be used to detect landmines and other small buried features. NRI-TL metamaterial structures may also be used to manipulate/enhance the radiation characteristics of nearby antennas or the scattering properties of objects, which may be useful in a defense or telecommunications context.""460513,""Iyer, RohinKrishnan"
"450642"	"Janicki, Ryszard"	"Concurrency theory, non-numerical approximation and mereology"	"Both concurrent systems, approximation problems and mereological systems abound in human experience but their fully adequate conceptualization as yet eludes us. Our increasing dependence on ever more complex systems in the management and control of human affairs and activities increases the urgency for developing more adequate and preferable more formal concepts to maintain reliable control over systems we have created. The solution of the problem of correct specification of the design and verification of its behaviour becomes crucial, and a satisfactory conceptual apparatus for rigorous specification and verification becomes essential.The project will explore theoretical issues involved in the formal reasoning about concurrent systems, approximate reasoning, and mereological systems. First general aim is to develop a framework, based on the concept of Generalized Causality modelled by Discrete Relational Structures (developed by Janicki and Koutny), to support the design and the verification of sophisticated concurrent systems. The second general aim is to strengthen foundations and improve applications of non-numerical approximation and non-numerical ranking techniques. The third aim is to provide a solid formal mereological reasoning that can effectively be used in Software Engineering and Computer Science. What connects all three topics is methodology, as the scientific method is basically the same, i.e., discrete mathematics, especially set, relation and automata theory and of course formal logic.""449252,""Janischewskyj, Wasyl"
"457221"	"Jeremic, Aleksandar"	"Inverse modeling of biochemical transport"	"This proposal addresses current need for computationally efficient inverse models of dispersion processes that are amenable to statistical signal processing. The forward dispersion models have been subject of considerable research interest in the past as they are adequate mathematical representation of diverse processes in various applications: environmental monitoring, homeland security, drug delivery, stock market price fluctuations, atmospheric condition monitoring, etc. Complementary to this approach is inverse modeling in which a set of unknown parameters is estimated using real data measurements. Namely in reality some of the parameters that affect dispersion are unknown (e.g., location of the pollutant leakage, dispersion coefficient for a particular drug, etc.) Due to the complexity of the aforementioned processes and their dependence on a variety of complex parameters and phenomena (e.g. turbulence) inverse models often require significant approximations which in turn affect our ability to accurately estimate unknown parameters and/or predict dispersion process in future.""470615,""Jeremic, Filip"
"451066"	"Johns, David"	"Advanced interface circuits for MEMS technology"	"Micro-ElectroMechanical Systems (MEMS) refer to tiny devices that combine micrometer-scale mechanical devices with micro or nano scale electronic circuits to sense physical quantities such as pressure, acceleration, rotation, etc. and turn them into electronic signals for processing.  Some recent examples of commercial applications for MEMS are tire pressure monitoring (together with RF signaling to the car console) available in many new cars, pressure sensors used as microphones in devices such as cell phones and hearing aids, inertial sensors used in airbag deployment as well as positional control in handheld games and cell-phones, and gyroscopes used for image stabilization for cameras as well as angular velocity measurement in handheld games.The use of MEMS are limited today partly due to power consumption, accuracy and cost. This research program investigates new circuits and architectures that will significantly improve MEMS power dissipation as well as improve accuracy performance.  With improved accuracy, new applications can be developed that are not otherwise possible.  For example, a highly accurate inertial sensor can be used to track position by integrating acceleration to obtain velocity and then integrating velocity to determine distance travelled.  In this way, ultra low power devices can track objects that would otherwise be impossible.  ""457781,""Johnson, Bradford"
"451010"	"Joos, Géza"	"Impact of distributed resources on the electric grid"	"Future electricity grids will incorporate new energy sources, including renewable energy, and a wide array of equipment with embedded intelligence and communications systems. This evolution will make electricity grids more controllable and manageable, allowing them to better respond to new constraints, including climate change (the concept of smart grids). Renewable energy sources, such as wind and solar, are a means of providing a source of clean energy, thus displacing fossil fuel based generation, used to produce more than half of all electricity consumed in the world. Renewable energy sources can be connected at the transmission level, as in the case of large wind farms, or at the distribution level. When associated with electricity storage, these resources, or distributed energy resources, can be embedded within the distribution grid. An integration approach, investigated in this proposal, is to create entities incorporating these resources, with well defined physical boundaries. These entities, or cells, can then be operated either as an integral part of the larger grid, or as autonomous systems. In addition to power generation, this approach would offer a number of benefits, including support of the grid under normal and emergency situations. In the case of a disruption in the main supply, the entity would be capable of operating as an isolated system, with at least the critical loads being supplied. If the entity, or cell, remains operational, it could be used to help restore the integrity of the main grid. This project will develop approaches and technologies to help integrate renewable energy and distributed resources into electricity grids, make a case for the added benefits of using such resources, and allow a transition towards an intelligent electricity grid. The new and large investments expected over the next years in the renewal of the electricity infrastructure will create significant opportunities for technology transfer and for the employment of highly qualified personnel, within electric utilities, equipment manufacturers, and energy consultants in Canada. Employment opportunities will also be created by the retirement of a large number of electrical power engineers within the next years.""463035,""Joos, Géza"
"458017"	"Joslin, Christopher"	"Adaptation of dynamically generated content"	"The focus of most current research directions in this area is based on a server/single-client relationship, whereby a client makes a request to a server for media, provides its contextual elements, and that media is processed and presented to the client. This arrangement avoids certain considerations when this relationship is not present, for example in server/multiple-client based systems (e.g. virtual environments) whereby each client is interacting with other clients, and the server is acting merely as a filtering and distribution platform; or for a client/client system (e.g. video conferencing) where two clients are interacting with each other directly. In both cases the concept of storing media in a database or file store, along with some kind of tagged description for the media, is no longer possible - and in essence while there might be certain elements stored on the server or even at the client, the media is either being generated dynamically (e.g. video conferencing) or as part of a mix (e.g. a virtual environment). In this program of research the aim is to tackle the issue of media that is generated dynamically, as well as contextual elements that can also change dynamically during a session. The concept of adapting scalable data according to the contexts of the other user(s) is completely valid and necessary; for example one client may be using a mobile device, whilst another uses a home entertainment system; in this case we can encode the media once and then adapt it for each target client - in both cases a quality experience should be available.The research program will initially focus scalable video coding, but the long term goals of the research will be to use generic concepts and apply them to other media items such as audio and graphics.""455627,""Josselyn, Sheena"
"457247"	"Jourdan, GuyVincent"	"Applied formal testing"	"This research project goal is to adapt some of the techniques of formal testing to better handle some real-life requirements. Many formal testing techniques are directed towards what is sometimes called ""debug techniques"": the goal is to fulfill some given criteria (all paths, all code, all def-use and many others), or uncover every faults under some specific assumptions (checking sequence generation). However, in practice non trivial applications are simply not expected to be fault-free, thus the purpose of a realistic testing campaign cannot be to uncover all faults. Given that only some of the faults will be found, it only makes sense to question which ones will be uncovered by a testing method. It is also desirable to direct the detection towards the ""good"" faults. In a realist setting, uncovered faults are always ranked by importance. We cannot expect to promote formal methods if these methods are blind and unable to detect the most important faults first. Our goal is thus to provide tools based on formal methods that are able to uncover some categories of faults, and to provide tools and data to evaluate how effective a given method is at uncovering faults in a given category.""465184,""Joushaghani, Arash"
"451414"	"Kamwa, Innocent"	"Wide-area information based control and optimization of stressed and smarter power systems"	"In a market-driven environment with unanticipated load flow patterns over wider geographic areas, network operators cannot rely on the old central planning approach with simplified model and static deterministic performance criteria. Instead, they turn to a wide-area information based approach, for assessing and managing the dynamic performance of an increasingly complex system without undue conservativeness. Optimizing the dynamic performance of bulk power systems is a demanding task, which requires (1) identification of accurate stability models of the system components; (2) performance assessment by predictive simulation and real-time monitoring of security boundaries; and (3) performance control using a coordinated hierarchy of local devices and overreaching special protection systems. Our research touches on all these areas with the aim of contributing to the emergence of smarter power grids operated closer to their limits to satisfy our reliability-sensitive digital society, while integrating new forms of sustainable energy (such as wind farms) without expansive non-ecological investments in new transmission facilities. The three proposed research paths are tied together by the broad goal of improving system performance using analytical concepts together with actual wide-area measurements: 1)On-line identification/monitoring techniques based on enhanced Phasor Measurement Units (PMU) incorporating generator dynamic state (rotor speed/angle); 2)Wide-area control schemes including, global stabilizing control using remote sensing and actuation, intelligent systems based stability controls able to adaptively adjust emergency protection to more accurate values, wide-area and agent-based defense plans intercommunicating with the energy management system to safely achieve a closer to limits operation. 3)On-line Dynamic Security and Venerability Assessment (DSVA) frameworks based on intelligent systems and wide-area measurements, that can respond effectively to fast changing conditions and stressing operations induced by market activity and intermittent generation.""456649,""Kan, Frederick"
"458444"	"Kara, Nadjia"	"Mécanismes intelligents de gestion de ressources dans des milieux de virtualisation d'infrastructures"	"Ce programme de recherche vise à développer des mécanismes intelligents et efficaces de gestion de ressources dans des milieux de virtualisation d'infrastructures (MVIs). Ces mécanismes sont essentiels pour résoudre les problèmes majeurs de partage dynamique de ressources et d'optimisation de l'utilisation des ressources dans les MVIs. Les objectifs spécifiques de ce programme de recherche sont reliés à la:1) Découverte de la topologie et des ressources dans un MVI : mettre en oeuvre de nouvelles approches de découverte de la topologie et des ressources sur lesquelles est basée la gestion des ressources dans les MVIs. Le but est de recueillir les données sur la topologie et les ressources qui lorsque combinées avec d'autres paramètres permettront d'identifier le contexte pour lequel  une gestion dynamique des ressources sera effectuée. 2) Gestion d'allocation dynamique et sensible au contexte de ressources partagées : développer des mécanismes dynamiques d'allocation de ressources basée sur les besoins des environnements d'applications (EAs) hébergés par des MVIs et sur l'identification des paramètres contextuels dynamiques de ces milieux. De nouvelles approches seront développées pour la gestion des ressources dans ces milieux tout en garantissant la qualité de service requise.  3) Convergence assistée dans un MVI : développer de nouvelles méthodes de gestion de ressources pour des communications inter-MVIs. Les mécanismes de gestion des communications inter-MVIs qui intégreront ces méthodes faciliteront la convergence vers des systèmes ouverts de communications.  4) Gestion de ressources inter-MVI : définir de nouvelles méthodes de gestion de la mobilité des EAs à travers différents MVIs et des ressources utilisées lors de leurs déplacements. Les mécanismes de gestion de la mobilité des EAs à travers différents MVIs qui intégreront ces méthodes favoriseront la convergence vers des systèmes ouverts de communications.""467453,""Karabanow, Alex"
"457152"	"Kashyap, Navin"	"The design and analysis of coding algorithms for reliable and secure digital communications"	"Information technology relies heavily upon error-correcting codes and cryptographic protocols to ensure the reliable and secure transmission and storage of information. Error-correcting codes are used universally for sending packets over the web, in writing data on CDs and flash memory devices, and other similar means of modern communication. Secret-sharing schemes are protocols that allow for the secure storage of cryptographic keys, and for secure multiparty computation, in which a group of participants want to compute an agreed function of their private data without in any way compromising the privacy of their data.Modern error-correcting coding schemes use iterative decoding algorithms that are based on graphical models, which bring the error-correction performance of the codes close to the optimum predicted by theory. One of the goals of this research is to develop graph-based iterative decoding algorithms having low implementation complexity for such well-known code families as Reed-Muller and BCH codes. Our approach for this is to use tools from the mathematical theories of graphs and matroids to analyze the complexity of graphical models, which then allows for an analysis of the complexity of the associated decoding algorithms. Many of the same mathematical tools can be used in the design and analysis of secret-sharing schemes. In this context, this research addresses the issue of finding secret-sharing schemes with the optimal information rate for a specified set of security requirements.The proposed research aims to impact the information and communications technology sector in three ways: (i) by providing the sector with new low-complexity error-correcting coding schemes; (ii) by developing design techniques for optimal secret-sharing protocols that meet a given set of security requirements; and (iii) by training highly qualified personnel in techniques essential for research and development in this sector.""473018,""Kashyap, Raman"
"456688"	"Khalid, Mohammed"	"Field programmable chips and systems: architecture exploration, CAD tool development and novel applications"	"Field programmable chips and systems are a very popular implementation medium for digital systems targeting a wide range of applications in consumer and automotive electronics, communications, networking, etc.   In order to utilize their full potential, research is needed for the development of new CAD tools to help the designers in mapping their designs effectively to the target systems. Also as the complexity of the designs implemented continues to grow, research is needed in developing new architectures that can efficiently implement a wide range of designs. Due to their huge logic capacity, current FPGAs are effectively Field Programmable Systems on Chips (FPSoCs). Hence to utilize their resources effectively design based on parameterizable and reusable Intellectual Property (IP) cores needs to be explored. Network-on-chip (NoC) is a promising interconnection scheme for FPSoCs implemented using multiple IP cores. Hence NoC architectures suitable for FPGAs need to be explored. Finally, novel applications in areas such as digital signal processing need to be explored. The objectives of our research in this area are: (a) Develop NoCBuild, a CAD tool for Design Space Exploration (DSE) of NoC components/building blocks.  (b) Develop a VHDL library of parameterizable NoC components that will be used by NoCBuild CAD tool. (c) Develop and where possible, acquire a suite of real world benchmark applications for evaluating NoC-based systems. (d) Explore NoC architectures for implementing a variety of multi-spectral image processing algorithms. (e) Develop a configurable IP core that provides high performance FPGA implementation of parallel turbo decoders. Explore novel applications of field programmable chips and systems in different areas of digital signal processing. One of the goals is to develop intellectual property cores for real world applications that can be potentially useful for designers in the industry. This work will contribute towards the development of CAD tools and NoC architectures that will be very useful to designers of the next generation of FPSoCs in Canada.""464847,""Khalid, Salim"
"452143"	"Khandani, AmirKeyvan"	"Application of information theory in wireless networks relying on a cross layer approach"	"Future wireless systems are expected to support much higher bit rates at a lower cost, which imposes tremendous pressure on the usage of spectrum. Multiple Input-Multiple Output (MIMO) antenna systems are widely recognized as the key solution to realize the increase in the spectral efficiency required in future wireless networks. In the recent years, research community has paid significant attention to study the Information Theoretic aspects of MIMO wireless networks. Network information theory generalizes Shannon point-to-point (two terminals) communication to systems with more than two terminals. This general framework allows considering transmission of more than one source, and/or over more than one channel. For many years, theoretical studies in this subject have shown potential for realizing high gains over conventional point-to-point communication techniques. However significant effort is needed to improve our understanding of these networks towards bridging the huge gap that currently exists between theory and practice in this emerging field. In addition to the huge computational complexity (which has to be implemented in part at the resource limited mobile unit), many rather unrealistic assumptions are made about the underlying channel and (fast/slow fading) and availability of the channel state information at the transmitters/receivers which should be better understood and tailored to practice. In addition, traditional communication networks have many levels of abstraction where protocol layers are designed as separate modules. More recently, cross layer optimization in communication networks has been widely considered as an attractive approach to improve network performance. The proposed research will address several problems related to the practical application of the above concepts to the design of the future generation of wireless networks, relying on a cross layer optimization. This involves problems in Multiplexing, Co-operative Transmission, studying the fundamental tradeoffs in these networks, Multi-user Diversity, Decentralized networking, and interference management/alignment.""451388,""Khandjian, Edward"
"457986"	"Kherfi, MohammedLamine"	"Recherche et indexation de documents visuels: vers des systèmes qui se configurent automatiquement"	"En raison de l'abondance de documents visuels (images et vidéos) sur le web et dans les collections électroniques, il est devenu indispensable de développer des outils qui organisent ce type de documents afin de pouvoir les localiser en un temps raisonnable. Plusieurs chercheurs, dont nous-mêmes, se sont intéressés récemment à cette question. Cependant, la plupart des outils développés manquent souvent de précision. Une première analyse nous a permis de constater que le même moteur peut donner de bons résultats avec une requête donnée alors qu'il donne des résultats médiocres avec une autre; il peut réussir à bien organiser certaines BD mais échoue complètement avec d'autres, etc. Une analyse approfondie nous a dévoilé que ces irrégularités sont dues au choix des ""ingrédients"" de la recherche : Un descripteur qui fonctionne bien pour un ensemble d'images peut ne pas donner de bons résultats pour d'autres ensembles; La même chose est valable pour les mesures de similarité; Pour certaines images, l'utilisation de descripteurs globaux donne des résultats satisfaisants alors que pour d'autres, il faut utiliser des descripteurs locaux; Dans certaines situations, l'usager ne s'intéresse qu'à une partie de l'image alors que dans d'autres situations, tout son contenu l'intéresse; Pour certaines images, les descripteurs visuels sont plus pertinents alors que pour d'autres, ce sont les descripteurs textuels qui donnent les meilleurs résultats, etc. Notre objectif est donc d'explorer la piste de la configuration automatique en développant un moteur de recherche capable de choisir les ingrédients optimaux à chaque fois. Notre recherche se déroulera en 4 étapes : 1) Développer une banque d'ingrédients composée de plusieurs sacs : un sac contenant plusieurs descripteurs, un sac contenant plusieurs mesures de similarité, un sac contenant différentes méthodes d'indexation, etc. 2) Développer un mécanisme de sélection qui choisit de chaque sac les ingrédients qui donnent les meilleurs performances, et ce en fonction de la situation. Cette dernière est décrite par l'image requête, la constitution de la BD, les préférences de l'usager, etc. 3) Évaluer les ingrédients choisis en les appliquant à la recherche et l'indexation d'images. 4) Généraliser aux autres types de documents visuels.""463114,""Khesin, Boris"
"456625"	"Koudas, Nick"	"Processing queries on text streams"	"In the last couple of years we have been experiencing an explosion of text information in the form of user generated content (UGC). Web logs (blogs) account in the order of 100 millions according to some studies, user participation in the top 200 social networks  is close to 1.3 billion people, microblogging services (such as twitter, jaiku, friendfeed, etc) are gaining popularity and message boards, review sites and product feedback forums are becoming ubiquitous. The term social media has been coined to refer to this information collective. A common denominator in all such applications is that individuals produce textual information (and associated multimedia content) as a function of time. Depending on the application the size, style, format and sophistication of text content  produced varies. At the University of Toronto we have been building BlogScope a system for the collection and analysis of social media. The system collects information from blogs, news sources, social networks and other UGC forums, extracts information that is publicly disclosed from the individuals producing the information (such as age, location, interests, profession, etc) and makes it available for querying. The system is currently used by thousands of people daily and it is available on the internet at www.blogscope.net. It currently processes more than 10 million documents per day and warehouses 8TB of data that evolves daily. The proposal will address issues of organizing this constantly evolving collective of information in a ways that facilitate information discovery. We will address issues of faceted information search adding spatial and temporal dimensions, develop algorithms for flexible temporal navigation and research issues of query processing and optimization in modern back-end infrastructures such as hadoop operating in the map/reduce query processing framework.""461073,""Kouisni, Lamfeddal"
"449872"	"Krzymien, Witold"	"Methods and algorithms for spectrally efficient broadband wireless networks"	"The primary objective of this proposal is the development of effective methods and algorithms essential for the future design of advanced broadband wireless packet data systems enabling bandwidth and power efficient high throughput access to the backbone telecommunication network for nomadic and mobile users of data and multimedia services. The techniques to be developed will be highly relevant to the design of all types of broadband wireless networks, including very high bit rate wireless local area networks (WLANs), but the main application target of the project are the fourth generation (4G) cellular radio systems and networks, and especially their future evolution. The scope of the proposal encompasses two major inter-related and presently rapidly developing areas in wireless communications: (1) reduced-complexity transceiver processing, resource allocation and user scheduling algorithms for multiuser MIMO and MIMO-OFDM cellular downlink through network coordination (also known as coordinated multipoint transmission/reception, or network MIMO); (2) MIMO multi-hop relaying and cooperative communication applicable to the cellular downlink.Expected results of the proposed work will facilitate development and eventual deployment of future innovative infrastructure-based wireless systems and network architectures, capable of achieving high spectral efficiency and reliable ubiquitous coverage at very high bit rates at a reasonable cost.The usual approach involving simplified theoretical analysis followed by computer simulations will be applied. Analytical approach is desirable, since it provides valuable insights into possible design options, and may expose ultimate performance bounds. Computer simulations are then applied to verify analytical results for simplified cases, and produce results for specific complex algorithms.""449073,""Krzyzak, Adam"
"456783"	"Ktari, Béchir"	"Cadre algébrique pour l'analyse de programmes informatiques"	"L'analyse de programmes est un domaine de l'informatique dans lequel on vise à produire des outils automatiques et semi-automatiques qui permettent d'avoir une meilleure compréhension de ce que fait un code. Par exemple, grâce à l'analyse de programmes, il est possible de vérifier qu'un programme est conforme à une politique donnée, de vérifier qu'une version optimisée ou obscurcie d'un programme est sémantiquement équivalente à sa version originale, et d'instrumenter un programme par insertion ou modification  de code pour contraindre son comportement à être conforme à une politique. Dans le contexte actuel, où de plus en plus de logiciels sont distribués et mis à jour par Internet, il est nécessaire de disposer de tels outils afin d'analyser les programmes avant qu'ils ne soient installés sur nos machines, et ultimement, de «corriger» leur comportement pour les contraindre à respecter nos politiques.Toutefois, si les outils d'analyse utilisés ne sont pas eux-mêmes validés par rapport à ce qu'ils sont censés faire (leur spécification), on ne peut se fier à leurs résultats: il faut donc les certifier. On note cependant que la plupart des techniques d'analyse de code sont assez complexes, rendant difficile la preuve de leur correction. De plus, elles sont en général très spécifiques à une problématique donnée.Le présent programme de recherche portera sur différentes problématiques liées à l'analyse de programmes en utilisant un même cadre algébrique. Outre l'élégance, l'approche algébrique ouvre la voie à de nouvelles possibilités intéressantes comme, par exemple, la possibilité de vérifier des programmes admettant des parties non encore implantées ou la possibilité de déterminer, par résolution d'équations, ce qui manque à un programme pour qu'il respecte une propriété donnée.""456693,""Ku, Hyejin"
"449176"	"Kwong, Raymond"	"Dependability and security in control and multimedia systems"	"Control and multimedia systems have become increasingly sophisticated and complex. Component failures in control systems can lead to financial loss or catastrophes resulting in loss of life. Insecure multimedia systems can be attacked, resulting in tampered documents or stolen content. Dependable and secure operation of these systems is critical for society. Control systems are dependable if they are highly-available, reliable, and remain safe and operational even when failures occur. Secure multimedia systems are resistant to attacks and can deliver the intended content to users. My proposed research will study fundamental issues which arise in the design of dependable control systems and secure multimedia systems. For complex control systems with inaccurate models, my methodology is to combine approaches from control and artificial intelligence to simultaneously detect faults and learn about the system. Controller reconfiguration then integrates diagnostic information and available components for dependable control design. For secure multimedia systems, I will focus on document authentication and fingerprinting against piracy in digital cinema. My approach to authentication is to embed imperceptible watermarks to protect the important data features. These watermarks are made secure using codes which incorporate cryptographic techniques. Fingerprinting of digital movies deters piracy but is vulnerable to collusion attacks. My approach is to develop a comprehensive forensic marking system which complies with industry standards for evaluation and code design. Large systems such as the power grid and telecommunication networks need intelligent diagnostic and control design tools to ensure reliable operation. Multimedia data in important application domains such as medical records, video surveillance, and digital cinema need secure protection through authentication and fingerprinting techniques. The results of the research proposed here will provide significant improvements on the dependability and security of these important technological systems.""465053,""Kwong, WaiMan"
"452604"	"Larochelle, Sophie"	"Photonics technologies for access networks and short range communications"	"Over the last decade, data communications enabled by optical networks have revolutionized our way of life. It has changed how we work, learn and communicate. Every day, new services and applications are fuelling the growth of optical communication networks, requiring an infrastructure that will deliver high bandwidth services to end users. The challenges facing optical networks are manifold. Packet-switched networks are currently considered as a means to provide efficient bandwidth management in data networks. In long haul links, traditional on/off keying modulation is being replaced by more complex formats that achieve record breaking capacity and spectral efficiency. But how can the final user access this extraordinary bandwidth? What will be the role of photonics in providing this connectivity? Solutions that are currently being studied include fiber optics passive optical networks with wavelength division multiplexing (WDM-PON) and mobile RF connections with fiber optic backbone (radio-over-fiber). This research is concerned with extending the reach and capacity of these networks. More specifically, we will focus our efforts on studying photonic devices that can enable new and efficient architectures or give access to new frequency bands. As an extension of this project we will explore how these technologies can be applied to other contexts such as sensor networks. To achieve these objectives, we will build on our current expertise in fiber optic components and optical communication systems.""453006,""LaRochelle, Sophie"
"463930"	"Lea, Rodger"	"Large scale ubiquitous computing infrastructure"	"The vision of Ubiquitous Computing whereby sensors and other computing technology built into the world around us enable smart buildings, offices and other public places to recognize people and provide them with services is a compelling and potentially paradigm shifting vision. However, although the subject of significant research effort, we are still a long way from seeing a practical realisation in everyday life.  We believe that the issues with real world deployment stem partly from two underlying factors; firstly, there is little real world deployment of the basic fabric required for the UbiComp vision, i.e. accessible and connected embedded sensors, actuators and processors; secondly, in cases where there has been real-world deployment of UbiComp infrastructure, end-users have failed to engage and have reported significant frustration in using applications that are often poorly designed, brittle and fail to meet their needs.  To address these issues we plan to exploit  the growing use of smart sensors, sensor web technologies and technologies such as RFID which are being deployed as part of the Internet of Things (IoT). Our research will investigate the most appropriate system model for a combined IOT/UbiComp framework, system mechanisms and technologies that support middleware that scales across a wide range of devices from sensors nodes to UbiComp server nodes as well as programming models to support wide area dynamic composition of devices and services to meet end user needs. To address the issue of end user take up we will  explore end-user models for UbiComp applications that support the development of a DIY (Do it Yourself) or end-user engagement strategy -and associated tools and technologies- that allows end-users to build, deploy and use UbiComp applications that are suited to their needs. Our approach draws on the mash-up model of Web2.0 or the iPhone & Android market-places that have fostered a thriving application development community. We believe our approach will lead to a vibrant community of academic and industry partners, developing and deploying new technologies and provide new opportunities for Canadian industry.""452102,""Leach, Gary"
"452732"	"Lefebvre, Roch"	"Sparse representations for audio"	"Over the past decades, advances in digital audio processing have allowed numerous applications to emerge, mature and in many cases become commodities in everyday life. Cellphones, smartphones, MP3 players, Voice over IP (VoIP) software and even Youtube all have in common advanced audio processing in their core. Specifically, speech and/or audio compression algorithms allow using transmission bandwidth and storage space sparingly while providing users the audio quality they expect for a given application. The need for performant audio processing algorithms, and in particular compression algorithms, can be expected to grow with demanding applications such as digital multichannel radio broadcast for mobile users (in cars, for example) and wireless multimedia of all sorts to be used on mobile devices.The purpose of this research project is to study new approaches for analysing, decomposing and representing audio signals in order to obtain sparse and very sparse representations suitable for compression, but also for signal enhancement, sound source separation and signal classification. Instead of attempting to improve on the classical approaches based on block-by-block analysis using transforms and prediction filters, the focus will be on adaptive signal segmentation, perceptually derived basis functions and audio object representation. The resulting algorithms will thus produce audio signal approximations which will range from synthetic sound objects (as in MIDI, for example) to high-fidelity source reproduction, rather than from high distortion to low distortion. To achieve the next significant gains in sparse audio representations, waveform tracking constraints have to be relaxed, and ultimately completely replaced by excitation pattern similarities in the auditory system. This research project aims to contribute knowledge and technological advances towards that goal.""473109,""LefebvreCécire, Josée"
"457170"	"Leung, Debbie"	"Quantum communication theory"	"Quantum mechanics has the potential to revolutionize information processing as we know it today.  This brings many applications; most notably communication system secure against all possible physical attacks and fast algorithms for solving some hard problems.  Quantum communication is an integral part of quantum information processing.  Many interesting quantum communication tasks have been identified.  They use different resources and achieve communication with varying complexity and security.  We have a basic understanding of optimal quantum communication, but we are far from knowing how to fully harness the novel quantum advantages.This proposed program aims at a better understanding of optimal quantum communication, by quantifying the optimal resources and providing optimal methods for various tasks.  Studying the ultimate limits for communication also reveals the underlying fundamental physics such as causality and the thermodynamics of error correction and encryption.  The program will investigate (1) optimal communication rates given some combination of resources (including auxiliary classical communication channels and shared correlations), (2) methods for secure communication in the presence of imperfections, (3) the nature of quantum information, and the special quantum effects exhibited by it.""452956,""Leung, Debbie"
"452068"	"Leung, Henry"	"Nonlinear signal processing for complex systems"	"Systems to-date including finance, transportation, health care, defense and social networks consist of a large number of interacting components. These complex systems are usually nonlinear, ill-posed and chaotic in nature. Conventional signal processing, developed based on relatively simple models for optimal performance, has limited usage in these systems. In fact, optimizing individual components of a complex system does not ensure system level optimality. The objective of this project is to develop a new formulation for nonlinear signal processing. A fundamental difference between nonlinear and linear signal processing is that there are many different functional forms for nonlinear systems while linear system representation is unique. We will formulate nonlinear signal processing as a problem in the functional space. The idea is to represent signal level information through higher level functions, in order to easily allow multi-level signals such as features/tracks and decisions to interact homogenously and to have processing methods selected automatically. A novel system of systems approach will also be developed by representing complex systems as a sum of the different components with individual objectives. An optimal framework will be developed for signal processing in complex systems based on an integrated multi-objective approach. In order to overcome the communication constraints on energy among subcomponents, analog wideband communications is proposed here to reduce complexity while keeping high data rate which otherwise will be used to convert information between analog and digital domains. The proposed research is expected to develop theory for complex systems to support emerging multi-disciplinary applications in engineering, management and biological sciences. From a Canadian perspective, this research will provide tools for various fields such as transportation networks, aerospace and manufacturing industries, city infrastructure management and wireless sensor networks.""465508,""Leung, KaLun(Victor)"
"450681"	"Li, ZeNian"	"Digital video analysis based on contexts"	"Digital video carries rich multimedia information and it involves insurmountable amount of data.  In recent years, researchers have advocated that contextual information in the video is vital for understanding its content so its processing and retrieval can be more reliable.  Because of its additional time dimension, digital video indeed facilitates contextual analysis.  However, context itself is an elusive subject.  Although simple visual features (color, texture, shape, etc.) and their geometric and temporal relationships can provide some contextual information, they are often limited and unreliable.We propose two novel concepts: Saliency Context and Activity Context, because they provide additional and essential cues for visual object-based video analysis.  Research issues pertinent to their descriptions and matching will be investigated. This research is a continuation of our work in visual object extraction and reconstruction from Active Video, where videos are obtained with purposive camera movements and aimed at objects of interest.  Multiple cues and descriptors at multiple scales will be incorporated to better represent the contextual information.  Moreover, efficient matching algorithms based on convex programming and machine leaning will be explored.The long-term objective of my research is to advance the knowledge in visual object-based image and video processing and analysis.  It is motivated by the cues from the intelligent human vision system.  In this proposed research, projects in several application areas such as video summarization and context-aware visual tracking will be conducted to demonstrate the potential for more efficient and effective representation and matching of visual objects in digital video, and to seek possible technology transfer.""464968,""Li, Zhanbiao"
"456222"	"Liang, Ben"	"Multi-tier mobile resource management for broadband wireless communication"	"Overwhelmed by an ever increasing number of constantly upgraded smart phones and mobile devices, the 3G wireless systems current being deployed in Canada and around the world are already showing strain in keeping pace with the growth of data-hungry multimedia applications. The 4G wireless standards currently under development promise great increases in the wireless data rate, at 100 Mbps to 1 Gbps for a variety of mobile and stationary users. This is achieved by utilizing a new multi-tier network architecture, where multiple levels of overlapping cells of different sizes and capacities co-exist to provide universal broadband services. In contrast to recent proposals of networking design with heterogeneous wireless technologies, such as the 3G-WLAN integration, this new 4G architecture allows the same licensed spectrum across different tiers to provide more efficient network control and seamless user experience. There is a body of growing recent research efforts in heterogeneous networking, but most have not considered the new challenges in the current 4G paradigm or provided sound design guidelines based on rigorous mathematics. The main goal of the proposed research program is to create new theories and technologies toward a holistic cross-layer approach for optimal resource management in the 4G multi-tier environment. A special focus will be placed on developing careful mathematical modeling and analysis frameworks for the new system, leading to original insights and innovative solutions. We will investigate into mobility modeling, admission control, and transmission data rate adaptation in the new multi-tier framework. The outcomes of this program are expected to bring substantial advances toward new standards and design policies for future wireless communication networks. It will also enable the training of telecommunication researchers and engineers for sustained development of the Canadian high-tech economy.""454934,""Liang, Dong"
"457358"	"Liang, Jie"	"Multimedia communications: sensing, compression, transmission and applications"	"Multimedia communications has evolved into an indispensable part of our daily lives, as the rapid development of Internet, communications and mobile multimedia devices. In particular, applications involving multi-users or multi-cameras have gained tremendous momentum, including multi-party video conferences, wireless visual sensor networks, video surveillance, multiview videos, and 3-D TVs.        However, existing designs of the data acquisition, compression, and transmission of multimedia communications systems are mainly based on Shannon's classic point-to-point information theory, which is no longer sufficient in many emerging network or multi-user applications. As a result, there have been growing interests in the more general network or multi-user information theory.        The objective of this program is to contribute to both the theory and applications of various aspects of multimedia communications. In the theoretical part, we will focus on the following topics: compressive sensing, distributed source coding, and joint source-channel coding for multi-user channels such as relay and interference channels using both information theory and game theory approaches. In addition, by collaborating with our industrial partners, we will develop novel algorithms for video coding, multiview video coding, and scalable video coding, and apply them to various networks and services.        Our research will integrate interdisciplinary theories such as multi-user information theory, signal processing, communications, computer vision, optimization and game theory. Moreover, working on both the theoretical and practical fronts enables us to identify new research problems from the practical applications, and to use the insights from our theoretical research to improve our practical designs.        The program provides vast opportunities for students to gain experience in both inter-disciplinary theoretical research and practical development of multimedia communications.""451507,""Liang, Ming"
"457105"	"Liebeherr, Jorg"	"Towards an overlay network architecture"	"Self-organizing overlay networks have emerged as a powerful new paradigm for deploying network services on a large scale, yet the full potential of this technology remains largely unexplored. This project will advance the state-of-the-art of self-organizing virtual overlay networks  and show that design principles for self-organizing networks can be evolved   into a blueprint for a new protocol architecture for future communication  networks and services. We will develop algorithms and protocols for a network architecture that is entirely based on the principles of self-organizing overlay networks. Using analytical methods, simulation, and measurements of testbed implementations we will provide new insights into the capabilities and limitations of self-organizing networks.  A key challenge addressed in this project is the development of algorithms and protocols that can maintain a connected overlay graph in the presence of multiple  heterogeneous substrate networks. To show that self-organizing networks can deliver high delay and throughput performance at the lower layers of the system architecture, we propose the development of an `overlay switch' with hardware support for self-organizing overlay network protocols.   We will explore algorithms that enable overlay networks to  scale to millions of nodes.   In addition, we will devise and evaluate algorithms for overlay networks that result in an efficient use of network resources.  A second thrust of this research program is the advancement of the stochastic network calculus, a new theory for the evaluation of delay and throughput performance of existing networks, as well as  future networks that may be different from the networks and protocols used today. We will investigate applications of the stochastic network calculus techniques to  gain insights into the scaling properties of network delays,  develop better bandwidth estimation methods, and improve feedback-based  congestion control algorithms. ""460012,""LieChinCheong, Sharon"
"456350"	"LightThompson, Janet"	"Data interaction models for predictive system in pervasive computing environment"	"Pervasive computing is an application-driven field which is slowly proliferating into our everyday life. Pervasive systems are complex due to the multidisciplinary nature of the diverse set of applications and lack of standard definitions, algorithms and architectures. Data is collected in various forms over short and long term period from a number of devices, often with wireless and mobile capabilities. The data interaction among these systems requires prudent iterative approach. Besides, most pervasive applications are still in a nascent stage and exist mostly in labs or in specialized environments or in the form of single isolated applications. Hence, research is crucial as these systems mature to address common data interaction models to achieve efficient intelligent pervasive systems. A lot more work is needed before wireless networking can enable configuration-free data interaction between alien devices in pervasive applications.        The goal here is to address the challenges in data collection, context aware data association, data aggregation, data integration, data management, and privacy protection in pervasive systems. Data integration models are needed that allow easy interaction of data between the myriad system components so that critical application specific predictions can be obtained quickly. Use of information-flow control models are also required to alleviate privacy concerns.        Research will be conducted to study and develop novel data integration models to improve the prediction performance in pervasive environments in the two main application domains that I am currently working, namely, the emergency response system and the non-intrusive patient monitoring system.  The performance will be studied in terms of resource saving, improved intelligence, response time, and false predictions. The expected findings will be intelligent predictive models developed from efficient data interaction models for mission-critical applications.""459837,""Liko, Tomas"
"455119"	"Lim, TengJoon"	"Towards intelligent wireless systems"	"This project designs and evaluates technologies that will enable a more resource-efficient wireless communication infrastructure, both in the backbone network and at user premises, through such techniques as spectrum sharing, real-time firmware and software adaptation to maximize the use of power and bandwidth, and intelligent system design. It expands on ongoing work in the PI's research group, which is already well-recognized internationally for important work in interference mitigation, MIMO (multi-input multi-output) processing, transmitter design for multi-user networks, spectrum sensing for cognitive radio, cooperative wireless networking, and multi-carrier transceiver design.Three Ph.D. students will be funded in this project. In the short term, i.e. the next two years, one will work on spectrum sensing and other areas related to cognitive radio, another will be tackling the design and analysis of a downlink transmitter when it has imperfect knowledge of the channels to all users, while the third will be looking into the adaptation of both transmitter and receiver to changes in the operating environment in order to minimize the transmitted power while satisfying quality-of-service requirements for each user.The project will complement the PI's collaboration with industry partner Redline Communications Inc., funded jointly by an NSERC CRD grant and an OCE grant. While that project will develop medium-term solutions such as a hardware testbed, the problems identified therein will serve as wellsprings for long-term projects suitable for a Ph.D. student, to be funded by the proposed NSERC Discovery grant. In addition, the current proposal will help to strengthen international collaboration between the PI's group and others in Singapore, Sweden and the United States.""469288,""Lim, Violet"
"454752"	"Lin, Man"	"System-level-low-power scheduling algorithm design for networked heterogeneous embedded systems"	"Low power consumption is one of the most important design factors for embedded systems, especially in this global warming era.  Besides the computing elements, the memory access, communication over system bus or network links and device access all consume energy which cannot be neglected.  Therefore, low-power design should consider the system-level energy consumption.  Most system-level energy scheduling algorithms focus on systems with single CPU or homogeneous systems.  Advanced embedded systems are becoming more and more complex. They are networked and may be composed of subsystems with various processors like single-core processors, multi-core processors, GPU, E-Textiles, etc. The networked and heterogeneous nature of such systems poses a great challenge on system-level low-power design.  In this project, we will investigate new techniques to design system-level, low-power scheduling algorithms for networked heterogeneous embedded systems. The effect of various high-level architecture features, system connections, as well as scheduling policies for system-level energy, will be studied. A framework will be proposed to specify system models and constraints and to design system-level, low-power scheduling algorithms. The specifications of the system model include the task model, the energy model, the resource model, the energy model of the devices and batteries, reliability and security constraints, etc. Existing DVS scheduling and low-power management algorithms will be studied and adapted, and new system-level energy minimization algorithms will be developed for various configurations of the system. The proposed techniques and framework will be tested and evaluated, based on simulation environments.The results of this project will make contributions for low-power embedded system designs and can be applied to the information technology industry, such as health monitoring systems and smart security devices.""468047,""Lin, Nan"
"455943"	"Liu, Xiaoping(Peter)"	"Human factors and performance enhancement techniques for haptic-enabled systems"	"Among the five major senses, touch is of particular importance: it is the only channel by which we can not only feel an object, but physically explore, manipulate and even change it.  This research focuses on haptics-- an emerging technology that enables a user to interact with virtual environments, remote objects or micro-/nano-worlds through the sense of touch. Specifically, we propose to develop some principles, algorithms and tools that are essential for a haptic-enabled system to realize highly realistic haptic interaction with virtual environments. Due to the broad scope of potential research topics, we limit our efforts to a subset of problems about human factors and system control. The work targets the application of medical simulation and the expected research results will be verified by the development of a cricothyroidotomy simulator.""456196,""Liu, XingYang"
"456739"	"Liu, Yanni"	"Protocols for efficient, reliable, and secure communications in emerging wireless networks"	"Most licensed bands of the radio frequency spectrum today are underutilized. The research in cognitive radio (CR) technologies seeks to achieve intelligent dynamic spectrum access, hence higher throughput and more efficient spectrum utilization. It may also enable the co-existence of different wireless technologies, which can drive the development of diverse future applications. Vehicular ad hoc networks (VANETs) have many important applications in transportation, such as vehicle safety, automated tolling, enhanced navigation, traffic management, and in mobile information and entertainment (infotainment) services such as in-car Internet access, mobile office, collaborative expedition, and location-based services.  This has stirred intense interest from both government and industry sectors. Both CR networks and VANETs are emerging wireless technologies. Each possesses unique characteristics and poses unique challenges. To realize the great potential, significant advances in virtually every protocol layer from the physical up to the application layers within the end-to-end communication paradigm are required. The proposed research is directly tailored to the needs of these next-generation networking technologies, services and applications. It addresses the critical aspect of protocol design for efficient, reliable, and secure communications in CR and vehicular ad hoc networks. Over the past few years, for both CR networks and VANETs, a large amount of research went into the development of lower-layer protocols such as those of physical and data link layers. There are very few studies on the upper-layers. The proposed research will fill this gap; although information from lower-layers will be taken into account, emphasis is placed on the design, development, and performance evaluation of the upper layer protocols, including network, transport and middleware layers, for emerging CR and vehicular wireless networks. The functionalities developed within this research will significantly ease the task of developers for CR and vehicular ad hoc networks applications.""471729,""Liu, Yi"
"450674"	"Lubiw, Anna"	"Algorithms in computational geometry and graph drawing"	"My research is in design and analysis of algorithms -- specifically in the areas of computational geometry and graph algorithms.  Graph representations are ubiquitous in computer science, engineering and the sciences -- a few examples are: road/electical/internet networks in engineering, and molecular structures in chemistry and biology.  Geometry, sometimes intrinsic, and sometimes imposed, is often a valuable part of a graph representation.  A big part of my work is about devising algorithms to find, manipulate and utilize such geometric representations of graphs.  Specifically, I propose working on algorithms to represent two graphs that share some vertices and edges, with the constraint that the shared part be represented consistently.  In computational geometry, a recent area of concentration is the study of algorithms to change geometric configurations over time.  ""Morphing'' is the popular term for some special cases of this, though more broadly, this area includes motion planning problems, folding problems, and problems of interpolating between 2-dimensional slices of a 3-dimensional object -- with applications to robotics, manufacturing, and medical imaging, respectively.  I propose working on algorithms to morph between two representations of the same graph while preserving some geometric structure such as planarity, angles or lengths.In both the geometric setting and in graphs, shortest paths are a continued source of interesting and practical problems.  I will pursue on-going work on finding shortest paths on terrains, such as the earth's surface, while observing height constraints, such as going downhill but not too steeply.   I will also work on shortest paths in graphs for the situation where some subpaths are forbidden, but the forbidden set is not known in advance.  This has applications in optical network routing and vehicle dispatching.""470762,""Luby, Amanda"
"463826"	"Ma, Burton"	"Integrating modelling and knowledge into computer-aided orthopaedic surgery"	"The goal of the proposed research program is the development of new computer-aided orthopaedic surgical systems. The components of such surgical systems can include pre-operative planning, intra-operative navigation, and post-operative evaluation using computer assistance. In systems implementing a pre-operative planning phase, medical images are processed to produce computer models of the anatomy. The surgical procedure can be virtually simulated to create an optimized plan of the procedure, and patient-specific instrumentation can be designed. Systems implementing intra-operative navigation may rely on intra-operative imaging, tracking systems to localize anatomy and surgical instruments, robotic devices to perform surgery, or patient-specific instrumentation. Systems implementing post-operative evaluation may process medical images, intra-operative tracking data, and various outcome measures to relate patient outcome to surgical technique. A major theme in the approach of the proposed research program is the incorporation of knowledge and modelling in surgical systems. Knowledge of anatomy can be used to achieve automated recognition, localization, and inference of anatomy in medical images. Of particular interest is the utility of such representations for modelling deformities ranging in severity from mild (joint erosion in osteoarthritis) to severe (multi-fragmentary fracture). The novelty of the proposed research will be the creation of computer-aided surgical systems with a high degree of automation enabled by knowledge integration. The current generation of computer-aided orthopaedic surgical systems rely on trained personnel to segment medical images, create surgical plans, and design and fabricate patient-specific instrumentation; automating these tasks will potentially reduce the costs and technological barriers discouraging the widespread adoption of computer-aided surgical methods. A system that can automatically recognize, localize, and infer clinically meaningful information about deformed anatomy would be a novel engineering contribution to the state of the art in medical image computing.""457267,""Ma, David"
"454756"	"MacGregor, Michael"	"Algorithms and architectures for packet classification"	"Global sales of routers and switches, estimated by looking at the sales figures of the industry leaders, are in the range of $20 billion to $30 billion annually. The objective of the proposed program is to push the state of the art in packet classification, which is the fundamental technique used in these devices. Packet classification, very generally, is the operation of comparing the contents of selected fields in a packet to a set of filter conditions, deciding whether the packet matches one or more of those conditions, and then selecting the best action in the case of one or more matches. The near term prospects are for links running at terabits per second, and new forms of addressing that will allow for hundreds of millions of subnets. This means that classifiers will have to deal with billions of packets per second per link, and that filter sets will grow from tens of thousands of rules to millions. New algorithms and implementations of packet classification are required to enable this future.""452040,""Macgregor, Robert"
"451644"	"Mahseredjian, Jean"	"Accelerated computation of transients in large scale and complex power systems"	"This project is related to the computation of electromagnetic and electromechanical transients in power systems. It is targeting simulation tools and methods used on conventional modern computers with multi-core CPUs. Only the off-line and wideband simulation methodology is considered within the constraint of highest precision in accordance to the studied frequency range.The power system engineer is faced to solve problems of increasing complexity level. In recent years the frequency range of modeling with EMT-type (electromagnetic transients) programs has been constantly increasing due to improved models, numerical techniques and modern network complexity. The inclusion of machine voltage regulation controls allows studying overvoltages in conjunction with automatic system separation controls. EMT-type methods are also required for various studies, such as, motor startup, network islanding, single-pole reclosing analysis, statistical/parametric studies, harmonic analysis for industrial installations and power swing detectors. In many cases, such as wind parks, it is required to conduct detailed simulations with power electronics devices and sufficiently small time-steps. Such simulations are extremely demanding on precision and computational speed aspects.The range of EMT-type methods is expanding into the precise simulation of electromechanical transients which are traditionally conducted using approximations and positive sequence networks. Recent research has demonstrated that it is now feasible to simulate very large scale networks using EMT-type techniques for the precise computations of low frequency transients.The objective of this research proposal is to accelerate EMT-type solutions using a combination of methods. In this project the research will target the following key aspects: sparse matrix based solutions of large scale nonlinear networks, initialization of control systems and solution of differential equations.""464552,""Mahy, Caitlin"
"454780"	"Mann, Richard"	"Computational perception of motion"	"This research aims to develop new models for computational vision, with a focus on methods for the perception of motion.  We will build on our earlier work in the area of event recognition that is based on first specifying common sense physical knowledge.  Our approach will be to examine how to segment the trajectory of a moving object into motion boundaries, such as starts and stops of action.  Our primary methodology will be the use of models of machine learning referred to as the Bayesian approach.  In particular, we will explore three concrete areas of application: analyzing and gestures in human-computer interaction as part of the software for conducting presentations; recognizing brush strokes used in tablet PCs with the aim of designing improved interfaces for these devices; aligning images as part of the application of registering images taken in medical domains.  Our research thus contributes to the improvement of various real world applications and as well advances the state of research in computational vision by determining new insights into the value of the Bayesian approach.""450990,""Mann, Robert"
"461456"	"Manuch, Jan"	"Combinatorial models and algorithms in bioinformatics"	"My future research plans call for a continued focus on problems in bioinformatics and computational biology. Besides the theoretical values, results of the proposed research can have significant practical importance. The inverse protein folding problem and in particular hand-in-glove designs have significant relevance for understanding protein interactions and our solutions can help to target new protein designs for disease using in-silico techniques. Our algorithms for haplotyping have drawn the attention of biologists and we believe there is significant potential to help to narrow their search for disease markers. The complexity and algorithmic results on RNA pathway problems help in understanding RNA function in biological settings and in design of programmed molecular systems. Several practical questions we have considered in these various domains have an interdisciplinary character, and they are leasing us to explore exciting connections to fundamental problems in other areas.""470240,""Manuel, Skahiish"
"455221"	"Marbach, Peter"	"Theoretical foundations of online social networks"	"Online social networks have revolutionized the way we interact and share information over the Internet, and social networking applications such as YouTube, Flickr, MySpace, Facebook, etc., have millions of active users. While already being enormously popular, we currently have a very limited formal understanding of online social networks. The goal of this research project to fill in this gap by working towards a theoretical foundation for the understanding and algorithm design for online social networks. The two main topics of the project will be on providing a mathematical models and results on how online social networks are formed, and on how we can use the understanding of the topologies and characteristics of online social networks to design efficient algorithms and protocols for content search, sharing, and distribution. Below we provide a briefdescription of these two main themes.As part of the project, we propose to develop novel mathematical models of how online social networks are formed, and be able to formally characterize the network structure, topologies, and properties of online social networks. While there are models available that characterize the macroscopic properties of social networks, such as the well-known small world phenomena, there are currently no satisfying models for the microscopic properties such as on (a) how individual users decide on the network connections they create, and (b) what networks properties and topologies emerge from the individual decision and actions made by users.In addition, we propose to use the knowledge about network topologies of online social network to design efficient algorithms and protocols to share and distributed information. Particular applications that we will focus on is (a) content search, (b) content distribution, (c) recommender systems, and (d) community identification.Methodologies that we will be using for our analysis include game theory, random graphs, expander graphs,We believe that the formal understanding, mathematical models and characterization, and insights into online social networks can help to created and design novel online social networking, and communication, applications that go well beyond of what today's social networking applications are capable of doing. Our long term goal is to design algorithms and protocols that allow to build online social networking applications that behave, and have capabilities, similar to the social networks that we use in our everyday life. As such, our models and results will also provide insights into how and why the social networks that we form and use in our everyday life are so efficient for information search. sharing, and communication.""461504,""Marble, Andrew"
"452622"	"Marquez, Horacio"	"Sampled-data estimation and control of nonlinear plants"	"This proposal is concerned with multirate nonlinear control systems design and state estimation of nonlinear plants with special attention to systems subject to limited communication capacity. More specifically, we will consider the following problems:Nonlinear Observer Design and associated filtering problems:  Observer theory is a classical problem in system theory that finds application in various areas. In recent years we have developed a comprehensive theory of filter design for nonlinear Lipschitz systems. We will focus on extensions of our work in this area and study the following problems: (i) Nonlinear observers and filters for nonlinear Lipschitz systems over networks, and (ii) observers for one-sided Lipschitz systems. The first problem extends our theory to systems where information is received through a communication channel with limited capacity, a problem of great interest in the systems literature. The second is an important extension to a new class of systems that generalizes Lipschitz systems and presents important advantages.Multirate Sampled-Data Control Design: The past few years have seen the emergence of the theory of nonlinear sampled-data control systems. We have contributed to this theory by proposing the use of multirate sampled-data controller and showed that multirate systems have advantages over their single-rate counterparts. Most of our work has focussed on stability of multirate systems. In this proposal, we will focus on multirate sampled-data control design and associated network problems. More specifically, we will focus on (i) the extension of stability results to the network case, and (ii) the formulation of a general theory of nonlinear multirate design based on (a) (Lyapunov-based) constructive methods, and (b) H-infinity synthesis. ""468381,""Marquis, Frédéric"
"452136"	"Mathieu, Pierre"	"Electromyography and elastography investigation of arm and trunk muscles"	"Our research program is oriented toward the study of how upper limb and back muscles are activated.  Interest in the arm's muscles derives from the challenge facing many persons when they lose one arm and sometimes both of them. During an amputation, muscles are tied around the end of the stump. They can still be contracted but no useful movement is produced. However, with the electromyographic (EMG) signal at the origin of the contraction, a myoelectric prosthesis can be activated. Amputation often leaves only a short or no forearm and the few but voluminous muscles of the upper arm become the best candidates as control sites. Our research consists at developing ways by which many surface control sites could be identified from these voluminous muscles by using surface electrodes arrays and elastography maps derived from ultrasound (US) signals. By facilitating the operation of multifunctional prostheses, our research could contribute to the improvement of the personal and professional life of many upper limb amputees. We are also concerned with scoliosis, a 3D deformation of the spine. Occuring mainly at the growth spurt, it affects 2-3% of the population. The deformation perturbs daily activities, causes physical and psychological pain and invasive surgery is required in severe cases. Prevention is difficult due to the many factors that could be involved. One of those elements is alteration in the muscles attached to the spine. To investigate this factor, data will be collected from Duchenne muscular dystrophic (DMD) patients. Due to an inherited recessive trait, their skeletal muscles are wasting over the years and many develop a scoliosis. The evolution of a cohort of these patients will be tracked over 3 years during which EMG signals and US images of the trunk muscles will be collected at 3 intervals of 12 months. Interpretation of the EMG results will be supported by anatomical and physiological information extracted from the US signals. With the knowledge acquired, new interventions could be devised to improve the wellbeing of DMD and of all other scoliotic patients. Pierre A. Mathieu (514) 343-6369, mathieu@igb.umontreal.ca""450402,""Mathieu, Pierre"
"449612"	"MehmetAli, Mustafa"	"Resource optimization in wireless networks"	"The proposed research is efficient resource allocation in wireless networks which builds on the research I have been doing on wireless ad hoc networks. Wireless ad hoc networks lack fixed infrastructure such as base stations. In this type of networks, users are both sources of information as well as participate in relaying the information to the final destination. We have studied important design problems related to two new types of wireless ad hoc networks, wireless sensor networks (WSNs) and vehicular ad hoc networks (VANETs). The wireless communications is heavily constrained by the available spectrum and high channel error rates. The advances in technology are creating new techniques to improve efficiency of wireless communications, network coding, mesh networks and cognitive radio. Network coding proposes to code several information packets into a single packet. Since wireless networks operate in a broadcast medium, the destinations may be able to decode received packets through opportunistic listening. It has been shown that coding may improve the network throughput. Wireless mesh networks are expected to form a wireless local backbone network which will provide broadband connectivity. Recent studies also have shown that the licensed spectrum is underutilized, while unlicensed spectrum is getting too crowded by the emerging new wireless services. Cognitive radio promises to sense the unused parts of the spectrum in real-time and make it available to the unlicensed users. In the proposed research, we would like to study several optimization problems related to implementation of network coding, wireless mesh networks and cognitive radio.The contributions of the proposed work will benefit the telecommunications industry in Canada which remains a strategic economic sector.   ""466761,""Mehr, Kevin"
"452172"	"Merlo, Ettore"	"Analysis, testing and evolution of security vulnerabilities in web applications"	"The purpose of the proposed research is to improve software productsand processes through software analysis and testing.New challenges have been encountered in developing Web applicationsespecially with respect to security verification because of thespecific aspects of Web programming languages and because of thedistribution of applications and data through networks.Early vulnerability detection and repair are highly beneficial,particularly in areas with interaction with the public, such asbanking and finance, medical records, educational records, government,and critical social infrastructures.I propose to investigate Web application vulnerability analysis thatcan be easily applied after application modifications. Vulnerabilityaddresses common problems as well as more sophisticated roles andsecurity privileges combinations.Vulnerability results will be communicated to developers and managersso that they apply appropriate countermeasures or fix flaws duringapplication development.""460001,""Merrett, Craig"
"456796"	"Misic, Vojislav"	"Sensing, scheduling, and performance optimization in cognitive wireless personal networks"	"This proposal addresses the research issues related to the design, development, performance analysis and optimization of wireless personal area networks that use opportunistic/cognitive spectrum access, which will hereafter be referred to as Cognitive Personal Area Networks, or CPANs. Particular attention will be given to issues of spectrum sensing, adaptive scheduling, and performance evaluation and optimization. Cognitive radio technology can improve spectral utilization in many personal area network applications, and thus alleviate the shortage of spectrum opportunities that impedes further development of wireless communications. We propose to augment cognitive technology with frequency hopping spread spectrum techniques, which provide flexibility, scalability, survivability, and resistance to interference and jamming, even when using inexpensive devices operating on battery power. Our research will contribute to improved quality of life through better home networks, increased coverage, and quality of service of wireless sensor networks, and provide efficient and unimpeded communication channels for ad hoc networks for emergency situations. In this manner, this research will contribute to improved productivity, increased profitability, and further growth of the Canadian economy.""466580,""Misik, Warren"
"464748"	"Mohamed, Yasser"	"Distributed power grids: modeling and control design"	"Driven by economic, technical and environmental reasons, large amount of increases in the electrical energy demand will be met by distributed generation (DG) in near future. Potential renewable and clean energy sources, such as fuel cells, photovoltaic arrays, micro-turbines, full-scale wind turbines, variable-speed efficient diesel generators and energy storage devices, are interfaced to the grid via power electronic converters. On the other hand, driven by the urgent need to improve the reliability and efficiency, power distribution systems are moving towards an extensive use of electronic distribution system control devices. With high penetration of these newly emerging devices, future distribution systems can be seen as converter-dominated power grids with hybrid continuous-discrete control devices.      Large-scale integration of DG units, storage and electronic control devices will be of significant impact on the structure, performance and operation practices of future energy systems. Dynamic interactions between DG units, system control device and loads may exist and can lead several low- and high-frequency instabilities. The intermittent nature of renewable generation is another key challenge facing distribution system operation and control practices. Harmonic interactions and high-frequency power quality issues may degrade the service quality. Such integration issues create a fundamental challenging question, which is how to control future distributed power grids in a way that ensures a reliable and efficient grid. This multi-dimension control problem spans the control design of individual DG units, system-level controller design and coordination, energy management and supervisory controllers.     The main objective of this research program is to develop software analytical tools, advanced control schemes and energy management algorithms to overcome integration barriers and help sustainable and clean DG technologies make their contribution to our energy system in a way that enhances the overall grid performance.""458306,""Mohamed, Yasser"
"463682"	"Mohassel, Payman"	"Practical secure computation"	"There are many important algorithms (applications) with privacy-sensitive inputs that come from different sources. A number of examples can be found in applications such as health care, genomic computation, data mining, web services, digital libraries, and e-commerce. These algorithms are often run on large sets of data that come from different sources. For instance, a data mining application might need to run a clustering algorithm on input data from rival businesses, or from government agencies with conflicting privacy mandates. An application in health care might need to run statistical analysis on several health records without violating the privacy of individual patients. In several countries there are now legislations in place that regulate the use and disclosure of private information by companies and government agencies. The sensitivity of such data limits these organizations' ability to share, or engage in collaborative studies on them. These problems can be formulated as instances of a fundamental problem in the realm of cryptography and distributed computing, namely ""secure multiparty computation"". The research on this problem has led to important and classic results. However, despite the impressive theoretical achievements of the last two decades, the majority of existing constructions are too complex and inefficient to be adopted in practice.The long term goal of this research is to identify the underlying reasons behind this discrepancy between theory and practice and to design efficient protocols that bridge this gap. Towards this goal we mainly focus on three research directions: (i) Designing efficient mechanisms to protect against strong but realistic adversaries such as malicious faults, side-channel attacks, and realistically weakened variants (ii) Developing customized protocols for important problems: We plan to design secure computation protocols for problems in data mining, genomic computation, network algorithms, key distribution, electronic voting and more. (iii) Implementing the protocols we design is an important measure of their practicality and helps us study important properties such as their scalability and the tradeoffs between the communication and computation costs.""468619,""Moher, Deanna"
"451345"	"Morris, Kirsten"	"Feedback control of systems with multiple time scales"	"The control of systems such as structural vibrations and sound waves is challenging due to the presence of an infinite number of modes. Even though the controller is intended to affect the lower modes, the control signal and also disturbances can  affect higher modes. This can lead to a decrease in performance, and  sometimes instability, if the effect of the higher modes is not handled correctly. The location of actuators and sensors for control systems such as structures can often be chosen and, because system performance depends on these locations, it is desirable to use a mechatronic approach that integrates  controller design with actuator/sensor selection.  Generally, an approximation must be used during this step. Unfortunately, not all numerical schemes or controller design methods yield consistent results as the order of the approximation is increased.  I plan to develop algorithms that use approximations such as finite elements to design control systems with the best actuator/sensor locations.       Smart materials, such as piezo-electrics and magnetostrictives, are also infinite dimensional systems. Furthermore, these materials display hysteresis.  Hysteresis is fundamentally a phenomenon that is due to multiple equilibrium points with a system time scale that is significantly faster than that of the control variable.  It is a factor in many finite-dimensional systems as well; for instance electronics, friction and mechanical play. Methods for controlling hysteretic finite-dimensional systems that consider multiple time-scales will be developed.  Models for smart materials based on thermodynamics that include spatial variation lead to nonlinear partial differential equations  with hysteresis. Nonlinear partial differential equations with multiple equilibria  arise in many other applications, including the use of smart materials in structural control, flow during freeze/thaw cycles and reaction-diffusion systems. There are few results on stability and controller design for these complex systems.  I propose to make progress by using techniques developed for finite-dimensional hysteretic systems and by concentrating first on several specific problems.""458000,""Morris, Quaid"
"454770"	"Mouhoub, Malek"	"Preference-based (temporal) constraint solving under change and uncertainty"	"Solving real life constraint problems  naturally implies tackling the challenging task of constraint processing in the presence of change, various kinds of uncertainty and preferences, as well as different aspects of time. In the past years several techniques have been developed to tackle each of these challenges separately. However, these latter challenges often  co-exist and need to be handled together within the same problem. Failing to do so will prevent us from faithfully representing and solving many  real world constraint problems.  My main objective is to develop a unique constraint solving framework with the ability to manage, in a dynamic manner: problem requirements as well as user preferences, temporal information, and uncertainty due to missing information, lack of knowledge or variability caused by events which are under nature's control. In addition to being expressive and efficient, the framework should be implemented as a GUI-based solving tool that does not require strong skills in constraint programming from the user. This tool should  be capable of guiding and assisting the user during the solving process and providing him/her with justifications and explanations  taking preferences into account (for instance, by providing the reason why it is impossible to get a better solution to a given problem). In order to meet these objectives, I propose a constraint-based approach where problem requirements are hard constraints while user preferences are expressed through the C-semiring and the CP-nets, and can be quantitative, qualitative or conditional as in ``if Condition then I like A more than B''. Through the extension of the Allen Interval Algebra, both the numeric and the symbolic aspects of time are managed together with temporal granularities and periodic data. The uncertainty is handled with both the probability and the possibility theories. The reasoning mechanism relies on new solving techniques that I will develop based on branch and bound  and metaheuristics. The success of this research program will achieve  significant advances in the state of the art and will be very beneficial to efficiently solving a wide range of problems under constraints including configuration,  reactive scheduling and planning, transportation and space missions.""456959,""Mould, David"
"456773"	"Mousavi, Parvin"	"Towards integrative data analysis for predictive modeling in biomedical computing"	"The proposed research is aimed at developing innovative methods to support our understanding of complex biological phenomena. The new era of large-scale biology is characterized by insurmountable amounts of data including high throughput genetic data, large public databases, medical images and clinical signals. Data abundance stands in contrast to the limited knowledge about the biological events underlying disease. The proposed research seeks to close this gap through the development of feature extraction methods that exploit multi-modal biomedical data for informative attributes, and modeling approaches that integrate these attributes to explain disease processes. The computational methods sought will advance the state of the art in machine learning, image and signal processing, and software engineering. Methods are discussed in the framework of prostate cancer and multiple sclerosis; however, they are extendible to other disorders. The unparalleled integrative approach that uses features from multiple-modalities, publically available databases, and clinical information enables formulate accurate models of MS progression. Such models will lead to earlier diagnosis of disease, and will facilitate the discovery of personalized therapies. In addition, the novel approaches to acquisition and analysis of ultrasound signals provide an enhanced understanding of the existence and the extent of high-grade prostate cancer. As a result, better clinical management decisions can be made to drastically reduce mortality rates while balancing it against the risks of unnecessary overtreatment. Training of highly qualified personnel is a primary focus of this work; three PhD and four Master's students, as well as three summer undergraduates will be involved in this research program.""465368,""Mousavi, Payam"
"454632"	"Mukhopadhyay, Asish"	"Geometric optimization and applications"	"My area of research is the study of algorithms for computational problems with a geometric content, particularly those requiring the optimization of one or more parameters. I study them per se, trying to  find faster, better and simpler  algorithms, both exact and approximate,  than existing ones. I also implement the algorithms that I discover since implementations unravel complications that are often ignored in theoretical studies. Lately, I have got interested in solving geometric optimization problems over geometric data streams. In this model,  we do not have enough space to store all the data streaming by. The question is what interesting computations can we do despite this limitation. For example, one might be interested in maintaining an approximate spanning ball, when we are only allowed to store the current ball. In another direction, I try to find application of geometric algorithms in other areas with the motive of finding interesting new geometric problems to solve. For instance, I have recently been involved with finding a unique way of determining the placement of n points on a line with the fewest pairwise distance queries. This is a problem that arises in computational  biology in attempting to solve the restriction site mapping problem. This has motivated me to explore other problems arising in Computational Biology with a significant geometric  content. ""471675,""MulayathVariyath, Asokan"
"452635"	"Murphy, Gail"	"Software evolution concern summarization and explanation"	"Canadian society has become dependent upon software intensive systems, such as cellphones, online banking, and ultrasounds to name just a few. To keep up with our changing world, software developers must continually evolve these systems. When evolution tasks are performed incorrectly, there are consequences. For example, an incorrectly completed evolution task at the Canada Revenue Agency (CCRA) resulted in the suspension of on-line personal tax filings in early March 2007. As a result of this error, taxpayer refunds were delayed and the CCRA was forced to incur additional costs to fix the problem. To complete evolution tasks with higher quality, software developers need to communicate efficiently and effectively about the intended change and they need to perform the technical work associated with the change precisely and completely. Currently, these activities are needlessly complicated because developers are working with all of the details associated with all the kinds of information needed for the task. This research program aims to improve developer productivity and the quality of a developer's work by enabling developers to work in terms of tasks rather than having the developers mired in the details of each task. This research program will develop techniques and tools to support the representation of the fragments of software project information needed for an evolution task. We refer to this representation as an evolution concern. The representation will include automatically produced summaries and explanations of the information comprising an evolution concern, providing the abstraction that is currently lacking in a developer's work day. We will evaluate whether the techniques and tools we built to support evolution concerns can ease development activities through laboratory experiments on open-source projects and field studies within the context of industrial developments. This research will expand Canada's $100 billion dollar software production industry.""465369,""Murphy, Jeffrey"
"457445"	"Muscedere, Roberto"	"Multi-dimensional logarithmic number system; low power 2D hardware text and image acceleration for ubiquitous mobile devices"	"As ubiquitous mobile devices have become increasingly popular in today society and the introduction of high speed data networks (such as 4G) on the horizon and higher definition display screens, common low-level functions need to be optimized in order to maximize battery life. The object of this proposal concentrates on two major areas, data encryption and visual acceleration. The Double Base Number System has great potential in simplifying the calculations needed to perform the basic data encryption arithmetic. Simplicity in hardware implementation directly relates to lower die area and the power required for the device. These are key factors in this market. The nature of this number system reduces the number of transistors and their connections to each other, which reduces both power consumption and processing delay. High quality text generation and high-resolution image manipulation will become increasingly more important as screen capacities and data network speeds increase. Special circuits designed to render and place text as well as decode highly compressed images for multi-mega-pixel images is a significant need for future devices. The divide between ""desktop"" and ""mobile"" internet functions will need to be eliminated in order for these mobile devices to reach their full potential. The inherent speeds and low power operation of silicon have begun to flatten out, so we cannot rely on the fabrication processes alone to enhance future technology. We must look at fundamental ways to improve processing of common tasks through other methods.""465785,""Musgrave, Jeffrey"
"455022"	"Müller, Martin"	"Search and simulation in games and planning"	"These are exciting times for research in heuristic search. Research goals that seemed completely out of reach just three years ago have been achieved through huge improvements in search techniques, and new applications have become possible. ""Search and Simulation in Games and Planning"" proposes research in the fast developing field of simulation-enhanced search methods. Statistical ""Monte-Carlo"" methods, which explore a search space by repeated random sampling, have proven to be a very powerful, dynamic technique that overcomes the limitations of more static previous approaches. These techniques have lead to rapid progress in applications as diverse as the ancient game of Go, game-independent General Game Playing, and even classical domain-independent planning.While very powerful, these new approaches are still relatively poorly understood, and there remain important applications where these techniques either cannot be applied, or fail to scale up to more complex problem instances. The aim of this proposed research is to study and further improve simulation-enhanced search methods, both in theory and in practice. One focus of our research will be on hybrid systems that can combine the strong points of previous approaches and the new Monte-Carlo search methods.""473069,""Mulligan, Catherine"
"463917"	"Nabki, Frederic"	"Wireless microelectromechanical integrated sensors"	"Like lasers or integrated circuits, microelectromechanical systems (MEMS) are a disruptive technology, which enhances existing systems and enables new applications. It is projected that the MEMS market will experience significant growth over the next three years (14 to 19% per year), and will involve many different emerging applications such as micro displays, fuel cells, fluidics, and sensors. MEMS sensors have had an increased presence in the consumer market, with a significant number of commercialized devices such as motion controlled handhelds, airbags deployment systems, and microphones. In addition, because they are mobile, MEMS sensors are well suited to wireless interfaces for the transmission of telemetry. MEMS are fabricated with similar technologies to those used for the fabrication of integrated circuits. As such, complementary metal-oxide-semiconductor (CMOS) chips are a natural choice for implementing the wireless interfaces and the data processing circuitry included within MEMS sensors. However, the integration of CMOS and MEMS technologies remains difficult because of microfabrication process incompatibilities, MEMS-specific circuit design intricacies, and high cost of system packaging. This research program aims to investigate the design of circuits and MEMS transducers for the implementation of integrated MEMS sensors with wireless capabilities. The research program is structured into three different axes: 1) integrated circuits for sensors, 2) MEMS fabrication, and 3) system integration.Ultimately, this research program will facilitate the creation of fully integrated wireless MEMS sensors in order to enable low-cost, small form factor systems, which can be used in a variety of environments and applications. This will yield advances in the fields of MEMS, sensing and low-power communications, which represent significant contributions to research in Canada.""452124,""Nabutovsky, Alexander"
"454515"	"Najm, Farid"	"Computer-aided design for advanced large-scalre integrated circuits"	"An Integrated Circuit (IC) is an electronic component that typically consists of a large number of circuit elements that serve collectively to perform a useful function. ICs, alsocalled silicon chips, are the building blocks of almost all electronic equipment todayincluding computer and communications hardware, both stationary and mobile. Over the last30 years, IC designs have evolved from very small (a few transistors) and very slow (kHz)to very large (100 million transistors) and very fast (GHz). This was made possible by thecontinuous shrinking of semiconductor technology over the last 50 years. Today, state ofthe art ICs include features that are only 32 nano-meters (nm) across (one nano-meter is1/1000000000 meters), and next generation technology will probably be at about 22 nm. As ICs have become complex, correspondingly, the design process for ICs has increased in complexity so that it now takes a team of about 1000 engineers to design a chip as complex as a Pentium. A large number of software tools are used by IC designers to assist in thedesign process.  These are referred to as Computer-Aided Design (CAD) tools, also called Electronic Design Automation (EDA) tools. Research in the CAD field has continuouslyimproved the capabilities of the tools.  Nevertheless, EDA tools of today are unable tocope with the ever-increasing complexity of IC designs. This research is targeted atimproving our ability to design ICs in the nanoelectronics regime, by advancing the CADtools in various ways, relating to the on-chip power distribution network and tovariability in circuit timing arising from manufacturing process variability. Such toolsmust be developed if the historical trends of advance in IC design are to be maintained,in support of future communications and computing technology.""462251,""Najmaei, Nima"
"452008"	"Narayanan, Lata"	"Algorithms for wireless ad hoc and sensors networks"	"Advances in wireless networking technology have made the vision of mobile and ubiquitous computing a reality. Wireless networks in the home and public spaces are now commonplace, and emerging technology and the applications they will enable will change our world in hitherto unknown ways. My research is centered on wireless ad hoc, sensor, and delay tolerant networks, and ranges from applications of such networks to routing and other communication protocols, to algorithms to determine who gets access to the wireless medium.Ad hoc networks can be defined as networks that are created as needed, by the nodes of the network, usually without any assistance from the existing Internet architecture. Wireless sensor networks have the additional feature that nodes are equipped with sensors that can monitor environmental characteristics such as light, temperature, humidity, or motion. Nodes in both kinds of networks may be mobile, and may or may not have fixed positions. Nodes are generally severely resource-constrained and run on limited battery power, particularly in sensor networks. In the paradigm of delay-tolerant networking, even an end-to-end path is not assured at any given moment. Finally, since nodes are autonomous entities, they may choose to be uncooperative in network protocols. Communication between nodes must overcome these challenges and be achieved by multi-hop routing.I propose to find efficient algorithms for achieving communication in wireless ad hoc, sensor, and delay-tolerant networks. I will study ways to ensure cooperation in wireless networks, and to prevent or detect selfish behavior. Finally, I will investigate different applications of wireless sensor networks, particularly intruder detection and barrier coverage using moveable sensors. ""451254,""Narbaitz, Roberto"
"452702"	"Nikolaidis, Ioanis"	"Usable and efficient abstractions for wireless sensor networks"	"Wireless sensor networks are formed by inexpensive, underpowered, and rather unreliable wireless devices/nodes that are deployed in all kinds of environments to allow the remote acquisition of sensed data. Transmissions are expensive because they require a significant amount of the energy available to the nodes. To cope with this problem, sensed data, such as temperature, motion, pressure, etc. are collected and aggregated within the network, with the intention to reduce the total amount of data that needs to be eventually transmitted. Application programmers who wish to develop applications for wireless sensors networks need to be able to translate the application requirements into computation and communication tasks for each node. Yet, because of the limited resources of those nodes, they do not have the luxuries afforded by high-end computing devices that run fully capable operating systems. The question thus becomes what should be the minimum set of capabilities that would allow a wide range of applications to be developed, and are possible to incorporate in the limited resources of sensor nodes. At the same time, one should be able to properly and easily reason about the application logic. That is why the minimum set of capabilities should be accompanied by a model of computation and communication. In essence, the communication and computation model acts as the framework in which applications will be developed. Our proposal addresses precisely this issue, i.e., what abstractions and model for development of applications is suitable for wireless sensor nodes. We use our experience that spans from theoretical results to implementations of such networks. The proposal places emphasis on how to leverage the large number of sensor nodes to provide a ""communal"" form of resources over which an application can be developed, while at the same time being capable to deal with failures and energy saving strategies in a fashion that does not obstruct the applications.""463566,""Nikolova, Natalia"
"457484"	"Okhmatovski, Vladimir"	"Novel methodologies for fast electromagnetic modeling of signal propagation on interconnects"	"The continuously increasing demand for higher functionality and lower cost of consumer and industrial electronics stipulates the evolution in microsystem design methodologies. As microsystem designs are exhibiting higher data exchange rates and tighter component integration, the modeling techniques must be changed adequately to account for the occurrence of tighter physical coupling between the devices. Thus, the large fragments of the microsystem designs must be modeled simultaneously while maintaining the electromagnetic accuracy of analysis.  To overcome the constraints imposed on the analysis by the use of simplified traditional approaches capable of modelling only small microsystem fragments, we propose to develop a new generation of prototyping methodologies which can expediently handle the associated computational complexity. These methodologies will adapts today's fastest methods of computational electromagnetics to modeling of integrated electronics and map them onto powerful high-performance supercomputing architechtures. The main thrust of research will be in generalizing the recently proposed well-conditioned integral formulation to the case of multilayered substrates, augmenting them with skin-effect inclusive current flow models, and porting thus created algorithmic framework to the multiprocessor machines. This specialized software-hardware synergy will allow electronics designers to conduct electromagnetic modeling of integrated circuts of unprecedented sizes and complexity. Such modeling methods are expected to be adopted by the electronic design automation industry and establish a new standard as the available commercial tools greatly lack the capicity to address the associated complexity. The proposed research program will help close the gap between the needs and capabilities of electronics designers and has a potential to cut the multi-billion dollar losses incured by the majority of today's design houses and semiconductor foundries due to costly re-designs and delayed time-to-market.  ""455630,""Okine, Erasmus"
"449397"	"Oommen, John"	"Learning, neural and pattern recognition systems"	"During the term of the award, I propose to undertake various projects which concern both the theory and applications of Learning Automata (LA), and Pattern Recognition (PR) as listed below:1. I propose to develop a formal LA-based theory for Tutorial-like systems in which an imperfect ""Teacher"" teaches from inaccurate teaching material, and where each individual probabilistic ""Student"" found in a ""Classroom"" of such ""Students"" can learn from the ""Teacher"" and his ""Colleagues"", where each of these entities need not be a human being but for example, a component in a cybernetic system.2. The families of Pursuit and Estimator-based LA have been characterized by using LA which use action probabilities, but which also maintain estimates of the reward probabilities. In the past, these estimates have been of the Maximum Likelihood nature. I propose to develop the theory of the Bayesian Estimator LA, in which the estimates used to design the Estimator LA are obtained in a Bayesian manner.3. The Goore Game (GG) is a non-trivial non-zero-sum game, which unlike the games traditionally studied in the AI literature (e.g., Chess, Checkers, Lights-Out etc.), is essentially a distributed game. LA have been used to play the game when all the parameters are unknown. I propose to use LA to solve the GG when the ""players"" can also be traitors, and to study its applications in sensor networks and distributed optimization.4. I propose to use LA to solve the COMMONS game, a generalization of the Prisoner's Dilemma. This game simulates a society in which Entities have to work cooperatively so as to utilize common shared resources.5. I propose to use LA to enhance the Growing Self-Organizing Map (GSOM) which is a neural network clustering algorithm based on Kohonen's Self-Organizing Map.6. Historically, the concept of designing Neural Networks (NN) which are inherently chaotic and which also possess PR capabilities by abruptly switching to periodicity, is new. Our prior research has led to the only reported NNs that demonstrate Chaotic PR phenomena. I propose to study various problems in this area.""461415,""Ooms, Kristopher"
"463851"	"Ordonez, Martin"	"Fuel cell hybrid schemes for renewable flex power generation"	"Global concerns relating to the uncertain cost of fossil fuels, energy security, and environmental sustainability have brought ""green"" alternatives to the forefront of the energy sector. Canada's commitment to reduce green house emission by 20% by 2020 requires fast incorporation of renewable power in the electricity distribution system. Distributed and intermittent renewable energy sources including photovoltaic arrays, wind turbines, fuel cells, and ocean tidal generators require energy conversion and storage technologies to transform raw unregulated power into quality power needed to supply stand-alone AC loads and inject power into the grid uninterruptedly. The main technical challenges associated with successful integration of renewable power are intermittency, stability, power flow control, power quality, utilization, and efficiency. When two or more power sources are combined in a hybrid system to mitigate intermittency effects and improve stability, further technical challenges arise from their interactions, resulting in the need for reliable coordinated operation.The goal of the proposed research program is to investigate and develop a flexible power architecture that combines fuel cell technologies and intermittent renewable sources to produce uninterrupted power generation. The program targets research and development, and final implementation of a flexible, reconfigurable power conversion system that concentrates on definitive technical challenges associated with intermittency, interactions, need for elastic scalability, and control for coordinated operation. As a result, the program will provide valuable and timely technical solutions to incorporate multiple hybrid renewable sources to mitigate green house emissions. Anticipating the high demand for engineers in renewable power, the program would help to train highly qualified personnel and deliver graduates with strong R&D skills to the province and the country. The research program fits into the government's strategy on science and technology by helping to accelerate the deployment of renewable power. As well, it meets NSERC's objectives by bringing originality and innovation to address one of the most pressing and challenging areas in modern society.""466693,""ORegan, SachaMelanie"
"451232"	"Panangaden, Prakash"	"Probabilistic systems and applications"	"Probabilistic systems are becoming more common as we move towards a world in which embedded real-time systems, process control systems, flight management systems and communication systems become more common.  Establishing the correctness of such systems poses new challenges for verification technology especially when we combine continuous space and real-time aspects with probability.  The purpose of this proposal is to develop the mathematical framework for reasoning about such systems.  This project is not about randomized algorithms but rather about reasoning in the presence of uncertainty with a view towards verification.  In the past several years I and my collaborators have developed a theory of systems that combine physical continuous aspects with the discrete nature of software systems.  We have developed ways of measuring how close the behaviour of two systems is and we have techniques for approximating a complex system with a simpler one.  We have also explored applications to AI.  We have also looked at the special problems of security where we have developed techniques for describing how well a system prevents the leakage of information.In this proposal I plan to continue this work with special attention on the combination of probability and nondeterminism.  Sometimes we cannot know the probabilities, nevertheless we need to quantify notions like risk and uncertainty.  This combination would be particularly useful in many modelling tasks and in reasoning about security.  I will also investigate concepts like knowledge and information and how it flows as agents interact with each other.A minor theme of the current research is exploring ways of reasoning about quantum computation which is a very special kind of probabilistic system.""460755,""Panario, Daniel"
"452043"	"Parsons, Jeffrey"	"Using classification principles to improve semantic information integration"	"The problem of data semantics (or meaning) is one of the pressing issues both in effectively representing information about a domain in an information system (data and software), and in supporting large-scale sharing of information across multiple independent sources, such as those available over the Internet. Addressing the problem of determining the meaning of data requires overcoming two challenges: (1) specifying semantics for a single information source; and (2) providing mechanisms for reconciling the semantic specifications provided by independent sources.      In the proposed research, new approaches to understanding the semantics of information sources will be proposed. In addition, methods to reconcile independent sources will be developed and evaluated. The research will build on my earlier research by focusing on three areas. First, my students and I will develop methods to enhance the semantics of information models by exploiting the inference capabilities afforded through the classification of instances. Second, we will apply the concept of classification-based inference to provide a semantic layer to tagging mechanisms used in popular tools for annotating content in online social networks (e.g., del.icio.us and Flickr.com). Third, we will apply and evaluate the methods developed in the first two phases of research to business contexts. In particular, we will use the classification principles to develop domain ontologies for the online travel industry in conjunction with an industry partner.     As the accessibility and scope of networked resources grow, the need for effective methods and tools to locate and manage information is becoming more critical to both individuals and organizations. This research aims to enhance Canada's expertise in the Information and Communication Technology sector in general, and in information management in particular, by supporting the effective classification of information resources in order to improve the ability to search, filter, and use these resources.""465753,""Parsons, Lance"
"463918"	"Patel, Hiren"	"Achieving predictability, repeatability and performance for hard real-time embedded systems"	"In order to use chip-multiprocessors (CMP)s for hard real-time embedded computing (HRTEC), we need to innovate computer architectural techniques that exhibit predictable and repeatable timing behaviours, and those that simultaneously enhance performance. This is because HRTEC makes timeliness a criterion for correctness, and ensuring that timing requirements are always met is of critical importance. These timing requirements are ensured through accurate worst-case execution time (WCET) analysis. Unfortunately, modern uniprocessors and CMP architectures are designed with the perspective that faster is always better; thus, employing architectural techniques such as speculative execution, cache-coherency protocols, and best-effort routing. The resulting designs are poorly suited to HRTEC because WCET analysis either requires heroic efforts or is just intractable. In addition, such designs do not scale to CMP architectures.To address this, I advocate re-evaluating architectural techniques with particular emphasis on enabling predictable and repeatable timing behaviours. By having predictable timing behaviours, designers can obtain tight WCET estimates under various stimuli and operating conditions, and with repeatable timing behaviours where multiple executions of the system under the same operating conditions yield identical timing behaviours, testing can be used for correctness. Therefore, the proposed research programme focuses on innovating architectural techniques that improve performance while simultaneously increasing predictability and repeatability for hard real-time embedded systems. In particular, I will concentrate on: (a) devising an interleaved-multithreading (IMT) VLIW architecture with ISA extensions for improving predictability, repeatability, and high performance, (b) designing a dynamic random-access memory controller that tightly couples the controller with the IMT pipeline for better predictability and high performance, (c) scaling to a CMP, and (d) implementation and evaluation on an FPGA platform.""472046,""Patel, Mitesh"
"463806"	"Pattabiraman, Karthik"	"Building error-resilient applications on many-core platforms"	"Emerging applications demand increasing amounts of computational power, which cannot be satisfied by today's micro-processors. To keep up with this demand, many-core processors with tens or hundreds of cores will soon become a reality. However, many-core processors will experience high rates of hardware errors, due to the fundamental limits of physics and engineering. Traditional approaches that mask hardware errors from software incur high power-overheads, which limit their applicability in many-core processors. Therefore, future applications must be capable of anticipating and tolerating hardware errors in order to leverage the computational capacities of many-core platforms.The proposed research program will investigate an application-centric approach to detect, diagnose and recover from errors in many-core processors. The detection approach uses compiler-based techniques to insert runtime checks into applications. The approach is completely automatic and requires no intervention from the application developer except for annotations on what data is critical to the application, i.e., important for the application's correctness. The diagnosis approach uses formal verification techniques to trace back the cause of failures and isolate the failure-causing errors. Finally, the recovery approach reconfigures the application to mitigate errors optimally based on online performance and power models. The main advantage of the proposed research program is that error detection, diagnosis and recovery are all performed entirely in software, which results in substantial power savings compared to traditional approaches.The results of the proposed research program will enable a diverse range of applications from high-performance codes to desktop applications to execute reliably on many-core processors. This in turn will facilitate the rapid adoption of many-core processors, and accelerate progress in science and engineering. ""470871,""Patterson, Charla"
"463919"	"Polouchine, Ilia"	"Force reflection algorithms for networked bilateral teleoperators and haptic interfaces"	"Force-reflecting teleoperation and haptic interaction with objects in virtual environment are two related areas of technology which have numerous applications to manufacturing, robotic surgery, health care and rehabilitation, education, search and rescue robotics, and other areas. The proposed project aims at development of new schemes for networked force-reflecting teleoperators and haptic displays that are built upon the principle of projection-based force reflection. This principle assumes decomposition of the force reflection term into ``interaction''  and ``momentum-generating'' components and subsequent attenuation of the latter. It was previously demonstrated that application of this principle to the design of force-reflecting teleoperators in the presence of network-induced communication constraints leads to substantial improvement in stability and performance characteristics.  The long-term objective of this project can be defined as comprehensive development of the principle of projection-based force reflection and its application to the design of networked force-reflecting teleoperators and haptic interfaces. More specific goals will include elaboration and justification of the projection-based force reflection principle, development of new projection-based force reflection algorithms, their experimental evaluation and comparison, and applications to the design of new schemes and algorithms for cooperative force-reflecting teleoperation over networks and for haptic interaction with objects in virtual environments.""465722,""Poloz, Yekaterina"
"452092"	"Poulin, Pierre"	"Complex appearance modeling and animation"	"The realistic appearance of objects is strongly influenced by how light behaves with (i.e. on and under) its surface. Light transport depends of various physical properties of the surface. In computer graphics, they are mostly modeled by geometrical optics, such as the distribution of its meso- and micro-structures, each structure having its own physical properties. This hierarchy of scales for a surface is a fundamental concept; it distinguishes an object from its surface. However when observed from different distances and view angles, this appearance can have a highly non-linear visual behavior.This research program aims to revisit this entire hierarchy of scales to efficiently construct, pre-compute, compress, represent, and render realistic surfaces at any scale, including how external temporal phenomena affects the appearance with local and global deformations, deteriorations, gravity, heat, wetness, etc.Projects are proposed to improve the current state of the art at four levels of appearance modeling: BRDFs, svBRDFs, BTFs, surface parameterizations. A global view of the process will also consider smooth transitions between these levels of modeling details.The proposed surface design pipeline fits very nicely with how current graphic artists create scenes, so it should be intuitive to use, and allow for fast and wide adoption within the rendering community. It should make available the creation of complex surfaces and increase the range of surface realism of next generation computer imagery. The new introduced representations and developed expertise in compression will apply to real-time hardware rendering, as well as to offline quality rendering, but with large number of surfaces. These expertises are highly sought after in the competitive computer graphics industry.""452588,""Poulin, Richard"
"463921"	"Pratte, JeanFrançois"	"Advancement of instrumentation for medical imaging"	"A Positron Emission Tomography (PET) scanner is a nuclear medicine diagnostic tool that produces 3D images of functional process in the body. In comparison, a Computed Tomography (CT) scanner is a medical diagnostic tool that produces 3D anatomical images. A dual-modality PET/CT scanner has the benefits of producing 3D images of functional process anatomically located in the body. The nature of the work in this proposal is the realization of a novel detection module that will be used in future PET/CT and CT scanners. The proposed detection module integrates a new digital photodetector and the required miniaturized electronics that will detect the information essential to create the images. This detection module is now possible, thanks to recent progress in semi-conductor technology, where multiple integrated circuits can be stacked and interconnected vertically (in 3D). This research is important because it will enable PET/CT and CT scanners with unprecedented performances, which will have a direct impact on Canadians' health through better diagnostics and a reduction of the radiation dose absorbed during the scans, a significant outcome. This research is also important for the training of graduate students, the next generation of scientists and engineers who will push the limits of medical imaging instrumentation and advanced integrated circuits design. This research proposal represents a major advancement in the field of medical imaging instrumentation and will contribute to keep Canada a world leader in medical imaging apparatus and advanced integrated circuits.""471342,""Precup, Doina"
"455114"	"Precup, Doina"	"Developmental reinforcement learning"	"We propose to address the major problem of building artificial intelligent agents that exist for an extended period of time, during which they continually improve their world representations.  We will use the theoretical framework of reinforcement learning, which allows agents to learn by interacting with an unknown, stochastic environment, and receive reward feedback form it.  Unlike in existing reinforcement learning methods, we will not try to infer a true environment state.  Instead, agents will build their own internal state, which will be sufficient to make predictions.  Prediction errors will be used to guide the agents' exploration, in a process mimicking curiosity.""457587,""PredoiCross, Adriana"
"454596"	"Primak, Serguei"	"Measurement, modelling and simulation of wireless channels with applications to modern communication"	"Progress in the development of communication systems, and their omnipresence require consistent update and development of new innovative tools, techniques and models for the simulation and the evaluation of cutting-edge designs. A surge in the demand for remotely operated devices, especially in the communications, control, health and green power industries, brought new challenges in understanding of the effect of communication systems, and the need for the investigating of state of the art communication techniques. Growing demand for the development of 4G and beyond calls for communication systems with smaller cells, smart antennas, mobile-to-mobile, relay assisted peer-to-peer communications, cognitive capabilities. Increase in diversity and in integration of the traffic require that new, innovative models of communication channels be developed, and leading-edge means and ways provided along with analytical techniques, allowing to analyze their impact on the overall system's performance. It is expected that the proposed research will make significant impact in the areas of high importance to Canadian industry and science.""466185,""Primmer, Heather"
"463694"	"Quitoriano, Nathaniel"	"The direct growth of high-quality semiconductors"	"Research in semiconductor nanostructures is developing quickly with recent demonstrations of nano-lasers, semiconductor nano-transistors, and nano-sensors.  The applicant has led some of these advances by engineering nanowire growth to build a transistor and measuring the mechanical properties of single-crystalline, Si nanotubes which have potential use as resonant sensors.  These technologies utilize unique properties at the nanoscale.  The applicant will now take his strong background in lattice-mismatched semiconductors and semiconductor nanostructures and apply them to demonstrate new methods of integrating high-quality semiconductors.  The technological impacts of integrating semiconductors together would lead to sweeping changes in the design of semiconductor devices influencing devices like solar cells, computer processors, cell phones, and telecommunications.  The applicant is proposing a number of technologies to starting at the nano-scale, where unique properties enable the direct growth of high-quality semiconductors.  For example, one proposed technology will demonstrate the growth of high-quality, single-crystalline semiconductor films on amorphous substrates by scaling up a technology, already demonstrated at the nanoscale.  This technology will revolutionize solar cells, since it will enable high-efficiency solar cells on large, glass substrates.  The applicant will demonstrate the utility of these structures and technologies by building working devices, such as solar cells, lasers, and photodetectors on Si and amorphous substrates.  In the process of demonstrating these devices, students, working with the applicant, will receive broad, interdisciplinary training in materials growth, materials characterization, device fabrication and device characterization.  The enclosed proposal outlines compelling projects the applicant will explore leading to substantive, commercializable technologies and broad, industrially-relevant training.   ""471070,""Quon, Peter"
"450440"	"Rappaport, David"	"Computational geometry and data analysis"	"I study the geometry in problems with the intent of producing correct and efficient computer programs. Applications such as computer graphics, computer vision, optical character recognition, robotics, computer-aided design, and computer-aided manufacturing, network design, and data analysis all have some geometric component. My work focuses on the geometric structure or geometric patterns that can be used to solve these types of problems. The nature of my research is primarily theoretical in the sense that general methods and techniques are given. These results may not immediately apply to a particular problem. However, with the appropriate modifications, these results will be used for a variety of applications. Furthermore, these results are not based on any existing computer hardware or operating system. Thus the results have the added advantage of being useful in the future on some currently unimaginable machine.      The problems I work on are motivated by a particular class of applications where interactions between components in a system are often displayed as a graph, that is, the relationships between elements of the system are represented in a mathematically rigorous way. Examples of the use of this type of representation are subway maps, project time lines, communication networks, or the hierarchical management structure of large organizations.      I will maintain an annual cadre of three graduate students, both at the Master's and Doctoral level. These students will pursue research that is either entirely theoretical, an application of theoretical results, or both. The training these students will receive will enable them to access, master, and create results from the rich extant literature in the field of computational and combinatorial geometry. One continuing and two new Doctoral students will be trained to work as researchers in computational geometry in an industrial or academic setting, two continuing and five new Master's students will be trained to either continue their studies in a Doctoral program or enter the work force as IT professionals. ""452661,""Raptis, Leda"
"463855"	"Rayside, Derek"	"Programming with specifications"	"The objective of this research is to improve software quality and reduce development cost.Software is becoming an increasingly important part of civil infrastructure: from medical and avionics systems to cars and roads with embedded sensors, from Blackberries to desktops to the mainframes that calculate our taxes, software is everywhere.Software failures can cause or contribute to serious accidents that result in loss of life, environmental damage, or financial calamity.  A widely cited 2002 report from US National Institute of Standards and Technology (NIST) reported that inadequate testing of software costs the US economy $20--$60 billion per year.  As software permeates more of our lives the risks posed by low quality software only increase.A 2007 study commissioned by the US National Academy of Sciences (Software for Dependable Systems) argued that the way forward is to produce objective dependability arguments for software artifacts (rather than the alternatives of certifying practitioners or production methods).  This study also noted that the majority of problems with software quality arise because the specifications for the software do not match the user requirements.  Executing specifications, as we propose here, provides mechanical means for programmers to establish objective arguments that their specifications are consistent with user requirements.The tools developed as part of this research programme will make Canadian high-tech industry more competitive in what is increasingly a global marketplace.""467360,""Raz, Amir"
"455135"	"Reformat, Marek"	"Fuzziness and context-awareness in decision-making schemas for semantic web"	"An introduction of the concept of Semantic Web has brought predictions, as well as promises of automatic finding and accessing information and services on the web. Uncertainty is an integral component of information and knowledge. Many concepts we deal with are without precise definitions, or with missing or inaccurate data. Such a situation is also present on the Internet where many sources of information could be corrupted, or partially and temporally inaccessible. However, uncertainty is not only associated with data and information stored on the web - also users cause ambiguity and imprecision. In many cases, users' decisions regarding selection of most suitable alternatives heavily depend on current circumstances, understanding of a situation, needs and requirements, as well as users' ""mood"" for selecting something new and/or unusual. All those aspects of a selection process are naturally ambiguous. In order to make the web a user-friendly environment where users can easily, quickly, and precisely find things they are looking for, new web utilization tools have to be developed. The objective of the proposed research is to develop a framework supporting design and construction of fuzzy-equipped components capable of performing context-aware multi-criteria decision-making tasks in the presence of impression - imprecision that ""comes"" from the web, and imprecision ""caused"" by a user. The research will focus on applying techniques of fuzziness, evidence theory and machine learning to argument general knowledge (domain ontology) with user's behavioral patterns; and elements of fuzziness, evolutionary computing and substitution to generate new - ordinary as well as unconventional - context-dependent solutions to web-based decision problems.The ultimate goal is to formulate a fundamental methodology for construction of human-centric systems, i.e., systems that represent users on the web - work on users' behalf, continuously search for information interesting to users, adapt to any changes in users' preferences and needs, and handle ubiquitous imprecision. ""466163,""Refvik, Shannon"
"449991"	"Reilly, James"	"Application of signal processing in neuroscience"	"This proposal develops a fundamental thrust on the interface between neurological science and signal processing and biomedical engineering, in that we propose the development of new methods in machine learning (ML) for application to problems in neuro-science.  One area of application is in the field of psychiatry, with specific attention to the treatment of depression.  We also propose using ML methods to offer new insight in the broader framework of neuro-science.  With respect to the psychiatry application, major depressive disorder (MDD) is a serious and debilitating psychological condition. It is not known a priori to which type of the many available anti-depressant medications a particular patient may respond.  Thus, in prescribing a treatment for MDD, the psychiatrist must by necessity resort to a trial-and-error procedure.  This can result in long delays before remission, with attendant suffering on the part of the patient. The applicant and his colleagues have developed a preliminary ML-based procedure that can predict the response of a particular patient to a medication using only the patient's electroencephalogram (EEG) obtained before treatment begins.  This enables prescription of an effective treatment immediately.   However, new ML methods with significantly improved performance must be developed before the full potential of this capability is realized.    That is the first objective of this proposal.We have previously demonstrated that certain neurological conditions such as MDD, schizophrenia, epilepsy, tinnitus, etc. can be identified using ML methods from the EEG.  An additional objective is to collaborate with neuro-scientists to determine whether the relevant features extracted from the EEGs of subjects afflicted with these conditions can be used as clues to gain further insight into the pathology of these disorders. ""449607,""Reilly, Norman"
"457446"	"ReyhaniMasoleh, Arash"	"Efficient and reliable computations for cryptography and coding: architectures and algorithms"	"With the rapid advances in information, computer and communication technologies, huge amount of data is being processed, stored and exchanged among users which makes the data integrity and information security become very important issues. To protect the data and make it secure, various cryptographic primitives and error detecting/correcting codes are being incorporated in many standards and protocols and implemented in a wide range of platforms from powerful computers used in the servers and base stations to the limited ones used in smart phones and wireless devices. The cryptographic and coding schemes are naturally computationally complex functions and hence the system performance heavily relies on their efficient computations, specially in resource constrained embedded systems, such as smart cards, next generation passport, RFID tags, and wireless sensor networks, where the power consumption, memory, and bandwidth are very limited. One of the research directions of this proposal is to develop efficient algorithms and architectures for cryptographic and error control coding systems and then design, test and implement them for the above mentioned platforms. The complex nature of software algorithms and hardware architectures of cryptographic and coding systems makes the reliability of their implementations a challenge. Another focus of this research is to develop reliable techniques for software and hardware implementations of these systems. More importantly, since cryptographic systems perform in harsh and hostile environments, such robust schemes are required to counteract fault attacks and natural faults. This is a very important research area because the fault attacks have become the most significant threat to many practical cryptography-based security systems. Furthermore, with advances in IC manufacturing technology and improvements in the density and speed, occurring natural faults will become more likely in the products made by new technologies. Therefore, for the existing and future applications of cryptographic systems, developing reliable algorithms and architectures to counteract these threats is highly important. This research will lead to more secure and reliable systems with lower cost and higher performance.""464334,""Reynolds, Conor"
"455137"	"Rohling, Robert"	"Dual-source 3D ultrasound and adaptive imaging for guidance and elastography"	"The medical ultrasound market is expected to grow robustly from $4B to $6B by 2012 (InMedica, 2009). The emerging areas of ultrasound are anesthesia, emergency medicine and surgery (15% annual growth). Growth is fuelled by adding new functions to the ultrasound machine, and by improving the image quality. At the same time, ultrasound is experiencing a revolution in miniaturization, with new ultra-portable systems now being offered by some of the major manufacturers. The combination of new functionality, better image quality in a small low-cost package would make ultrasound attractive to doctors who have not used ultrasound for a given task before. With such systems, the range of clinical applications for ultrasound would increase. This project is aimed at providing new functionality and improved image quality through the simple concept that two ultrasound transducers are better than one. In particular, the specific objectives of this grant are to make a leap forward in the capability of ultrasound through the use of (1) custom transducer design using two ultrasound heads, (2) adaptive image processing, and (3) novel methods to depict tissue stiffness from ultrasounds signals, while maintaining the existing benefits of real-time imaging, low cost and portability. This research is significant because such a new line of ultrasound transducers and imaging methods can benefit patient care in many areas. For example, I am already exploring how ultrasound can improve minimally-invasive robotic surgery, detection of cancer by detecting stiff lumps, and guidance of needle insertions for biopsy and anesthesia. All of these areas could benefit from the proposed research in the grant.""463945,""Rohrauer, Gregor"
"455383"	"Rosehart, William"	"Multi-objective optimization under uncertainty for electrical energy systems with renewable energy sources, load participation, and plug-in hybrid electric vehicles"	"Electrical Energy Systems are on the edge of a significant revolution in their operation and planning.  Great interest is being directed towards increased integration of renewable or ""clean"" energy sources such as wind- and solar-based generation, and they are expected to be a substantial component of future electrical power systems.  Power system operators and planners must address many new challenges when faced with large integration of wind- and solar-powered generation in electrical grids because of the variability associated with their power output and geographical limitations where they can be located.  The challenges can lead to economic and stability problems if not properly addressed.The objectives of the proposed research program is to first develop a deeper understanding of relationships between economic cost, environment costs, and stability when incorporating stochastic elements such as wind and solar generation, and Plug-in Hybrid Electric Vehicles (PHEV).  Multi-objective algorithms, appropriate for power system planning and operation, given large penetrations of stochastic elements, such as wind and solar generation and PHEV will be developed.  The aim of the algorithms is to enable the development of tools that will maximize the benefit society can gain from Plug-in Hybrid Electric Vehicles and renewable energy sources such as solar and wind.""472984,""Rosei, Federico"
"463823"	"Roy, ChanchalKumar"	"Change, similarity and redundancy in software systems"	"Studies show that up to 80% of software development cost is spent on software maintenance. One aspect of software maintenance is dealing with duplicated code. Copy-and-paste of code sections with or without minor changes is a common software development practice, and over time large software systems develop many such ""software clones"". While such cloning is often intentional and can be useful in many ways, it can cause significant problems in the maintenance of large software systems because it increases the risk of update anomalies, increases program size and complexity, and contributes to high maintenance costs. Clone detection is thus an important problem in software maintenance research, and serves as the basis of a wide range of software engineering tasks. In the last several years, we have significantly advanced the state-of-the-art in the detection and analysis of near-miss software clones, those where minor additions, deletions and modifications have been made in the copied fragments. In this proposal, we will continue to focus on the analysis and management of software clones, both during software maintenance and during original software development. In addition, we will extend our research to related aspects of software maintenance and evolution such as program comprehension, semantic differencing, migration from legacy systems to services, and objective comparison of software tools. We will validate our work using studies on large scale open source systems as well as in the industrial context at VendAsta Technologies Inc. of Saskatoon. This research is aimed at the rapid evolution of software systems in response to business and technological change. Using such automated techniques, software evolution and adaptation can be made more cost effective, more timely and less error prone, helping the Canadian software industry be more competitive and better able to quickly react to changing market demands while addressing critical software quality issues. ""466414,""Roy, Christian"
"451892"	"Rudie, Karen"	"A framework for decentralized, dynamic discrete-event systems"	"My research focuses on systems whose behaviour can be thought of as sequences of events or actions, which occur with various probabilities under timing constraints. I develop mathematical models to represent such discrete-event systems in which control involves multiple agents in order to automate problem-solving and testing of solutions. I will use my models to develop public emergency response procedures and to improve software development for distributed systems. In emergency response protocols, such as those needed for an epidemic, we need to model systems with many agents (e.g., patients, care workers, health units), where events occur with probabilities (e.g., if exposed to a disease, a patient may have a 40% chance of contracting the disease), where actions sometimes need to occur at specific times (e.g., patients in a hospital must eat at specific hours), and the group of agents changes over time (as patients recover or die and new patients become ill). My research will aid in determining protocols, procedures and policies for public emergencies. Improved emergency response protocols will benefit Canada by optimizing resource allocation and minimizing the harm done to people by crises such as disease outbreaks, water contamination, and massive electrical outages. Software engineering problems share the same types of features as emergency response problems: there are multiple agents (concurrent tasks, called threads), they change over time (because some threads terminate and others are created), and the system needs to be controlled so that undesirable sequences of events are prevented from occurring. My framework for discrete-event systems will help automate the software development process to guarantee solutions that are correct by construction. Since software is at the heart of the technical infrastructure in modern life (e.g., systems that run Canadian banks and hospital equipment, security systems), a method that ensures correctness will provide safer, more reliable systems for the many computer programs that we rely on.""472515,""Rudner, AdamDaniel"
"455170"	"Sadaoui, Samira"	"Trust management and matchmaking system for multi-attribute reverse auctions"	"Online auction frauds, representing the largest part of all Internet frauds, are on the rise because auctions are vulnerable to opportunistic behaviors. Thus, auctions are considered trust-critical systems. Trust is difficult to establish because transactions occur among complete strangers. Nowadays auctions are facing serious frauds including: bid shilling, sniping and shielding, auctioneer cheating, auctioneer-bidder cheating and lack of privacy of submitted bids especially the losing ones. Shilling and auctioneer cheating are the hardest types of frauds to detect. These problems, reported in existing auction houses, make auctions really unfair and unreliable. To increase trust in auctions and overcome the problems listed above, we want to develop a trustworthy online auction house for multi-attribute reverse auctions, i.e. in the context of competing sellers and multiple attributes to describe the products being auctioned. All existing work on trust management for auctions concerns forward auctions, i.e. with competing buyers, and price-only auctions which are simple and cannot be used in many real-world situations. Our trustworthy multi-attribute reverse auction house is composed of the following components:(1) a new mechanism for Internet auctions that copes with sniping, shielding, auctioneer cheating and lack of privacy problems; (2) a real-time fraud detection system that detects at run time two cheating activities: shilling and outbidding; (3) a real-time trust evaluation system that dynamically computes the trust scores of sellers; (4) a framework that evaluates the fairness of our auction protocol, and (5) a matchmaking system that searches for the best sellers' products that match the buyer's request. The matched sellers  who have acceptable trust values are then invited to the auction. We will utilize game theory to model the rules of our negotiation protocol and verify desirable theoretical properties,  formal methods to be able to realize the activities of components (2) and (4), and multi-agent technology to design, implement and simulate each component of our auction house.""459748,""Sadat, Fatiha"
"456832"	"Sahinalp, Cenk"	"Bioinformatics algorithms- from sequence to structure and function"	"This proposal aims to address in part a number of fundamental computational challenges we will be facing in molecular biology, biochemistry and genetics in the coming few years. In particular we propose to develop novel algorithms and data structures for making high throughput sequencing technologies cost effective and useful for detecting variations, in particular structural differences between individual genomes and transcriptomes to map human genetic variation and to understand genomic orgins of disease.We also we will aim to develop novel algorithms for biomolecular network analysis, RNA structure and interaction prediction and small molecule analysis, going beyond the QSAR methods we have been working on. Our approach will be based on rigorous mathematical modeling of the problems which will then be tackled through algorithms with performance and correctness guarantees. The techniques we will use are typically based on approximation algorithms, randomized algorithms, combinatorial optimization, data structures and computational learning theory.""453749,""Sahinalp, Cenk"
"456280"	"Salah, Aziz"	"Service management system: protocol similarity search"	"This proposition program covers and focuses manly on protocol search and queries.Current tools support search based on the service interface which is adequate for searching stateless services. However, the interface is absolutely not enough to be representative for stateful services which are characterized by their behavioral models. Conceptually, the behavioral model of stateful service is a protocol that can be expressed in formalisms such as which finite state machines, petri nets, activity diagrams, $pi$-calculus, etc.Given a protocol, some researchers have already investigated how to find out services that exactly match that protocol with respect to a kind of bisimulation relation. Such relation is very strong and restrictive. Consequently, there is very little chance that a match occurs. Such data (which are protocols), like images and videos, cannot be represented in canonical format that is   meaningfully searchable on the basis of database queries seeking an exact matches. To overcome this limitation, we propose to investigate how similarity metrics can be computed for each service protocol of the registry according to a given protocol of the query. A designer would be invited to closely consider the highly similar protocols, which are the result of the query, as they may fulfill his needs. In the case of protocol search, it would be very improbable that an exact match occurs while the registry may contain relevant protocol for the designer's needs. Avoiding the situation of miss, means more cost and effort savings, and more business opportunity and partnership in the context of SOA. The preliminary study reveals that this proposal will fill a gap at least in service discovery support.  Even though this proposal takes SOA as an application domain, its fundamental results are general and applicable whenever a protocol similarity search is relevant such as in component based approaches. ""463787,""Salahpour, Ali"
"454798"	"Sazonov, Andrei"	"Multifunctional OLED displays with low power consumption"	"The objective of the proposed research program is to drastically reduce the power consumption of flat panel displays used in portable electronics and TV sets. We propose achieving this goal by developing novel organic light emitting diode (OLED) displays with three major innovations:i) transparent (""see-through"") thin film transistor backplane that can be fabricated at low temperatures on glass or plastic substrates; ii) integration of this backplane with thin film solar cells and touch sensors on the same panel; iii) development of novel backplane pixel architecture with light guiding structures to direct the light towards the eye and towards the solar cell. This will allow capturing the light from the ambient and generate the power for the standby mode operation, and capturing the light emitted by OLED and otherwise being lost. These innovation will lead to displays with reduced power consumption and hence smaller carbon footprint of flat panel displays used in portable electronic devices (personal communicators, laptop computers) and in TV screens, and eventually lead to novel products.""471075,""ScaffidiArgentina, Sarina"
"458731"	"Schost, Eric"	"Efficient algorithms in computer algebra and their applications"	"My field of research is computer algebra: very broadly, this stands for the study of mathematical  problems that can be solved on a computer in an exact manner. Many problems in the academy or in industry require such solutions: important fields of applications are robotics (to plan the motion of robots) and cryptography (to design cryptographic protocols). However, some of these problems remain out of the reach of even the best software.The objectives of this proposal are the development of efficient algorithms and implementations to tackle such problems.""453381,""Schost, Eric"
"454678"	"Sedig, Kamran"	"Design of cognitive tools for everyday information-based activities"	"We live in a knowledge society. We are heavily dependent on information in many spheres of our activities. Most of our everyday activities involve both the mind and information. We are awash in a tsunami of information. Simultaneously, we are expected to carry out our activities more efficiently than ever before. To cope, we need tools. Cognitive tools (CTs) can help. These software tools sit at the interface between our mind and a digital information space and mediate the interaction between the two. Examples of CTs include interactive mathematical visualization software, decision-support systems, and digital libraries.Researchers need to know how to design CTs so that these tools can effectively mediate our information-based activities. Currently, researchers do not have enough knowledge about how to design the human-information interaction component of CTs to support a variety of knowledge-oriented activities. In order to have scientific knowledge in this area, researchers need to develop design frameworks, build different kinds of tools for different activities, and conduct empirical studies to assess the effectiveness of different design strategies. My research proposal is to do the aforementioned research with regard to three kinds of cognitive tools: digital libraries, digital games, and tools for exploration of mathematical ideas. This research can significantly contribute to our understanding of how to design CTs. Well-designed CTs can transform and scale our knowledge-oriented productive activities, not unlike the revolutionary effect that physical tools and machinery have had on our material productive activities. As Internet-based CTs improve, these tools can virtually affect the knowledge activities of tens of millions of people throughout the world. In short, this research has wide-reaching applications in the support of our everyday information-based activities.""468086,""Sediqzadah, Saadia"
"458004"	"Selvaganapathy, Ponnambalam"	"Microsystems for drug discovery and biological studies"	"Microfabricated Lab-on-Chip devices for genomics and proteomics have been widely researched and developed over the past decade and have gained significant acceptance among biological and biomedical researchers. The next grand challenge is the miniaturization of laboratory automation technologies for handling and manipulating living biological cells, embryos and small organisms that would enable faster analysis, lower cost, greater precision and parallelization (high throughput) over existing methods. These devices are essential in many biological studies and procedures such as embryology, in-vitro fertilization, transgenics, stem cell engineering/ transformation and in toxicology. They are also critically important in the drug discovery process where increasingly, small model organisms such as Zebrafish and C.elegans are being used for determining potent drug candidates from millions of molecules in the chemical compound libraries. Our laboratory has recently emerged at the forefront of this nascent field due to the development of innovative Lab-on-Chip devices, one for the microinjection of Zebrafish embryos and another for the electrical micromanipulation of C.elegans worms. Building on our recent successes, the short-term aim of our research program, is to investigate and understand the fundamental science behind interaction of mechanical and electric forces with biological living organisms and use it to design innovative Lab-on-Chip devices. Additional aims include developing hybrid micro and nanofabrication technologies for their manufacture and to investigate electrically controlled micro and nanofluidic technologies for controlling the transport, positioning and injection into these organisms. Over the long term, we will develop integrated microsystems that are capable of performing specific bioanalysis on cells, embryos or worms in a low-cost, high-throughput and accurate manner and customize it towards specific applications in drug discovery or in other biological studies. By lowering the cost of research and enhancing capability and speed, these systems can significantly enhance discovery of new drugs and understanding of biological processes.""451083,""Selvaraj, Gopalan"
"450150"	"Shafai, Lotfollah"	"Applied electromagnetics, advanced antennas and simulated materials"	"The broad objectives of this research are to develop accurate mathematical models for electromagnetics (EM) interactions with complex media, and to investigate their solutions, in order to understand their properties and to use the acquired information for design of novel sensors and antennas, and real and artificial materials that can improve the performance of current designs and enable the design of the next generation EM hardware with superior performance. A few specific areas will receive more and detailed attention. To improve the performance of phased array antennas, a new concept based on printed patch layers will be investigated and developed. This study will determine the important characteristics of this matching layer for eliminating the scan blindness, limitations in the phased arrays and improving their performance. A new hardware design concept will be further studied and implemented. This concept formalizes the traditional intuitive engineering design rules and cast them into mathematical molds, thereby allowing performance studies on the basis of mathematical principles, to generate more optimal solutions. Significant research will be devoted to electromagnetic material and device design. Using electromagnetic principles new and artificial materials with novel properties will be tailored, using common materials, and studied for their properties and hardware design. They will be used to design the next generation hardware, especially ultra wideband and multiband antennas, sensors and microwave components.Major research will also be devoted to electromagnetic modeling of arctic sea ice and effects caused by climatic change. A significant portion of this research will involve physical and electromagnetic properties of the ice, using data recorded on board the icebreaker Amundsen, by different radars and ice samples. This research will generate accurate electromagnetic models for the arctic sea ice and help in realistic prediction of the ice conditions that are important for navigation and environmental studies.""453669,""Shafai, Lotfollah"
"455312"	"Shakshuki, Elhadi"	"Cooperative intelligent distributed systems for context-aware and calendaring system"	"This research program explores several issues introduced by my most recent research on the use of cooperative intelligent agents for (1) integrating agents with wireless sensors networks (WSN) and radio frequency identification (RFID) to enhance context-aware services, and (2) calendaring management systems. 1) Enhanced context-aware services with agents, WSN and RFIDWSNs are increasingly seen as a solution to the problem of performing wide-area monitoring and surveillance within many environments. RFID solutions have become essential in monitoring and tracking large volume applications. Intelligent agents can autonomously acquire data from these networks, and perform information processing tasks such as fusion, inference and prediction. Our aim is to investigate how to simplify and enable on-demand access to RFID stored information and integrate it with the information sensed about the environment to enable identity and context-aware applications, and to enhance WSN efficient algorithms using RFID. This research will lead to new, efficient and effective context-aware service techniques. The outcome of this research will advance techniques and approaches of agents with WSNs and RFIDs, and provide enhanced methods for many applications, such as improving the accuracy of health records and to improve the accuracy and efficiency of location tracking and condition monitoring of products.2) Intelligent calendaring management system Despite the widespread adoption of calendar or schedule management software systems, they still require significant user input, especially when users are attempting to schedule group meetings. One of the major challenges associated with scheduling is to resolve conflicts. Our goal is to significantly enhance the calendaring experience by developing a probabilistic interval algebra multi-agent system approach using agents with several efficient negotiation techniques.""453601,""Shalaby, Ahmed"
"458802"	"Shepherd, Bruce"	"Polyhedral methods for optimization and algorithm design"	"The PI's  research is in the broad area of discrete optimization.Three focus areas are as follows. The algorithmic theory ofnetwork routing and design. This area has historically given birthto key problems and techniques in discrete optimization. Inaddition, modern technologies have continually  inspired newfundamental algorithmic problems in the area. Key areas of the PI's researchrobust optimization, constrained network flows, and disjointpaths.The PI also studies the general connections between  polyhedral combinatorics and convex geometry to approximation algorithms for combinatorial optimization.Finally, much of the PI's past work has aimed to develop some theory for the interdomain routing protocol (BGP) used by the Internet. ""469068,""Shepherd, Gena"
"455686"	"ShiriVarnaamkhaasti, Nematollaah"	"High performance time series data analysis and pattern discovery"	"This research is motivated by the increasing demand for high performance time series data management and analysis techniques. A time series is a sequence of values of a variable measured often at regular time intervals, e.g., yearly, monthly, weekly, daily, and hourly. Advances in technology in development of automatic sensors and devices for data collection and storage have provided us with huge amount of time series that need to be processed to extract the real benefits buried in the data. Time series appear in a wide range of numerous applications in finance, meteorology, telecommunication and wireless networks, security, etc., in which the data is analyzed and searched for similarity matching, correlation queries, and pattern discovery.  Two major challenges in management of single and multiple, large scale time series in such applications include modeling and processing, where modeling aims at describing the data and providing insights into its generation mechanisms while processing aims at prediction and forecasting. Time series data has two important characteristics, namely high dimensional and temporal, that affect the performance, explained as follows. Each time point of a time series can be viewed as a dimension, and hence a time series of n values is a point in an n-dimensional space. Processing points in a high dimensional space is extremely difficult, noting that n is large in practice. The temporal property of time series implies that consecutive and near by data values are similar to each other within some predictable threshold. This implies possibility of redundancy and hence opportunity for reduction in dimension and space. This primarily provides storage efficiency, however, its real benefit is time reduction.  Storage and time efficient techniques are crucial in success of time series applications. The literature is vast and rich on such techniques for handling Gigabyte or close to Terabyte time series. This research attempts to raise the bar even higher to surpass handling Terabyte data towards Petascale time series. The theoretical and practical developments in this research will advance the state-of-the-art and technology for high performance time series data management. ""471068,""Shirley, Ben"
"449398"	"Shoja, Gholamali"	"Leveraging social network theories to address data delivery issues in mobile ad hoc networks"	"We propose to investigate the problem of cross layer design of communication protocols that can reliably expedite end-to-end delivery of data in a mobile social network environment. Since the performance of such protocols is closely related to users' social relationships and movement patterns, our first task will be to develop a realistic mobility model for human interactions in a social networking context. As it turns out, previously proposed models are generally based on random walk mobility models which are contrary to human behavior, and as such their validity is suspect. To address this problem, we intend to leverage well known social networking concepts and previous work in modeling of human mobility to come up with a realistic model. We will test the validity of our model by using empirical data from large scale studies of human mobility patterns such as MIT's Reality Mining project. Next, we will proceed to design and develop a cross layer communication protocol where a novel social aspects layer interacts with all 7 layers of the standard OSI protocol stack in order to facilitate data delivery in a delay tolerant network. Our results will enable end-to-end delivery of data in a mobile social network environment where continuous end-to-end connectivity cannot be guaranteed. In the longer term, we propose to extend our investigations into social networking problems in pervasive computing settings. We intend to research the problem of social training of intelligent mobile devices carried by humans. We call these devices sociable guardians and we believe them to have important practical applications in low cost continuous medical monitoring, search and rescue operations and communications in hostile environments.""470701,""ShojaeiBaghini, Ehsan"
"463663"	"Song, Wei"	"Ubiquitous multimedia service provisioning over heterogeneous wireless networks in a diverse mobility landscape"	"This proposed research will investigate the delivery of high-quality multimedia services over wireless networks in a diverse mobility landscape. The service delivery will be enabled ubiquitous (i.e., anywhere and anytime) by using diverse wireless technologies. In recent years, wireless networks have been proliferating and offering a variety of access choices. Next-generation wireless networks are envisioned to be heterogeneous, integrating multiple broadband wireless accesses and providing ""always best connectivity."" Although there are some initial efforts by cellular carriers to provide multimedia services, they are still embryonic and far from profitable. In this project, we will develop a spectrum of techniques to facilitate multimedia communications over heterogeneous wireless networks. It will effectively address emerging new challenges in hostile wireless environments such as high mobility and error-prone transmissions. The limited network bandwidth will be intelligently allocated to mobile users to enable cost-effective multimedia services with a satisfactory quality.The novelty and significance of this research lie in the following areas. First, expanding previous research focus on static deployment of indoor hotspots, this project will also consider heterogeneous interworking for mobile hotspots, taking into account circumstances of high mobility, e.g., in an earth-bound vehicle or an airborne flight cabin. Second, advanced methodologies such as cross-layer design and reinforcement learning will be used for multimedia service provisioning. Third, cost-effective implementations via distributed cooperative relay will be investigated to facilitate commercialization of new technologies. Distributed control decentralizes control intelligence to user terminals and thereby greatly reduces the deployment cost. Following the above methodologies, we will develop innovative technologies to provide economical and high-quality multimedia services to mobile users. There will be a significant potential to spawn new revenue sources for the Canadian telecommunications industry and foster its sustainable competitiveness in the global market place.""457997,""Song, Yang"
"450241"	"Stewart, Neil"	"Robustness of subdivision-surface methods"	"The aerospace and automotive industries in North America rely heavily on solid modelling systems, i.e., computer-aided design (CAD) systems. The mathematical models used in these systems, to represent objects, are prone to error. This is true for at least three reasons: 1. because the curves representing the intersections (joining) of two objects must be approximate, due to their very high degree (complexity) 2. because computers can only do arithmetic approximately, and 3. because different systems use different and slightly incompatible methods to represent the modelled objects. The errors often appear when a model is transferred from one computer system to another. When errors appear, a human must intervene to correct them, and this causes costs to rise very rapidly. The economic costs of errors in solid-modelling systems, including the transfer of models between systems, has been estimated by US government agency studies to be over one hundred million dollars per year in the automotive industry alone.The goal of this research project is to establish rigorous mathematical theorems to guarantee the validity of results produced when doing operations on solids based on subdivision surfaces. Subdivision surfaces are a representational method that is quickly becoming dominant in the film-animation and computer-game industries, and which will certainly become more common in the industries mentioned above. The intuitive idea behind these representations is very simple: we start with an approximate mesh, and then repeatedly refine it and smooth it, and apply various operations so that the mesh takes the shape we want. It turns out, however, that to guarantee reliable performance of the operations, fairly difficult mathematical theory is required. In this research we will try to develop this theory.""465811,""Stewart, Robyn"
"450851"	"Stojmenovic, Ivan"	"Data communication in wireless ad hoc, sensor and vehicular networks"	"Ad hoc networks are multi-hop wireless networks consisting of wireless autonomous hosts, where each host may serve as a router to assists traffic from other nodes. The applications of sensor networks are envisioned primarily for monitoring the environment (e.g. motion and fire detection, chemicals, temperature) or as embedded systems. In vehicular ad hoc networks (VANETs), vehicles are equipped with wireless devices for communication to other vehicles and to road side units. VANETs allow the vehicles to become integrated within an Intelligent Transportation System (ITS). This research plan concentrates on the network layer of ad hoc, sensor, actuator, robot, and vehicular  networks, and applies a cross layering design, considering realistic physical and medium access protocols and power efficiency. In data communication problems, such as routing, geocasting, multicasting, and broadcasting (sending a message from a source node to one destination node, all nodes in a region, or several destination nodes, respectively), the primary goal is to fulfill a given communication task successfully between nodes in ad hoc network. The secondary task is to minimize the communication overhead (since bandwidth in wireless communication is typically limited) and power consumption by battery operated nodes. There are also a variety of data dissemination, data gathering, data aggregation, robot assisted or self placement and relocation issues in sensor networks. In VANETs, geocasting is used for warning dissemination while routing provides location based services. High variability of mobility speeds, and intermittent connectivity pose unique and new challenges for the design of data communication protocols in VANETs. Localized algorithms, where nodes make decisions based on local knowledge,  will provide scalability, which is not achieved with standard centralized or distributed solutions. Beaconless algorithms do not require neighbor knowledge and address node mobility. Simplicity is also our important criterion. Parallel advance of useful modeling, from unit disk graphs to realistic physical layers, provides tractability and approaching application level for designed protocols.""470915,""Stojmenovic, Milica"
"463969"	"Sundaram, Shreyas"	"A holistic approach to security and reliability in networked control systems"	"The world around us is replete with complex networks (both natural and engineered) that arise as a result of local interactions between various agents; examples include gene networks, social networks and the internet.  In the near future, networks will form the basis for an extraordinarily large number of life- and mission-critical applications, ranging from transmitting patient diagnostic data in hospitals via multi-hop wireless networks, to coordinating teams of autonomous aircraft for search and rescue operations.  Disruptions in such applications (either by intent or by accident) could have dire consequences, and thus every effort must be made to ensure that the overall system is resilient to components that behave in malicious or unpredictable ways.  Networked control systems are a particular class of distributed systems that contain physical processes and digital controllers that interact with each other over communication networks.  The task of the controllers is to ensure that the physical processes behave as they are supposed to by sending signals to them via the communication networks.  Such architectures are a natural fit for the control of critical infrastructures (such as the electric power grid), where serious physical or economic damage can result if the processes are not regulated correctly.  However, designing secure and reliable networked control systems necessarily requires a holistic approach due to the complex interplay of control, communications and computation.  In light of this challenge, the objective of the proposed research is to bring together perspectives from various disciplines to study and design secure networked control systems.   The results of the research will potentially lead to greater reliability, security and efficiency in the control of large-scale critical systems.""463261,""Sundararaj, Uttandaraman"
"450855"	"Syrzycki, Marek"	"Analog and mixed-signal circuits for semiconductor detector microsystems"	"The latest advances in metallurgy of compound semiconductors and also in micron-scale photolithography enable fabrication of pixelated semiconductor radiation detectors with a capability to make the high-resolution nuclear imaging feasible. Intelligent sensor interface electronics, able to detect the particle energy and the 3-D position of the interaction event, must connect the semiconductor radiation detector with the digital world. The required functionality of front-end pixel electronics is exacerbated by a large number of the parallel signal channels, each one associated with one pixel of the detector. While the number of pixels currently reaches the range of a few hundred, future applications requiring high-resolution imaging will need to accommodate thousands of channels on chip. The proposed research program focuses on the design and manufacture of sensor interface electronics for semiconductor detector microsystems. This will comprise of the analog and mixed-signal processing elements, featuring low noise performance, extra-low power consumption per channel, resiliency to high-voltage environment, and ability to integrate a few thousand of signal processing channels within a single microsystem. Low-voltage and low-power CMOS integrated circuits for pulse-mode pixelated radiation detectors will be built to provide a time and space pulse resolution capability necessary for high-resolution gamma-ray imaging. Our planned research efforts focus on developing and testing new analog and mixed-signal CMOS circuits leading to the large-scale solution that would be hybrid integrated with pixelated gamma-radiation detectors. The resulting improvements in CMOS microsystems are expected to facilitate future commercial implementations in X-ray and gamma-ray imaging systems for applications in biology, nuclear medical imaging, and for homeland security. The research approach will be intensively used for training highly qualified researchers.""472055,""Syslak, AnneMarie"
"452662"	"Tahar, Sofiène"	"Formal verification of physical systems and devices"	"Physical systems and devices are increasingly being used in safety-critical domains, such as electronic medicine equipment and automated transportation. The verification of such systems has predominantly been accomplished by analytical techniques or simulation testing. However, as engineering systems are getting more complex the confidence level in such traditional verification techniques is rapidly decreasing. These limitations can, however, be overcome by using formal methods for the modeling and validation of physical systems through deductive reasoning. In this research program, we aim in particular at using higher-order logic based theorem proving to formally analyze and verify physical systems. Higher-order logic is a system of deduction with a precise semantics and is expressive enough to be used for the specification of almost all classical mathematics theories. Theorem proving is the field of computer science and mathematical logic concerned with precise computer based formal proof tools that require some sort of human assistance. In the proposed research, we are interested in analyzing the probabilistic and statistical behavior of systems by formalizing in higher-order logic queuing and information theory fundamentals. Immediate applications include the analysis of telecommunications systems performance, roundoff errors of arithmetic computing or error coding in digital media. We also plan to formalize specific mathematical theories widely used in the target domains of optics and aeronautics to be able to reason about properties of optical interconnects and flight control stability, respectively. Finally, we aim at investigation prospects of using numerical analysis with automated theorem proving for checking the reliability of nanoelectronics circuits. We believe that our approaches will advance the state-of-the-art in systems specification and verification, thus significantly enhancing the confidence level in the correctness of safety critical products. The direct beneficiary of this research will be the Canadian telecommunications, optics, microelectronics and aeronautics industry. Furthermore, this proposal will contribute towards the training of a number of skilled personnel available to Canadian industry and academia.""472251,""Taheri, Farid"
"454771"	"Talbi, Larbi"	"Les systèmes de communication sans fil intra-immeuble EHF"	"Notre programme de recherche couvre deux axes importants pour la technologie qui opérera à 60GHz dans un avenir très proche, la caractérisation du canal de propagation et le développement de réseaux d'antennes innovatrices. Pour le premier, nous aurons à construire un système de mesure capable de sonder la réponse impuslionnelle. Les résultats obtenus (gradient des pertes, la vitesse des évanouissements, l'étalement temporel des délais de propagation et la bande de cohérence) seront cruciales pour le designer de cette technologie. Ils lui permettront de connaître les limites de cette bande de fréquence en termes de la couverture du signal et de la capacité du canal. Le deuxième axe concerne les réseaux d'antennes. En effet, nous avons développé un intérêt soutenu pour l'élaboration de techniques moins coûteuses, à l'instar des antennes intelligentes, pour combattre le problème de l'évanouissement multitrajet par l'introduction de structures innovatrices d'antennes. Présentement notre intérêt se dirige vers deux types de technologies pour le réseau d'antennes, réseau d'antennes imprimés conformes cylindriques et réseau d'antennes en GIS. Le premier semble être une solution prometteuse pour avoir, d'une part, une meilleure contrôlabilité du diagramme de rayonnement, et d'autre part, un gain élevé. Nous allons dans ce contexte utiliser comme éléments rayonnats résonateurs diélectriques DRAs dont les pertes diélectriques sont beaucoup moins prononcées. La conception sera effectuée à 60GHz. Le deuxième type de réseaux d'antennes est celui basé sur la technologie GIS utilisée pour concevoir plusieurs circuits microondes. Nous avons toute une expertise dans ce domaine et nous souhaiterons le mettre au service de la conception d'un autre réseau d'antennes complètement en GIS et à 60GHz. Ce programme constituera une grande opportunité pour la formation du personnel hautement qualifié dans un domaine de pointe qui est la technologie 60GHz.""470261,""Talbot, Alexandre"
"452689"	"Tawbi, Nadia"	"Security policy enforcement mechanisms"	"We have come to rely on computer systems in almost all our activities. Their performance and theirquality have a great impact on daily life, economy, health and even national interests. These systems are increasingly complex, highly connected and characterized by an extensive use of mobile code. Applications, even critical ones, involve untrusted off-the shelf components and can even download parts of their code during execution. In this context, security issues are a major concern and and effective solutions to  protect users are an urgent need. The most common security threats are unauthorized access to the system resources and information, corruption of information and denial of service. Protecting users against security breaches is one of the most challenging issues in Information technology. We want the protection mechanisms to be automated, reliable, robust but still flexible enough to allow authorized actions. Existing mechanisms offer poor protection. Virus detection tools can only protect against previouslyknown attacks, firewalls are either very restrictive or very permissive. Designing secure applications is  error prone. The literature contains many examples of applications in which flaws were only detected  after years of intensive use. The broad aim of my program is to  provide provably correct techniques and tools aimed atprotecting users exposed to executing untrusted applications. Progress along this line requires a constant interplay between practice and theory. Namely, this research program will address the following issues:- Extending the enforcement power of in-lined monitors to larger class of security policies based onan a priori knowledge of the program acquired by static analysis.- Characterizing security policies, in meaningful ways, in order to devise efficient enforcementmechanisms for particular classes.""472838,""Tawhid, Mohamed"
"451617"	"Thomson, Douglas"	"Mobile and miniaturized materials analysis"	"Miniaturized electronics and low cost wireless technology have dramatically influenced our lives in the past 50 years, enabling innovations such as the computer and ubiquitous telecommunications. Over the next few decades they will also enable another revolution in miniaturized and mobile materials analysis. Materials will no longed have to brought to laboratories for analysis. This grant will focus on electronic systems for miniaturized and mobile materials analysis, with particular emphasis on passive wireless systems and microwave frequency biomaterials analysis in microfluidics. Passive wireless sensors will be attached and implanted and allowing rapid onsite analysis. Miniaturized microwave frequency microfluidic biomaterial analysis systems will be rapidly deployable for portable diagnostics. These systems will have broad impact on society ranging from increased safety of civil infrastructure to personalized treatment of disease. Passive wireless sensors offer a compelling alternative to traditional wireless sensors in applications such as monitoring civil structures where the supply of power to the sensor is challenging. A focus of this grant will be extending the range and accuracy of passive wireless sensors and demonstrating new passive wireless sensing modalities. These sensors will address materials sensing in civil infrastructure by measuring corrosion, pH and strain. In power distribution the ability to measure temperature and strain on overhead transmission lines will lead to enhanced safety and increased capacity. Electrical properties of living systems are of rapidly expanding interest in medical diagnostics and biophysics. Merging this with micro?uidics offers a possibility of markerless analysis at the single-cell level, leading to applications such as miniaturized systems for blood counts including white cell differentiation. A focus of this grant will be to improve the sensitivity of a microfluidic microwave impedance detection scheme introduced by our group and to push this approach to its theoretical limits. This work will have application in systems ranging from personalized treatment to pathogen detection.""460098,""Thomson, Euan"
"450456"	"Tompa, Frank"	"Flexible, efficient text databases"	"Applications relying on text must be built upon principles of data organization similar to those that are fundamental to conventional database systems. For document repositories and other collections of structured text to be managed as databases, we must advance the theory and practice of text data structuring (including representation, language, and implementation strategies).   Thus one long-term objective is to support document storage and management by adapting and applying relational database experience to this domain of information. This objective includes designing and prototyping suitable document database systems. The challenge is to discover how the complexity of text, with its intricate structure and diversity of expression, can be efficiently managed. The second, complementary objective is to create systems that provide information fragments as virtual texts (""text views""). These text views can be assembled dynamically to meet various applications' needs, and they can be used as update portals into the database.   In the short term, aspects of these objectives will continue to be addressed through three themes: (1) design and implementation of improved algorithms for protecting text from unauthorized access and modification; (2) design and implementation of improved systems for information retrieval, applying practices used in database systems; and (3) design and implementation of text transformation systems for loading structured text databases, for integrating text databases, and for supporting materialized and unmaterialized views.   The continued worldwide interest in diverse applications of XML suggests that this research remains timely and crucial to an important economic growth area in Canada. Publishers, data providers (e.g., via the World Wide Web), organizations that rely on any form of text-dominated knowledge base for conducting their internal and external business, and software system providers will benefit from specific tools that arise from this research as well as from the theory, which will provide a framework for designing their databases.""457090,""Toms, Andrew"
"450359"	"Tourlakis, George"	"Calculational logic"	"Predicate logic is used widely in Computer Science to formalise and prove properties of programs, specify the contents of a database, or represent a body of knowledge. A particular ""implementation"" of predicate logic-calculational logic-relies on Leibniz's principle of ""replacing equals by equals"" that allows the user to prove assertions in the same way that one verifies the equality of two expressions in high school algebra or trigonometry, namely, by calculating a chain of equalities. However, when we reason formally within calculational logic we often need to break our chain of equalities and invoke a rule that will spawn a new chain of equalities, disjoint from the original. This happens, for example, when one chain ends with an assertion A, and the new chain starts with the generalisation of said assertion, i.e., ""for all x, A"". While the verification of an assertion is equivalent to that of its generalisation, there is no formal way to write this down as an equality in classical logical calculus. On the other hand, the similarly sounding equality ""A = for all x, A"", that we can state within calculational logic, is invalid! Users of logic often stumble on this and related ""disconnecting"" steps in reasoning. Thus, one motivation for our research is methodological, to provide a more reliable framework for proving theorems-a kind of good engineering practise in logic. Another motivation relates to our choice of specific methodology (modal predicate logic), and is to contribute to the understanding and development of this fairly new area in logic, which unlike its well studied propositional counterpart is still under construction. The fundamental approach in our research has been to augment the expressive power of classical logic by adding the modal operator, so that we can now write a formal equality that says ""the verification of an assertion is equivalent to that of its generalisation"". Our research is also concerned with practical issues: We are carefully designing a ""practical"" calculational logic that will allow its end-user such as a student of computer science to reliably write correct, well annotated calculational proofs. This methodology may also find application in the areas of program verification and automated reasoning.""459048,""Tousignant, Cody"
"450148"	"Turksen, Burhan"	"Computing with words for decision making under uncertainty"	"Computing With Words for Decision Making Under Uncertainty (CWWDM) proposal is aimed at the development of computationally effective methods of decision-making under uncertainty. Primary application areas are: conception and design of autonomous systems for natural language processing applications specifically on human oriented information analysis, information extraction from unstructured text, etc.Decision-making under uncertainty is a mature field. Currently, the methods are based on standard probability theory and bivalent logic. The problem is that in the employed models, it is assumed that the underlying probabilities, gains and losses are known precisely. However, real world probabilities are generally based on perceptions, which are intrinsically imprecise. They reflect the bounded ability of human sensory organs, and ultimately the brain, to resolve detail and store information. Imprecision of perceptions is passed on to probabilities. Standard probability theory does not offer effective techniques for computation with imprecise probabilities. Proposed research is aimed at the development of new approaches based on fuzzy logic. In these approaches, a formalism which plays a pivotal role, is that of Computing with Words (CWW), which is fuzzy-logic-based. In CWW, the objects of computation are propositions drawn from a natural language. A basic assumption in the proposed research is that imprecise probabilities are described in a natural language. This assumption opens the door to application of CWW to decision-making under uncertainty.CWWDM is an important resource for Canada. It enriches the national research enterprise. By proactively partnering with scientists in related disciplines, CWWDM addresses problems of critical importance to Canada.""458738,""Turley, Eva"
"464750"	"Tweed, Douglas"	"Algorithms for adaptive near-optimal control"	"In almost every field of science or engineering there are processes we would like to control in an optimal way, e.g. maximizing speed or accuracy or fuel efficiency. But optimal control is computationally so demanding that it is out of reach except for simple tasks. The next best thing may be near-optimal control, where we compute a sequence of better and better controllers, moving ever closer to the optimal one. There are many algorithms for this purpose, but the most efficient and versatile is probably the method of generalized Hamilton-Jacobi-Bellman (GHJB) equations. Here I show that this method is in an important sense indirect, and can be improved by using a more direct form of supervised learning. The GHJB method is based on the fact that if we have a feedback controller, and we learn to compute the gradient grad-J of its cost-to-go function, then we can use that gradient to define a better controller. We can then use the new controller's grad-J to define a still-better controller, and so on. But GHJB works indirectly in the sense that it doesn't learn the best approximation to grad-J but instead learns a related function and from that infers a suboptimal estimate of grad-J. I show how it is possible to learn the gradient directly; e.g. we need signals that report grad-J(x) for different states x of the controlled process, and I show how to obtain them using a formula similar to the Euler-Lagrange equation. I compare this direct algorithm with GHJB on test problems from recent control papers, and I show that the direct method yields controllers that are more nearly optimal and simpler, requiring (on one complex task) 10 times fewer function evaluations and adjustable parameters. But much more testing is needed, and there is a great deal of work to be done improving and extending this approach.""465647,""Twieg, Brendan"
"457556"	"Vaidyanathan, Mani"	"Understanding  nanoscale elecctronic devices for future technologies"	"For the past 40 years, silicon transistor technology has driven the electronics industry, with continually decreasing transistor size and continally increasing signal-processing power.  This trend, well-known as ""Moore's law,"" has primarily been enabled by our ability to ""scale down"" the size of the conventional metal-oxide-semiconductor (MOS) transistor.  However, as the critical dimensions of conventional transistors move into the nanometer regime, and as fundamental limits on scaling are approached, industry and university research in electronic devices has shifted to examining alternatives.  New candidates being considered to augment or even replace silicon transistors include those made from carbon nanotubes and carbon nanoribbons, those utilizing compound III-V semiconductors, and those exploiting the property of electronic spin.A critical role in providing a roadmap for future technologies that might utilize any of the above (or other) devices is modeling and simulation, especially in cases where direct experimental data for comparison to silicon is difficult or presently impossible to obtain.The purpose of the proposed research program is hence twofold:  (1) to use modeling and simulation of emerging transistors to PROVIDE UNDERSTANDING that will help identify the best possible paths for future electronics; (2) to TRAIN highly qualified personnel (Ph.D. and M.Sc. students) in state-of-the-art modeling and simulation techniques required for emerging and future electronics.  The general field of research into which this program fits is known as ""nanoelectronics,"" and was identified by NSERC several years ago as ""an emerging area of Canadian and international importance.""   The knowledge and training from this research program will help Canada actively participate in the continuing evolution of electronics into the nanoscale.""465897,""Vail, Kathleen"
"461511"	"Valizadeh, Pouya"	"Investigation of strain-engineering techniques for modification of the characteristics of FETs in polar III-nitride material system and analytical modeling of device peculiarities"	"During the past decade, polar AlGaN/GaN heterostructure field effect transistors (HFETs) have been the focus of the device research community as alluring candidates for high power, high voltage microwave applications. High temperature stability and large breakdown voltage of wide-bandgap AlxGa1-xN material system, promote the applicability of AlGaN/GaN HFETs as vital candidates for switching applications under high-voltage/high-temperature conditions. In light of the growing demand for reliability improvement of the sensory equipment and surge in popularity of electric vehicles, power-prudent solid state electronic circuits capable of operating at temperature and voltage ranges beyond those offered by silicon technology are in demand. To the best of the PI's knowledge, a systematic investigation of the application of wide-bandgap polar III-Nitride material system to switching-mode electronics from the standpoints of process-development requirements for offering normally-off (zero current at zero gate voltage) HEFTs and challenges of reproducible fabrication of normally-off/normally-on (nonzero current at zero gate voltage) pair has not been performed. Results of this Discovery Grant proposal will lead to improved methods for analysis, design, and evaluation of solid state electronic devices and circuits in polar material systems for the efficient exploitation of the material properties of this relatively new family of electronic materials. Moreover, analytical models will be developed for incorporation of drain-current collapse and excessive gate leakage of AlGaN/GaN HFETs.""467508,""Vallée, Réal"
"452093"	"VandePanne, Michiel"	"Scalable physics-based modeling of skilled movement"	"Humans and animals move through their world and interact with their environment in robust, graceful, and highly skilled ways. We do not yet know how to design realistic animated characters, biomechanical human simulations, and robots that can mimic this degree of skill. However, recent developments suggest that significant progess is being made towards mimicing human-like dexterity in simulations, or, in some cases, with robots.  In this proposal, we use robust simulations of bipedal walking as a point of departure for developing richer movement skill sets, including highly agile walking and running, natural movement through urban environments, and dexterous hand manipulation.  We target next generation of computer animation software as our primary application, but we expect that the same models will also be useful for understanding and correcting pathological gaits, the design of walking prostheses, obtaining a deeper understanding of human motor control, and creating more skillful robots.""453221,""vandePanne, Michiel"
"456788"	"Vaughan, Richard"	"Energy efficient large scale multi-robot systems"	"The logistics of energy supply will dominate the practicality of any robot system at sufficiently large scale. Therefore, improving the efficiency with which energy can be supplied and used could have a significant impact on the size and nature of tasks achieved by robots. To explore this idea, I propose to investigate methods for maintaining complete mobile robot systems with populations of up to 100,000 - three orders of magnitude larger than typical in the field.  The experimental domain is autonomous mobile robot foraging and transporation: a model for many real-world applications. The key problems to be studied are (i) how to effectively distribute energy to individual robots; and (ii) how the robots can use their stored energy efficiently. The proposed approach has three complementary threads: (i) Recharging strategies (macro-level): seeking optimal strategies for individual robots to decide when to switch between working and recharging so that energy use of the whole population can be traded off with performance. [Time scale: minutes to days.](ii) Reducing spatial interference (micro-level): designing scalable strategies for improving the performance and energy efficiency of navigation by reducing local spatial interference between robots. [Time scale: milliseconds to minutes.](iii) Tools (meta-level): developing the necessary simulation, interfacing, analysis and debugging infrastructure for experiments with 100,000 simulated robots and tens of real-world robots.""456526,""Vaughan, Rodney"
"455178"	"Wainer, Gabriel"	"Methodologies for real-time discrete-event simulation"	"Real-time systems (RTS) are very advanced computer system applications with hardware and software components interacting in a tight fashion. The manual development of this kind of systems is time consuming, error prone, and expensive. Different formal methods have been proposed for RTS development, but they have shown to be difficult to apply as the complexity of the system scales up. Modeling and Simulation (M&S) techniques and tools have been proposed to address this issue and have been proven to be useful to verify and validate RTS, including the environment they interact with. We want to investigate systematic methods and automated tools to develop RTS using a model-based methodology. We are interested in defining new theories, methods and techniques for building simulation models that can later be applied in real-time environments, and in systems integrating complex physical systems with RTS, combining formal modelling techniques and simulation. Although several efforts exist in this area, many questions remain open. The main issue is that none of the proposed solutions has considered problems of transient overloading, checking of timing constraints, and fault tolerance. This requires defining new theories, algorithms and tools to guarantee responses to inputs within specified time constraints, supporting faults and providing graceful degradation. We will also analyze how to define real-time models, and we will provide automatic verification for the executing models, including methods for continuous and hybrid systems. We will build a set of tools that can be applied to develop real-time software, and simulations with hardware in-the-loop, which will be used as a proof-of-concept of our theoretical framework. The research has the potential to influence a variety of applications fields (communication, emergency planning, traffic control, biomedical, etc).""472323,""Wainwright, Lindsay"
"463863"	"Waldispuhl, Jerome"	"Algorithms for exploring the mutational landscape of RNA molecules"	"Our understanding of the mechanisms regulating cell activity has considerably improved in the last two decades. Ribonucleic acids (RNAs) have emerged as one of the most important biomolecules, playing key roles in various aspects of the gene transcription and regulation processes. For instance, ribozymes  (or catalytic RNAs) are involved in the cleavage of messenger RNAs (mRNAs), and riboswitches undergo structural changes to regulate gene expression. To achieve their functions, RNAs use sophisticated structures which are determined by their sequence. Any modification of a sequence may result in a structural change and a loss (or an improvement) of the function. The development of tools to estimate the effect of mutations on structures, or conversely the influence of structure conservation on the mutational process, is essential for understanding the mechanisms of molecular evolution or to develop bioengineering applications such as the design of RNA molecules.We recently introduced a novel computational framework (RNAmutants) that enables to explore simultaneously the mutational (sequence) and structural landscapes of an RNA molecule in polynomial time and space, thus opening the door to large-scale studies impossible to perform with previous approaches.In this proposal we aim to develop and apply this framework. First, we will extend current limitations of RNAmutants such as its structural model, and enable the prediction of RNA tertiary structures. Then we will apply our methods (A) to make significant progress in our understanding of molecular evolution, and (B) to develop breakthrough methodologies to address problems encountered in emerging fields of Biology such as Synthetic Biology or Metagenomics. Both themes are expected to have strong impacts. The first one (A) aims to expand our understanding of molecular evolution and current scenarios on the origin of life (i.e. RNA world hypothesis). While the second theme (B) aims through its applications to radically change our comprehension of the living systems and eco-diversity.""456778,""Waldman, Stephen"
"455191"	"Wan, Justin"	"Numerical models and methods for cell image analysis"	"Common diseases such as stroke, diabetes, asthma, and cancer result in substantial health care expenditure in Canada every year. These diseases are often caused by environmental factors interacting with genetically susceptible individuals. Understanding how the regulatory gene networks affect cell function will facilitate treatment of common disorders. However, linking genes, diseases, and treatments is time-consuming. This involves collection of massive amounts of data from native and genetically perturbed cells. The recent DNA microarray technology allows cell biologists to quickly record cell activities by high-throughput real-time imaging systems. With thousands of images generated in each experiment, the traditional manual or interactive computer-assisted tracking would no longer be feasible. Efficient and reliable software for tracking cellular behaviour becomes critical in analyzing the cell images.The proposed research will investigate sophisticated mathematical models and numerical algorithms for image processing of cell images, which are effective for brightfield microscopy and can cope with the complexity of the cellular topologies. The objective of this research is to develop robust, automated cell image processing software that can help cell biologists effectively and efficiently analyze cell morphology from a large volume of microscopic images, thus facilitating better understanding of fundamental biological processes. It is likely that development of new drug discovery and therapies in several areas will be significantly aided by these numerical models and methods. In addition, this research advances numerical techniques for processing complex image objects, and solving high order nonlinear equations.""453062,""Wan, Justin"
"451284"	"Wang, Fangju"	"Disambiguation in natural language user interfaces of service robots"	"In the proposed research, we develop disambiguation techniques for building natural language user interfaces of service robots.Service robots are used by ordinary people in their homes and offices, etc. They help people do house work, care seniors, conduct office duties, and so on. Most of the users of service robots are not specially trained personnel. For such users, a desirable way to control or communicate with service robots is through natural language interfaces. That is, users use instructions/commands in natural languages to control service robots. A core function of a natural language interface is to translate user natural language instructions into machine commands. So far, techniques for developing robot natural language interfaces are far from mature. A major challenge in building a robot natural language interface is handling ambiguity in natural language instructions. A natural language instruction may be ambiguous when it can be interpreted in different ways, and when it contains grammatical/lexical errors. Ambiguity may result in incorrect translation. In this research, we develop a set of new techniques for disambiguation. In developing our techniques, we take into consideration the special features of robot applications, in addition to the general features of natural language user interfaces. In our techniques, knowledge about actions and behavior plays a key role in handling ambiguity. The techniques will help a robot natural language interface understand, in a knowledgeable way, natural language  user instructions containing ambiguity. The technique can be used to build service robots that are more user-friendly and more accessible to ordinary people.""454911,""Wang, Feiyue"
"457447"	"Wang, Jane"	"Signal and information processing methods for digital media security and brain connectivity modeling"	"Both Information Management and Biomedical Technologies are identified as key strategic areas in Canada, whereas developing advanced signal and information processing tools is vital due to the increasing analysis complexity and modeling challenge of current digital media and biomedical data.   The proposed research will extend the applicant's current work on statistical signal processing theory and applications, specifically targeting several important topics in the areas of digital media security and brain connectivity modeling.     The massive proliferation and extensive use of digital media arising from its easy-to-manipulate & distribute nature also poses new challenges to digital media security. To enforce copyright compliance for media sharing, we focus on developing innovative, trustworthy watermarking and content-based fingerprinting technologies. The proposed research addresses strategic and timely issues of practical importance and will enhance the accessibility, management and security of digital media sharing. With the recent revolution in neuroimaging techniques, there is greater recognition of the vital role of signal processing for modeling brain connectivity, which is critical for the understanding and assessment of brain functioning. We focus on developing novel, fundamental signal processing and graphical models for accurately inferring brain connectivity from neurological data (e.g. EEG) by addressing key challenges, including sparsity, temporal variability, blind source separation, error control and group analysis. As general signal processing connectivity tools, the proposed methods will have wide applications in science and engineering, especially for biological network analysis such as gene regulatory networks and corticomuscular connectivity.     The long term goal of this research program is to develop novel, fundamental signal and information processing theories and methods that will have a lasting impact on the field and that will be used by other researchers and engineers in real-world applications. Given the inherent multidisciplinary nature, the proposed research focuses on the training of interdisciplinary, highly-qualified personnel.""455561,""Wang, Jian"
"451567"	"Wang, Shengrui"	"Mining high-dimensional data, sequences and data streams / Prospection de données de grande dimension, de séquences et de flux de donées"	"In recent years, the size of databases has increased rapidly and data sources capable of continuously producing large flows of data are becoming increasingly common. For instance, the Yahoo home page receives 166 million hits per day and about 48 Gb of clickstream collection per hour. Other examples include traffic and weather data, satellite observations, banking transactions and genomic sequences via high-throughput technologies. There is an increasing need for efficient data mining methods and knowledge discovery systems to utilize this data. Cluster analysis, which is aimed at separating data into meaningful groups, is a major sub-domain of data mining. Clustering high-dimensional data, sequence data and data streams is very difficult because conventional measures of the distance/similarity between data objects are not meaningful, the structure in these data sets is complex and real time processing is often required. These difficulties are further amplified if the data type is categorical rather than numerical. This research program aims to develop practical and efficient methods for clustering data sets of such complex types. We will investigate several important issues including extraction of most significant patterns/features, definition of appropriate similarity measures, detection of outliers, and development of efficient ways to cluster categorical data, massive data, and data streams. The proposed research will be carried out in a context closely related to real world applications, including living assistance to elderly and disabled persons in a smart-house environment, analysis of genomic sequences, discovery of social networks in Web forums, assistance to police in the search for suicidal persons, etc. The anticipated output of this research is a better understanding of the potential and the limitations of the current techniques. It will create new data mining methods and techniques as well as systems that will have impacts on socioeconomic development, health care, business intelligence and scientific research.""462905,""Wang, ShengYuan(Alex)"
"454745"	"Wareham, Harold"	"Parameterized complexity analysis in cognitive science"	"Every day, we as human beings are constantly performing computations as part of our everyday lives -- we search cluttered desks for misplaced keys, we make plans for visiting a set of stores to get everything we need for an upcoming party, we decide that comparing a neighbor's child to a runaway bull is (or is not)  appropriate. We do these computations quickly and (for the most part) correctly, and they seem so simple -- however, after 50 years of research, the subdiscipline of Computer Science called Artificial Intelligence (AI) has yet to replicate in robots and software systems many human cognitive abilities that we take for granted. This is not just a software or hardware problem -- psychologists working in Cognitive Science have equal difficulties deriving general theories explaining how we do the things we do as quickly and as well as we do.The typical approach to handling such difficult problems in both AI and Cognitive Science is to invoke fast approximation methods called heuristics which always give solutions quickly (though not necessarily the best ones). However, this may not be necessary -- given all the restrictions on the types of inputs to and required outputs from human cognition in practice, there may yet be best-solution methods that are slow in general but fast for the particular situations humans typically encounter every day.In my research, I use techniques from the theory of parameterized computational complexity to find such methods for solving difficult problems encountered in Cognitive Science and AI. I will not only be tackling problems arising from deriving theories or software implementations of human cognitive abilities, but also analysis problems for very large datasets of observed human behaviors  (which give further insights into how we think and act the way we do). The methods derived in my research should lead not only to better theories of human cognition but also to better implementations of human abilities in robots and other AI systems.""465773,""Wares, Trevor"
"459434"	"Wasserman, Wyeth"	"Computational analysis of regulatory DNA sequences controlling gene transcription"	"All cells and organisms possess a DNA-based set of information that describes the proteins that can be constructed, as well as information for making RNA molecules that perform important roles in the biochemical processes of life.  The information present in the DNA sequences is static -cells have the same DNA present without regard to the place, time or conditions in which the cell exists.  To respond to the environment, cells selectively turn on or off (or up or down) genes, allowing production of RNA and proteins suitable for the conditions encountered.  For instance, yeast producing alcohol from sugar in wine fermentation turn on genes that facilitate this process.  The research proposal addresses the interdisciplinary application of computers to the analysis of the regulation of genes, with a specific emphasis on the transition of information from DNA into RNA (the process called transcription).  The research program includes three majn areas of activity. First, software is developed to allow for the identification of specific DNA sequences that appear near genes that are turned on in a specific condition.  Finding these ""on/off switches"" is a first step in deciphering how cells  respond to changing conditions. This approach builds on artificial intelligence methods for pattern recognition. In the second step, we address non-random properties of DNA that complicate the analysis. In many organisms, genes that are ""always on"" have different mixtures of A,C,G and T, compared to genes that only turn on when needed.  Searching for patterns is complicated by this mixture difference. Solving this seemingly minor problem could greatly improve the interpretation of expensive laboratory-generated data from across the life sciences.  Linking of the on/off switch DNA sequences to the proteins that ""flip the switch"" is the focus of the third aim.  We will computationally analyze text from published papers to predict which proteins control the process.  The research program impacts research across all of life sciences, from agriculture to bioengineering, and from microbiology to biochemistry.""450248,""Wassersug, Richard"
"455096"	"Westwick, David"	"Identification of nonlinear systems operating in closed loop"	"Many systems, robots, aircraft, electrical motors and generators, behave in a nonlinear fashion.  Controlling these systems efficiently often requires an accurate mathematical model of their behavior, that can accurately predict the system's response to any given input or stimulus.  In relatively simple cases, it is possible to build these models using principles of basic engineering, mathematics and physics.  When the system becomes too complex for this analytical approach, the model must be obtained experimentally, using measurements of the system's inputs and outputs.  This is system identification, which is used wherever precise mathematical descriptions of complex systems are required.  System identification has been applied successfully to several classes of nonlinear systems, provided there is no feedback from the system's output to its input.  Unfortunately, this output feedback is the basis for most control systems.  While many systems can be tested without feedback, there are many systems that are inherently unstable, high performance military aircraft, and magnetic bearings, for example, and cannot be operated without some sort of stabilizing feedback controller.  In other cases, electric generators and chemical plants, there may be both economic and safety reasons that require closed-loop operation.This research aims to develop system identification methods that can be used to construct precise mathematical models of linear and nonlinear systems using only measurements of their inputs and outputs, all while they are operating inside feedback control loops.""467863,""Westwood, Alana"
"455031"	"Wineberg, Mark"	"Modeling evolutionary algorithms for dynamic and stochastic optimization"	"The aim of this research is to study and model the behavior of evolutionary algorithms (EA) in stochastic and dynamic environments, with an eye to better understand and so better control evolutionary behavior under these environments. The models used in this research are based on statistically significant observations of production-level EA systems as opposed to a bottom-up analysis of the mathematical (probabilistic) properties of a very simplified EA. Furthermore they are not just performance (output) models of the EA but also comprise the properties that make up the state and dynamics of the EA within a complex environment.      The project can be divided into two broad areas: population modeling for dynamic environments (DE), and statistical modeling for stochastic environments (SE).     With population modeling, we first aim to properly analyze the fitness landscape (FL). Current FLs are based on the topology of the problem, not the topology of the search space. Furthermore, the crossover operator does not readily lend itself to a distance measure, which is needed to define a topology. Once a new FL is developed, a DE provides an interesting framework for utilizing the new model. With it we should be able to discern the evolutionary motion towards the global optimum and factor it out of the population motion, thus exposing the environmental motion. The EA could then be modified to be predictive, evolving to where the global optimum is heading, as opposed to being reactive, evolving to the optimum's current location.     The second thrust of this research is the use of statistical modeling to capture and possibly control the behavior of an EA system in a SE, i.e. a probabilistic fitness function. Recently there has there been a renewed interest in this area mostly focused on the similarity with DEs or with the goal of finding ""robust"" solutions. However, there are more ramifications than are commonly realized, especially in the areas of multi-objective optimization and automated program induction using Genetic Programming (GP) especially when applied to the problem of symbolic regression.""466607,""Winegard, Timothy"
"461374"	"Witte, René"	"Natural language processing services for semantic computing"	"Dealing with the overwhelming amount of information readily available today is one of the biggest challenges for individuals, as well as organizations. Current database and information retrieval systems enable users to quickly obtain vast amounts of information in textual form -- but a serious bottleneck remains by reading, interpreting, and using the collected information.   Clearly, users are in dire need of more advanced, semantically-oriented systems that help them analysing, transforming, and creating knowledge from large amounts of (textual) data. Foundations for this kind of support have been developed for many years in fields like text mining, ontology, and the Semantic Web. Yet widespread support has not yet materialized in the tools and desktop environments available in current systems. This is due to a gap between current knowledge analysis research, which aims at algorithmic foundations and is rarely concerned with software engineering questions, and software engineering research that likewise barely targets the systematic engineering of knowledge-intensive systems and their particular requirements.   This research aims to close this gap. It is one of the first to address both software engineering of semantic systems, as well as knowledge management foundations using ontology and text mining approaches. Foundations and applications are developed in close collaboration with domain experts.  The research will progress from short-term objectives, analysing requirements and developing foundations for fields like biomedical research, cultural heritage data management, and business intelligence, to long-term foundations in semantic system engineering. A central focus and testbed of our research targets the conceptual and technical integration of semantic technologies with common desktop environments, including word processors, email clients, Web browsers and software development environments, which will fundamentally change the way users interact with their systems.""461411,""Wittenberg, Katherina(Karin)"
"457318"	"Wohlstadter, Eric"	"Optimization of distributed data access for web applications"	"Web-based applications are the de facto platform for distribution of, and interaction with, large-scale data sources. Traditionally these applications had been designed based on the well known 'three-tier architecture'. Invariably, the back-end tier in the architecture would have been implemented as a relational database, providing powerful features for optimization of data queries. Without such features, developing Web applications is much more difficult; leading to increased development time, cost, and decreased responsiveness for end-users.      Recently, several advances in application hosting practices have diversified the mechanisms where data resources can be stored and accessed by Web application developers, making data access and storage more decentralized and heterogeneous. We plan to address three major factors that change existing development practices: use of third-party Web services, third-party data storage hosting (i.e. 'cloud' storage), and support for offline client-side storage (i.e. 'HTML5 Web Database'). In this new environment, traditional centralized optimization approaches cannot be applied because of the diversity presented by resource interfaces and loose coupling between application providers and data resource providers.      The objective of this research is to enable optimization of distributed data access for Web applications. Our intention is to allow small/medium sized businesses and government departments to offer information access in this new Web environment with usability and scale that is competitive with the handful of large Web-centric companies. If such companies were to monopolize the Web based on their technical edge, it could be detrimental to the spirit of competitiveness and open information exchange that has made the Web an asset to society.""452189,""Woit, Denise"
"458089"	"Wu, FangXiang"	"Development of methodologies for modeling and design molecular biological systems"	"Genomic-alerted diseases (e.g., obesity, cancer, HIV, H1N1) stem from the dysfunction of molecular biological systems, not only their isolated components (e.g., genes, proteins). With advances in high throughput measurement techniques such as microarray, ChIP-chip, and mass spectrometry, large-scale biological data have been and will continuously be produced. Such data contain insightful information for understanding the mechanism of molecular biological systems and have proved useful in diagnosis, treatment, and drug design for genomic-alerted diseases. However, even if our ability were adequate for the task to measure the functional states and interactions among biological network components, computational limitations alone would prohibit us to understand the behaviours of molecular biological systems because their complexity exponentially grows with the number of network components and interactions among them. With such various types of biological data, my research has been focused on developments of advanced methodologies for understanding the details and principles of molecular biological systems while aiming at designing (or controlling) them to have a desired behaviour (e.g., robustness ot diseases). The long-term objective of my proposed research is to advance methodologies for modeling, analyzing and designing molecular biological systems (networks). To achieve the long-term objective, three specific short-term objectives are designed for my proposed research: 1) inferring gene regulatory networks from various types of biological data; 2) molecular biological system identification and parameter estimation based on various types of biological data; and 3) analyzing and simulating molecular biological systems for designing them. While ambitious, the successful outcome of the proposed research will have considerable ramifications for designing and engineering molecular biological systems with the potential to benefit many areas of industry such as personalized pharmaceutical development and crop yield enhancement.""472634,""Wu, Gang"
"456220"	"Wu, Huapeng"	"High performance computations and architectures for computer and communication security"	"The major breakthroughs in the fields of elliptic curve cryptography, the standardization of elliptic curve cryptography based security services by major international organizations, and the wide adoption of the Internet have created demand for high speed, area efficient and reliable finite field arithmetic units. The objectives of the proposed research are: (i) to further extend the state-of-art for the architectures for characteristic two finite field arithmetic circuits using normal basis and redundant representation or its variants, and VLSI implementation of the arithmetic units for the fields recommended by NIST for elliptic curve cryptography. (ii) to further develop efficient algorithms and architectures for odd-characteristic extension fields arithmetic. (iii) to further investigate polynomial bases and its variants that could result in efficient finite field arithmetic. ""453238,""Wu, Jiangning"
"457492"	"Xu, Dewei"	"Power converters and system control for wind farms"	"Wind energy is fast growing world wide. It is predicted by the Canadian Wind Energy Association that wind energy will reach 20% of the total energy demand by 2025. With higher wind energy penetration, there are many challenges that need to be addressed, not only at the individual wind turbines but also at the wind farm level, including but not limited to: output variability, grid management and energy storage. The proposed research will focus on the development of innovative power converters, robust control strategy and reliable grid connection for wind farms.The proposed research will focus on three areas: 1) To develop innovative power converters to transfer the variable wind energy to the fixed grid. High power high voltage converters for existing and future high power wind energy systems will be investigated. Innovative converter topologies and control strategies are expected to improve the reliability and reduce the cost. 2) To improve the wind energy integration. Fault-ride through control scheme for large wind farms is expected. Energy storage, flexible AC transmission systems (FACTS) will be studied in this research. New voltage and frequency control methods with integration of energy storage and FACTS devices are expected to improve the operation of the wind farm. 3) To improve the stability of wind farms. The wind farm together with the photovoltaic system will be studied in the micro-grid. Innovative island operation and anti-island detection methods will be expected through this research. The proposed research is expected to generate certain innovative technology, which will be transferred to the Canadian wind energy industry and benefit Canadian economy in renewable energy.""451513,""Xu, Gu"
"454626"	"Yang, Simon"	"Intelligent navigation, communication and cooperation of multi-robot systems"	"A team of robots would work together to accomplish an assigned task rapidly and efficiently. In many applications such as search and rescue operations, a multi-robot system has obvious advantages such as faster operation, higher efficiency and better reliability than a single robot. The key challenges of multi-robot systems are the safe navigation, effective communication and efficient cooperation of these robots.The proposed research will make significant original and innovative contributions in real-time multi-sensor fusion, navigation, communication, coordination and cooperation of multi-robot systems, by addressing the following issues: (1) Real-time collision-free navigation of individual robots; (2) real-time communication among the wireless network enabled robots; and (3) intelligent coordination and cooperation of the multiple robots to accomplish a task that is usually multi-objective.         New approaches will be developed to achieve these three objectives. In particular, biological functionality, mechanism and strategy with reasonable justifications will be incorporated in the new technology development. The inspiration of the proposed research is that biological species can efficiently sense dynamic environments and take effective actions based on the inputs often with very simple mechanisms and with limited availability of information; and biological species exhibit coordinated behaviours and can cooperatively accomplish tasks that are beyond the capabilities of a single individual under limited implicit communication. Rigorous theoretical analysis and extensive experimental studies of the developed methodologies for real-time navigation, communication and cooperation of multi-robot systems will be conducted. The developed technologies will be implemented in the experimental multi-robot system in the applicant's research lab. The developed techniques will have many potential industrial applications. This research will provide an excellent venue for the training of highly qualified personnel in areas of robotics and intelligent systems, and offer good commercial and social implications.""461892,""Yang, Songlan"
"456971"	"Yavuz, Mustafa"	"Fabrication and testing of nano-building blocks: thin films, nanowires and nanotubes and their applications"	"There is a huge growing interest in thin films, nanowires and tubes-that sometimes have been named as nano-building blocks-, with functional electro-mechanical characteristics, since the assembly of these units into nano- and micro-scale devices and circuits could enable diverse applications in nanoelectronics and photonics. Most of these applications involve the use of hybrid structures in which materials of different compositions meet at interfaces. To fabricate the nano- and micro-devices from the 'nano-building-blocks, it is necessary to connect or assemble these blocks. However, to realize electronic applications, such as microchip interconnects the need to reproducibly fabricate connections between individual nano-building blocks has been identified as a major challenge. For instance, as previous studies show, making an electrically conductive connection between nanotubes is not straightforward. Instead of the desired low resistant contacts, high resistant junctions are often generated. Such a high resistance (~200kOhm) has to be interpreted in view of the small contact area of the order of 1 square nm.   For these reasons, this research aims to manufacture and test nano-building-blocks of different natures and assemble them together to make small scale devices such as sensors for bio-marking.""466483,""Yawney, John"
"454800"	"Yeh, ChiHsiang"	"Green wireless internet: architectures, protocols, applications, and implementations"	"In many parts of the world including Northern Canada, no utility grids are available so that conventional Internet access and mobile communications technologies such as 2G/3G may not be deployed due to the inability to power the required communications infrastructure. Communications and Internet access in these areas has to resort to satellites communications, which are expensive in terms of bandwidth per dollar, require higher power which translates to shorter device uptime, and specialized equipment that is relatively expensive and not commonly available to general population.  In this research program, we will develop technologies and protocols that can incorporate green energy technologies into mobile wireless systems and Internet applications. In particular, we will develop enabling techniques for WiMAX/3G/4G and beyond cellular networks to utilize isolated renewable energy sources including wind, solar, and geothermal power. The success of this green technology initiative can enable a variety of new applications utilizing wireless technologies and the Internet in such remote areas. Examples include bolder control, disaster rescue, climber safety, military applications, as well as civil applications waiting to be invented and to generate tremendous commercial and social values. Moreover, such isolated renewable energy-based technologies are of strategic importance for national security. Wireless networks powered by such technologies can continue to function regardless of utility failure due to accidents or terrorist attacks - realizing sustainable and distributed communications infrastructure without single points of failure.   Hopefully our research initiative will lead to advancements that resolve issues hindering the deployment of a pervasive wireless Internet that connects and serves every person in the world in a green and affordable fashion, eliminating digital divide and harnessing the power of collective human intelligence.""465640,""Yeh, Christina"
"463834"	"Yoon, TaeJin"	"Prosodic phrasing detection using acoustic source characteristics"	"The goal of this multidisciplinary research among linguistics and speech technology is to develop an efficient algorithm for prosodic phrase prediction using voice source characteristics. The human voice has evolved as a vehicle for conveying many different types of information, and human listeners have developed the ability to detect very small and subtle voice quality changes, and interpret their function. Our abilities lag far behind what the human ear can effortlessly do. Most of the studies to date have been very limited, either in the quantity of data analyzed, or in the kinds of source measures made. Advances in the field have been particularly hampered by the lack of availability of suitable analysis tools. Manual interactive techniques which has been generally adopted in the investigation to voice quality across languages permit a fine-grained analysis of the source, but because of their labour-intensive nature, are not suitable for the large scale studies that would be ideally needed for progress in this field (Epstein, 2002; Hardcastle & Laver, 1997; Ishi, Ishiguro, & Hagita, 2008). Therefore, it is an important task to develop an efficient tool for (semi-)automatic prosodic phrasing annotation, which are becoming more and more important for the speech community. For example, the prediction of prosodic phrase boundaries is an important task for natural language processing to convey the correct meaning and text-to-speech synthesis applications to increase the naturalness of the synthesized speech. It has been reported that the prosodic boundary is occasionally glottalized, but because glottalization is only occasional, and because glottalization is difficult to characterize using standard speech signal processing methods, this acoustic feature of prosodic phrasing has apparently not been extensively studied or incorporated into automatic prosodic phrasing prediction algorithm. Investigating prosody or computational modeling of prosodic phrase boundary through the study of these acoustic features is complicated by the fact that pitch, loudness, duration and voice quality is also affected by paralinguistic properties of the utterance such as the speaker's emotional state, and even by non-linguistic factors such as speaker's gender and age. ""467270,""Yorioka, Teruyuki"
"450445"	"You, JiaHuai"	"Answer set programming and applications"	"In Artificial Intelligence, we attempt to build intelligent agents, which, among other things, shall be capable of acquiring knowledge through various means such as learning, observation, or simply being told, and able to reason with it, for example, to make plans, explain observations, achieve goals, etc. To acquire knowledge and reason with it, we need to represent knowledge in a computer readable format.Answer set programming is a recent addition to the paradigm of declarative knowledge representation and constraint programming, with strength in solving computationally hard problems, which typically require  combinatorial search. The basic idea is to represent a given problem by a logic program that specifies the constraints and rules that must be satisfied by any solution, called an answer set, and an implemented system is responsible for efficient computation of solutions. In a broader sense, answer set programming targets all applications of answer sets to knowledge representation.The long term objective of this project is a deep understanding of the nature of declarative knowledge representation in form of answer sets, and eventually make it a realistic tool for solving significant practical problems. The possibility of such a tool has been demonstrated by the recent advances in  the state-of-the-art in building fast constraint solvers, including answer set solvers,  and existing applications.Specifically, we propose to study several critical issues on which the success of answer set programming intimately depends, including theoretic foundations, programming methodologies, complexities and expressiveness,  methods of efficient implementation, significant applications, and relations with other constraint solving frameworks. ""458535,""You, Lidan"
"456433"	"Yousefi, Shahram"	"Rateless codes for advanced architectures"	"Handheld computers, cell phones, and other communication (COM) devices have transformed our lives. Increasingly, subscribers expect high-quality access on an 'anywhere-anytime' basis. New generations of COM systems are expected to support applications such as wireless broadband access and HDTV in addition to minimal services such as voice and data. The spectrum is expensive and limited; yet demand for high-rate high-quality wireless applications is rapidly rising. Increasing signaling efficiency and improving resource allocation methods are key to making fast and reliable information networks a reality on an anytime-anywhere basis. The proposed research in this application  lies at the intersection of two very active and promising COM research directions: network coding and rateless coding. The signal design and  analysis laboratory (SDAL) at Queen's University has recently made major contributions  to these two areas and further research and development projects are underway. The focus of this application is to develop proper analytical tools and also design truly scalable COM networks serving large numbers of demanding users. Using  receive adaptivity with the aid of decentralized utilitarian protocols, significant  improvements in the QoS, delay, power consumption, and bandwidth usage are expected. This application approaches a solution based on network coding and rateless signaling schemes. Realistic large-scale information networks are to be considered. The following studies are  proposed: i) practical network coding schemes and resource allocation protocols, ii) novel rateless codes and decoders for noisy channels, and, iii) rate- and overhead-optimal rateless/network  codes for multi-hop networks. This research will keep Canada at the forefront of telecom technology. Industry and household subscribers will enjoy full mobility in access to information and applications at better rates while concurrently consuming less power. This proposal directly involves the training of at least 3 PhD, 3 MASc, and 5 BASc students. Due to the timeliness and significance of the proposed project, the highly-trained students will be a great asset to high-tech industry and academia in Canada.""467417,""Youssef, Amr"
"457362"	"Youssef, Amr"	"Cryptographic hash functions: analysis, design and implementation"	"Cryptographic hash functions can be considered as the workhorse of cryptography and they are sprinkled all over security protocols.  In spite of this, hash functions are arguably the least well-understood cryptographic primitives, and hashing techniques are much less developed than encryption techniques. Cryptanalysis during the early 2000s has shown that widely used hash functions offered only a very limited security margin. A completely new chapter in the history of analysis of Merkle-Damgård (MD)-like designs was opened when Chinese cryptanalysts managed to enhance multi-block differential cryptanalysis to a point that finding collisions for MD5 became very easy and a substantial reduction of the security margin was obtained for SHA-1. Soon after the first collisions for MD5 have been found, it was shown how such collisions can be used to mislead integrity checking software and to produce documents with popular formats that collide under MD5. Furthermore, MD5 collisions were also used to create a rogue certification authority. In response to these dramatic cryptanalytic attacks, the National Institute for Standards and Technology (NIST) has decided to develop one or more additional hash functions. Consequently, in November 2007, the NIST has announced the launch of the SHA-3 competition to select a new hash function family. The objective of this research is to investigate the design and analysis of efficient families of provably secure hash functions. In particular, we aim to (i) Formalize both the design and the practice of cryptanalysis of hash functions by introducing more sound concepts from mathematics (ii) Produce a theory to quantify the resistance of iterated hash functions to a large set of cryptanalytic attacks, and (iii) Develop a new alternative to the current MD-like iterative structures which can be used to bring practical hash functions closer to the random oracle model. The cryptographic primitives that will be investigated in this project are the main focus of the cryptographic and security communities from both academia and industry. Consequently, the findings of this project are expected to be of high significance because of its impact on a very wide range of security products and applications.""456144,""Youssef, Maged"
"455131"	"Zhang, XiaoPing"	"Signal and information processing based on multiscale and graphical statistical models"	"The objective of this proposed research is to investigate new signal processing theories and algorithms based on multiscale analysis and graphical statistical modeling. Specific applications include multimedia information hiding and retrieval, video databases, data fusion and statistical filtering, and economics.The exponential growth of digital information raised tremendous demands and technical challenges in information analysis, processing, organizing, and retrieving, making multiscale analysis and graphical statistical modeling a particularly fertile and timely area of research in both theory and applications.  Multiscale analysis and graphical statistical modeling can generate new technology and algorithms that are promising in effectively elaborating the complex structure of large amount of digital data, thus contribute to the new generation information mining, retrieval and analysis applications. This research will build on and extend my previous research on multiscale analysis and adaptive statistical modeling for signal and information applications. In the proposed research, I will investigate the multiscale graphical statistical modeling, targeting specific applications in multimedia information hiding and retrieval, video databases, data fusion and statistical filtering, and economics. I will (i) Develop theoretical models that combine graphical models and multiscale statistical analysis and related model identification methods; (ii) Develop theoretical frameworks and specific criteria to employ the new multiscale graphical models for feature extraction and event analysis for each specific signal processing application; (iii) Develop practical new algorithms and system prototypes to solve emerging application problems. The proposed new technologies and algorithms based on multiscale analysis and graphical modeling will contribute to the new generation information mining, retrieval and analysis applications in broadband digital age.""464591,""Zhang, Xinzhi"
"457398"	"Zhang, Yang"	"Noncommutative structures in computer algebra"	"Computer algebra (also called symbolic computation) is a relatively recent research area in computer science and mathematics, where the mathematical computation is symbolic rather than numeric. Although the numerical computations provide efficient solutions in many practical questions, computer algebra strives to  address the different and arguably more difficult problems by providing exact solutions, or solutions parameterized by variables in the input. In the past few decades, this method has been developed very quickly and successfully applied to many other areas.In general, mathematics structures are divided into two parts: commutative and non-commutative. The non-commutative mathematics structures include, for example, matrix algebra, Lie algebra, rings of differential operators, and quantum groups.The goal of this research program is to address many new challenges in computer algebra presented by noncommutative structures and their applications, so called noncommutative computer algebra. I plan to consider the design, analysis and implementation of efficient algorithms, and the development of the corresponding mathematical theories, for a number of important non-commutative rings including matrices, Ore polynomial rings, quantum groups, Poincare-Birkhoff-Witt extensions, and more general differential-difference algebras.   As applications I am going to apply my algorithms and theoretical approaches to other areas such  as control theory, cryptography and statistics. Efficient algorithms will be implemented in the computer algebra system Maple.""471150,""Zhang, YaweiIvy"
"455097"	"Zhao, Qing"	"Control, identification and diagnosis of networked and hybrid systems for fault tolerance"	"This proposal seeks support for a research program on developing advanced fault tolerance techniques for technical systems/processes in networked and hybrid configurations. Driven by the global need for plant optimization and information integration, introducing networking into system automation and control becomes a prominent trend in process industries. As a result, we are now experiencing a dramatic change of technical systems from traditional homogeneous and centralized configurations to hybrid and distributed ones. The fault tolerance issue becomes more stringent in this case. Indeed, faults in one part of the system can propagate through and affect the other parts even in remote locations. As one important objective, we intend to study the constraints and resources brought by the networks, and propose new design tools for fault diagnosis based on network data transmissions instead of traditional wired signal transmissions. Fundamental issues, e.g. time-delay, data loss, and time-varying uncertainties will be considered. Another objective is to study fault tolerant control systems from a hybrid system perspective, where different system operations, e.g. multiple faults, are described by discrete modes while the system dynamic behavior under each mode is governed by continuous-time or discrete-time states evolutions. System identification for such a hybrid system and its application to fault tolerant control will be researched. In fact, fault detection can be treated as the discrete mode estimation problem while the switching control strategy can be adopted for control reconfiguration in this case. At last in this program we seek to  develop practical applications of fault diagnosis and fault tolerant control by applying the results to real-world problems. The proposed research program is inter-disciplinary that involves various topics in systems and control, signals, information systems and operations research. It can find vast applications not only in safety critical and highly autonomous systems but also in process control, manufacturing and energy industries, which nowadays have an increasing requirement on reliability and sustainability.""461372,""Zhao, Rongmin"
