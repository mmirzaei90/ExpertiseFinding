"id"	"researcher_name"	"application_title"	"application_summary"
"479165"	"Aagaard, Mark"	"Model-driven development of pipelined systems"	"Pipelining is a widely used performance optimization for digital-hardware systems that is used in systems ranging from simple signal-processing filters to high-performance microprocessors. Pipelining increases performance by overlapping the execution of multiple instructions, analogous to the way that multiple cars flow through an automobile assembly line.  Although intuitive when viewed as block diagrams, pipelines are difficult to design and verify, because subtle bugs can arise from the interactions between instructions flowing through the pipeline.    The goal of this research is to reduce the development effort and improve the quality of digital-hardware systems by creating theories, tools, and techniques for the model-driven development of pipelined systems.  Model-driven development is a style of development that focusses on models of a system, rather than just the final implementation.  Model-driven development distinguishes between a model of an individual system and a metamodel that models an entire class of systems.  A metamodel captures features that are common to all systems in the class and identifies points of variation between systems.  We make daily use of metamodels without being burdened by the meta-level nature of our thinking, or even being aware that we are using a model.  Conventions for cartographic symbols on road maps are an example: all areas are drawn at the same scale, water is blue, thicker lines mean larger roads, etc.  These conventions are a metamodel for maps, and an individual map is a model for a specific geographical area.    We will create a family of metamodels that describe the behaviour of pipelined systems in general and of specific performance optimizations for pipelines.  We will encode these metamodels in CAD tools that will enable engineers to reuse high-level design strategies and cleanly separate the process of designing a system from that of implementing the design.  The research will improve design quality by reducing the opportunities to make mistakes and making it easier to discover opportunities for new performance optimizations.""495126,""Aamodt, Tor"
"479066"	"Achar, Ramachandra"	"Advanced methodologies and computer aided design tools for signal integrity analysis of high-speed circuits and systems"	"During the recent years, the intense drive for high-frequency and miniature designs has been at the forefront of new developments in simulation and verification methodologies focused on microelectronics, communication and computing systems. The design requirements are becoming very stringent, demanding higher operating speeds, sharper excitations, denser layouts and low power consumption. Consequently, traditional boundaries between the circuit/EM/mechanical and thermal design considerations are rapidly vanishing. High-speed effects such as crosstalk and attenuation, if not addressed properly during the design stage, can cause logic glitches which render a fabricated digital circuit inoperable, or they can distort an analog signal such that it fails to meet specifications. To address the above difficulties, a new research program with following initiatives is proposed to develop new-generation computer-aided tools and methodologies targeting high-speed modules: (a) Development of fast/accurate signal and power integrity analysis tools, (b) Development of parallel circuit simulation and optimization tools with emphasis on signal integrity and (c) Computer-aided design tools and methodologies for mixed-domain and multi-disciplinary analysis.The proposed research program is focused on strategic issues, to advance the state-of-the art in various aspects of high-speed applications and wide range of electronic design levels. The impact of the proposed research would be to facilitate the development of new generation multidisciplinary tools for concurrent analysis and validation of high-speed modules encompassing all levels of design hierarchy, such as on-chip, multi-chip modules, interconnects, packages and printed circuit boards. Finally, it is to be duly noted that an improved efficiency of design automation algorithms indirectly percolates in decreasing the design cycle time of new products, enabling further innovation leading to an increased influx of new electronic and communication products entering our markets, ultimately resulting in a healthier economy.""484715,""Achari, Gopal"
"479546"	"Adi, Kamel"	"Sécurité des systèmes et réseaux informatiques"	"Ces dernières années ont connu un accroissement spectaculaire dans l'utilisation des systèmes et réseaux informatiques et ce dans tous les aspects des activités humaines. Cet accroissement s'est aussi accompagné d'une complexité accrue des technologies de l'information qui sont désormais à caractère distribué, ouvert et mobile. Malheureusement, ce développement a aussi engendré de nouvelles vulnérabilités exposant ainsi ces systèmes et réseaux informatiques à un nombre croissant et à un éventail plus large de menaces, lesquelles posent maintenant de nouveaux problèmes de sécurité. Le nombre de vulnérabilités informatique a progressé de manière fulgurante ces dernières années et à l'ère du numérique,  ces failles engendrent un impact dévastateur sur toute la sphère économique. Nos recherches visent le développement et l'application de méthodes formelles et rigoureuses pour la résolution de problématiques liées à la sécurité informatique. Plus spécifiquement, Notre objectif de recherche est de mettre en place un cadre formel pour la spécification, la vérification, la validation et le renforcement  de politiques de sécurité dans les systèmes informatiques. Ce cadre doit prendre en charge trois niveaux et vues du système informatique allant du niveau organisationnelle et passant du niveau réseau et middleware jusqu'au niveau application. À terme, notre programme de recherche aura le mérite de faire bénéficier, pour des considérations et des besoins pratiques, des formalismes théoriques rigoureux tels que les algèbres de processus,  les fondements sémantiques, la logique, l'interprétation abstraite, les théories de types, etc. L'objectif ultime étant le développement de méthodologies et d'outils logiciels pour garantir la sécurité des systèmes et des réseaux informatiques. ""480398,""Adkin, Allan"
"479037"	"Afsahi, Ahmad"	"High-performance and scalable communication subsystems for exascale computing"	"High-Performance Computing (HPC) is the key to many scientific discoveries and engineering innovations. It is used to tackle computationally-intensive problems in fields as diverse as drug discovery, modeling of global climate system, seismic processing for oil and gas, green energy, genomics and bioinformatics, and astrophysics. Scientific/engineering simulations are mainly written with the Message-Passing Interface (MPI) library. Parallel processes in these simulations compute on their local data while extensively communicating with each other through the MPI library. Communication adversely affects the performance and scalability of MPI applications running on HPC clusters. With the availability of multi-core and soon many-core architectures offering increasing parallelism at all levels, HPC clusters consisting of hundreds of thousands of nodes with millions of cores and complex network topologies are poised to break the Exaflops (10^18 floating point operations per second) barrier in the coming years. With the emergence of such highly hierarchical clusters, MPI has to be optimized for performance and scalability in order to cope with the ever-increasing demands of large-scale simulations. The proposed research is highly original and innovative in the sense that it addresses key issues in MPI, by including topology-awareness for process mapping, by incorporating dedicated queues for partner processes, by quality of service provisioning partner/non-partner traffic, and by developing fiber-based asynchronous progression techniques. The outcome of this research will be relevant to various sectors in Canada, including Environment Canada, Compute Canada, Canada Genome Sciences Centre, oil and gas industries, and ultimately the Canadian public at large. It is expected that the findings from this research will have significant impact on the target community, and that it will lead to new directions for future research. The proposed research is ideal for training HQP in that it has a strong foundation that translates immediately into practical applications and implementations. There is a high demand for graduates in HPC and networking, and the HQP trained will be well positioned to compete for jobs in academia and industry.""485863,""Afshaninaghadeh, Peyman"
"473510"	"Ahmad, Omair"	"Design and implementation of signal processing algorithms and architectures for communication and other applications"	"The field of digital signal processing (DSP) has experienced explosive growth during the past couple ofdecades. The DSP techniques have become integral parts of the products and services that we need orencounter in our daily lives. The research efforts of the applicant in this area during the past five years have led to some very concrete results that have been shared with the international scientific community, both from academia and industry, and have given rise to new ideas and directions that need to be further investigated. The overall objective of the proposed research is to develop efficient algorithms and architectures and to lay sound mathematical foundations for reliable processing of speech, image/video, image and genomic signals, and for their efficient software and hardware implementations for multimedia communication and other applications.The distortion of images by additive or multiplicative noise is common during their acquisition, compression, storage, transmission and reproduction. Presence of the noise makes the automatic interpretation of image data very difficult. Pitch estimation of noise-corrupted speech, and identification and modeling of noisy speech systems are vital for reliable design of speech communication systems. Emerging communication and information networks deal with high volumes of digital speech, image and video signals and with their transmission through over-congested communication channels. Fundamental to these activities are the signal compression, and pre- and post-processing of such signals. Efforts will be put to reach solutions to these problems through the design and implementation of DSP algorithms that are cost effective and superior in performance. Regulation of gene expressions is a dynamic process. Understanding of the gene interactions that contribute to certain diseases provide valuable information for pharmaceutical and biotechnology industries. Signal processing techniques for the analysis of genomic signals and for modeling of the dynamics of gene regulation will be developed. Discrete transforms are essential to signal processing tasks. To this end, new parameterized transforms capable of optimizing the performance of given applications will be investigated. Fast and efficient techniques for computing these transforms will be undertaken. Research efforts will also be directed towards efficient software and VLSI implementations of the algorithms developed in this proposal.""492655,""Ahmad, Sayyad"
"478575"	"Ahmed, Maher"	"Multiple sensors system for sign language recognition"	"Sign language consists of symbolic gestures to establish communication by combining the shape, orientation and movement of hands and arms, as well as facial expressions to fluidly express a speaker's thoughts. American Sign Language is the primary means of communication for more than half million deaf people in the United States and Canada. My research is focused on communication with the hearing-impaired by developing a software tool that can interpret sign language. There are two methods of collecting information about hand gestures, data gloves and video cameras. Data glove is a glove equipped with sensors that sense the movements of the hand and interfaces those movements with a computer. The data obtained from data gloves are concise and requires little post-processing. However, the data gloves are limited by the number of sensors and they do not capture facial expressions which some signers use to communicate additional information. Some recent research has been shown that video cameras can capture hands positions as well as facial expressions. However, such cameras are prone to typical computer vision problems such as occlusion and sensitivity to lighting conditions. These vision-based methods typically use image processing techniques to segment the hand from the image background, and extract features such as the fingertips for use in gesture recognition. The proposed system depends on the inputs from the gloves and a single camera. It consists of four modules. First, the real time hand tracking and extraction algorithm is applied to trace the moving hand and extract the hand region. Secondly, Fourier or wavelet descriptors will be adopted to characterize the spatial features. A motion analysis will be conducted to characterize the temporal features. The new feature vector will be created with both spatial and temporal features. In the third module, the description of hand shape, orientation, and movements are used as inputs to the training module for the segmentation and recognition. The collected data will be transferred to the fourth module for classification and the word corresponding to the sign will be determined. This research will allow a hearing-impaired person to communicate with other people.""484236,""Ahmed, Marya"
"479336"	"Aimez, Vincent"	"Nanointegration for sustainable photonics solutions"	")This DG proposal builds on expertise accumulated at uSherbrooke to develop improved structures for optoelectronics integration on silicon devices, as well as for high efficiency photovoltaic cells for the next generation of solar energy supplies. Our ability to make advanced nanostructures provides a toolbox of solutions that we can bring to bear on this technology objective. The work connects closely with industry leaders in the domain (both in Canada and abroad) as well as with other university groups engaged in system deployment aspects of these devices.     )As a result, we are part of a major Canadian initiative offering global leadership in the topic of Concentrator PhotoVoltaics (CPV) and our researchers enjoy a stimulating range of connections with other academics and industry players, providing valuable opportunities for career development of the HQP generated by this work. We are fortunate that the emergence of this industry interest occurs at a time when our experience with technologies developed for Light Emitting Diodes (LEDs) and high performance transistors offers a convergence of solutions applicable to industry for the related challenges of optoelectronics on silicon as well as the new solar power marketplace.      )Our proposed approach aims at lower-cost processes for integrating multiple functions onto a single substrate, and as well as enabling more cost effective and reliable photovoltaic devices, we anticipate that the techniques developed will also be usable in other device contexts, such as applications in the biosensing and biomedical arenas.  The device development proposed will take advantage of a very close relationship with the advanced epitaxy group at uSherbrooke, the SUNLab group in Ottawa and will help to position Canada on the international scene by training experienced graduate students in device technology relevant to the new MiQro Innovation Collaboration Centre at Bromont, (a $382M investment to build a world-class innovation centre leading to the commercialization of electronic products of the next generation).""487355,""Ainslie, Philip"
"479068"	"Aitchison, Stewart"	"Photonic nanowires for optical sensing and signal processing"	"Developments in nanotechnology underpin many areas of science and technology. The ability of accurately structure surfaces, materials and devices on the length scale of 1 - 100 nm opens up new device and system possibilities in the areas of ICT, biomedical sensing and environmental monitoring. The program of research outlined in this proposal covers the use of nanofabrication processes based around electron beam lithography and their application to develop high refractive index contrast waveguide structures. There are three areas where student projects will be focused over the next five years, a) electron beam lithography and process development b) photonic wires for wavelength conversion and 3) Photonic wires for optical sensing applications. In 2009 we officially opened our new electron beam lithography system, which allows features down to 10 nm to be defined across large areas. The high beam current, and low stitching errors possible with this tool allow a wide range of structures to be patterned, including nano-structured surfaces for biology, sensing and photonics.  The quality of the underlying fabrication processes dictates the quality of the final waveguides, resonators and photonic crystals. We plan to optimize the ebeam patterning process to reduce the minimum feature size, improve writing time and reduce edge roughness that leads to high scattering losses in photonic waveguide structures.  The use of high refractive index contrast waveguides to implement wavelength conversion has many advantages. The small core size increases the local intensity, the waveguide structure can be used to dispersion engineering the waveguide to enable phase matching and resonators can be used to further enhance the conversion efficiency. The ability to engineer the dispersion and field profile in a nanowire waveguide also has applications in optical sensing. By narrowing the waveguide, incorporating a photonic crystal, or defect state it is possible to control the overlap of the optical field with the sensing material. The program of research outlined in this application will provide an ideal training ground for students who will gain skills in nanofabrication, integrated optical circuit design and optical testing.""483488,""Aitchison, Stewart"
"481818"	"Ajib, Wessam"	"Efficient resource allocation schemes for cognitive radio networks: agile spectrum sharing and enabling technologies"	"In recent years, wireless applications have become increasingly crucial while the number of wireless users has constantly been growing. However, the radio spectrum is a scarce natural resource; and its majority is allocated to well-identified licensed users. In general, licensed users are unwilling to share their spectrum with other unknown users and a large part of the licensed spectrum is not fully utilised. Cognitive Radio (CR) technology (based on software defined radio) presents a practical approach to spectrum scarcity and to its underutilisation. This is achieved by implementing the concept of dynamic spectrum access such that unlicensed CR users can dynamically access the licensed spectrum, without causing harmful interference to licensed users. This research program focuses on developing original and innovative efficient resource allocation schemes for the integration of CR technology in emerging wireless networking techniques. First, we will investigate CR nodes operating with cooperative communication - that builds a virtual multi-antenna system by making use of the antennas available in the neighbour nodes. This integration creates a cognitive cooperative system where efficient resource allocation algorithms are critical for extracting the benefits of both the cognition and the cooperation. Second, we will study the integration of dynamic spectrum access into femtocell cellular systems. Femtocell systems provide a high-throughput high-mobility service to answer the more-challenging requirements of new wireless applications. Each femtocell contains one low-power low-cost high-throughput base station connected to the wide cellular network through a reliable high speed connection. Femtocell benefits can be significantly curtailed without efficient resource allocation, spectrum access and interference control schemes. The benefits of this program include the contribution to fundamental and practical knowledge, the resulting good insights and better understanding of the dynamic spectrum access integration, the training of several graduate students in the emerging forefront areas of wireless technology, highly demanded in Canada, and the resolution of industrial problems related to efficiently implementing CR technology in wireless networks.""479547,""Ajila, Samuel"
"479624"	"Alhajj(Elhajj), Reda"	"The power of data mining and machine leaning techniques in social network modeling and analysis"	"This research program studies how the social network model and data mining techniques complement each other to produce a powerful framework that can successfully tackle a wide range of applications in different domains. Though, research on social networks started in anthropology and sociology in 1930s, the recent development in technology has influenced the field and rapidly increased its popularity. A social network consists of a set of actors called individuals and how they are linked to reflect relationships inspired from the domain within which they are studied. For example, people may be linked as students to reflect their enrollment in courses; proteins may be linked based on their functions within the body. On the other hand, data mining techniques are effective in discovering implicit knowledge hidden in the analyzed data. We argue that the social network model can be enriched with more valuable knowledge that can be extracted from the underlying data by employing data mining and machine learning techniques. The analysis of social networks may lead to valuable discoveries that may have essential social and economic impacts. From a social perspective, the discoveries may highlight membership in terrorist groups, common hobbies, family relationships, occupations, friendship, etc. From economic perspective, the analysis may lead to better identification of certain target customer groups, the development of drugs, exceptional weather conditions, unusual trends in the stock market, etc. ""492788,""Alhamami, Mosa"
"477028"	"Andrews, James"	"Development of high-quality, accessible software"	"I intend to carry out research in the three areas of software testing, software fault localization, and accessible computing.Software runs on our computers, our handheld devices, the cash registers of our stores and restaurants, and on every computer connected to the Internet.  In order to make sure that this software is the highest possible quality, software writers test their software extensively -- so extensively that it becomes one of the main cost drivers of software development.  We plan to identify ways of improving the cost-effectiveness of testing effort, and ways of ensuring that software developers have accurate, well-supported data to use in making decisions about how to test.When a software bug is found, software developers often have to look over thousands of lines of program text in order to fix the bug.  Many researchers seek to build ""fault localization"" tools that make good guesses as to where in the program text a fault is and communicate these guesses to the programmer.  We plan to study these tools in industrial settings, and combine the strengths of several such tools in an integrated environment.People with disabilities, such as blindness or motor impairment, often find it difficult to use computers, since modern user interfaces are fine-tuned to be used by able-bodied users.  We plan to continue work on Johar, a platform which allows application program developers to create accessible computer applications without having to know the needs of specific user groups.  Using Johar, researchers will be able to try out new computer accessibility technologies and compare them to old ones more accurately and efficiently, and disabled users will have greater and more up-to-date access to computer and communication technology.""487013,""Andrews, Jeff"
"482999"	"Ansermino, JohnMark"	"The monitoring messenger: Mobile patient monitoring for the intensive care unit"	"The modern intensive care unit (ICU) is a disruptive arena of continuously competing noises, alarms, signals, and patient data. Within this chaotic arena, the healthcare team must attend and respond appropriately to the competing needs of different patients. Monitoring devices, ventilators and infusion devices collect and display an array of data for each patient via their own separate displays. Each device attempts to direct the attention of healthcare workers towards relevant changes in device function or patient status, for example, by triggering an alarm. In reality though, these systems have many failures; there is a lack of coordination between these devices and the healthcare team they are there to support, and the frequency of false alarms, task interruptions and noise are a nuisance to healthcare workers and a hindrance to patient safety. We know that more technology, by itself, does not necessarily translate into better care or improved outcomes. Increased data may, in fact, increase misunderstandings and mistreatment of patients. If a team of healthcare workers is to provide optimum treatment, this information must be readily conveyed to them.In this project I propose to develop an improved, portable interface for monitoring critically ill patients. Information from multiple monitoring sources, administration devices and clinical information systems will be integrated into a single, mobile monitor: the Monitoring Messenger. The Monitoring Messenger will intelligently process this information in a decision support application, providing an intuitive display of vital information, alerts and reminders. This will optimize the collaborative working environment of the healthcare team by interrupting users only when an important clinical event has occurred or essential information is required.  Instantly accessible to bedside nurses, charge nurses and even remotely-located physicians, the Monitoring Messenger will enhance individual patient care, improve team work and provide an overall perspective of priorities for the ICU.""474463,""Anstee, Richard"
"481819"	"Ardakani, Masoud"	"Novel channel coding techniques under practical assumptions"	"Our modern life relies heavily on reliable, fast and efficient data communication. Some of the most important inventions of the 20th century are indeed motivated by this need (e.g., telephones, digital data storage devices, the Internet, and various wireless devices). Shannon's noisy channel coding theorem states that error-free communication is possible, even in the presence of noise. Moreover, depending on the strength of the noise, there is a fundamental limit on the amount of the information that can be conveyed error-free. This limit is called the channel capacity. The key to error-free communication is channel coding, i.e., adding redundancy to the data to make it robust against the channel noise. Since Shannon's theorem (1948), there have been many scholarly works on design of channel coding solutions. However, it was only in the past decade that modern coding methods made approaching the channel capacity possible. The existing literature on modern coding, however, almost always assumes that each transmitted symbol is read by the receiver exactly one time. In practice, due to inevitable timing errors, some symbols may be read more than once or not at all. Such insertions and/or deletions have a devastating effect on existing decoders, causing them to fail almost surely. As a result, timing recovery modules are designed with special care and at high cost to reduce the possibility of timing errors. This practice is against the main philosophy of channel coding, which suggest adding redundancy to data instead of paying for a more reliable channel. This research program aims at developing a profound understanding of channels with timing errors and devising efficient coding solutions that can correct timing errors. The successful outcome of this project will contribute to the progress of the important field of channel coding. It will reduce the cost of communication systems, while making them more reliable. The results of this research program will also impact a number of other fields of research. ""480508,""Ardelli, Bernadette"
"474421"	"Atwood, John(Bill)"	"Secure and accountable multicast data distribution"	"Multicast data communication has potential for substantially reducing network resource requirements when used to distribute identical data to many participants.  Although it has been used in specific, controlled environments, it has not been offered as a general service by Internet Service Providers, since the necessary security and accounting functionality is not available.  Solving this problem requires an integrated approach, with attention paid to Security and AAA (Authentication, Authorization, and Accounting) in the entire protocol stack.  During the previous grant period, solutions have been validated for the following areas: key management, participant access control for both senders (content servers) and receivers (end users), data distribution tree security, router-to-router control message security, interaction with financial representatives (e-commerce interaction), deployment when not all parts of the network support multicast data distribution, through the use of an integrated model for distributed session management.During the coming grant period, there will be three main areas of activity:1) Formal validation of the security of the distributed session management proposals;2) Formal validation that an overall system built by integrating the control, security, and AAA approaches is secure against a number of common threats and attacks;3) Design and validation of a framework for support of distributed key management and adjacency management, with the goal of securing the inter-router control messages for a wide class of routing protocols.The long-term goal is the demonstration of the feasibility of a secure, accountable multicast data distribution system.  Achievement of this goal will permit the introduction of multicast data distribution in commercial systems, with direct benefits to the operators of such systems (lowered costs and possibility of revenue generation), and indirect benefits to the user communities, through lower costs and by enabling applications that would otherwise be infeasible.""487066,""Au, Samuel"
"481647"	"Bajic, Ivan"	"Ergonomic multimedia"	"As defined by the International Ergonomics Association, ""ergonomics is the scientific discipline concernedwith the understanding of interactions among humans and other elements of a system."" The main theme of this research program is ergonomic multimedia. More specifically, our goal is to develop tools and techniques for multimedia processing from a human perspective.Multimedia is omnipresent, and the users can't seem to get enough of it. The extent to which a device is able to process and present multimedia has become a major selling point and a key differentiator in the consumer electronics industry, as demonstrated by Apple's products in recent years. In the past few decades, the main challenge regarding multimedia was communication - how to get it to the user? While that continues to be a challenge, new problems are starting to emerge. Even today, and especially in the future, the main question related to multimedia will not only be how to get it to the user, but how to fit it to the user? How to make it more appealing and easier to use?The development of sophisticated measurement techniques, such as eye and gaze tracking, is facilitatingverification and testing of existing models of human attention, sensing, perception, and behavior. On thetheoretical side of the proposed research program, we plan to improve existing models of human interactionwith various media types, and develop new ones, especially cross-media, context-adaptive, and device-specific models. On the practical side, our goal is to utilize the aforementioned models in the creation of new and efficient algorithms for capturing, processing, encoding, communicating, presenting, and interpreting multimedia signals in a human-centric way, and to create tools that enable users to interact with multimedia in a natural and intuitive manner.This type of research requires a new breed of researchers with a solid understanding of psychology andcognitive science in addition to expertise in engineering and computing. Building on our experience inmultimedia compression and processing, and our recent collaboration with researchers in HCI and psychology, the proposed research program aims to provide training for such researchers and equip them with necessary skills to become successful innovators and leaders in this emerging field.""484189,""Bajtos, Barbora"
"481321"	"Bakhshai, Alireza"	"Modeling, design, and control of plug-and-play power converters for hybrid renewable distributed energy"	"Traditional ""central"" power plants are typically located far from the load centres, and generally use fossil fuel to generate electricity.  These systems are inefficient and bring considerable environmental impact. The use of distributed renewable energy sources offers a variety of environmental, social and public benefits that are increasingly in demand as their greenhouse gas emission reduction, economic development, and environmental benefits become more widely known. Within a short time span of four years, from 2004 to 2008, global photovoltaic (PV) and wind power capacity grew 600% and 250%, respectively.  In 2008, Canada has reached installed wind energy capacity of 2369 MW, and the average annual market growth for PV technologies has been over 20% for more than a decade. Individual wind and PV systems can be unreliable due to their variable nature.   It is generally observed that wind and sun intensity have a complimentary nature; thus when integrated into a single hybrid system, the intermittent but complementary nature of wind and solar provides a more reliable system. Distributed generation (DG) covers different types of generation including but not limited to renewables. The concept of a hybrid green micro-source offers great potential for providing energy to remote rural areas in a vast country such as Canada. A micro-source with  'plug-and-play' flexibility is very useful, since various kinds of energy sources can be added to or removed from the DG system at any time without affecting the performance of other component units. A plug and play micro-source is feasible only if each generating unit component operates as an autonomous entity. Thus, each unit must have an isolated and self-operated control scheme. This research proposal addresses this challenging issue, and aims to answer the following questions: How can a plug and play micro-source of various non-identical units be designed and controlled? What are the associated energy management and system protection considerations? How are costs to be minimized? And, what are the resulting energy savings?""486097,""Baki, AbulBasarMohammad"
"479344"	"Bakr, Mohamed"	"Efficient modeling and design optimization of computationally intensive high frequency structures"	"One of the main Computer Aided Design (CAD) elements is the process of modeling.  Modeling allows us to predict the response of a structure for given values of the system parameters without having to actually build it and measure it.  Another procedure of equal importance is that of design optimization.  Using an optimization algorithm (optimizer), we are able to determine the optimal values of the parameters of the system under design that would result in the desired response.  The process of optimization involves time intensive repeated simulation of the system for different parameter values.       We aim in this work to develop novel techniques for efficient and accurate modeling of computationally intensive high frequency structures.  These techniques will utilize conformal meshes of transmission lines capable of accurately modeling structures with arbitrary shapes and material properties with as little memory storage as possible.         We will also apply novel optimization approaches that exploit the unique properties of electromagnetic (EM) solvers.  Efficient adjoint sensitivity analysis techniques will be extended to structures with dispersive materials whose material properties vary with frequency.  We will develop methodologies for constructing ""surrogate"" models that are capable of accurately predicting the EM model response over a wide region of the parameter space but are much faster to evaluate.   Our developed algorithms will reach the desired optimal values using only few EM simulations.    We expect that the proposed research will have a significant impact in reducing the modeling and optimization time of many high frequency components such as large antenna arrays, the modeling of the interaction of EM waves and the human body, and the modeling of the electrically large photonic and optoelectronic devices.  This will reduce the time-to-market time of products and add valuable tools to the designer of high frequency structures.""492296,""Bal, Navpreet"
"480131"	"Baltes, Hansjorg(Jacky)"	"Active balancing and complex motion planning for adult sized humanoid robots"	"The generation of stable walking and motion planning for adult sized (> 1.4m) humanoid robots presents unique challenges, because the state space is high-dimensional, the available sensor feedback is limited, and the robots are underactuated. This proposal aims at investigating new approaches to fundamental problems involving these robots. First, how can we design and implement algorithms so that humanoid robots cannot only move over flat even surfaces, but also move over uneven terrain? The solution to this problem will allow robots to travel across grass surfaces, forest paths, and rugged terrain. Second, how can we develop algorithms so that robots can plan novel motions from scratch rather than having to pre-program them? This will allow humanoid robots to pick up objects, plan its footsteps, and to even climb up a rock wall.Both areas have received significant attention in recent years, and progress has been made - but versatile, robust, practical solutions are still elusive. The research described in this proposal aims at greatly increasing the range of balancing and actions that a robot can execute by allowing some of its movements to be open-loop rather than closed-loop.To overcome the need for highly accurate global models of the kinematics and dynamics of the robot as well as its interaction with the environment, we propose to use multiple models with different resolutions that model specific important parts of the behaviour of the robot.Applying this idea to active balancing will allow us to overcome restrictions imposed by the 3D inverted pendulum model and the zero moment point (ZMP) constraint and to generate more stable, faster, more efficient and more natural looking walking gaits and balancing behaviours. The controller will generate balancing reflexes that will bring an unbalanced robot back into the controllable balanced region.We propose novel motion planning algorithms that uses the controllable balanced regions rather than regions based on kinematic and dynamic constraints as state space for the planning process. ""482230,""Baltzer, Jennifer"
"481910"	"Ban, Dayan"	"Design, fabrication and characterization of high-performance terahertz quantum cascade lasers"	"Electromagnetic waves in the terahertz frequency range (1-10 THz, 1THz = 1E12 Hz) have attracted intense interest because of their potential for many important applications: bio-chemical species detection, astronomical spectroscopy, terahertz imaging and optical wireless communication, to name a few. Application progress of terahertz technologies has been hampered by the lack of compact, low-energy-consumption, solid-state based and easily-operational terahertz sources and detectors. The situation has started to change recently, particularly after the invention of the first semiconductor quantum cascade laser working in the terahertz frequency range reported in 2002. The development of terahertz quantum cascade lasers has proceeded rapidly, nevertheless, there are still many technical challenges to further improve device performance. Among them, to achieve room-temperature lasing and high-power operation of terahertz quantum cascade lasers might be the biggest challenge. This proposed project will tackle the technical challenges by means of combined experimental and theoretical approaches. The objectives of the proposal include 1) to develop a comprehensive knowledge base on the underlying physics of intersubband transition and carrier dynamics in the quantum semiconductor heterostructures; 2) to develop a simulation package that can numerically calculate the important parameters of the quantum heterostructures and the overall performance of the quantum cascade lasers for comparison to experimental results; 3) to explore new quantum structures (such as a structure based on double phonon relaxation for carrier injection and extraction) that could outperform the existing quantum active-region designs in order to push higher the operating temperature; 4) to fabricate and characterize quantum cascade lasers with new structures and to apply the devices for practical applications. This research will provide a broad base of new knowledge in the THz frequency range with potential impacts in the areas of telecommunication, biological and medical sciences and security. It also brings in excellent training opportunities for HQPs to acquire strong photonic knowledge and highly-demanded hands-on skills.""492449,""Ban, Frank"
"475369"	"Basu, Anup"	"Perceptually and probabilistically guided multimedia"	"Over the next five years I will develop biologically and probabilistically motivated multimedia systems that include the following novel advancements:(i) Probabilistic and perceptual factors in 3D medical image analysis, extending techniques, such as random walks and Gaussian shape modeling, to better represent specific medical conditions related to infant brain, spinal cord etc., and thereby improve segmentation techniques.(ii) Incorporate the perception of silhouettes in the process of 3D point cloud simplification.(iii) Extend wireless 3D transmission strategies developed for mesh data structures to work with point-cloud representations.(iv) Calibration and synchronization of multi-camera environments using simple, widely available objects (such as a globe) and combining sensor data to create a dynamic integrated 3D scene.(v) Designing optimized peripheral vision stereo displays.(vi) Perceptually guided Motion Capture (MoCap) data compression, with the goal of making real-time decompression feasible even on hand-held cellular devices. The significance of my work will lie in:(a) Robust 3D medical image segmentation and ground truth generation algorithms.(b) Reducing bandwidth, memory requirements and optimizing perceptual quality in 3D multimedia transmission and visualization; with applications in panoramic 3D TVs.(c) Robust 3D transmission strategies under packet loss; resulting in improved 3D edutainment over wireless devices.(d) Robust calibration and intelligent coordination of a network of cameras, allowing better 3D free-viewpoint video capture.(e) Developing better interactive 3D environments in Edutainment.""496426,""Basu, Onita"
"481841"	"Bendada, Abdelhakim"	"Leading-edge developments of infrared vision for demanding scientific applications"	"Infrared (IR) imaging is a valuable tool for the nondestructive testing (NDT) of materials and the temperature mapping of the scenes under inspection. Basically, the observation is implemented either with a passive or active mode. The passive mode consists in exploiting the IR radiation distribution as naturally emitted by a scene. In the active mode, an external heat or light source is required to perturb the natural equilibrium of the scene under observation and produce IR radiation differences (thermal or non-thermal) between the features of interest and the background. IR vision still faces several limitations at both the detection and interpretation stages, in particular for demanding applications. Short-term objectives of the proposed research are oriented towards the investigation of new ways for solving some of the key issues; they are in agreement with and ensure the continuity of the long-term goals of our research activities. The current proposed research program provides promising potential for the following original and valuable activities: 1) IR inspection of scenes with 3D complex shapes: the objective is to recover the inspected surface geometry and to correct the thermal images acquired during a thermal NDT test so that the detection of hidden defects is completed in a suitable way; 2) IR inspection of high speed heat transfer: the goal is to use stroboscopic techniques to artificially increase the apparent frame rate of our IR imagers; 3) IR inspection of microscale scenes: innovative development is foreseen by using the new trends in super resolution. The central goal is microscale thermal imaging; 4) Very far infrared (Terahertz) inspection: the recent progress in the terahertz field would allow the adaptation of our IR vision systems to terahertz measurement; this would be beneficial for our activities in NDT; and 5) IR multispectral inspection: the objective consists in pursuing our activities in the multispectral field by developing a tomography inspection method for silicon based structures. The significance of the activities proposed in this application is two-fold. First, the basis of the work has a strong theoretical foundation; and secondly, the expected results could be well applied in various industries.""479341,""Bendall, Andrew"
"476640"	"Berini, Pierre"	"Active surface plasmon devices"	"This proposal is devoted to the investigation of active plasmonic devices, particularly amplifiers and oscillators (lasers) operating with surface plasmons. The objectives are: to research and demonstrate low-ASE (amplified spontaneous emission) polarisation-independent surface plasmon amplifiers, to research and demonstrate tunable 1st order Bragg gratings and 2nd order grating couplers, to research and demonstrate tunable surface plasmon lasers, and to fabricate surface plasmon amplifiers, gratings and lasers. Conducting the work described in this proposal will result in significant advances, of interest to the plasmonics and the broader photonics, telecom and sensor communities. A low ASE (noise) amplifier is of significant interest for telecom applications, and a high quality tunable coherent source is of great interest for integration with e.g. biosensors, but also as an independent component given its expected performance (strongly polarised, low noise, broadly tunable). Over the duration of this program, 2 Ph.D. and 4 master's students will complete their studies, and 4 new students will begin. The research outcomes and the highly qualified personnel produced will be of strong interest to the Canadian high-technology industry sector, particularly to the electronics and photonics industries, which are of strategic importance to Canada's innovation agenda.""488346,""Berland, Anne"
"473593"	"Bhattacharya, Binay"	"Design and analysis of efficient algorithms for location-routing problems"	"Efficiency in modern industrial operations requires that resources be deployed in optimal manner. This has resulted in extensive work in operations research on resource allocation with a focus optimal or near optimal planning. I propose to study algorithmic aspects of a combinatorial problem called location routing problem. Location-routing research area encompasses the classical facility location and routing areas. The location of service providers is often influenced by transport costs. Clearly, dealing this problem separately, will often result in suboptimal algorithm. There are a host pf practical industrial problems which can benefit from the theoretical results in this area.We will first build on our research in the areas of facility location and vehicle routing. Significant progress has been made, but still many problems remain in these areas. A particular focus will be on well structured classes of graphs such as partial k-trees. The long term objective of this research remains the goal of integrating the location planning with tour planning aspects taken into account. An additional focus on mobile sensor networks raises novel combinatorial questions arising from the ad-hoc nature of network.""487529,""Bhattacharya, Kankar"
"476131"	"Bochmann, Gregor"	"Software development for distributed applications and scalable execution platforms"	"The general objective of this research project is the improvement of methods and tools for building distributed computer applications. Most of these applications involve communication, either between humans (for instance in the case of telephony and teleconferencing), between humans and computer systems (for instance in the case of typical Web applications, e-shopping, video on demand, and video broadcasting), or between several computer systems (for instance in the case of distributed automated business processes, or in the case where peer-to-peer systems or cloud computing provides processing resources for a large number of popular applications). These applications are realized using distributed databases, computers, smart phones, electronic tablets, and many other devices; and they require complex software for controlling the interactions between these different system components. Our objectives address the two complementary issues: (1) Improvement of the development methods for distributed applications, and (2) improvement of the distributed execution platforms that are used for implementing such applications. Under the first objective, we consider a hierarchical description of the application requirements in terms of collaborations among different parties and their sub-collaborations. Having identified the system components that play the roles of these parties, our automatic development tools should be able to produce the behaviour descriptions for the different components in terms of message exchanges with the other components and local interactions with the user or a database system. Under the second objective, we study peer-to-peer systems and develop distributed management algorithms that would be suitable for using these loosely coupled systems to provide a reliable execution environment for application services ""in the cloud"". ""483958,""Bock, Nicholas"
"480140"	"Boley, Harold"	"Metrics-controlled Web rule translation"	"The Web was initially used to distribute documents for direct human consumption. Recent Web developments such as the Semantic Web and Web Services are distributing formalised knowledge, in particular rules, for machine consumption to assist humans. Formal rules can be employed to extend databases by deriving data that are entailed by stored data according to variously expressive logics.However, even for a fixed logic, different communities use different rule languages and engines, which has led to isolated 'silos' of rule formalisms. Language translators are thus called for to make rules more widely available by enabling interoperation between the silos. But automatic translations are typically not perfect: two languages may realise different logics; an incorrect mapping between language constructs may be used; there may be errors in the implementation of a translator; and there are various other possible problems. Hence, translation quality needs to be addressed in an explicit manner based on metrics. These metrics can then drive systematic translator improvement.This proposal investigates the quality of rule translation through four interwoven research areas: (1) Explore rule languages that are likely to become a standard (e.g., W3C's Rule Interchange Format, RIF), thus reducing the number of translators between interoperation partners. (2) On the basis of these rule languages, study translation techniques and translator implementations. (3) Propose quality metrics for the translators, e.g. combining precision and recall of queries, and give feedback on the languages. (4) Cross-evaluate (1)-(3) via RIF and OpenRuleBench test cases and the Web 3.0 use case WellnessRules2.Metrics, especially quality measures, can act as drivers of innovation in science and engineering. Focusing on rule interoperation with measureable quality, this research will lead to improved rule languages, translators, and applications.""493440,""Bolghari, SeyedehGhazal"
"475463"	"Booth, Kellogg"	"Collaboration technology and multi-user interfaces"	"Interaction techniques for shared displays in collocated environments present a number of challenges. The proposed research examines the affordances of tabletop, large wall-mounted, and hand-held displays to determine how each best supports specific tasks during face-to-face collaboration, and how each can augment the other. Investigations of novel multi-touch interaction techniques and techniques for use at a distance from the display are being conducted, especially in collaborative environments where mutual awareness of each other's workflow must be traded off against interference and distraction by one user's interaction with another's workflow. Privacy and security issues arise in ad hoc collaborations, where mutual trust cannot be assumed. In these situations the usability of privacy features becomes an important concern that is being studied.Classroom and meeting room presentations on shared displays are of special interest. Research in actual classrooms with special-purpose software to support multiple projectors extends standard PowerPoint presentations to much larger screen areas more like traditional multi-blackboard ""chalk talk"" lectures. Specific pedagogical hypotheses and new ways to engage students as active participants, rather than simply passive receptors, will be tested. Novel uses of personal response systems (""clickers"") to support ""hands on"" learning by large groups of students are being designed to examine how technology can enrich classroom experience.Augmented, mixed, and hybrid reality each offer another avenue for research on shared displays. In this case the displays are super-imposed or embedded in physical objects. Interaction techniques that flow seamlessly back and forth between the real and virtual representations of the objects or the information underlying them are being studied. Recent work on multi-projector augmented reality techniques for architectural plans will be continued in a number of directions.""486102,""Booth, Kellogg"
"477417"	"Bouchard, Martin"	"Advanced noise reduction and speech extraction for hearing aids and speech communication devices"	"The research to be conducted from this research proposal is for the development of innovative and efficient solutions for noise reduction and speech extraction in digital speech processing devices. In particular, the emphasis will be on hearing aids, although several of the expected research outcomes are also applicable to other speech devices such as the ones used in hands-free or mobile telephony and automatic speech recognition systems. Hearing aids have been available for a few decades, but the hearing aid adoption rate still remains low today. While there are several socio-economic factors that affect the decision of opting whether or not to use a hearing aid, one factor is that under some challenging yet common acoustic environments, the overall performance of hearing aids has remained fairly poor because of the limited performance of noise reduction and speech extraction algorithms. Other important applications share the same shortcomings as hearing aids: for example mobile telephony devices, Bluetooth headsets, and speech recognition systems, where the performance degrades significantly under some challenging acoustic environments. Even though a very significant amount of work has been achieved in the area of noise reduction and speech extraction over the years, there are still issues to be solved and algorithms to be improved for those systems to work satisfactorily in several difficult practical environments. The new research to be conducted in this project proposes innovations to address some of the main issues limiting the performance of noise reduction and speech extraction systems. A successful completion of the proposed research would go a long way in improving the overall performance of noise reduction and speech extraction technologies, leading to tangible improvements in several applications which are important both from a healthcare perspective and a commercial perspective.""486074,""Bouchard, Mélanie"
"488060"	"BouchardCôté, Alexandre"	"Next generation phylogenetic modelling using machine learning"	"The goal of phylogenetics is to draw inferences about the past from the diversity of the present.   Although phylogenetics is best known from its application to the reconstruction of biological histories (from biodiversity), phylogenetic also has ramifications for the problem of reconstructing linguistic histories (from the world's linguistic diversity).  The program I propose involves both biological and linguistic reconstructions.  I hypothesize that the two problems have sufficient similarities to justify a joint study, but also have sufficient differences to foster innovation.  In both biology and linguistics, the current state of phylogenetic research is stimulating.  Increasingly, researchers have at their disposal datasets of new types, that is, new sources of data such as linguistic records (typological or phonological), geographic locations, results from SNPs studies, and combinations of these.  Increasingly, researcher also have access to datasets of new scales, that is, datasets containing large numbers of species/populations/languages (collectively called taxa), or large amounts of data for each taxon (genome-wide and vocabulary-wide studies), or both.Efficiently using phylogenetic datasets of new types and scales is still largely an open problem.  It is an important problem, since it has the potential to advance our understanding in fundamental areas of science and also to impact biotechnology.  The aim of this program is to unlock the potential of phylogenetic datasets of new types and scales.  The approach will be to apply and build on recent developments in machine learning.  Since phylogenetic inference has challenging characteristics that are only found at the cutting edge of machine learning, this is likely to result not only in innovations in phylogenetics, but also in machine learning.""493101,""BouchardMarmen, Mariève"
"478991"	"Boucheneb, Hanifa"	"Vers l'intégration de méthodes formelles dans le processus de développement de systèmes temps réel complexes"	"La complexité grandissante et le caractère souvent critique des systèmes temps réel (systèmes de transport, systèmes de production, systèmes de communication, etc.) rendent indispensable une démarche de conception et de validation rigoureuse permettant d'en assurer la maîtrise, tout au long de leur cycle de vie. Les méthodes formelles, parce qu'elles s'appuient sur une base mathématique rigoureuse, sont largement reconnues comme étant une alternative incontournable pour développer des systèmes fiables. Cependant, l'intégration de ces méthodes dans les processus de développement dirigé par les modèles, utilisés en industrie, se heurte à plusieurs problèmes. Selon les concepteurs de systèmes, les méthodes formelles sont difficiles à comprendre et à appliquer, peu expressives, trop complexes ou mal adaptées aux besoins de spécifications. Par ailleurs, leurs techniques de vérification sont complexes, indécidables, souffrent du problème d'explosion combinatoire ou se limitent à des systèmes finis. Dans ce contexte, ce programme de recherche propose d'investiguer l'intégration de méthodes formelles dans le processus de développement de systèmes temps réel. Il vise, d'une part, à banaliser l'utilisation de ces méthodes en les enfouissant dans un processus de développement et, d'autre part, à parfaire et à étendre leur utilisation à des systèmes temps réel de plus en plus complexes. L'atteinte de ces objectifs généraux implique la réalisation des objectifs spécifiques suivants :1)    )Établir / adapter / utiliser des techniques sûres de transformation de modèles de conception (UML, SysML, AADL, etc.) en modèles formels. 2)    )Concevoir et développer des techniques de model-checking ou de synthèse de contrôleur plus efficaces permettant une meilleure scalabilité que celles proposées dans la littérature. 3)    )Investiguer la combinaison de techniques de model-checking / synthèse de contrôleur et de preuve de théorème. 4)    )Implémenter, tester et valider les approches de vérification proposées.""485688,""Boucher, Christina"
"488086"	"Bouguessa, Mohamed"	"Knowledge discovery from online social network"	"The increasing amount of communication between individuals in e-formats (e.g. email, instant messaging, blogs, etc.) has motivated computational research in social network analysis. Social network analysis techniques aim to search communities of shared interests or leaders within communities. Social networks are often represented as graphs, where nodes represent individuals and edges represent the relationship between them. Such graphs are massive, in which node may contain a large amount of text data. Many existing social network analysis techniques focus either on the social network topology measured by communication frequencies or the content generated by the users. However, neither information alone is sufficient for finding accurately communities of shared interests and leaders within communities. The information in the text and the linkage structure re-enforce each other, and this leads to higher quality result. In addition to this, existing social network analysis techniques are only effective in analyzing graphs which are from a single source and relatively complete. Furthermore, most existing approaches assume that the structure of the network is static. However, online social networks change continually and links within the network come from different online sources. For example, consider links from Usenet to the blogosphere, links between tweets and news articles, etc. There are also some applications in which the whole network is not available at one time, but available in the form of continuous stream. Such applications create unique challenges, because the entire graph cannot be held in main memory. What is needed to make social network analysis more effective is to develop techniques that take into account textual content, uncertainty, incompleteness, heterogeneity of data sources and the need of developing specialized algorithms for Web applications that involve continuous stream of edges. Our goal is to address these issues by developing appropriate models and algorithms for mining effectively online social networks.""482179,""Bouguila, Nizar"
"476133"	"Boukadoum, AMounir"	"Heuristic-based and nature-inspired techniques for system design and overall data processing"	"Current technological advances are leading to evermore complex systems, bringing forth modeling and analysis problems that were either marginal or ignored in the past. A need has developed for new approaches and tools to improve current practices for system design and data processing algorithms in situations of too many degrees of freedom, nonlinearities, noise, auto-adaptation, context aware learning and decision making, etc.    Our research aims to study heuristic-based and nature-inspired techniques for system design (both hardware and software), and overall data processing. It relies mostly on artificial neural networks (ANN; neuro-inspired), particle swarm optimization (PSO; etho- and evolution-inspired), artificial immune systems (AIS; bio-inspired), and fuzzy logic (cognition-inspired). Our postulate is that most system design, modeling, and analysis problems can be represented as classification or optimization problems for which natural equivalents exist and have been solved over the course of evolution, or that they can be handled by unconventional formalisms. We aim to complement current formal techniques when they prove to be too slow, prone to combinatorial explosion, or not suited to readily exploitable closed form solutions. A complementary aspect of our research includes the study of neurologically-inspired cognitive processes for inclusion in artificial intelligent entities (self-adaptive sensors, virtual animats, physical robots...)    We will validate our results on various applications, of which intelligent design tools, sensors and sensor networks, substance and pathology detection and identification with ANN, non-linear component characterization and circuit design with ANN and PSO, design fault detection and correction with AIS, heuristic approaches to task allocation and scheduling in MPSoC design, and a glucose monitor implant. We expect out research to advance the state of the art in the areas of soft programming and intelligent signal processing, and have useful applications for computer aided design, sensor design, hardware and software design, substance and pathological condition detection, and artificial intelligence creation.""484906,""Boukerche, Azzedine"
"479557"	"Broucke, Mireille"	"A control theory for complex specifications"	"Control theory is a discipline bridging mathematics and engineering and its function is to supply the theories, frameworks, principles, and algorithms for analysis and control synthesis problems arising in a spectrum of application disciplines. Its clients include energy systems, biomedical engineering, mechatronics, robotics, aerospace systems, nanotechnology, transportation systems, process control, manufacturing, biology, ecology, economics, and other areas.Control theory has focused over the last 200 years on problems of regulation and tracking. Common examples of regulation are maintaining a constant temperature in a building or maintaining a desired speed in a vehicle cruise control. Optimizing a solar cell performance by rotating its surface to continually face the sun during daylight hours is an example of tracking. Tracking and regulation are simple control system specifications.  As control systems become more integrated within high-end engineering systems as well as consumer products, they are expected to achieve richer specifications that intertwine regulation and tracking with safety requirements, startup procedures, logic rules, and so forth. Good examples of complex specifications are the startup of a chemical or nuclear reactor, requirements for an automated parking system, or transition between different gaits in a bipedal robot. Because of the historical emphasis on regulation and tracking, even the simplest control problems with non-traditional specifications have no known solution method.      Our research program concerns the development of a control theory for complex specifications. This is a multistep endeavor involving both the discovery of fundamental mathematical structures within a control system that determine if it can achieve a complex specification, as well as the development of concrete design methods that can be used by practitioners in diverse application fields. ""489253,""Brough, Luke"
"478567"	"Brown, Lyndon"	"Spectral analysis of non-stationary signals and applications"	"Spectral analysis is one of the most fundamental analysis tools available to scientists. Despite its ubiquity, there are inherent difficulties with this tool. Traditional spectral analysis tools can have difficulty with signals whose spectrum changes quickly with time.  A recent major advance in the field of spectral analysis is the Hilbert Huang transform (HHT). This algorithm uses a heuristic algorithm called empirical mode decomposition (EMD), to decompose the signal into sums of 'narrowband' signals for which a good solution to the spectral analysis problem, the Hilbert Transform, exists. Several significant limitations have been identified with the algorithm. It is computationally expensive. The algorithm is not able to separate signals with frequency differences of less than 10%. The algorithm can fail to separate signals with low relative energy. Artifacts are introduced into the signal at the end points. The researcher has developed an alternative means of decomposing that overcomes these limitations. Most significantly, this approach is not computationally more expensive than the fast Fourier transform (FFT), and for signals with sparse spectral components, can be significantly more efficient than the FFT. This approach, however, also presently suffers from limitations. These include a limit on the number of spectral lines that can be present in the signal and the necessity of good initialization of this locally convergent algorithm. The two main goals of this research are to  reduce and eliminate the existing limitations of this algorithm as greatly as possible and apply this new approach to applications such as power systems frequency and impedance measurements and resistance measurement in resistance spot welding. The results of this research will be a spectral analysis tool as powerful as the HHT but with much greater frequency separating power, and the computational cost of the FFT. Its application will solve the long standing problem of measuring weld voltage for resistance spot welding of vehicles. This has the potential of increasing the quality and safety of automobiles, of creating savings of millions of dollars per year and giving Canadian manufacturing plants a competitive advantage against international competition.""491550,""Brown, Marc"
"481879"	"BurtonJones, Andrew"	"Expanding conceptual models used in information systems analysis with temporal concepts"	"In the early stages of developing information systems, systems analysts often create conceptual models. Such models include constructs and rules for describing a domain (usually in graphical form), free of design and implementation concerns.  Conceptual models are vital because they help analysts understand user requirements and communicate them to designers. Research shows that understanding user requirements is the key to successful information system design.Good conceptual models represent domains clearly and completely.  However, while time is a feature of all domains, and methods exist for representing time in systems design, most notably in temporal data models, no detailed conceptual modeling method exists for representing time.  This is alarming because time is a critical part of almost all domains and, thus, should be explicitly represented in conceptual models.  Extending prior research on modeling time (especially in temporal data modeling), this research will undertake five studies that, together, 1) identify what elements of time must be represented in conceptual models, 2) develop a method for representing these elements, and 3) empirically validate the method's usefulness and usability.The research will involve theoretical work (applying theories of ontology and cognition), design (creating conceptual modeling methods), and testing (evaluating the methods empirically).  The intended contributions of the research are:- For research: providing the first theoretically-grounded and empirically-validated conceptual modeling method for representing time- For practice:  contributing a method that will help analysts to understand the temporal aspects of organizations and, thereby, enhance their ability to design systems that meet organizations' needs.""483439,""Buschmann, Michael"
"485093"	"Chaharmir, MohammadReza"	"Investigation of beam scanning in reflectarray using tunable elements"	"Miniaturized and integrated high performance (multi-standard, multi-band) antenna systems, development of novel intelligent (adaptive) and reconfigurable antenna array structures, flexible antenna systems, and conformal structures are among the key enabling technologies for future generation wireless information systems and sensor networks. Recently, antenna engineers and scientists have become more interested in the Millimeter Wave (MMW) bands for future broadband commercial communications. MMW systems are the next generation of wireless technology that can provide up to multi-Gbps wireless connectivity for short distances between electronic devices. MMW imaging has also become an active field of research due to the combined features of high resolution and penetration at MMW band. It has potential applications in public security and non-invasive parcel inspection.There is an ever-increasing demand for microwave and mm-wave RF front-ends, which are reconfigurable, show increased performance, and are also affordable. In this framework, the main general objective is to investigate the possibility of designing relatively low-cost electronic beam steering reflectarrays that have the high reliability and resolution required for MMW applications.Reconfigurability can be accomplished by employing solid state tuning devices (pin or varactor diodes, FET) or MEMS technology. In parallel, control algorithms and their digital hardware implementations will be developed in order to perform beam calibration and real-time steering.""482590,""Chahine, Richard"
"475748"	"Chan, ChristineWaichi"	"Development of a method for ontology-driven knowledge engineering and its application to the greywater reclamation system"	"The objective of the proposed research program is to study use of ontology for supporting the development of semantic web applications and to develop a novel method for ontology-driven knowledge engineering.  The method will provide guidance for knowledge and software engineers in developing web-based  knowledge-based systems.  As a case study, the method will be applied to the development of an intelligent monitoring system for the water reclamation system. The proposed research program will enhance understanding of how the use of an ontology can be effective for supporting development of a web-based knowledge-based system.  The methods and systems developed would contribute to building information technology infrastructure necessary for supporting automation of industrial process systems in the energy and environment sectors, which is important for enhancing productivity of the Canadian industries.The application focus on the domain of greywater reclamation is timely nowadays when more than 50% of the world's population will soon be facing water shortages.  Greywater reclamation is the process of treating soiled water from sources such as kitchen and bathroom sinks, so that the treated water can be used for irrigation, toilet flushing etc.  Many cities in Canada are facing increasing sewage loads, to the extent that the sewage infrastructure is reaching maximum capacity and city growth is severely limited.  The decision support system for monitoring of the greywater reclamation system will help operators better monitor the reclamation system, identify operational problems in a timely manner, and avoid downtime and costly maintenance.  As a result, operation of the greywater reclamation system will be more stable and cost-effective, and higher quality reclaimed water will be produced.  This would help reduce both fresh water consumption and loads on city sewer systems, enabling growth and expansion of Canadian cities.  This has significant socio-economic and environmental benefits for Canadians.""486452,""Chan, ChunShing(Aaron)"
"479488"	"ChanCarusone, Anthony"	"Energy-efficient I/O for supercomputing"	"Supercomputing infrastructure has reached an industrial scale.  The video, search, and cloud computing services upon which our modern economy relies are delivered from warehouse-sized facilities housing thousands of individual servers. The energy consumption of these facilities is enormous, both in economic and environmental terms.  Unfortunately, energy-efficiency has not been a primary concern in the design of these distributed supercomputers. Consequently, they operate at 15-65% of their peak energy efficiency most of the time.     This project will improve the energy-efficiency of distributed supercomputing environments by targeting the interconnections within them.  Without progress on the digital input/output (I/O) circuits at either end of these interconnects, their share of overall server power will increase to over 50% in the next decade.  We will develop I/O integrated circuits and subsystems that are capable of near-zero energy consumption when idle, sub-nanosecond wakeup times, and scalable to meet the demands of the future.     The massive industrial scale of parallel supercomputing infrastructure makes the impact of this research dramatic, both economically and environmentally.  Consider the central processor in a single high-end compute server, which by 2020 will demand an aggregate I/O bandwidth of 1TB/s.  Using today's technology, the I/O alone will consume roughly $800 in electricity over the computer's 5-year lifetime.  This research project can offer a $750 energy savings from a single computer - more than the total CPU cost.  For the environmentally minded, this represents a savings of 7500 kg in CO2 emissions.  With thousands of such servers in each data center, the worldwide numbers are staggering. The total energy per year consumed by compute servers is 220 TWh, roughly 10% of which is attributable to I/O.  Hence, even research that improves I/O energy efficiency by only 1% in these installations yields a savings equivalent to the average electricity consumption of 20,000 homes.  Our research promises improvements far exceeding 1%.""485774,""Chanda, Debashis"
"476222"	"Chandra, Ambrish"	"Development of controllers for renewable energy conversion systems with power quality improvement"	"Wind  energy, due to its free availability and clean and renewable character, ranks as the mostpromising energy resource that could play a key role in solving the world energy crisis. According to Canada Wind Energy Association (CWEA), Canada's wind energy industry has already broken its annual growth record in 2009 with 950 MW of new wind energy capacity, with a total capacity of 3499 MW till September 2010. The CWEA in its 'WindVision 2025' document sets a goal of producing 20% or more of the country's electricity from wind power by 2025. Due to significantly higher and more constant wind speeds, reduced planning restrictions, and the shortage of suitable sites for wind farms on the land, off-shore wind farms are becoming very attractive.  Europe has held the lead in off-shore wind, having installed more than 830 turbines with grid connections to nine European countries. Canada has also started investing in off-shore wind farms with many projects in line. To transit such bulk power over great distances, in sea,  creates challenges for both system operators and wind farm developers, apart from inherent wind's intermittent nature. From the off-shore collection point to on-shore grid, VSC-HVDC technology has many advantages over other transmission links such as, HVAC or HVDC with thyristor-based line-commutated converters (LCC). It is becoming more and more important to develop advanced control techniques so that the integration of off-shore wind farms with the on-shore grid is more reliable, efficient and free from power quality problems. Therefore, the objectives of the proposed research work is 1) to conceive, analyze/evaluate, fabricate/test proper converter/inverter topology and its control for VSC HVDC system (simulated on a real-time digital simulator - RTDS), for various types of off-shore wind farms. 2) to simulate an on-shore power system network on a RTDS. 3) to develop proper control techniques to integrate off-shore wind farms (of objective 1) with an on-shore power system network (of objective 2). 4) Performance evaluations of integrated off-shore wind farms and on-shore power network for various operating conditions. ""487947,""Chandra, Pooman"
"481659"	"Chen, Jie"	"Developing low-intensity pulsed ultrasound circuits to enhance cellulosic bioethanol yield for renewable energy"	"The global economy has been growing at an unprecedented rate over the past decades. With the increasing detriments of rampant fossil fuel usage, renewable energy is urgently needed. Biofuel is one of the most promising alternative clean energy sources in addition to solar and wind energy. However, the bioethanol conversion rate is too low, which limits the biofuel to be a viable solution to renewable energy. Over the past five years, the principle investigator (PI) has collaborated with researchers in the Faculty of Medicine to discover that  low-intensity pulsed ultrasound (LIPUS) can promote dental tissue regrowth, gene delivery, and stem cell proliferation. The PI's role in these projects is to lead the design of the prototype LIPUS generation circuits, and implement them first on printed circuit board and then miniaturize the design towards nanoscale system-on-chip design. Our research has resulted in several papers, two patents and several news publications (including the cover page report in ""Globe and Mail"" on June 28, 2006, and featured in 2006 Reader's Digest Canadian Medical Breakthroughs). The LIPUS patent has been licensed by a Canadian company. Recently, we discovered that the LIPUS technology can also stimulate microorganism growth by at least 30% over non-ultrasounded samples. The microorganism investigated is T. Reesei, the cellulosic enzyme producing fungi commonly used in converting plant biomass to bioethanol.In this NSERC grant, the PI will focus on optimizing the LIPUS design to increase bioethonal production. In order to scale-up the design for processing large amount of biomass, an industrial-scale fermentation tank is required. The PI is going to custom-design low-power LIPUS generation circuits to make the process more efficient. In addition, the PI is going to design sensing circuits to monitor the fermentation process. The resulting design will greatly impact biofuel industry. This research also offers a unique opportunity for engineering students to apply engineering methods to the emerging field of renewable energy.""484942,""Chen, Jing"
"479806"	"Chen, Liang"	"Theory of electoral college framework-based multi-classifier ensembling and its applications to subjective pattern recognition"	"The proposed research will be applied to a class of pattern recognition applications, which we call subjective pattern recognition, with face identification and content-based information retrieval as typical examples. We know very little about how our brains process this type of information; indeed, the standards for judging the similarity/dissimilarity are subjective -- manpower is usually required to verify any conclusions made by machine. Although new algorithms are developed every year, much of the research follows a general ""trial-and-error"" procedure: Given a data set, for a known algorithm, we try various parameters; if we are not satisfied with the accuracy, we try a new algorithm; and then for the new algorithm, we try various parameters; and so on.  The proposed research will lead to a new type of algorithm for subjective pattern recognition, which has predictable high stability and therefore is guaranteed to perform (i.e. accuracy) at a high level.    The research proposed is closely related to the Electoral College (EC). The EC voting format has been used for many years in political elections: a nation is partitioned into regions; the winner of each region is determined by a majority of its voting population; the final winner is selected according to the weighted sum of each candidate's winning regions based on the winner-take-all principle. It has also been used in many areas of scientific research. This research attempts to develop a model, our so-called EC framework-based multi- classifier ensembling, to improve the regular EC format in that general decision making approaches rather than simple vote counting will be used in each region for local decision making, and advanced ensembling technology rather than simple winner-take-all rule will be used for combining local decisions into final ones.    On the theoretical front, our model will be the first that integrates classifier ensembling techniques with the EC framework; it will also be the first that illustrates the stabilities and applicabilities of the EC framework- based multi-classifier ensembles. The proposed research will establish guidelines for adopting our model in pattern recognition applications, particularly in subjective pattern recognition.""478479,""Chen, Liang"
"488228"	"Cobzas, Dana"	"A diffusion-based statistical shape model for medical image analysis"	"Medical imaging plays a key role in modern medicine, being part of  many diagnoses and treatments. While the imaging hardware and acquisition methods have seen a tremendous development in the last decade, this is not matched by the image processing software. Therefore, in clinical practice, much image processing work is still performed manually due to inadequate software. To realize the full benefit of these new medical imaging methods, new and better image analysis methods need to be developed.We propose a medical image analysis framework based on modern computer vision techniques. Our methods make use of anatomical similarity between humans parts and organs to define a shape model. This shape model is defined in a continuous way and captures both global and local variability between individuals. We will use this shape representation to develop robust medical imaging analysis methods and to statistically study disease-induced changes in anatomical shapes. This will provide a way to better understand the influence of certain diseases. This proposal introduces both theoretical advances of main medical image processing methods (segmentation, registration, shape analysis) as well as practical modern solution for multiple sclerosis and liver cancer though our medical collaborators. In addition, it opens the opportunity of training graduate students in an interesting multidisciplinary environment.""492175,""Cocchiarella, David"
"475326"	"Cockburn, Bruce"	"Hardware-accelerated prototyping, characterization and optimization of multiple-antenna communication transmitters and receivers"	"The main objective of the proposed research programme is to develop, extend and demonstrate a methodology for using field-programmable gate arrays (FPGAs) to greatly accelerate the simulation of wireless communication systems.  The designers of multiple-antenna wireless communication systems face a rapidly escalating challenge where the designs of proposed transmitters and receivers must be verified and optimized over a wide range of posssible design configurations and wireless channel conditions.  The performance of each candidate design must be verified using long-running simulations with pseudorandomly generated data communicated over realistic but complex models of the wireless channel.  To ensure statistically meaningful bit and and symbol error rate (BER and SER) measurements, the simulated communication systems must process an extremely large number of transmitted data bits.  This situation is made worse by the use of powerful error correcting codes, such as Turbo codes, which further reduce the effective BER and SER for the same channel conditions and hence lengthen the required simulations.  Consequently conventional software-based simulations can easily take many days and even weeks of time on even the fastest computer workstations.The proposed research program investigates the use of hardware emulators, implemented as synthesized circuits on FPGAs, to accelerate the simulation of baseband wireless systems by several orders of magnitude.  The resulting simulation times will be much less than the times required using conventional software-based simulation models on computer workstations.  With the FPGA-accelerated prototyping methodology, the performance of proposed wireless systems can be measured and verified much faster over a larger number of possible scenarios.  The hardware-accelerated simulation methodology should allow manufacturers to better verify and optimize the much more complex, upcoming wireless products before the designs are finalized, and then bring them to market faster and with a greater chance of success than would otherwise be possible.""484545,""Cockburn, Jeffrey"
"477932"	"Condon, Anne"	"Prediction and design of nucleic acid secondary structure and folding pathways: from theory to tools"	"My long-term research goals are to provide effective computational methods for predicting function of nucleic acids - DNA and RNA molecules - and for designing nucleic acids with novel functions. Prediction methods are essential to biologists who wish to better understand cellular RNA function, such as the central roles of RNA in gene regulation, and causes of malfunction. Design methods enable scientists to create new molecules with useful functions, e.g., in disease diagnosis and therapy. My research particularly aims to advance the nascent field of molecular (DNA and RNA) programming, which makes possible scalable, general-purpose molecular designs that could transform the ways in which we monitor and mediate molecular dynamics within the cell or other micro-environments.Molecular function derives from structure, i.e., molecular shape, and from folding pathways, i.e., sequences of structural changes over time in a changing environment. Thus two short-term objectives of my proposed research are to provide accurate models of the thermodynamic and kinetic processes that dictate how molecules fold to form their structures, informed by the latest experimental data, and to develop algorithms which use these models to predict structure and folding pathways. A distinctive feature of our approach is the linking of thermodynamic model inference and algorithms development, creating a feedback loop that strengthens both. A third objective is to understand how to perform simple logical functions with DNA strands in a way that minimizes the number of strands used, thereby reducing cost, waste and probability of error. Because of the central roles that nucleic acids play in proper functioning of the cell and in disease, because accurate structure and folding pathway prediction methods will accelerate understanding of these roles, and because nucleic acid design has the potential to yield new molecules with diagnostic and therapeutic functions, my work can ultimately be of broad benefit to biologists and health scientists.""473750,""Cone, David"
"474276"	"Cook, Stephen"	"Computational complexity and logic"	"I plan on continuing my research in both computational complexity and proof complexity.  My current work in complexity theory is motivated by the problem of separating logarithmic space (deterministic and nondeterministic) from polynomial time.  My collaborators and I have formulated a computational problem (the Tree Evaluation Problem), for which we have a conjectured space-optimal algorithm requiring superlogarithmic space.  We use the branching program model of computational space, for which a superpolynomial lower bound on the number of states implies the desired separation.  We have proved such a lower bound for restricted branching programs, and will work toward gradually removing the restrictions.In proof complexity I will continue working with my students on projects motivated by understanding the complexity of concepts needed to prove combinatorial theorems.""474475,""Cook, Wade"
"473965"	"Cormack, Gordon"	"Evaluation and improvement of search tools for critical information"	"We investigate the effectiveness of using search technologies to identify, as nearly as possible, all relevant documents within a very large electronic dataset, while at the same time identifying no irrelevant documents. Applications include spam filtering, legal search, medical informatics, law enforcement, security and intelligence. For these applications it is necessary not only to identify many relevant documents and few irrelevant ones, but to estimate how many of each are identified, and how many are missed.The crux of the problem is to measure precisely and accurately the effectiveness of approaches for their intended purpose. We propose to design and deploy an evaluation appliance suitable for measuring the efficacy of search tools, without intervention from the user. An evaluation appliance allows different technologies and methods to be compared in parallel experiments, including some that are embedded in sensitive processes to which evaluation experts have no direct access.The anticipated outcomes of this research are: precise measurements of the effectiveness of current technologies for exhaustive search; improvement in the state of the art in technologies for exhaustive search, demonstrated by precise measurement; tools to provide prior estimates and ongoing estimates of the effectiveness of particular search strategies for specific tasks.""493932,""Cormier, Derek"
"474612"	"Czyzowicz, Jurek"	"Mobile agents algorithms for networks and geometric environments"	"In distributed computing portions of a computational task are assigned to several processors. To accomplish the goal of such teamwork, some means permitting the exchange of information between the participating processors are usually in place. One possibility is to send messages between the stationary processors located at some nodes of the network. Another option is to admit processors moving along the edges of the network, or travelling inside some geometric environment. Such mobile processors (agents) may exchange the information when they meet (or they are sufficiently close to each other) or they leave the information at some places of the environment so that it is available to other agents visiting such places in the future.      The subject of this research project is the study of algorithms for mobile agents working in networks as well as in the geometric environments. The main objectives of the project concern the feasibility of the fundamental problems of for mobile agents as well as the efficiency of their algorithmic solutions. The measures of efficiency involve algorithmic time complexity, memory and power used by the agents. The problems include collective exploration of the environment, rendezvous or gathering, pattern formation (moving of the agents in order to form a particular arrangement in the environment, e.g. to permit execution of some specific task).      The applications of mobile agents are numerous. In robotics the mobile devices perform a task of terrain exploration or some target search. The software agents migrate over the network and their typical objective is  remote data collection. Other applications come from wireless and ad-hoc networks, nanotechnology, medical computing, distributed spatial control, and many others.       The results of this project will permit the deployment of more efficient teams of mobile agents in distributed computations, resulting in reducing network traffic and improved network reliability. The feasibility results will provide a better understanding of what is achievable in the field of mobile agents computations.""478805,""Daayf, Fouad"
"474511"	"Dehne, Frank"	"Auto-tuned parallel algorithms for hybrid multi-core/many-core processor clusters"	"The main goal of parallel computing research is to create enabling technology for solving data intensive and/or computationally hard problems in the Natural Sciences, Engineering, Medical Sciences, and Social Sciences. My research objective is to contribute towards that goal by developing general parallel algorithm design methodologies and parallel algorithms for specific problem areas.For the next funding period, I propose to study auto-tuned parallel algorithms for hybrid multi-core/many-core processor clusters. Such hybrid processor clusters consist of nodes that contain both, multi-core and many-core (GPU) processors. The focus of our proposed fundamental research will be on non-numerical problems such as computational geometry and graph algorithms. These problems often involve irregular data movements and complex data structures that are particularly challenging for parallel computing. I propose to extend this parallel algorithms research for hybrid clusters by integrating the study of auto-tuned parallel methods. The goal of auto-tuning is to design efficient parallel algorithms and software that adapt automatically to different hardware configurations. Rather than developing new parallel algorithms for every new parallel architecture, auto-tuning aims at designing portable parallel algorithms and software for a large class of current and (hopefully) future parallel machines. The auto-tuning approach is currently most advanced for linear algebra and other matrix based problems but there is very little work published so far on auto-tuning for non-numerical problems such as computational geometry and graph algorithms proposed for this project. Due to the irregular data movements and more advanced data structures, auto-tuning parallel algorithms for these problems will be considerably more challenging. To demonstrate the significance of our research and facilitate knowledge transfer, this project also has a research component on parallel scientific computing, focusing on the design and implementation of an auto-tuned parallel protein interaction prediction algorithm for hybrid processor clusters.""475695,""Deibel, Donald"
"474848"	"Dimopoulos, Nikitas"	"Towards exascale computing systems"	"The proposed research program will focus on issues pertaining to Exascale Computing that is, computing systems attaining performance that is 1000x the performance of the present day PFlop/s systems.Specifically, we shall focus on methods that will contribute in the realization of such systems through performance enhancing techniques applicable to interconnection networks, scheduling, and low power design All are critical issues and are researched intensively internationally.In a cluster system, the interconnect plays an important role in delivering data to the computations. Our research program will develop methods that deliver data to the computations carried out by the many cores of the system with a minimum of latency. We shall investigate both the send and the receive sides of the communication channel. At the send side, prediction will allow the hiding of the setting-up-the-channel overhead while at the receiving end, directly injecting the payload to a section of the storage hierarchy that is the closest (and the fastest) to the computation that will consume it ensures minimal delays.Concomitant to the delivering of data to the computation, is the scheduling of these computations especially in the era of many-core processors. For maximum efficiency, one needs to ensure that a computation scheduled on a particular core is not waiting for data, or instructions. Our objective is to develop methods ensuring that computations are optimally scheduled on the available resources.As far as power is concerned, our objective is to develop a design environment that will automatically produce low power systems utilizing the information that is present within the description of the computation. We are developing techniques that identify the portions of the system that are not active during portions of the computation, so they can be switched-off. We are targeting this environment for embedded systems, but also to specialized computation engines that will be part of hybrid nodes as we move to address the challenge of exascale systems.""491361,""Dinavahi, Venkata"
"475674"	"Dssouli, Rachida"	"Dynamic composition of systems and services"	"The main objective of this research proposal is to contribute to the growing research activities in the domain of systems' composition taking into account their proprieties and semantics. More precisely, the aim is to generate valid compositions of systems that preserve desirable proprieties at run time. In this project we will be focusing on systems' services, web services as a mean of high level systems' composition, and mobile service composition.The outcomes of this research will be frameworks, approaches and formal methods based development lifecycle to achieve accurate, reliable, efficient and dynamic real-time composition of services. Research topics will address challenges in web services, mobile services, and systems composition.The long term objectives of this research are: the composition of systems using complex models, establishment of sound theories, and development of synthesis, verification, validation and testing algorithms for dynamic composition.The short term objectives are to develop approaches for the composition of web services and mobile application services that take into account user functional and nonfunctional requirements. I would like to validate some of the outcomes by experimentation and to apply the approaches to real applications, such as safety critical services and to develop mobile services. I would also like to develop mobile services to support equitable business in rural areas of developing countries. NB! This abstract is different from the one of the 180 form.""495144,""Du, ChenFei"
"478986"	"Egli, Richard"	"Meshfree physically-based animation of natural phenomena in computer graphics"	"Title: Meshfree physically-based animation of natural phenomena in computer graphics   My research program for the next five years will focus on a mesh-free method for physically-based animations of natural phenomena. We recently developed a new and simple kernel-based method for solving partial differential equations. My plan is to further extend the scheme in order to apply it to simulating natural phenomena dynamics in computer graphics such as fluids, solids and cloths.     Smoothed Particle Hydrodynamics (SPH) is popular for simulating liquids due to its simplicity when considering the simulation and the visualization of the fluid. SPH is also considered as being well suited when dealing with irregular domains and topological changes. But using SPH when dealing with sharp domain edges does not give as accurate results as it does with mesh-based variational methods. Hence particle-based methods for simulating fluids were enhanced with meshes to deal more accurately with sharp boundaries.    Our Mesh-free Kernel-based method is more accurate than the SPH approximation, yet its use offers a similar simplicity. In the proposal, I emphasize simplicity and ease of use, believing that it is a key ingredient in having a true impact especially in the field of interactive applications. In the short term, we will work on more accurate computation near the boundaries, without having to resort to a mesh. This is possible with the method due to a promising feature of the method: its flexibility and in particular, the possibility to consider an upwinding of the kernels.     Finally, I will generalize the method to encompass other matter than fluid. Particle-based methods have been used for simulating unified models, which facilitates the interactions between different types of matter (Fluid-Solid, Fluid-Cloths, etc.).  It is also important to mention that, regarding interactive applications, this method can be used to develop animation algorithms, and will adequately map to the Graphics Processing Unit (GPU).""480853,""Eglington, Bruce"
"481422"	"Eizenman, Moshe"	"Eye tracking systems"	"The long-term objectives of my research program are to develop clinically useful non-invasive methods of recording and analyzing normal and abnormal eye movements and visual scanning patterns, in infants, children and adults. The systems and methods that my group is developing are optimized for studies in ophthalmology, neurology and psychiatry.  Remote eye tracking systems that do not require user calibration procedures (calibration free) have a limited tracking range and cannot operate reliably outside the laboratory (due to sunlight interference). We plan to develop distributed, calibration-free eye tracking systems with extended tracking ranges that can operate reliably both inside and outside the laboratory.  The development is based on recent work by my group that established the theoretical foundations for calibration-free gaze estimation systems and novel methods to estimate the orientation and position of eye-features in 3D-space.  In the next five years we plan to use our advanced eye-tracking systems to measure objectively and accurately eye misalignment in infants and young children. Early detection of eye-misalignment can support early diagnosis and treatment that can help maximize the potential of the visual system. The indoor-outdoor (universal) eye-tracking system will be incorporated into an assistive communication device to enable severely motor impaired individuals to communicate through the use of their eye movements. The methods that we are developing to analyze visual scanning patterns will be used to study a subconscious surrogate of mood disorders, attention bias, that might provide an early indication for treatment efficacy in patients with depression.Graduate students who participate in my research program have excellent opportunities to be trained in interdisciplinary research and to develop expertise in signal processing, detection and estimation of biological signals, control of eye movements and vision.""477616,""Ekeland, Ivar"
"478601"	"ElSakka, Mahmoud"	"Model-based image segmentation"	"Image de-noising and image segmentation represent a crucial part in many image processing and computer vision applications. The goal of de-noising is to smooth homogenous regions of an image while preserving the region boundaries (i.e., edges). Meanwhile, segmentation techniques aim to extract the boundaries of homogenous regions. In traditional pipelined de-noising/segmentation schemes, the segmentation step may suffer from a loss of vital information due to the de-noising step, or due to the excess of noise. Combining these two steps should preserve much of this lost information compared to the traditional methods.        In this research, we plan to combine speckle image de-noising and image segmentation processes in an iterative fashion at various levels of granularity, rather than simultaneously. This way, de-noising and segmentation sub-results would be utilized by both schemes at all levels. This should facilitate the ability to smooth irrelevant details from the image while enhancing and successfully segmenting the desired object. In addition, we plan to incorporate prior knowledge and optical flow information to the image segmentation/de-noising process. This direction should improve the accuracy of the produced segmentation results, especially when the object of interest has weak edges. We will also plan to improve the de-noising diffusion process by adaptively calculating diffusion coefficients. This step would improve de-noising results at a moderate complexity.          The proposed combined segmentation-diffusion approach will be applied on real echocardiographic and carotid ultrasound images. These images tend to be plagued by speckle noise and other anomalous artifacts due to the sporadic nature of high frequency sound waves. Hence, they present a perfect environment for validating and testing our proposed research. Our proposed schemes would help improve the reliability of various clinical measures such as a left ventricular ejection fraction or a carotid artery stenosis assessment. The proposed segmentation schemes will be also validated and tested on synthetic images with simulated speckle noise.""479259,""Elsalakawy, Ehab"
"487970"	"Emadi, Ali"	"Novel switched reluctance propulsion motor drives for electric and hybrid electric powertrains"	"There is an evolving paradigm shift in transportation toward more environmentally friendly vehicles with electric and hybrid powertrains. In fact, powertrain electrification results in more efficient, higher performance, cleaner, and safer vehicles. However, major technical challenges for the mass commercialization of electric, hybrid, and plug-in hybrid electric vehicles (EVs, HEVs, and PHEVs) include the development of electric propulsion motor drives that are low cost, reliable, rugged, fault tolerant, and scalable. The proposed research program is focused on a novel family of switched reluctance motor (SRM) drives to overcome these major challenges. The proposed research objectives are aimed at developing SRM drives with excellent traction performance and lower cost compared to the currently used permanent magnet brushless and induction motors. The absence of windings or permanent magnets on the rotor and simple machine construction make SRMs lower cost and their high peak torque-to-inertia ratio and wide speed range make them well suited for propulsion applications. However, high torque ripple, acoustic noise, and vibration have prevented the widespread use of SRMs. Furthermore, conventional SRMs have lower power density compared to the permanent magnet brushless machines. Therefore, there is a desire to reduce torque ripple and acoustic noise and improve torque production of SRMs. The proposed new SRMs have a higher number of rotor poles than stator poles. Such novel stator and rotor pole combinations provide SRM designs with higher torque, lower torque ripple, and better performance while also being inherently less expensive for high volume production. The proposed SRM technology has several advantages compared to the conventional known SRMs. These benefits include higher power density, lower torque ripple, higher efficiency with lower copper loss, and improved thermal performance. It is expected that these significant improvements will lead to the development of advanced low cost, reliable, and fault tolerant SRMs for propulsion applications of EVs, HEVs, and PHEVs.""477726,""Emadi, Ali"
"487987"	"Falk, Tiago"	"Distributed quality-aware multimedia communications"	"The multimedia communications industry is going through a phase of rapid development and new services are emerging continuously, such as mobile video telephony and internet protocol television. Additionally, multimedia services are increasingly being transmitted through heterogeneous networks which generate signal degradation combinations that adversely affect the end-user's perceived quality of experience (QoE). Traditionally, subjective tests have been used to characterize multimedia QoE. Subjective tests, however, are not suitable for online applications and objective models are needed for in-service quality monitoring. With such objective models available, network parameters can be systematically adjusted to optimize rate-quality performance, thus improving multimedia stream delivery and enhancing QoE. We refer to this scenario as 'quality-aware multimedia communications.'In this context, the main goal of this project is to develop an objective audio-visual quality model that can be distributed throughout a network. With distributed processing, locations where different quality degradations occur can be detected. Being able to detect specific and relevant quality-degrading artifacts in the multimedia stream is an important step towards QoE feedback and root cause analysis. Specific artifacts, however, are perceived differently by users depending on their level of expectation  and affective state. In order to adjust QoE models at the user level, an additional goal of this project is to develop objective models of user expectation and affective states. The models will be based on innovative acoustic and physiological signal monitoring techniques. The user-centred paradigm proposed here has the potential to redefine multimedia communications and bring to bear significant economic gains to multiple industries in Canada and worldwide.""482537,""Fall, Mamadou"
"479302"	"Far, Behrouz"	"Model based approach to verification of distributed and multi-agent systems"	"Increasing demand for Distributed software systems (DSS) has led to the development of several Agent Oriented Software Engineering (AOSE) methodologies. Unfortunately AOSE methodologies do not cover the full software life cycle and usually exclude verification, monitoring and testing. This research provides a cost effective solution to distributed system verification by detecting emergent behaviour during the requirement and design phase. A practical approach for DSS development is describing its requirements using scenarios. However, scenario-based specification and behavioural modeling based on it are prone to subtle drawbacks including incompleteness and partial description. This research is focused on the emergent behaviour that can arise when behaviour model of a system with multiple components is synthesized from its scenario-based specification. The goal is automating the process of modeling, analyzing, detecting and resolving the emergent behaviour in distributed systems. The following research questions will be addressed: (Q1) what is the exact cause of emergent behaviour in distributed systems? (Q2) How to model and analyze the system in order to find emergent behaviour? (Q3) How to detect and resolve the problem? The deliverables are: (1) a 4 step process (i.e. modeling, analysis, detection and resolution) to detect emergent behaviour in DSS; and (2) a software system that is integrated with the conventional UML-based and major AOSE methodologies to provide seamless support for design verification. The proposed research has a solid theoretical basis and an implemented prototype. Through detailed industrial case studies in various real-life domain problems such as information retrieval and distributed manufacturing systems we will show the efficiency and effectiveness of the approach. The proposed research can (1) contribute to system level safety of DSS; (2) help wide spread adoption of multi-agent systems (MAS) in software industry by removing the vexatious emergent behaviour hurdle; and (3) can potentially be used to suggest possible positive emergent behaviour scenarios during MAS simulation and data mining.""484977,""Farag, AhmedMostafaTawfik"
"479370"	"Fevens, Thomas"	"Computational geometry and applications"	"My research program is centered on the area of Computational Geometry where I am exploring how geometrically motivated techniques and data structures may be used to solve basic and applied computing problems. In the primary area of Computational Geometry,  these problems include characterizing and developing algorithms for optimized partitioning problems for polygons, such as optimal partitioning. This work has applications in solid modeling and mesh generation. Many of the application area problems that I am interested in are in the areas of Networks and Visual Computing where I apply Geometric Computing ideas and techniques to find solutions. In the area of Networks, I continue to work on routing problems in position-based mobile ad hoc networks (MANETs). For 3-D position-based MANETS, it is known that guaranteed delivery is not possible with local routing algorithms so I am exploring new local routing algorithms for three dimensional MANETs with high delivery rates and relatively short path lengths, while modeling the networks as realistically as possible, with irregular transmission ranges, transmission delay, etc. In related work, I am building on my recent work to develop new geometric spanning subgraphs and explore their properties, including their effect on routing algorithms.  In the application area of Visual Computing, I am studying problems related to visual data processing in 3-D Graphics and Medical Imaging. In 3-D Graphics, I am applying the Machine Learning techniques such as locally linear dimension reduction to the problem of modeling dynamic deformable geometric objects, based on single or multiple views. This work has applications in Computer Animation and Scientific Visualization. In the specific area of Medical Imaging, I am extending my recent research on Computer Aided Medical Diagnosis to Histopathological images, particularly high resolution microscopic slides of urine (to detect sediments and/or crystals) and blood smears (to detect diseases and cancers). Using the tools of Image Processing (to determine geometric features, etc.) and Machine Learning, I am developing Diagnosis tools to assist Clinicians who examine many such slides.""483035,""Fiala, Mark"
"481374"	"Field, Timothy"	"Stochastic differential equation and random field theory applied to waves in random media"	"The focus of the research is the description of waves interacting with random media, throughout space and time. Scattering of waves (far field interaction) has many important physics and engineering applications, including radar, astronomy and wireless communications. The statistical characteristics of a scattering population play a crucial role in descriptions of scattering processes. The population may be realized in different ways depending on the problem - in maritime radar as facets on the ocean surface, in astronomy as the atmosphere, and in wireless as the number of available propagation paths for the microwave. Each case can be described in much the same mathematical terms, however. For populations that interact with each other we apply new mathematical techniques to better understand their dynamics and large time behaviour. The work draws on fundamental techniques in theoretical physics - the Ising model and the path integral method, in tandem with the stochastic calculus, to describe such populations and their interaction with waves of a variety of kinds. These studies are of vital significance in radar applications, especially satellite interferometric synthetic aperture radar (InSAR) with relevance to geothermal imaging and climatology. In wireless communications the techniques have important relevance to time varying channels in non-stationary receiver-transmitter environments with implications for coding and channel capacity calculations. Novel applications in acoustic propagation and speckle phenomena include sonar, in the generation of synthetic images (e.g. of the ocean floor with relevance to tsunami prediction), and ultrasound medical imaging, via stochastic inference techniques. In NMR (near field interaction), the population of interest consists of spins, and the resulting amplitude is spin noise. Chemical studies should have important consequences in imaging applications, particularly in functional magnetic resonance imaging, of key benefit to the Canadian health research community. In summary, the research addresses the behaviour of waves in random environments, and harnesses such description for extraction of useful information in a variety of physical applications.""493773,""Fifield, Eric"
"479092"	"Fisher, Brian"	"Visual analytics models and systems"	"The proposed application-aware fundamental research project will support laboratories studies, user modeling, and design and evaluation of ""proof of concept"" visual analytics prototypes. It relies on the synergy between visual information system development and empirically-based cognitive modeling in visual analytics (VA) ""the science of analytical reasoning facilitated by the interactive visual interface"".  We begin with laboratory studies that evaluate visual information systems for user understanding and cognitive task performance.  The results guide our design of improved visual information systems and more accurate models of users' perceptual, cognitive, and communicative processing of information presented to them by those systems.Models of user performance are often used to predict average performance of a population of users. We add to this a focus on individual differences expressed as parameters of the user model, i.e. a ""personal equation"" of interaction. By requiring our models to account for individual differences we add a new dimension to user modeling that holds particular promise for meeting the needs of specific subpopulations with special needs (e.g. low vision) and abilities (e.g. enhanced spatial attention and eye movement performance by skilled air traffic controllers). The reciprocal translation of knowledge between computing and the emerging cognitive systems science of VA will generate insight into new ways to manage the complexity of VA system engineering in particular and broadly integrative ICT R&D processes in general, methods for user training, ways of customizing interactive technologies for a given user population and organizational structure, and integration of visual analytics into organizational processes and practices.""487531,""Fisher, Brian"
"474043"	"Francis, Bruce"	"Distributed systems and control"	"The major area of this proposal is distributed robotics, meaning networks of mobile robots that cooperate to perform a task. The best-known example is perhaps robot soccer, where teams of mobile robots play against each other. A more practical example is a convoy of military vehicles transporting material through hostile territory to the front line; the vehicles are unmanned and are driven by on-board computer/camera control systems. And a third example is a team of robot rovers equipped with isotropic antennas that are required collectively to form a single antenna with a focussed beam. The proposal also contains a minor area: multivariable control design for power electronics.Three projects are proposed: 1) Dynamic path-following for convoys of robot vehicles. 2) New theory for robot formations. 3) Control design for switched-mode power supplies.""474311,""Francis, Don"
"485386"	"Gardoni, Mickaël"	"Méthodes pour un système d'informations informelles dynamique"	"Dans l'environnement hautement compétitif actuel, les entreprises qui réussissent sont innovantes dans la mesure où 75% de leurs revenues proviennent de produits qui n'existaient pas 5 années auparavant. Ces innovations naissent en grande partie pendant les phases amont de la conception lorsque l'essentiel des informations existent uniquement sous forme informelle. Au cours des 15 dernières années, le candidat a mené ses recherches sur les systèmes d'informations informelles car il existe un manque criant de méthodes de communication, de gestion des connaissances et de créativité-innovation pour supporter les informations informelles. Surtout dans un contexte d'innovation où les participants sont répartis géographiquement. Pour mener à bien ce programme de recherche, les 4 thèmes Structuration, Partage, Accès et Capitalisation des informations informelles constitueront les 4 axes de recherche : 1)- Axe Structuration - construire une modélisation unifiée des informations informelles permettant leur circulation entre les différentes méthodes de communication, de gestion des connaissances et de créativité-innovation. 2)- Axe Partage - construire une méthode qui permette l'échange à distance d'informations informelles en mode synchrone (par exemple, pour supporter le remue-méninge à distance) 3)- Axe Accès - évoluer vers un accès à l'information en mode «poussé» (l'information va dynamiquement vers son utilisateur) plutôt que le mode actuel «tiré» (l'utilisateur va chercher l'information stockée statiquement dans les méthodes.). 4)- Axe Capitalisation - construire des liens informationnels partant des méthodes de gestion des connaissances vers les méthodes de créativité-innovation pour favoriser la création de connaissances nouvelles en agrégeant les connaissances existantes. L'ensemble des caractéristiques de cette dynamicité va profondément impacter la gestion des informations informelles, qui sera plus en accord avec les pratiques des générations arrivant sur le marché du travail utilisatrices en continu de collecticiels communautaires tel que Facebook. Cela ouvre par la même de nouvelle voie de recherche et une véritable optimisation du processus de conception qui sera évaluée en vrai grandeur.""477240,""Garduno, Rafael"
"487971"	"Garg, Siddharth"	"Scalable and adaptive power management for highly parallel single chip multi-core processors"	"Highly parallel single chip multi-core processors  have become the driving force across product domains ranging from low-power multimedia and mobile devices to chip-multiprocessors used in high-endservers. Such multi-core systems are typically characterized by a high degree of spatio-temporal heterogeneity in application characteristics - from one core to another, and from one application phase to another. In addition, due to the increasing impact of random within-die manufacturing process variations, the leakage power and timing characteristics of otherwise identical cores on the same chip can be vastly different. Finally, due to the increased impact of reliability phenomena like bias temperature instability (BTI) with transistor scaling, the characteristics of the cores on a chip vary with time. In the face of the unpredictability and variability sources mentioned above, guaranteeing (or optimizing for) the power and performance specifications of a multi-core system cannot be done at design-time, and must instead be enabled through fine-grained, run-time adaptive, resource management techniques. The managed resources could be power, performance, temperature, remaining lifetime before failure etc., although in this proposal we focus primarily on power and temperature. The primary contribution of the proposed research is to develop scalable and adaptive control that will enable the next generation of highly parallel multi-core platforms to deliver predictable power/performance in the face of unpredictability and variability arising from the manufacturing process, environment and application behavior.  Compared to prior state-of-the-art, the novel contributions of the proposed research are: (i) the focus on the scalability of the adaptive control mechanism, in terms of area, power and performance overheads, to systems with potentially hundreds (or more) cores integrated on the same chip; (ii) holistically addressing all major variation sources within one framework; and (iii) demonstrating the feasibility of the proposed techniques using FPGA-based hardware prototyping.""489402,""Gariépy, Alexandre"
"479600"	"German, Daniel"	"Source code licensing as an essential aspect of modern software development"	"Most large software applications are not built from scratch; instead, they are created by combining several components. These components are of various types, such as libraries, database management systems, stand-alone programs, and network services. Over the last decade, various research efforts have focused on the technical aspects of supporting and improving component-driven software development processes. However, little attention has been directed toward its legal complexities. This aspect has been complicated by the availability of widely-used open source components (such as those of the Eclipse and the Apache Foundations) that are used in both commercial, and open source applications. The creators of component-based applications must frequently combine components with differing licenses (often commercial and open source) to create a new software application with its own license. Each of these licenses will impose certain constraints that should be honoured by the resulting system. Over time the licenses of components (or the resulting system) might change, old components dropped, and new ones added, making it necessary to consider licensing issues as another important concern of the software evolution and maintenance cycle. The main objectives of this research program are 1) to create models that abstract the challenges and issues of creating and evolving systems based on components with multiple licenses, and 2) methods and techniques to address and solve these challenges. This includes:* Modeling the constraints that software licenses impose on the creation of component-based systems.* Modeling the licensing requirements of each component, according to their license.* Extending current methods for modeling software architectures to include licensing requirements.* Identifying and developing types of software architectures that address licensing issues.* Incorporating licensing issues into the software evolution cycle.""474375,""Germida, James"
"487994"	"Gherbi, Abdelouahed"	"Model-based engineering techniques for embedded and real-time software"	"Embedded and Real-Time Systems (ERTS) are nowadays used in virtually every activity of our modern daily life. Examples of the applications using ERTS include the automotive, avionics, health monitoring, and home appliance to mention just a few. Software is increasingly used to implement most of the functionality and therefore is increasingly complex. This complexity can be mastered and controlled using a model-based engineering approach. This approach is supported by a diversity of standard modeling frameworks including for example the Object Management Group (OMG)'s System Modeling Language (SysML) and the UML profile for Modeling and Analysis of Real-time and Embedded (MARTE) systems on one hand and the Society of Automotive Engineers (SAE)'s Architecture and Analysis Description Language (AADL) on the other. While this rich diversity of modeling frameworks is necessary to cope with the heterogeneity and multi-disciplinary nature of ERTS, their integration, interoperability and inter-consistency emerge as a new and major concern.  The objectives of the proposed research program are: (1) to investigate techniques which could enable the consistent integration and combination of these modeling frameworks, (2) to define a new method for the verification of the consistency of multi-view ERTS models, (3) to  develop and validate a model-based approach to enable the application of diverse schedulability analysis techniques of ERTS models, and finally, (4) to develop techniques based on formal methods for the analysis and verification of concurrency-related issues in ERTS models. Ultimately this research project aims at developing new engineering techniques for embedded software systems of high quality. This research program can moreover contribute to enhancing the standard specifications by providing input to the standard organizations such as the OMG and SAE. This research program will be also an opportunity to train highly qualified personnel who will join the vibrant sector of the embedded and real-time systems market, including the Canadian aeronautic and automotive sectors, and thus sustain and contribute to Canada economic prosperity.""494457,""Ghesquiere, Mike"
"479349"	"Giannacopoulos, Dennis"	"Hybrid parallel adaptive finite element analysis and design for high-speed microelectronic system interconnections"	"The objective of this research program is to develop new, highly innovative, accurate and reliable numerical methods for the efficient simulation of high-speed microelectronic system interconnections (MSI). Simulations based on circuit theory have been effective for verifying signal integrity in systems operating at maximal speeds determined by constituent devices such as logic gates and transistors. However, with today's shrinking feature sizes and increasing clock frequencies, a limiting factor for many high-speed microelectronic system designs is interconnection delays rather than device switching speeds. Further, interconnection effects such as reflection, cross-talk, dispersion and attenuation are now leading sources of signal corruption and a significant cause of system performance degradation at higher operating speeds. Standard circuit analysis and conventional simulation techniques are not sufficient for accurately predicting microelectronic system performance when these conditions prevail. Today, the state-of-the-art in MSI research lies in the development of innovative numerical methods capable of accurately, efficiently and reliably simulating the interconnection electromagnetic waveforms within the sophisticated 3-D micro-fabricated structures of modern high-speed microelectronic systems. Moreover, when this information is produced effectively it can have a major impact by significantly reducing the overall time and cost of the design process.  The study of the electromagnetic behavior of MSI structures is a critical component for the synthesis and design of present and future generations of high-speed microelectronic circuits and systems. The overall goal of this research is to develop advanced high-performance computing methods capable of efficiently simulating the electromagnetic behavior of increasingly realistic models of microelectronic systems over future generations of hardware that will become available. Ultimately, this research is intended to benefit the microelectronics industry by providing practical and effective simulation tools that can be used with confidence to predict the performance of newly proposed designs to within specified engineering tolerances in a timely fashion.""475856,""Gianoulakis, Christina"
"488092"	"Goldthorpe, Irene"	"Group III nitride heterostructured nanowires for light emitting diodes"	"Gallium nitride (GaN) and Indium Gallium Nitride (InGaN) light emitting diodes (LEDs) are currently a multibillion dollar industry with applications in cell phone back lighting, traffic signals, message display boards, and projection television.  The development of solid state lighting to displace incandescent and fluorescent lamps for general illumination represents an even larger worldwide market, accompanied by a huge energy savings and corresponding greenhouse emissions reduction due to the lower energy consumption of LEDs.  Conventional LED technology is presently limited by cost, yield, and performance issues that are related to the materials deposition process.  The emerging field of semiconductors nanowires allows arbitrary materials and alloys of high optical quality to be deposited inexpensively.  This will bring new wavelengths and higher energy efficiencies to LEDs.  Nanowire heterostructures will be grown by metal organic chemical vapor deposition, characterized through microscopy and electrical characterization techniques, and be incorporated into devices.  This project will study the materials science of these nanowire heterostructures, and investigate strategies to increase the efficiency, brightness, and spectral emission range of nanowire LEDs.""473887,""Gole, AniruddhaMadhukar"
"487973"	"Gosselin, Benoit"	"High-performance biomedical microsystems to build the next generation of assistive technologies"	"This program consists in implementing high-performance mixed-mode and RF microsystems to build the next generation of assistive technologies (AT). Such AT will include wireless implantable/wearable sensory/actuating devices and miniature brain computer interfacing technology that will operate wheelchairs and various communication devices to help people with disabilities. For achieving the lowest power and the smallest size devices, our approach relies on ultralow-power VLSI circuit design, 3D-IC integration through CMC Microsystems and novel high-density assembly and packaging strategies based on multi-chip, systems-in-package and hybrid structures. For multi-modal sensing/actuating functions, we will design ultralow-noise amplifiers, accurate potentiostats, low-power digitizers and high-fidelity current sources. To enable full implantability and contactless operation, we will employ nano-scale CMOS processes to build ultralow-power wideband pulsed RF transmitters and we will use the latest low-power commercial components to build low-cost high performance receivers. Our microsystems will harvest power from a high-efficiency inductive link and/or small batteries. We will assemble the resulting microsystems with microelectrodes, sensors and MEMS to enable for highly miniature devices, and enclose them within biocompatible packages. The novelty of the proposed work is located at the following levels: 1) Reliable multi-modal sensing interfaces are not available yet for neuroscience research, 2) far field data transmission and inductive coupling will be combined in a single interface, 3) smart power saving mechanisms will be developed to achieve high-efficiency, 4) 3D-IC CMOS integration along with new vertical/horizontal multichip microsystems, system-in-packages and hybrid structures will allow building very dense and innovative medical microsystems. The high significance of our work resides in enabling new AT that will improve their users' quality of life and reduce healthcare costs. In regards of the novelty enclosed in our program, we expect to have a significant impact in the areas of micro/nanoelectronics, and biomedical engineering.""492750,""Gosselin, Catherine"
"479564"	"Goswami, Dhrubajyoti"	"Advancement of parallel systems skeletons"	"The idea of design patterns has been widely researched and applied in parallel application design and development. In most existing works on patterns in parallel programming, the emphasis is on aiding an application developer during the design and development phases of an application. In addition to the traditional applications of patterns, this proposed research extends the use of patterns towards (i) systems, dependability and performance specific aspects of parallel programs, targeting two categories of users: application and system designers/developers; and (ii) assisting in  non-trivial, practical, and useful parallel application domains. More specifically, the proposed research focuses on the following systems and dependability specific aspects: (a) fault-tolerance: fault-tolerance strategies differ for different patterns in order to minimize overhead. For example, fault-tolerance strategy for a master-worker pattern with no interaction among workers will differ from the scenario where workers form distinct communication subgroups. This necessitates proper design of strategies specific to patterns; (b) atomicity: since patterns abstract the behavioral requirements of an application, atomicity requirement checking of parallel programs could be facilitated by the use of patterns; this needs extensive research. From the performance side: (c) scheduling strategies for different patterns differ on a heterogeneous computing environment in order to minimize idling time, reduce communication cost and thus enhance efficiency. For example, a task farm with independent dynamic tasks needs to be scheduled differently from a pipeline or a systolic array. Hence designing of appropriate scheduling strategies is crucial for performance. In the non-trivial applications category: (d) two such application domains are recently being investigated and will be reported in the main proposal.  In all of these works, the targeted hardware platform is a commodity heterogeneous parallel computing environment, consisting of nodes with multi-core CPUs and GPUs of dissimilar architectures interconnected via high-speed network, where achieving efficiency through proper utilization of resources is a challenge and dependability is a requirement for the developed application.""488748,""Gotanda, Kiyoko"
"475837"	"Goussard, Yves"	"Improvement of reconstruction methods for three-dimensional computed tomography"	"In the past few years, three dimensional computed tomography (CT) has become widespread in many medical and industrial settings. Even though existing techniques are very useful, they still present significant limitations. In addition, their usefulness may be limited by computing requirements, cost or invasiveness. Our goal is to develop improved data processing methods for both established and emerging imaging modalities.For this purpose, we will focus on three major elements of CT reconstruction methods: 1) Data formation model - We will develop a formulation in which the geometric elements (the way in which the investigated medium moves with respect to the measurement apparatus during a tomography experiment) are decoupled from the physical aspects (the interaction phenomena underlying the measurement process). This will result in added flexibility and reduced computing time, as well as potential parallel implementation.2) Optimization methodology - We will expand previous comparisons of optimization methods and investigate new ways of utilizing them, particularly in the difficult case of diffraction tomography. In addition, derivation of appropriate preconditioners linked with the structure of the data formation model will yield acceleration of the optimization methods.3) Parallel implementation - Using high-level tools, we will study the potential of massively parallel implementation of the geometric transformations present in the data formation model using the power of graphics processor units. This should result in significant reduction of the computation time of the reconstruction methods.The resulting methods form the basis for new imaging techniques that will be developed through collaborations in the medical and industrial fields.""495446,""Goussev, Dmitri"
"481273"	"Granger, Éric"	"Adaptive multi-classifier systems for biometric recognition"	"Biometric recognition of individuals provides a powerful alternative to traditional authentication schemes that are presently applied in many security and surveillance systems. In practice, the performance of biometric systems typically declines because they face complex environments that change during operations, and they are designed a priori using limited data and knowledge of underlying data distributions. Biometric models are often poor representatives of the biometric trait to be recognized. For accurate recognition, these models should be adapted over time in response to new or changing input features, data samples, priors, classes and environments. This research program seeks to investigate adaptive multi-classifier systems (AMCSs) that can achieve a high level of performance in real-world biometric applications, and efficiently update biometric models in response to emerging information from the operational environment. These AMCSs evolve an ensemble of binary classifiers (EoCs) per individual, where classifiers are co-jointly trained using population-based evolutionary optimization. During the enrolment of an individual to system, a new dynamic multi-objective PSO-based training strategy generates a diversified pool of base classifiers through batch learning of data samples. Then, in response to new data for that individual, this strategy either generates an additional pool for combination with previously-learned classifiers, or evolves the pool of previously-learned classifiers through incremental learning. A subset of classifiers is then selected from an individual's pool according to specialized measures of accuracy and diversity. New incremental Boolean combination techniques are employed to adapt decision-level fusion functions over time, in response to new or changing pools. To account for limited data and skewed distributions, incremental BC is applied in ROC or other spaces. Although the robust adaptive techniques described in this proposal can be applied to a wide range of applications, face recognition, signature verification and biometric fusion are the focus of this research. To accelerate all steps of this program, new AMCSs will be validated with real biometric data on high-speed GPGPU platforms.""473799,""Granot, Daniel"
"473391"	"Green, Mark"	"Interactive visualization of large scale scientific computations"	"The state of the art in high performance computing (HPC) has been advancing rapidly with modern clusters having thousands if not tens of thousands of processors.  The large number of processors and the corresponding large amount of main memory allows researchers to run very large models over a large number of time steps potentially producing many exabytes of data.  This large data volume causes problems for traditional approaches to scientific visualization where the data is collected on disk during the computation and then transferred to a visualization workstation for post processing.  The process of writing the data is now consuming a significant amount of HPC resources and due to the size of the data transferring it over existing computer networks to a visualization workstation is no longer feasible.   The Mercury system supports the visualization of very large models by performing at least part of the visualization computations on the same cluster as the scientific computation.  This approach to visualization is called in situ visualization and until recently has not received much attention in the research community, but with increased data set size it has now become an important research topic.This project investigates software architectures and tools for in situ visualization and how existing visualization techniques can be transferred to this new environment.  The Mercury system is one of the first general purpose in situ visualization systems and will form the basis for many of these studies.  The software and techniques produced in this research program will improve both the quality and efficiency of the visualization systems used by scientists and engineers.""475845,""Green, Mark"
"475287"	"Grenier, Dominic"	"Applications du traitement du signal et de la fusion d'informations à la localisation haute-résolution"	"Les projets proposés consistent à l'utilisation conjointe du traitement d'antenne et de la fusion d'informations de capteurs multiples différents pour augmenter la fiabilité, dans le but de localiser soit un tireur embusqué (``sniper''), soit un usager de téléphone portable en détresse.Les particularités du canal acoustique ou du canal radiomobile devront être pris en considération: milieu inhomogène qui déforme les fronts d'ondes, réverbération provenant de la diffraction importante des ondes. En radiomobile, le problème provient des nombreuses réflexions du signal d'un usager agissant comme autant de copies corrélées, avec présence de plusieurs usagers en simultané, d'où une quantité élevée de signaux d'arrivées partageant un spectre assez étroit.Le traitement du signal utilisant un réseau d'antenne (appelé traitement d'antenne ou ``Array signal processing'' car le capteur peut être autre qu'une antenne) est envisagé pour estimer les paramètres tels l'angle ou délai d'arrivée des sources (DOA ``Direction-of-Arrivals'', TOA ``Time-of-Arrivals'', TDOA ``Time-Difference-of-Arrivals''). En exploitant les statistiques du second ordre (tels MUSIC, ESPRIT, MLM, MinimumNorm ...) ou d'un ordre supérieur des signaux, le pouvoir séparateur est accru d'où une estimation dite à haute-résolution (HR).La fusion d'informations consiste à combiner des informations issues de plusieurs sources afin d'améliorer la prise de décision et augmenter la fiabilité. Cette combinaison vise à obtenir un système plus robuste aux informations imparfaites: imprécision des données, incertitude relatif à la véracité de l'information, ambiguïté conduisant à plus d'une interprétation, informations impartielles, et conflit conduisant à des contradictions. La fusion s'appuie sur des outils théoriques développés dans un formalisme mathématique choisi selon le besoin : théorie de l'évidence (Dempster-Shafer), des ensembles aléatoires, logiques floues, etc.""489621,""Grenier, François"
"475378"	"Gupta, Arvind"	"Combinatorial algorithms in bioinformatics"	"One of the great scientific feats of the 20th century is the understanding of the quantitative nature of biological systems. The resulting explosion of information has challenged scientists to develop novel in-silico tools to model, analyze, and infer the underlying processes. Dubbed bioinformatics, the result is a multi-disciplinary effort bringing together mathematics and computer scientists with biologists and chemists. My long-term research interest on algorithms for analyzing combinatorial structures has, over the past ten years, increasingly centered on cellular processes; problems my lab  is considering include:1. Haplotypes give the structure of individual chromosomes and as such are the basis for genetic disease susceptibility. In-vitro techniques are prohibitively expensive for determining individual haplotypes and our focus is on the development of efficient computational tools.2. Drug synthesis, an expensive endevour, is now turning to computational tools. Our long-term project on Inverse Protein Folding attempts to construct a desired protein given information about the protein's properties. Such proteins are the basis for novel drug design as they will exhibit pre-determined behaviour.3. We have developed a novel technique to cluster extremely large high-throughput data and used it for flow cytometry data. The result is a general purpose tool that can, for example, identify populations of cells that express similar intercellular or surface proteins.We are interested in the efficiency and efficacy of algorithms in real-life settings. Students graduating from my lab have a mix of life-sciences and computational skills, a combination sought out by industry and academia. ""495047,""Gupta, Ashwin"
"473984"	"Gupta, Madan"	"Further studies dynamic neural networks and fuzzy logic with applications to neuro-control and neuro-vision systems in robotics"	"Dynamic neural networks and fuzzy logic are the two recently developing theoretical tools which have found a wide range of applications in the areas such as control of complex dynamic systems, vision systems, robotics, medical image processing, and decision making problems in industrial systems. Over the past two decades, our group at the Intelligent Systems Research Laboratory has developed an extensive theoretical basis in these two important areas, have merged these two mathematical tools for developing the fuzzy neural systems and have applied these theoretical tools to problems of complex dynamic systems. Over the next five years, we wish to extend these theoretical tools in the field of dynamic neural networks and fuzzy neural systems, and apply them to some industrial, medical image processing and space problems.    Further theoretical development of dynamical neural structures: Theoretical work will be carried out in the development of new dynamical neural architectures, study of their associative memory capabilities and stability characteristics. The novel design of dynamic neural networks will lead us to study their dynamic properties such as stability conditions and equilibrium capacity.    The neuro-controllers with new dynamic neural structures and with learning and adaptive capabilities will be developed. Further work will be conducted on the design of new neural architectures for their applications to receptive fields, associative memory, visual memory and other visual functions.     As during the past, several graduate students, postdoctoral fellows, visiting professors/scientists and several faculty members of the College will play an important role in the activities of these proposed research plans. This country Canada is in need of some qualified persons with high-level of research experiences in the field of neural networks, fuzzy logic and robotic systems, and as during the past, this proposed research has promise the produce such qualified personnel.""493768,""Gupta, Manika"
"473397"	"Hadley, Robert"	"Cognitive architecture, neural realism, and consciousness"	"RESEARCH OBJECTIVES:  The proposed research involves two major goals.  The first entailsresearch into forms of informational representation which arguably display the greatest promise for attaining the kind of robust combinatorial thought processes displayed in human mental processes.  Several of my prior publications have stressed the importance, for researchers working  with computer models of brain structures,  of discovering explanatory cognitive models which can achieve  Strong Semantic Systematicity.  This form of systematicity involves the ability to understand complex sentences which contain words in novel positions, and to properly interpret complex structures within those sentences.    In recent years, several researchers (including Thagard, Eliasmith, and Neumann) have obtained encouraging results with forms of  mental representation known as holographic reduced representations (HRRs).  Yet, despite these results, major neurological problems remain to be resolved.  Research proposed here will explore solutions to these major problems, with emphasis on patterns of  connective fibers thatare required to produce HRRs, and on the mathematical transformations of these representations.   The second major goal of the proposed research concerns the possible consciousness of artificially intelligent agents, including robots.   In recently published work (Hadley, 2010) I have argued that the existence of focused conscious states (intentional consciousness) is a crucial component in important forms of mathematical conceptualization and reasoning. I argued, in addition, that the efficiency of specific forms of discovery within mathematics intrinsically involves the kinds of unified perceptions which occur within conscious states. If these arguments are correct, then any artificial, computational agent which could, in principle,  match the cognitive abilities of competent human mathematicians would need to possess focused conscious states.  Research proposed here will extend the arguments I have published into related areas of creative discovery,  and will investigate the relevance of brain states to possible consciousness in robots.""473398,""Hadzilacos, Vassos"
"480266"	"Hamel, Sylvie"	"Algorithmic and combinatorial studies inspired by problems of comparative genomics"	"My research interests are in developing algorithmic approaches and combinatorial tools to solve theoretical problems inspired by biological problems arising in the field of comparative genomics. One of the basic aim of comparative genomics is to extract information from the comparison of genomes of different species. This information concerns the genomic sequence themselves, the order of appearance of genes in different genomes, the structure of small molecules (like RNA), etc. The general problem of extracting meaningful information from the resulting mass of biological information gives rise to many important theoretical problems on the fine combinatorics of words, permutations, trees or graphs. A first portion of my research program concerns the study and comparison of special trees and graphs representing non-coding RNA (ncRNA) structures. This study is of particular importance for biology because important cellular functions of ncRNA depend on their structural and combinatorial properties. Moreover, the fast expansion of complex databases of ncRNA structures (from barely 3 known families in 2002, to over 1,400 today) warrants the development of faster algorithms for the comparison and the analysis of such structures. A second portion concerns the study of permutations or signed permutations representing the order of appearance of genes in different genomes. Here, the problem consists in finding the median of a set of permutations under a distance d and is inspired by the gene order problem of comparative genomics, where the difference in order of appearance of genes in the genome of different species is used to evaluate the evolutionary distances between them. Finally, the expected theoretical results that will arise from the development of more efficient comparison algorithms in the field of bioinformatics is bound to help to solve similar problems in other field of computer science. We have already shown how our results may be applied in the context of software engineering where the maintenance of system requires the search of  recurrent motifs in a program code.""494348,""Hamelin, Caroline"
"488006"	"Hancock, Mark"	"Ubiquitous virtual tools: Providing ad-hoc multitouch interaction on nearby surfaces"	"There has been a recent surge in research prototypes and commercial products (e.g., smartphones, tablets, video game consoles) that enable people to interact with information using their fingers, hands, arms, and whole bodies. Pocket-sized handheld projectors have also recently become available, leading to the possibility that any surface in one's vicinity can be transformed into a digital display that can also sense touch.The same technologies that allow a person to use their fingers to make gestures typically require that the information be controlled on a flat, rigid, clean surface. However, our everyday environments are typically far messier. For example, tables in our environment are typically filled with paper, books, coffee mugs, and other artefacts. If a person were to project digital information within and around this mess, it is not clear how they could interact in this environment. My proposed research explores how a person can repurpose any nearby surface into a digital interactive touch display. First, I will develop the necessary hardware to build working prototypes of systems that display and sense information everywhere in our surroundings, and software toolkits for accessing this information. Second, I will design, implement, and evaluate interaction techniques that allow people to fold, bend, and otherwise manipulate digital information on these ad-hoc displays. Third, I will use this technology to perform studies about people's perception of the virtual information in these non-flat, messy environments, and use the results of these studies to inform improved designs of these interaction techniques.This program is critical to near-future computing. As multitouch displays become ubiquitous in our everyday environments, a key issue will be how to make the information they display interactive and meaningful.""484381,""Hand, Susan"
"475731"	"Hayward, Ryan"	"Algorithmic discrete mathematics with applications"	"I am interested in algorithms related to two recent developments.In 2002, Chudnovsky et al. proved the Strong Perfect Graph Theorem, the biggest open problem in graph theory since Appel and Haken's 1976 proof of the Four Color Theorem in 1976.Related to this recent monumental result is an intriguing open problem: given a graphsatisfying the conditions of the theorem, is there an efficient simple (so, not using the complicated mathematics of the ellipsoid method) way to find a largest clique ?For this problem, I am seeing if I can solve this problem on a smaller class of graphs.In 2006, the world learned the strength of the computer Go programs Crazystone by Coulomand MoGo by Gelly.  Since then, there has been a revolution in computer Go that has spilled into many areas of computer science, with applications as wide ranging as finding suitable schedules for multi-plant power generation systems.  Underlying this revolution is Monte Carlo tree search, a newrandomized method for performing a search in a huge solution space.For this problem, I am experimenting with these ideas on a simpler game than Go.""494464,""Hayward, Tim"
"479571"	"Hertzmann, Aaron"	"Modeling human locomotion control"	"Movement---walking, running, and physically interacting with the world---is fundamental to human existence. Yet, a scientific understanding of the principles of locomotion control remains elusive, even for a task so seemingly-ordinary as walking. Until very recently, simulating a motion as basic as walking---in a natural, human-like manner---was unthinkably difficult.  However, exciting recent breakthroughs in animation and robotics have brought human-like control within reach.Building on these breakthroughs, this project will develop the first full-body controllers for a wide range of human locomotion activities, from basic day-to-day motions, such as locomotion and terrain navigation, to complex and skilled movements, such as athletics and dancing. This work will address technical problems at the frontiers of motion planning in animation and robotics, while also developing new hypotheses in control of human movement. Furthermore, these models will be used to learn motion controllers from data, and to automatically estimate 3D poses from monocular video (a.k.a. markerless motion capture).  Hence, this work will advance the state-of-the-art in several disciplines at once.This research has several potential applications.  Flexible and controllable characters that respect physical constraints would be of enormous value for animation in interactive environments, such as urban simulations and video games. Feature-based controllers could be used to create bipedal robots that move with the dynamic capabilities of humans.  Vision-based tracking and estimation algorithms could be used for analyzing human gait and diagnosing disorders, as well as for feature-film special effects. Furthermore, insights gained from this work could help us understand the biomechanics and motor control of human movement.""483599,""Herwig, Falk"
"487989"	"Hoey, Jesse"	"Machine learning, dense sensing and decision theoretic planning for large-scale assistance systems"	"Assistance for elderly persons is becoming an important and pressing need with serious effects on society because of the changing demographic towards an aging population (the so-called ""Silver Tsunami''). The burden of care is shifting from the professional arena (e.g., hospitals) into the home and community. Technology to support people in their need to live independently is currently available in the form of personal alarms and environmental aids. Looking to the future, we can imagine intelligent, pervasive computing technologies using sensors and effectors that help with more difficult cognitive problems in planning, sequencing and attention. For example, a elderly woman with dementia may require help in recalling important steps in her everyday life. A set of sensors, such as cameras and switches, embedded in her home could extract information about her behaviours using computational models of temporal sequences.  This information could guide the provision of pre-recorded audio prompts, to which she is known to be responsive.  The system could adapt to her as she changes over time, or as her environment changes (e.g., if she buys new furniture). There are three artificial intelligence (AI) problems that remain to be addressed in order to build such effective assistance systems.  The first problem is that of sensing of the environment, specifically in relation to planning in a human assistance task. The second problem is that of planning and decison making in large-scale domains with high dimensional sensor data. The third problem is that of learning effective assistance mechanisms by eliciting prior knowledge and by integrating knowledge and empirical evidence into sensing and decision making.This research program will provide solutions to these three foundational problems with the eventual aim of offering an exciting and viable way to enable persons to live independently for longer, thereby maintaining quality of life.  ""495742,""Hof, Fraser"
"477354"	"Holt, Richard"	"Mining Computing Configurations"	"Large scale computing systems, notably IT (Information Technology) systems, consist of elaborate configurations made up of hardware servers connected by networks, operating systems and databases, service software such as Customer Service Management and so on.  These configurations evolve with time, and must be constantly monitored and managed to correct faults and to adapt to new demands.  It is challenging for analysts to manage and track these changes in order to maximize the reliability and performance of these systems.  Increasingly, these configurations are stored in special purpose databases such as Configuration Management Databases (CMDBs).  In essence, the CMDB records the present and past architectures of the computing system.  It is used to help the analysts control and maintain the system.  The research proposed here will use data mining techniques to explore and monitor CMDB-like databases with the long-term goal of assisting analysts to do a better job of running these systems.  One of the jobs of the analysts is to plan changes to the system.  Such a change requires updating a set of items in the system.  It is essential that the plan for the change must include all the items that must be updated to accomplish the whole change.  If some items that should be updated are inadvertently overlooked, the attempted change may fail, causing outage, loss of sales, customer dissatisfaction --- even failure of the commercial enterprise.  The proposed research will use mining techniques to provide suggestions and advice, based on past patterns of change and activity as recorded in the CMDB to help the analysts avoid problems such as incomplete changes.  More generally, the proposed research will search for historic patterns of activity within computing configurations in order to suggests tools and information for decision support to aid analysts.""481747,""Holt, Richard"
"475270"	"Hutchinson, Norman"	"The next generation of storage"	"This proposal addresses the tension between the rapid evolution of computing infrastructure and the very slow evolution of storage systems by investigating what I believe is the future of storage systems.  We must begin to understand how to construct and deploy storage systems that actually meet the needs of users, e.g., availability, convenience, flexibility, and security, as well as the needs of system administrators, e.g., correctness, efficiency, manageability, and virtualization-awareness.       The architecture that I propose works at the file abstraction level (in contrast to many systems, including a few of my previous ones) that work at the lower-level block abstraction level), separates the storage of files from the naming systems that we use to identify them, and provides a rich abstraction for manipulating namespaces to help address the complexities of finding, organizing, and managing the files that populate our storage systems.     Files are stored as immutable objects in a single instance store and may be replicated into the Cloud.  When files are edited a new version of the file will be created.  Objects in the single instance store will be replicated to provide increased availability, and will be versioned automatically to provide recovery from user mistakes or malware.     Above of the single instance store we provide flexible namespace composition using a namespace database. Namespaces are named via a URL, can be shared, and in addition to encompassing standard volumes or file systems, can be synthetic, that is, defined as queries over all files, such as ""All Excel files from the accounting team'', or ""All Excel, Word, and Powerpoint files that contain references to the 2011 Proposed Budget''.  Synthetic files can also be used to aggregate data across individual virtual machines, such as, ""all virus-scan log files'' or ""all browser configuration files'', thereby providing system administrators powerful tools to ensure that organizational policies on security and privacy are being followed.""495703,""Hutchinson, Robin"
"474141"	"Ionescu, Dan"	"Applications of computers in industrial environments"	"The objectives of the proposed research are to investigate selected theoretical and experimental topics related to the application of computers in an industrial milieu. This is a continuation and a development on a new level of the applicant's on-going research works. This research area witnessed dramatic changes and evolutions in the last few years on both the hardware and software sides. Complex applications for the enterprise supervision and control, such as CRM, ERP, BI, etc pushed the computer technologies towards new boundaries. Recent evolutions in the distributed computing paradigm fundamentally changed the way that enterprise supervision and control architectures were designed. Growing adoption of cloud based services, virtualization and autonomic computing requires new architectural models, while new dimensions such as user mobility and collaboration have stretched the research in this field at its extremes. Industry analysts (Gartner Quadrants) report that there is a strong need for converged, collaborative, unified and self-managed products or platforms capable to seamlessly provide supervision and control applications across the enterprise. Consequently, for the present phase of our research, we propose to investigate new distributed computing models disrupting the traditional way of offering services and applications at the enterprise level. The research will be focused on fundamental and experimental aspects of new models for autonomic computing, peer-to-peer, web-services and cloud computing technologies in the design of enterprise supervision and control architectures.By analogy with the communication equipment we call information fabric the blending of communication and computing resources in a web based service. We therefore investigate subjects related to the architecture and its components, in which the control of the information fabric is central. We consider the fabric being formed by on-the-cloud clusters controlled by autonomic computing servers endowed with special self-organizing algorithms. These subjects will be the objectives of this proposal. A part of our research efforts will be dedicated to the investigation of reconfigurable architectures as applied to the control of information fabric.""483906,""Ioschikhes, Ilya"
"474641"	"Iravani, Reza"	"High-voltage direct-current (HVDC) grids"	"The increasing reliance on electric power systems to reliably and efficiently meet the growing demand for electric energy and to accommodate new types of generation has resulted in major operational, control, and protection challenges for large interconnected power systems. These challenges cover a wide range of technical, technological, environmental, and  economical subjects, for example, the need to (i)  deal with  limitations in acquiring new right-of-ways or more effective utilization of the existing corridors, (ii) transmit and/or re-route bulk renewable-based power over long distances and wide geographical areas, (iii)  facilitate market operations, (iv) prevent hidden and cascading failures, (v) improve control over the interconnected power system dynamics, and (vi) utilize the power utility infrastructure more efficiently.The high-voltage direct current (HVDC) transmission system, integrated as the HVDC grid within the existing AC power system, is envisioned to address the emerging issues. The main objectives of this research proposal are to (i) identify the required technological developments, (ii) develop control and protection strategies, and (iii) develop analytical models and study tools for the HVDC grid to enable its integration in the existing AC power systems. The significance of this research undertaking is that it (i) enhances the existing Canadian research excellence in the field of HVDC systems, (ii) provides training for highly qualified personnel, and (iii) facilitates migration of the existing electric power system to the next generation power system.""486677,""Ireland, Benjamin"
"481703"	"Izquierdo, Ricardo"	"Interfaçage entre microsystèmes et nano-bio matériaux"	"Au cours des dernières années, les nanotechnologies ont connu un développement soutenu. Elles peuvent être utilisées pour l'élaboration des nouveaux matériaux ou des nouveaux dispositifs pour la microélectronique, la chimie, la biologie, etc. Cependant, afin de pouvoir tirer profit des récents développements du domaine des nanotechnologies dans des applications réelles, il est primordial de pouvoir les interconnecter de façon efficace avec le monde microscopique et macroscopique. Étant donné qu'elles impliquent des dimensions très petites (c.-à-d. quelques nanomètres), les nanotechnologies doivent souvent être implantées à l'aide d'interfaces basées sur des microsystèmes. Ainsi, la recherche en intégration micro-nano sera focalisée sur l'interfaçage et l'interconnexion entre les structures de plus en plus petites produites à partir des nanotechnologies et les microsystèmes nécessaires à l'implémentation de plusieurs applications. Les microsystèmes peuvent être définis comme des systèmes intelligents miniaturisés qui combinent des fonctions de détection ou d'actuation avec des fonctions de traitement. Ce type de composantes se profilent comme une nouvelle classe de systèmes et dispositifs multifonctionnels utiles dans un grand nombre d'applications allant du traitement de l'information, l'analyse et la détection chimique et biologique, la découverte et administration de médicaments, l'ingénierie des tissus, la synthèse des matériaux et produits chimiques et la conversion et stockage d'énergie. Dans le cadre de ce programme de recherche nous procéderons donc au design et fabrication de microsystèmes mais avec une emphase sur les procédés qui permettront d'intégrer des dispositifs optoélectroniques organiques et des nano-bio composantes sur ceux-ci.""475720,""Jabaji, Suha"
"488181"	"Jacob, Zubin"	"Nanostructured hyperbolic metamaterials: physics and devices"	"Nanotechnology has paved the way for artificial materials which have electromagnetic, mechanical, thermal and acoustic properties beyond those which are ordinarily found in nature. According to many published technological roadmaps, merging photonics and nanotechnologies holds the promise:- to usher in a new generation of imaging devices for biomedical applications, to drive nanophotonics research for sustaining Moore's law and even address pressing societal needs of solar energy harvesting. The central theme of this research proposal is the understanding of the essential physics for new photonic devices based on nanofabricated metamaterials, where the bulk macroscopic material properties are governed and tailored at will, according to the constituent nanostructured building blocks.Metamaterials are artificial structured media which allow unprecedented control over the flow of light at the nano and macro scales. The artificial nanostructured media to be investigated as part of this proposal are Hyperbolic Metamaterials (HMMs) which derive their properties from a counterintuitive electromagnetic response, not found in nature. The aim of this research program is twofold: to understand fundamental properties of  HMMs and simultaneously explore HMM based devices in three seemingly disconnected applications:  imaging, quantum optics and renewable energy. On the imaging front, we will look at how artificial materials can extend the capabilities of conventional microscopes; in the quantum optics direction we will look at efficient ways of harnessing light at the nanoscale and along the renewable energy direction we will explore devices which can efficiently harvest solar energy when assisted by artificial nanostructured media. The central theme surrounding these applications is the unique electromagnetic states characteristic of Hyperbolic Metamaterials.  ""496348,""Jacobi, Zachary"
"479043"	"Jacobson, Jr, Michael"	"Algorithms in number theory and cryptography"	"In the past few decades, cryptography, the study of data security, has emerged as an important concern in our society.  As information becomes increasingly accessible through the Internet, the need to protect and authenticate data is more imperative than ever.  The security of many methods in modern public-key cryptography relies on the supposed difficulty of certain computational problems in mathematics and, in particular, number theory.  These schemes are set up in such a way that presumably the only way for an attacker to break the given system is to solve an instance of one such problem.  Currently, only three number-theoretic problems are widely accepted as foundations for secure public-key protocols.  Furthermore, these three problems have strong theoretical relationships.  In many cases, being able to solve one efficiently implies the ability to solve the other two efficiently as well.  In other words, one dramatic mathematical discovery could render many current public-key cryptographic techniques insecure.I intend to investigate the suitability of certain problems in algebraic number theory as alternatives for public-key cryptography.  This will involve improving the efficiency of algorithms for computing with the basic objects of interest, studying theoretically and algorithmically the difficulty of the supposed hard problems, and conducting general investigations into the associated algebraic number theoretic objects.  For example, there are a number of extremely difficult computational problems in this area that have been proposed for cryptographic applications, but much more investigation is needed in terms of their security and efficiency.  By attempting to devise new, more efficient methods to solve these problems, I will provide other researchers with a much firmer foundation on which to base their assessments of these cryptographic protocols.  The end result will be a suite of well-understood, secure, and efficient cryptographic protocols that can be used to protect Internet communications, even if currently-used protocols are found to be insecure.""495096,""Jacobson, Kevin"
"478968"	"Jagersand, Martin"	"Incremental modeling of geometry and appearance from video"	"The focus of the proposed research is representations and computational methods for intermediate structures between 2D images and 3D models. Classically image processing and feature detection has been done first on a per-2D-image basis. Then in a single batch process a 3D geometry is computed from all the 2D features. In particular we will explore:(1) Free-space constraints. Each view of a 3D surface patch implies there is a free-space volume between the camera and that surface. Integrating such volumes yields an alternate 3D modeling to the conventional triangulation. While free-space is used in robotics for 2D ground maps, it is little explored in 3D computer vision, and constructing efficient and accurate incremental 3D volume merging algorithms is challenging.(2) Proxy geometries. The assumption that a computed geometry is accurately representing the real scene is fragile (and often incorrect). In Image-Based Rendering (IBR) a branch of computer graphics, a coarse proxy geometry is used to represent (index) ray-sets. A proxy geometry can be used to index depth displacement maps[22] and appearance information as well.(3) Statistical models of texture/appearance. Much computer vision uses the color-constancy assumption -- each surface point has a color. This is not true of physical surfaces in general, but more importantly algorithms often fail because color constancy is a particularly poor assumption under inexact geometries. Statistical models of appearance are less constrained. We have pioneered the use of linear subspace models for texture [Jagersand, CVPR 1997] and continued to develop this for 3D modeling [32,37]. Algorithms for non-linear spaces and incremental computation will be explored here.(4) Multi-level models. The use of a coarse 3D geometry proxy to index a displacement map, which in turn indexes a parametrized appearance model leads to an integrated model where different information is represented at different scales: Macro, Meso and Micro.""496545,""Jahazi, Mohammad"
"473891"	"Kaler, Karan"	"Droplet based surface microfluidic bio-microsystems"	"The development of miniaturized total analysis microsystems (µTAS) or often referred to as Lab-on-a-chip (LOC) is of increasing interest among the research community. This technology potentially offers new prospects for routine chemical analysis, drug testing, bioassay, and health care delivery, diagnostic devices including non-invasive and early detection of cancers.In this regard our research effort will specifically focus on the development and application of a novel droplet based surface microfluidic technology to: (1) Facilitate manipulation of biological cells, both healthy and cancerous, based on their measured dielectrophoretic signatures, (2) Provide multiplex sample handling for chip based biochemical assays and (3) assemble heterogeneous emulsion droplets and lipid bilayer vesicles for biosensing, drug encapsulation and delivery applications.The droplet based dielectrophoresis (DEP) fluidic microsystem when integrated with other microsystem sub-systems provides a low-cost platform technology, capable of rapid and sensitive diagnosis of early stage cancerous fingerprints of biological cells, nucleic acids (DNA, RNA) and/or proteins, and potentially ""in-field"": detection of pathogens and environmental toxins. The successful commercialization and deployment of such proposed microfluidic microsystems has the potentials to revolutionize the availability and delivery of cost effective, population wide, health care..""487332,""Kaliaguine, Serge"
"474719"	"Kaminska, Bozena"	"Design and integration of organic and inorganic devices"	"I propose the investigation of organic and printed electronics platform technologies based on organic and semiconducting, as well as printable inorganic, functional materials. My long-term goal is to develop a new generation of micro/nano systems based on inexpensive, printable, and self-powering devices that can be embedded into almost any fabric such as glass, polymer, or cotton. The focus of the proposed research, and areas where major breakthroughs are anticipated, will be on new low-cost fabrication processes, high performance design structures, and integration technologies. Key applications such as organic photovoltaic cells (OPVs), flexible displays, flexible batteries, and the integration of these components into fabrics, will be used to identify the real-world needs and challenges. ""476828,""Kaminska, Bozena"
"475889"	"Kapron, Bruce"	"Foundational studies in privacy and security"	"The need for security in computer and communication systems has never been greater. The increase in the amount of computation taking place in networked settings exposes systems to significant security threats. Fortunately, research in security and cryptography over the past thirty years has led to sophisticated techniques and tools which may be used in securing systems. In many cases, there are stable secure primitives which may be used in more complex security schemes and protocols. However, this is not the end of the story. The increasing complexity of security systems leads to a need for new tools for modeling, verification and testing. In particular, the security of the building blocks used in a system does not guarantee security in the end product. It remains a major challenge to develope tools and techniques for the formal validation of security of complex systems which use secure cryptographic primitives. The development of such tools and techniques is a major focus of our research programme.Recent developments in network computing and communication have lead to new security requirements. This is the case with the privacy of large amounts of personal information stored in databases and, more recently, available for mining in social networks. We are investigating the computational feasibility of techniques for anonymization, which make information provided by social networks available for analysis while protecting the privacy of individuals.As security becomes an increasing significant aspect of the development of computing and communication systems, there is a need to develop frameworks to support decision-making about when and how to deploy security tools. We are developing mathematical models based in game theory, decision theory and risk analysis, to provide such support.""493604,""Kaptur, Louis"
"479093"	"Kemme, Bettina"	"Data management in the cloud"	"Cloud computing has become a quickly evolving new computing paradigm. A cloud computing infrastructure can consist of hundreds to many thousands of compute nodes whose computational power is offered to customers through a pay-per-use model. Additionally, cloud providers often provide a whole range of  software services that customers can use to facilitate their software development. Resource management becomes a crucial task in such a complex application hosting environment. On the one hand, it makes sense to physically collocate small-scale applications as it allows the provider to exploit the full computational capacity of each machine. Such approach, however, requires dynamic allocation of computational and memory resources depending on workload patterns. It might also require the dynamic migration of applications between machines. On the other hand, there is an increasing number of large-scale applications, whose data might need to be distributed and replicated across tens to hundreds of nodes, and whose workload will demand many compute nodes. In this case, coordination, distribution and provisioning are crucial but difficult tasks. Database systems (DBS) present particular challenges in such cloud environment. This is especially true for online transaction processing applications (OLTP) that exhibit a high volume of short transactions with high update rates, as they require complex storage and memory management, and sophisticated consistency protocols. Current DBS are difficult to deploy and maintain in the cloud. In our research, we will develop DBS technology that is better suitable for cloud environments. One research direction will develop solutions that make DBS better candidates in resource-sharing environments targeting small-scale applications. This includes efficient and flexible migration mechanisms as well as performance models that allow for dynamic resource management. A second direction will develop a database management stack that provides the scalability, high throughput, high consistency and fast response times that are needed by large-scale applications with OLTP flavor. ""479613,""Kempf, Achim"
"487990"	"Khabbazian, Majid"	"Developing distributed algorithms for the proximate internet"	"The proximate Internet is a network of mobile/fixed devices in which devices can discover and interact with those that are within their vicinity. The primary objective of the proximate Internet is to connect us to the local physical world around us. The proximate Internet has numerous applications including social networking, mobile advertising, and controlling devices remotely. Consider situations where we are in a neighborhood searching for a Japanese restaurant, hailing an available taxi, wondering if a friend is close by, willing to receive sale or job advertisements, wishing to interact with a nearby person, or requiring to control nearby devices. These are only a few examples where proximity matters, hence we can greatly benefit from the proximate Internet.   The current centralized and distributed solutions for the proximate Internet, including WAN-GPS, and WiFi and Bluetooth based approaches, pose challenges for managing energy consumption, scaling with the number of devices, and users' privacy. For example, in the WAN-GPS based approach, devices determine their location using GPS and communicate their location to a ""God-Box"". In return, the ""God-Box"" informs devices of nearby mobiles and services. This raises the privacy concerns with God-Box tracking everyone.   The solution to the proximate Internet that we consider in this research proposal is a multi-hop wireless ad hoc network, in which all communications are only among nearby devices and the density of devices can be very high. In this proposal, we focus on designing energy efficient distributed algorithms for major operations in the proximate Internet, including neighbor discovery and paging, and use several techniques such as analog network coding, smart antennas and multi-channel utilization to make these operations fast, reliable and efficient in terms of bandwidth and energy consumption. The proposed research will be carried out in collaboration with graduate students and undergraduate research assistants.""490719,""Khademian, Behzad"
"479049"	"Khan, Gul"	"Multi-core embedded systems: computer aided NoC/MPSoC design and simulation techniques"	"An embedded computer is part of a larger system that supports overall functioning of the system. Embedded applications range from sophisticated aircraft flight control to consumer products. The incorporation of hardware/software co-synthesis process in the design of embedded systems is crucial for producing optimal embedded devices. We investigate co-synthesis approaches to design MPSoCs (Multi-Processor Systems-on-Chip) and multi-core embedded systems. MPSoC performance is determined by the ability of on-chip communication network that can fulfill the communication needs of processors and other cores. It is claimed that NoC (Network-on-Chip) based systems are economically feasible for several product variants. We investigate various approaches to the design and analysis of NoC systems. We propose to develop methods to design optimal NoC systems by considering chip layout, power, wire-lenths and other information.  A Tabu search based approach will be employed for NoC topology generation with an automated design technique, incorporating floor-plan information to attain accurate values of power consumption of NoC components including routers and links. A rendezvous interaction contention analysis method will also be applied that uses a Layered Queuing Network contention model to observe asynchronous interactions amongst the NoC components and identify possible contention points during NoC synthesis.  To aid the design process of complex systems-on-chip, we are also exploring NoC simulation environments using SystemC language. An in-house developed NoC simulation environment is being enhanced by including on-chip components' power and chip layout information. This research is motivated by the need to automate the design process for multi-core high performance single chip embedded devices. Embedded applications that would particularly benefit include aerospace, medical instrumentation, multimedia and telecom systems.""484380,""Khan, Hasan"
"475562"	"King, Valerie"	"Algorithms for large scale networks"	"Recent years have seen a rapid increase in the presence of large networks of diverse users. Such systems have the potential to speed up computation through vast parallelization and also to revolutionize how we as a society organize our data, economic activity and decision-making. With this promise comes an exposure to new threats in the form of increasingly sophisticated malware. As our dependence on large networks grows, so does our need to develop algorithms which are robust to powerful malicious adversaries.A starting point is a fundamental problem of coordination in distributed computing, the problem of bringing processors to agreement on a bit (Byzantine agreement) and a simple model of users, good users that follow the protocol and bad users controlled by a malicious adversary. This problem was commonly thought to have a communication bottleneck requiring all-to-all communication, making it impractical to consider for large size networks. Recently, we broke through some conjectured bottlenecks in this area. The theoretical limits of what can be done still remain open and we hope to develop both practical tools and theoretical understanding of what is possible. When we look at the network as a community of users with individual preferences, other interesting problems arise. Rather than ``good"" nodes, we may have factions of nodes that co-operate if they are friends or it is to their benefit. On the frontier of law and economics, we consider how nodes in a game can coordinate to improve their overall benefit by voting to change the current rules of the game. We  also ask how this and other coordination problems such as the decentralized scheduling of health care, may be done without losing privacy. While these problems have been studied by others, we will seek to implement them efficiently in large networks, without all-to-all communication.""493169,""King, Vashti"
"475675"	"Kranakis, Evangelos"	"Algorithmic methods in wireless network computing"	"This proposal is concerned with the design of ``algorithmic methodologies'' in wireless network computing inorder to enhance communication efficiency and reliability of wireless networks. Algorithmic methodologiesare common in all areas of computer science. On the one hand, wireless networking arises when connectivity isthe result of devices transmitting in unguided media, and on the other network computing refers to computersworking together and interacting over a network. It is of interest to explore how network computing principlescan lead to improved wireless network design. Questions underlying the efficiency of communication inwireless networks involve a combination of topological, architectural, methodological and coordinationparadigms. More specifically, some of the questions to be investigated include: 1) How are networkparameters such as diameter, connectivity and degree affected by random dependent (node and/or edge) faults?What is the impact of the probability distribution of the faults on the connectivity and diameter of the fault-freepart of the network? 2) How much receiver interference is caused by the transmission range and beam breadthof a sensor, how does it affect the quality of communication and how can extreme value analysis illuminate thelimitations of antenna design? 3) Can we improve the efficiency of sensor networks by judicious employmentof directional antennae and establish rigorous trade-offs involving a) robustness of network connectivity, b)antenna characteristics (directional versus omni-directional) and parameters (length of antennae range andbeam breadth), and c) overall network energy consumption when substituting omnidirectional by directionalantennae? 4) In what way can the use of mobile, co-operating agents take advantage of opportunistic anddelay-tolerant situations in order to establish a more reliable, mobile ad hoc network that attains goodconnectivity and coverage characteristics? How can the coordination of wireless nodes with mobile agents beused to establish secure network boundaries?""481517,""Krantzberg, Gail"
"485417"	"Kunz, Manuela"	"Algorithms and interfaces for improved image-guided surgery"	"This research program is directed to answering the question: ""How can we improve the planning and performance of Image-Guided Surgery (IGS)?"" IGS provides surgeons with methods for surgical planning and guidance for intra-operative interventions. In general, the procedure is a three step process: i) Medical Image Acquisition, ii) Planning and, iii) Navigation. During image acquisition, the anatomy is scanned using medical imaging (e.g. CT, MRI). In the planning phase relevant data (e.g. bone, cartilage) are extracted from the image dataset and a 3-dimensional (3D) model of the anatomy is constructed and surgical interventions are planned. During the surgery a tracking system, such as an opto-electronic camera, is installed in the operating theater and sensors are rigidly attached to anatomies and surgical instruments. The camera reads the information from these sensors and then computer graphics and registration algorithms are used to render the tracked position of the instruments on the virtual representation of the anatomy and/or the planned instrument trajectory.   IGS systems can help the surgeon to achieve the best post-operative outcome. However, available systems for IGS require expensive technical equipment and bring a great complexity into the surgical theater. For these reasons, these techniques are still not widely available for clinical use.     The goal of this research program is to explore alternative solutions, intended to provide adequate accuracy and high precision yet which will be easy to use and cost-efficient those allowing more patients to benefit from these technological advances. The research will be performed in two different themes: a) investigation of enhanced surgical planning algorithms and b) development and testing of novel intra-operative navigation solutions. Our goal is to ensure that more patients benefit from technological advantages in IGS.    One PhD student, three MSc, and six undergraduate students will be cross-trained in a multi-disciplinary setting ranging from fundamental computer science through biomechanical and mechanical engineering to surgery.""477386,""Kunz, Thomas"
"479606"	"Kutulakos, Kiriakos"	"Foundations of computational photography and videography"	"Over the last decade, computational photography has emerged as a research field that promises to revolutionize the way we capture and interact with photos and videos. With digital cameras becoming ubiquitous and turning into full-fledged computing devices, it is now possible to interject significant computation before, during and after image acquisition. This has led to a fairly radical re-thinking of the camera's imaging chain---from the optics that transform incident light before it reaches the sensor, to the control algorithms operating the camera, to the algorithms that process its photos and videos afterward.Although the field has now gained a lot of momentum, long-term progress requires tackling fundamental imaging and analysis problems that are still not well understood. In this proposal I focus on three of these problems: (1) analyzing light transport, ie., how light flows through the scene being imaged; (2) achieving multi-focal photography, ie. designing a camera that can take many differently-focused photos at once; and (3) inferring the time-varying 3D shape of people, objects and environments appearing in a video or personal photo collection.While our work will be guided by computational photography applications, the problems themselves represent a natural continuation of core research  in computer vision and computer graphics that I have been exploring with my group.""485595,""Kutz, Susan"
"479099"	"Labiche, Yvan"	"On integration testing object-oriented software systems"	"Most, if not all, of the software testing approaches are incremental in nature: either by following an agile (or extreme) development process or a unified process, testing activities are performed in increments. Any incremental class testing approach for an object-oriented software system has to answer the two following questions: What integration process, indicating in which order classes are (integration) tested, should be selected? Which test design techniques should be applied to unit and integration test classes? Although there is a fairly large number of reported works on both questions, much remains to be done. On the one hand, class (integration) testing techniques have mostly been described without any consideration for any integration process. On the other hand, integration processes have been suggested without much consideration for how they could be used in practice, along with class (integration) test design techniques. It appears that there is a lack of research and practical results on how a class integration process can be used to conduct class (integration) testing. Therefore, it is often difficult to answer any of the two aforementioned questions. This, along with a lack of tool support, is likely one of the main reasons why the notion of integration testing is not well understood by practitioners and therefore why integration testing activities are seldom performed (e.g., with the intend to reveal integration faults). This research aims at providing more trustworthy (research) results to test engineers and managers on how, and to what extent, class (integration) testing techniques benefit (or not) from using an integration process. The overall objective is to facilitate class (integration) testing of object-oriented software systems, thus likely leading to reduced testing costs and wider use by practitioners.""495451,""Labine, Guy"
"474379"	"Lacroix, Suzanne"	"Fibres, composants et dispositifs photoniques tout-fibre"	"Au cours de la dernière décennie du XXème siècle, la photonique a donné un essor spectaculaire aux télécommunications.  En dépit des apparences, la demande n'a cessé de croître dans ce domaine et il est essentiel de continuer la recherche et le développement de nouveaux produits pour répondre à ces besoins.   Par ailleurs, le potentiel de la fibre optique dépasse le domaine des télécommunications à haut débit: d'autres types d'applications des composants et des dispositifs tout-fibre voient actuellement le jour.  Par exemple, certains besoins, tels ceux de diagnostics et de traitements, que le domaine biomédical  manifeste, peuvent être comblés par les nouvelles sources tout-fibre.  Un autre domaine d'application encore plus «exotique» est celui de l'information quantique. Il comprend la cryptographie quantique qui répond au problème de la communication à distance absolument sécuritaire.  Dans ce cas, il s'agit d'utiliser les  mêmes source tout-fibre mais opérées en régime de fluorescence pour générer un ou deux photons à la fois.  La proposition de recherche vise à développer des composants et des dispositifs photoniques pour apporter des éléments de réponses à ces besoins sociétaux.""483344,""Lada, Rajasekaran"
"478967"	"Langer, Michael"	"Human-assisted computer vision"	"A longstanding goal of computer vision has been to create software that can interpret scene content from images with a performance that is similar or even better than the human visual system.  My research has been concerned with estimating scene properties such as the 3-D layout and shapes of surfaces,  their material properties such as color and reflectance, and the scene illumination.  The traditional goal has been to design computer vision algorithms that are as autonomous as possible, in particular, that rely as little as possible on human assistance to constrain the solution.  Many specific computer vision problems can indeed can be solved with very little human assistance, though there remain many important problems that cannot.  The proposed research will address a new and different goal, which is to achieve better computer vision performance by allowing for greater levels of human assistance than have been used in the past.  Such user involvement is appropriate for many tasks, that do not require an autonomous system and for which the goal is as accurate solution as possible.  Basic research questions will be addressed, such as what type and how much human assistance is required to achieve a given level of computer vision performance.  The research will develop new methods that combine human and computer vision.  We will use established psychophysical methods, namely shape probes, to  measure the scene geometry that is perceived by humans, and we will apply these perceptual measurements as constraints on the solution of the computer vision algorithms.  This is a significantly different approach to doing computer vision.  Rather than asking what performance level can be attained using no (or minimal) human assistance, we will open up the problem and examine how much various levels and types of human assistance can improve the performance of the algorithms.  The vision problems we will examine are very challenging, and include 3D stereo reconstruction of very cluttered scenes such as trees and bushes, as well as 3D shape and layout reconstruction in natural scenes using combined focus and texture cues.    ""474930,""Langevin, André"
"487966"	"Le, Long"	"Efficient and robust protocols for distributed control and optimization of ubiquitous wireless networks"	"Multihop wireless networks, especially those requiring low-cost infrastructure, provide an economical solution for wireless broadband access in various critical civilian and military applications. In spite of active research activities related to protocol engineering for these wireless networks in the past several years, their successful practical implementations are rather limited. Therefore, more fundamental research studies that can guide the design of low-complexity and practically-implementable network protocols for these networks are strongly needed. To this end, the main goal of this proposed research is to build a unified theory for system modeling, analysis and protocol engineering that leads to developing of practical network protocols for multihop wireless networks. Specifically, we propose to consider the design of communication schemes and network protocols in all layers of the protocol stack that enables us to successfully realize these physical wireless systems. We aim to understand the performance limits in terms of maximum achievable throughput region considering realistic physical layer models and advanced communication techniques beyond simplistic pair-wise interference models considered by most existing works in the literature. We will develop cross-layer control protocols that work appropriately over different time scales. These multi-layer protocols should be able to exploit the interactions among layers to enhance network performance with minimal loss of layering modularity while they can be easily implemented in a distributed fashion. In addition, they should be efficient with respect to several key performance metrics such as throughput, delay and packet loss and robust to different forms of uncertainties (e.g., in network topology and channel state information).  Finally, the proposed research will consider heterogeneous radio access technologies as well as dynamic spectrum access capability of wireless devices, which are important aspects of a future adaptive wireless system.  This proposed research will contribute to produce cost-effective wireless technologies and provide HQP in the advanced wireless technology sector, which helps boost the technology transfer and economic development for our country.""492235,""Le, Peggy"
"481912"	"Lee, Daniel"	"Real-time algorithms for spectrum and power efficiency in future wireless communications and networks"	"Wireless communications and network services are penetrating our society at an explosive rate, while the engineering community is setting ambitious goals for rapidly advancing their capabilities. My research program stems from two challenges in wireless communication systems and networks: first, wireless systems should deal with time-varying channel conditions; and second, the capacity of wireless communication is fundamentally limited by the spectrum bandwidth and transmission power.  Therefore, efficient use of wireless resources is extremely critical. To that end, the main objective of my research program is to discover practical and near-optimal algorithms that make real-time decisions in response to changing environments in order to dynamically allocate resources of wireless networks.  In addition, this research program will investigate fundamental and practical issues arising from dynamic resource allocation mechanisms.      New paradigms of wireless networks will need new algorithms for dynamic resource allocation. The application areas of this research program will include emerging technologies, such as cognitive radio and cooperative communications, and future technologies for extremely energy-efficient (green) communications.  ""486443,""Lee, Daniel"
"480415"	"Lee, Tim"	"Automatic analytical software for interpreting skin properties"	"Medical evaluation and diagnosis of skin diseases primarily rely on visual inspection for morphologic features such as colour, shape, border, configuration, distribution, elevation, and texture. The overall clinical approach is subjective and qualitative. Over the past 20 years, there has been rapid development in the medical imaging hardware. However, these new devices still require human experts for interpretation. There is a critical gap between rapidly advancing hardware development and the development of automatic analytical software.In this application, I propose to bridge the gap by working on image analysis programs for the new imaging modalities that could be used for early detection of melanoma, the most dangerous form of skin cancer. In particular, I will work on the following three areas: (1) I will develop new methods to analyse images of internal skin structures acquired by dermoscopy, a hand-held device often used by dermatologists to examine suspicious moles. (2) I will build a set of specialized programming tools for a new optical device that uses light to probe the skin's internal structures to detect abnormality and malignancy. (3) I will also create a new 3-D program to match moles between photographs in order to facilitate physicians who practice total-body photography to follow high-risk melanoma patients. When the proposal is completed, we will find new ways to analyse complex images ranging from 2-D to 4-D, and from greyscale to colour. The new knowledge will benefit the imaging community because the new methodologies can be applied to other image processing and analysis applications. In addition, our new computer programs, especially designed for melanoma detection as set out by the proposal, will move us one step closer to achieving a fully automated skin lesion diagnostic system, which will ultimately help diagnose melanoma in the early stages, thereby reducing un-necessary skin biopsies and decreasing mortality rate.""476469,""Lee, Timothy"
"477033"	"Lethbridge, Timothy"	"Evaluating and improving the Umple model-oriented programming language"	"Over the past three years, we have developed a technology called Umple. This technology unifies programming and modeling. It also allows incremental addition of new abstractions to programs, a process we call 'umplification.' Our initial observation is that Umple can speed up development and improve software quality. In addition, we have evidence that Umple helps students learn about software engineering because they can instantly generate systems from Umple to see the impact of their modeling decisions. Currently, Umple supports most class and state diagram notations. It integrates with Java, Ruby and PHP. It is available as an Eclipse plugin and also online as UmpleOnline.Objective: We wish to improve Umple and contribute to knowledge about this kind of technology in the context of both software development and software engineering education. Approach: We will first assess the effectiveness of Umple when users are performing the following tasks: a) creating medium-sized new systems; b) understanding an existing program; c) converting a program into a simpler program that incorporates Umple; d) learning about software engineering; and e) performing conceptual modeling.We will conduct a grounded theory study, usability studies, case studies in industrial contexts, formal experiments, and comparisons with competing technologies such as the ALF action language being developed for UML. Example experiments may include comparison of comprehensibility of a system written using a) Umple, b) a standard programming language, or c) a UML model-driven development tool. In an iterative manner, we will apply the lessons learned to make improvements to Umple.""493716,""Letourneau, Dane"
"479051"	"Li, Baochun"	"Thrift: optimizing performance in complex mobile internet applications with network coding"	"Users of complex Internet applications, such as navigation with augmented reality, have become increasingly mobile, using a wide range of mobile devices, including smartphones, tablets, and ultraportable notebook computers.  These applications require services from public or enterprise cloud computing platforms, in order to share application data, keep application states persistent across sessions, and collaborate with other users.  As users demand more complex functionality from these applications, and as the burden of storage and communication is shifted to the cloud, an integrated design of both cloud computing and Internet communication protocols is required in order to meet user expectations on performance.  The objective of this research is to design new algorithms and protocols that are optimized for performance in mobile Internet applications, and that combine the malleability of new mobile Internet applications and the availability of abundant storage and computation power in data centres in cloud computing platforms.  We propose Thrift, a new architecture that supports such a combination in a seamless fashion, and network coding is applied as a first-class primitive at the core of our design.  We take a holistic view that combines data centres, mobile devices, and data delivery paths in between, and target bottlenecks and disruptions that throttle the performance.  The Thrift architecture monitors important performance metrics in a mobile application involving a large number of users, analyzes live measurement data, predicts future resource demand using advanced statistical and inference tools, and then makes decisions that allocate resources and apply network coding, both in the cloud and along paths of data delivery.  As a testbed involving real-world users, a collaborative multimedia application will be developed on the iOS mobile platform.  As Canadians increasingly depend on mobile devices and applications running on these devices, our proposed work makes significant advances towards optimizing the user experience in a wide variety of such mobile applications.""490834,""Li, Bo"
"479569"	"Liu, Xiaoping"	"Nonlinear adaptive control and applications to biped robots"	"Some systems can be modelled by a set of ordinary differential equations, together with a set of algebraic equations, so called differential-algebraic equations (DAEs) and some by ordinary differential equations (ODEs).  All physical systems contain parameters, some of which can be measured and some of which are not accessible. Even for measurable parameters, sometimes it is not an easy task to obtain the exact values. Therefore, it is of practical importance to design controllers to achieve control objectives and to design adaptive laws to estimate unknown parameters. Controllers with adaptive laws are called adaptive controllers.      Adaptive control theory has been well established for linear ODE systems and has attracted a lot of attention for nonlinear ODE systems. Almost all the research results for nonlinear adaptive control are obtained with structural assumptions of either matching condition or lower triangular form. There have been no research results on adaptive control for either linear or nonlinear DAE systems so far.       Biped robots are robots which have two legs and are able to walk. The biped robots can be modeled by nonlinear ODEs for single support phase with only one foot on the ground and by nonlinear DAEs for double support phase with both feet on the ground. There are few results reported on adaptive control of biped robots.      The main goals of this project are to relax the structural assumptions by using the existing design methods or by developing a new design approach for adaptive control of nonlinear ODE systems, to develop a design method for adaptive control of nonlinear DAE systems, and to apply adaptive control theory to biped robots. The anticipated outcomes are to build a biped robot, to enrich adaptive control theory for nonlinear ODE systems, to create adaptive control theory for nonlinear DAE systems, and to reduce the gap between adaptive control theory and practical applications. Since many real-world processes, such as chemical processes, mechanical systems, power systems, and so on, are nonlinear, contain unknown parameters, and are subjected to disturbances and delays, this research program has its practical importance in industrial processes.""479840,""Liu, Xiaoping(Peter)"
"488062"	"Locasto, Michael"	"Supporting software security by measuring, coordinating, and enforcing software trustworthiness"	"The public often has little guidance when selecting security software to defend their computers.  What makes this problem so difficult is the lack of information on what specific parts of a computer system (and its software) various anti-virus, anti-spyware, and firewall software actually monitor and defend.  Information security experts typically recommend multiple layers of defense (i.e., ""defense-in-depth""), but there is little concrete evidence for judging what software to pick or determining the makeup of those layers.The key scientific issue here focuses on how to compose trust -- the notion that software will behave as expected -- in these systems. This work thus seeks to measure the composition of existing software protection mechanisms for how much of a computer they ""cover.""  This work also proposes models for how to design security software to cooperate (rather than compete) because their competition typically results in unstable or crashing computers due to conflicting modifications.  Pasting two or more security mechanisms together in the name of ""defense-in-depth"" and having them interact in unanticipated, counterproductive, or ultimately insecure ways does not seem very desirable.Although this research examines fundamentally hard scientific problems like how to compose trust relationships in software, the work in this proposal will have a very practical outcome: enabling end-users and organizations to more clearly understand what their chosen collection of protection mechanisms provides.""488339,""Locat, Jacques"
"474741"	"Lowe, David"	"Object category recognition with large training sets"	"An important long-term goal for the field of computer vision is to enable a computer to recognize broad categories of objects (for example bicycles, cows, or people) whenever they appear in images.  While this seems effortless for a person, the current performance of computer vision systems remains far below that of human vision.  The solution to this problem is important for numerous applications, such as automobile driver assistance, image search on the web, or aids to the visually impaired.  We believe that a major reason for the limited performance of existing systems is that they use inadequate training data.  This research will develop approaches for working with far larger training sets of images, such as tens of thousands of images per image category rather than the few hundred currently used.  This will require new learning methods and fast algorithms for scaling up to large amounts of data.  We will achieve other improvements in scalability by developing new approaches to feature selection, in which only a small subset of the best potential visual features are selected for each particular discrimination task.  Our research group has already developed some widely used software for speeding up similar problems, such as the Fast Library for Approximate Nearest Neighbours (FLANN).  We intend to build on our expertise in these areas to exploit large sets of diverse local features and scale training set sizes by at least two orders of magnitude over current capabilities.  Preliminary experiments indicate that this will give a large improvement in the accuracy of recognition and enable many new applications of importance to society.""480066,""Lowenberger, Carl"
"473781"	"Lu, WuSheng"	"Optimal compressive sensing systems for signal acquisition, reconstruction and processing"	"In a nutshell, a compressive sensing (CS) system acquires a signal of interest indirectly by correcting a small number of its ""projections"" rather than evenly sampling it at the so-called Nyquist rate which can be prohibitively high for broadband signals encountered in many applications. This new signal acquisition paradigm has revolutionized the way digital data are traditionally acquired. The objectives of the proposed research are threefold. First, we aim at developing optimal CS systems that require fewer-than-ever number of measurements that yet contain complete information of the data. This entails reduced number of sensing devices (or lower software complexity in the case of software implementation), hence improving processing efficiency and reducing cost. This goal will be achieved by exploring and maximizing a new measure for the incoherence between the measurement subsystem and an overcomplete dictionary for sparse representation of signals. Second, we investigate new methods to recover the data of interest from the limited number of measurements with better accuracy relative to existing algorithms. Our interest is in large-scale data, hence we must develop fast algorithms for them to be useful in real-time data processing. To this end, we shall develop a class of proximal-gradient algorithms for solving Lp type (with p < 1) mixed convex-nonconvex problems. Third, the algorithms developed are to be applied to problems in digital signal processing such as de- nosing, de-blurring and segmentation in medical imaging and communications such as wireless channel estimation and message recovery problems. These problems are current, significant and technically challenging in their respective fields either because of their involvement in large amount of data (medical imaging) or because of their real-time nature (wireless communication applications). The information processing communities will regard our research endeavors as significant when the proposed algorithms begin to play a crucial role in solving these and other related problems with satisfactory processing speed and performance.""490594,""Lu, Xiaodong"
"483123"	"Lyons, Kelly"	"Systematic Integration of Web 2.0 and Web 3D Tools in Service Systems"	"Today the service sector represents 78% of the Canadian economy. Despite this significant trend to services, R&D investment in the sector has not kept pace. We still do not have adequate methods and tools for describing service systems and representing the interactions that take place between and within them. At the same time, service organizations are increasingly operating in a global arena. The desire to lower travel costs and reduce carbon footprints has led to a growing interest in the use of social media (Web 2.0) and virtual worlds (the 3D Web) to support interactions among the people involved in service systems. Our research program at the University of Toronto leverages the interdisciplinary capability of the iSchool and the technological expertise in the Department of Computer Science.  This unique environment enables us to explore methods and tools for modeling and analyzing service systems as they operate today and to develop novel Web 2.0 and 3D web platforms to support and enhance their interactions over distance. Results of this research will contribute insights into service systems: enhancing the ability to model them; and, providing technological solutions to improve them.""487802,""Lyons, Kelly"
"478952"	"Ma, Bin"	"Bioinformatics tools for proteomics research"	"Proteomics concerns the characterization of all proteins in a complex biological system, and can provide important insight into diseases, identifying biomarkers for diagnosis and personalized medicine. Today, protein characterization is carried out by a mass spectrometry experiment followed by data analysis with software.  However, the data analysis has remained a principle bottleneck in the process.  To deal with this challenge, I have developed innovative algorithms and analytical software routinely used by thousands of researchers in research labs worldwide, from both academy and industry.     The long-term goal of the proposed research is to provide biological and health researchers with a powerful software platform for fully characterizing proteins with mass spectrometry. Backed with novel algorithms, the platform will be much improved in terms of accuracy and will have built-in quality control that guarantees high-quality results. Such a platform will enable and hasten the discovery process in many biological and health applications.  Within this research proposal four short-term goals and subtasks will be pursued:    (1) More accurate de novo sequencing: De novo sequencing identifies novel peptides that do not exist in any database. Our research will address the inadequate accuracy and lack of quality control in current methods.    (2) New database search method with low false discovery rate (FDR): For peptides existing in a database, we will research new methods to identify more peptides while reducing FDR.    (3) Post-translational modifications (PTM): PTMs and their quantity changes are very promising biomarkers. We will develop efficient computational methods to identify and quantify the modified peptides.    (4) Human proteome profile: In order to better characterize recurring human peptides, we intend to build a human proteome profile by analyzing all publicly available human mass spectrometry data.     Innovative algorithms are needed to achieve these goals. The research activities described in this proposal will enable effective training, placing HQP in a cross-disciplinary and collaborative environment.""485377,""Ma, Burton"
"488165"	"Mahajan, Aditya"	"Optimal control of dynamic teams under constraints and uncertainty"	"The objective of the proposed research program is to develop a systematic methodology for the  optimal control of decentralized systems where all agents have a common objective. Such systems, which are called dynamic teams, arise in almost all modern technologies, including networked control systems, communication networks, sensor and surveillance networks, environmental remote sensing, smart grids, transportation networks, monitoring and diagnostic systems, and robotics.This proposal considers three aspects of dynamic teams: (i) Understanding the role of information sharing in dynamic teams. This will provide insights into the trade offs between the infrastructure cost of sharing of information between the controllers and the benefits in terms of improved performance and simplification of design. (ii) Understanding the design of dynamic teams with constraints. This will provide insights into the choice of randomized strategies at the controllers and the choice of a randomizing process to meet the constraints. (iii) Understanding the design of dynamic teams with model uncertainty. This will provide a framework of adaptive control and learning in decentralized systems.The proposed research program will provide a broard training to undergraduate and graduate students in fundamental areas of system science and control engineering, thereby providing them with a solid foundation for their future envdeavors.""494749,""Mahal, Amritpal"
"474638"	"Maibaum, Thomas"	"Software architecture, model driven development and fault tolerance"	"Many practical and foundational problems in software engineering are essentially about representing the structure of software artifacts. Today's software intensive systems are very large and complex. They must be designed and implemented in parts that are manageable by engineers. This requires schemes for putting together parts to represent the whole. Analysis methods must be able to navigate between parts and whole to take advantage of complexity reduction in focusing on parts and then being able to ""promote"" the results to be applied to the whole. Structure may be imposed in at least two ways: the first is the usual hierarchical structuring typified by the part/whole relationship, a kind of vertical structuring, and the second may be seen as having various views of a whole, each representing different, but possibly overlapping, information about the whole, a kind of horizontal structuring.The main objectives for the proposed work include: (i) advancing the modelling and analysis of software architecture (a part/whole form of structuring), by undertaking the development of: (a) a new model of concurrency that distinguishes synchronisation of actions by identification and having them occur simultaneously, (b) an approach to aspects in architectural design founded on the notion of a transformation applied to an underlying architecture, and (c) a more abstract model of dynamic reconfiguration than adopted in earlier work;; (ii) developing further a component based (part/whole) modelling approach for fault tolerance and its analysis; and (iii) developing an understanding of multi modelling paradigms (which use horizontal structuring), such as those used in Model Driven Engineering, in terms of the underlying models described in various formalisms and the relationships between them,  The underlying mathematical ""technology"" to be applied, and the concept that ties these varied objectives together into a coherent whole is the use of category theory, to represent structure, and universal constructions, to provide operations for manipulating such structures.""485510,""Maibaum, Thomas"
"479270"	"Makarenkov, Vladimir"	"Methods and software for the analysis and classification of evolutionary and biomedical data"	"My research proposal involves four major components:1. A number of important evolutionary mechanisms can be only explained by the phenomenon of reticulate evolution which allows for additional evolutionary relationships among species compared to the classical model of a phylogenetic tree. First, I propose to develop two algorithms, based on the distance and probabilistic approaches respectively, for the prediction and validation of partial horizontal gene transfer events and the resulting intragenic recombination. The developed algorithms will be applied on a genome scale to estimate the proportion of mosaic genes in genomes of prokaryotes and phages, and used to assess the impact of HGT and intragenic recombination on the resistance of bacteria to antibiotics.2. Development of two methods and associated software for the analysis and representation of reticulate evolution by hybridization networks. First, I will design a method for computing the smallest number of hybridization events required to explain conflicting gene trees; and, second, I will develop a method for determining whether the relationships among the species at hand should be represented by a phylogenetic tree or by a hybridization network.3. Investigate the problem of multiple sequence realignment and of joint inference of a phylogeny and a multiple sequence alignment (MSA). The methods developed under this research component will be able to realign long genomic regions and carry out topological rearrangements of the phylogeny in order to optimize the agreement between the final tree topology and the given MSA. 4. Design and implement methods for the analysis and correction of high-throughput screening (HTS) data. I plan to develop two techniques for improving the hit selection procedure in experimental HTS. The proposed methods and related software will allow researchers to minimize the impact of systematic error affecting the selection of potential drug candidates.""478600,""Makaroff, Dwight"
"476570"	"Makrakis, Dimitrios"	"Secure and QoS-enabled heterogeneous communication networks for servicing the smart grid"	"Telecommunications has been traditionally at the top of Canadian high tech innovation, and is one of our most profitable markets nationally and internationally. Electric power generation and distribution is another very strong market for Canada. Considerable amount of electric energy is exported every year to the United States, and major Canadian power distribution companies have gained access into the US electricity distribution market.  Now the time these two very important industries cross each others path, in the form of the development of a new technology required to modernize the electric energy sector to successfully continue its growth in the 21st century, known as Smart grid.  Smart grid refers to a future electric grid, containing modern sensors, bidirectional communication capability, and powerful computational infrastructure, all three used in coherence in order to improve the efficiency, flexibility and stability of the electric power generation, delivery and use.""483953,""Maksym, Geoffrey"
"474164"	"Malhamé, Roland"	"Tools for the analysis, design and decentralized control of complex systems:  from decomposition-aggregations to mean field control"	"With the increasing level of interdependencies and interconnection of systems, be they energy or communication systems,  and their interconnection into  envisioned smart grids, be they manufacturing systems and their supply chains,  financial systems,  or environmental systems, there is a great need for effective tools to help the understanding, the design, and for scalability,  the synthesis of decentralized controls. Such interconnected systems are termed complex in this research  proposal.     Within the spectrum of possible complex systems, we consider two relatively extreme cases: type (i) systems, such as manufacturing transfer lines, or cascaded hydraulic dams which exhibit sparse and highly localized connections; type (ii) systems which on the contrary exhibit a dense network of interconnections with components exerting influence on and  subject to the influence of most other system components. A typical man made example of type (ii) systems is economies and markets, or traffic systems where individual decisions influence and are in turn influenced by the mass of other decisions; in nature, an archetypal example is the collective motion of herds, fish schools, and birds.      For type (i) systems, we propose  the exploration of decomposition-aggregation methods with our main engineering paradigm being manufacturing systems. For inspiration, we  also look  towards successful biological systems structures such as  metabolic networks, in an effort to abstract out good control architectures and good design principles later to be justified on theoretical grounds.As for type (ii) systems, we propose to use a methodology that we coinvented. It results from a blend of game theory initially developed for economics applications, and another very successful body of ideas in physics known as statistical physics constructed to provide effective macroscopic descriptions of large systems of interacting particles. The methodology is applied to the study of collective dynamics in fish schools and herds, with  envisioned engineering applications in robotics and communications.""486915,""Malik, Aqsa"
"475684"	"Marchand, Mario"	"PAC-Bayes learning and Kernel methods"	"Machine learning is concerned with the development of intelligent computer systems that are able to learn and generalize from collected data. This ability is required in many application domains. In computer vision for example, we are interested in learning to recognize a scene (and some objects within the scene) from a collection of annotated images. In bioinformatics, we would like to be able to predict which molecule can bind to a given protein by learning from a database containing several examples of molecule-protein pairs that bind together. In machine translation, we would like to be able to translate text from one language to another by learning this task from a set of text samples that have been translated by humans. Many important applications like these strongly rely on the performance of an underlying learning algorithm for constructing accurate predictors. The long-term objective of this research program is to propose learning algorithms that will meet the requirements of such large-scale applications that can improve our quality of life. To progress towards this goal, we plan to expand our previous work on learning algorithms that optimize a rigorous guarantee on the accuracy of predictors, called a risk bound. More specifically, we will focus on kernel methods and, by extending our previous work on PAC-Bayes sample-compression, we will develop PAC-Bayes bound-minimizing algorithms for learning the kernel. We also plan to propose novel learning algorithms for constructing structured output predictors and other predictors that are more general and potentially more powerful than the support vector machine.""474813,""Marchand, Richard"
"479574"	"Mejri, Mohamed"	"Renforcement formel et automatique de la sécurité informatique"	"Depuis l'avènement de l'Internet, la liste d'incidents  liés à la sécurité informatique ne cesse de s'allonger. L'Internet a non seulement donné d'innombrables nouvelles possibilités attrayantes et à faible coût tant pour les individus que pour les sociétés et les gouvernements, mais a aussi suscité de nombreuses idées chez ceux qui cherchent de l'argent facile, les terroristes, et ceux qui trouvent du plaisir à saccager les sites informatiques des autres.  Une faille, aussi infime soit-elle, dans un système informatique, peut engendrer des conséquences indésirables et parfois irréversibles  tant sur le plan humain que sur le plan financier. Pour minimiser le risque, plusieurs techniques et outils ont été mis en place au cours des dernières années. Leur objectif est de faire en sorte que nos systèmes informatiques fonctionnent ""tout le temps"" en produisant les résultats escomptés. La duplication du matériel et les tests des logiciels sont parmi les techniques les plus utilisées. Cependant, sans méthodes formelles, rien n'est garanti et des problèmes peuvent surgir à tout moment. Ce qui complique encore la tâche est la complexité des systèmes informatiques eux-mêmes dont les spécifications font intervenir des aspects multiples et compliqués tels que la concurrence, le temps réel, les codes mobiles et la sécurité. La complexité de ces aspects combinée à l'exigence au niveau de la qualité requise font en sorte que le développement d'un système de bon calibre est très coûteux et souvent inaccessible pour plusieurs petites et moyennes entreprises.La recherche ci-proposée vise à développer des techniques et des outils permettant de renforcer automatiquement une politique de sécurité dans un système informatique tout en garantissant le résultat en utilisant les méthodes formelles.""495470,""Mejri, Sahar"
"477180"	"Michaud, François"	"Integration of high-level cognitive functions in an autonomous robot control architecture"	"Over the last decade, mobile robots have evolved from being rigid, two-dimensional navigation platforms, to compliant and interactive machines that can accomplish more complex and sophisticated tasks, assuming that they can exploit their advanced capabilities intelligently. The ultimate goal is for such robot to be able to decide on its own what, when and how they can fulfill their duties in a secure, efficient and productive manner, with minimal human intervention. I have been developing a mobile platform that brings the necessary compliance, robustness and adaptiveness to handle autonomously the unconstrained and unpredictable situations of the real world. The platform is a compliant omnidirectional humanoid mobile robot capable of natural reciprocal interaction (motion, navigation, auditory, visual, language, touch, affect) with humans, and provides an excep- tional and advanced testbed to address the integration and coordination issues that a robot control architecture must face to operate in real life settings. The focus of this proposal is on the memorization and categorization of the perception and the situations experienced by the robot, 2 key high-level cognitve functions.  Memory is fundamental to reasoning, learning and cognition, but it still remains a poorly understood but required mecha- nism for robots having to understand its interactions in dynamic environments. I will study and develop approa- ches that will be integrated into a hybrid behavior-based architecture, to: exploit the concepts of short-term and long-term memory to achieve real-time processing and to derive a spatio-temporal representation of the robot's experiences from internal and perceived data; memorize and categorize events through the combination of visual, audible and tactile information; regulate satisfaction of the robot's intentions using emotional signaling, which can in term be used to categorize behavioral strategies. These approaches will be evaluated in the context of making an interactive robot assistant that will have to operate in our facility for the longest time possible. Performance will be measured in terms of the added capabilities brought by the high-level control mechanisms, and from the robot's overall ability to operate autonomously in open environments.""476882,""Michaud, François"
"478949"	"Mignotte, Max"	"Bayesian statistical fusion models based on constraints and multiple criteria in image processing and computer vision"	"My research program investigates the use of new unsupervised probabilistic or energy-based fusion models for understanding, analyzing, and manipulating still, moving and multidimensional or multi-modal images.  More precisely, this research program will attempt to propose new statistical models to integrate multiple (low or higher level) and complementary image cues (e.g., color, texture, edges, corner and interest point detection, symmetry detection, Gestalt laws of perceptual organization, etc.) and/or to exploit one or several low-level applications (segmentation, edge map, restored image, etc.) for the reliable estimation of a final high-level computer vision task (e.g., motion detection/segmentation, occlusion map, complex shape localization, 3D reconstruction, etc.).  These problems have a wide range of applications in several fields, including multi-modal medical image applications, geoscience and hyperspectral imagery and more generally in all multi-camera or multi-modal recognition and reconstruction systems of the next generation.""483427,""Mihailidis, Alex"
"479150"	"Miller, James"	"Engineering internet-centric software systems"	"Web 2.0 applications are set to revolutionize computing, especially within cloud computing environments. However, Web 2.0 applications are error prone.  Hence, this research program proposes a radical new approach to verification. It is argued that this approach needs to consider these systems in a language-neutral fashion rather than statically analyze their code base. Fuzzing or Fuzz testing is a popular, highly automated, language-neutral verification technique. However, traditional fuzzing approaches suffer from poor code and input coverage characteristics. It has been shown that traditional forms of fuzzing can be successfully extended by integrating other techniques such as symbolic execution. However, such extensions are often computational highly expensive. Recently grammar-based white-box fuzz testing has been proposed as such an extension; however, this extension has received limited attention in terms of its actualization. This program of research proposes to explore this actualization, and explore it significantly beyond its initial proposal. Its ultimate goal is to produce a highly automated, industrial-strength (highly scalable) testing framework and process. Specifically, it will address the following questions:-    ) How descriptions of the structure of the system's input can be used to improve code coverage;-    ) How to produce test cases which efficiently cover the input space of industrial-strength software;-    )How such computationally expensive approaches can be parallelized for efficient execution on multi-core and cloud computing environments;-    )How test oracles can be extended beyond simple pass / fail mechanisms found in traditional fuzzing;-    )How such automated tools can be incorporated into (existing) production processes; and-    )How automated guidance can assist the tester to make improved decisions with respect to inputs into, and results from, such verification processes.""474581,""Miller, John"
"474826"	"Miller, Michael"	"Synthesis and optimization of reversible and emerging technology circuits"	"Researchers anticipate that in 10-20 years current circuit technologies will reach their limits. It is thus important to consider novel approaches and technologies. Reversible logic offers an important alternative due to its relation to emerging technologies including quantum, optical, nano and bio gates and circuits. Reversible logic is also applicable to low power design in current technologies.   Quantum circuits (which are inherently reversible) are in their own right of critical importance since they support a computational model radically different from conventional digital circuits.  Quantum data can represent a supervision of binary states.  As a result, algorithms can be formulated that solve certain problems exponentially faster in the quantum domain than in the traditional binary logic domain.   The short term objectives of my research program are extend our design flow for reversible networks.  My students and I will investigate bounded search techniques in all phases of the synthesis process.  We will extend our recent work on optimizing circuits of elementary quantum gates to larger quantum gate libraries and gate libraries in other emerging technologies.    The first long term objective of my research program include investigating mathematical models and computational procedures for the direct synthesis of circuits in quantum and other technologies.  This work is novel in that it will employ our earlier work on decision diagrams for reversible and quantum circuits. My second long term objective is to examine architectural structures required to implement reversible computation, how they relate to high-level language constructs, and how our synthesis methods can be employed in implementing those structures in various technologies. This work is novel as it will explore the interaction of reversible circuit synthesis and computer architecture design in emerging alternative technologies.      The proposed research is significant as a foundation to the use of emerging technologies and to establishing reversible computation as a viable alternative to he traditional irreversible approach.""486319,""Miller, Michael"
"488097"	"Moha, Naouel"	"Antipattern-based approach for service oriented design and quality improvement"	"Service Oriented Architecture (SOA) is an emerging architectural style becoming broadly adopted as it offers the ability to develop low-cost, flexible, and scalable distributed systems by composing ready-made services. This architectural style can be implemented using a wide range of SOA technologies such as AJAX, OSGI, REST, RPC, SOAP, SCA, and Web Services. However, the emergence of such service-based systems (SBSs) raises several challenges. Indeed, like any other complex software system, SBSs must evolve to fit new user requirements and new execution contexts. These changes to accommodate new user requirements and execution contexts degrade the design and quality of service (QoS) of SBSs and often result in the appearance of common bad practice solutions, called antipatterns--by opposition to patterns, which are good solutions to common recurring problems. In addition to the degradation of the design and QoS, antipatterns resulting from these changes hinder the maintenance and evolution of SBSs and make it hard for software engineers to maintain and evolve software. Moreover, the dynamic nature of an SOA environment involves that antipatterns are also dynamic, and thus their detection and correction must be performed at runtime.    The long-term goal of this research program is to assess and improve the design and QoS of SBSs through the automatic detection and correction of SOA antipatterns. More precisely, this goal will be achieved by developing and applying a novel and innovative methodology supported by a framework to specify, detect, and correct automatically, at runtime, SOA antipatterns in large scale SBSs relying on different SOA technologies. The originality of this program lies in the dynamic nature of the SOA environment and its various underlying technologies. This methodology will deepen our understanding on antipatterns, design, and QoS in SBSs, and allow the development of tools intended for software engineers to assess and improve the design and QoS of SBSs handled by end-users, and ease their maintenance and evolution tasks.""488653,""Mohajerani, Amirhossein"
"480838"	"Mohammadi, Farah"	"Développement des méthodologies avancées pour l'analyse électrothermique des circuits électroniques"	"La tendance en circuits de plus en plus miniatures et denses (loi de Moore) continuera malgré les limites du développement technologique, les défis de conception et les coûts exorbitants. La miniaturisation des composants et circuits électroniques a mené à l'apparition du phénomène dit d'auto-échauffement comme limite critique à la performance et à la fiabilité des circuits et des microsystèmes électroniques de nouvelle génération. Pour chaque augmentation de 10 Â°C de la température, le taux de défaillance de produits électroniques double. En conséquence, une attention particulière est de plus en plus portée sur l'analyse électrothermique des transistors, interconnexions et circuits incluant les effets de Fourier et micrométriques.De plus, d'ici l'année 2016, l'International Technology Roadmap for Silicon (ITRS) prévoit que la longueur de canal pour les dispositifs CMOS sera aussi petite que 9 nm, que la puissance absorbée dans une puce de microprocesseur atteindra 288 W, qu'il y aura 10 niveaux d'interconnexions et que la surface d'une puce sera approximativement de 300 mm2 tandis que la température de fonctionnement aura les mêmes limites qu'aujourd'hui. Les stratégies destinés à relever les défis posés par les effets thermiques dans les systèmes électroniques exigent des efforts expérimentaux et informatiques conséquents pour prévoir la température sur une gamme d'échelles allant du composant, au circuit et au niveau architectural. Ce projet de recherche développera des méthodologies avancées pour l'analyse électrothermique des circuits et microsystèmes électroniques.Les objectifs de ce projet sont : a) Développer des modèles thermiques dynamiques compacts, b) Améliorer le processus de mesures thermiques et c) Développer de nouvelles approches pour déterminer le comportement thermique des circuits intégrés et des modules multi-puce.""488666,""Mohammadi, Milad"
"481188"	"Mori, Gregory"	"Recognizing human figures and actions"	"In this proposal we address the problem of human action recognition.  The grand goal in this area of computer vision is to build systems that can find human figures in either still frames or video sequences, and determine what action they are performing.  The ability to build such systems would facilitate many interesting applications.  This technology would be directly applicable to human-computer interaction, image and video retrieval and search, automated surveillance systems, video motion capture, and vehicle driver-assist systems.""487882,""Moriarty, Tara"
"478948"	"Moura, Lucia"	"Combinatorial and algorithmic aspects of covering arrays for software and network testing"	"Software implementation errors are responsible for most vulnerabilities in the security of information systems. Combinatorial testing methods can help minimize the cost and increase the effectiveness of software and network testing. A recent publication by the National Institute of Standards and Technology (NIST) of the US Department of Commerce on ""Practical Combinatorial Testing'' (October 2010) provides a thorough report on its effectiveness and its complementary relationship to other software testing methods. Research papers discussing combinatorial testing have been increasing rapidly from a few papers in 1995 to more than 70 in 2009 alone. In the past decade, a number of advances in combinatorial theory and combinatorial testing algorithms yielded effective methods for building compact test suites that guarantee excellent fault detection capability based on combinatorial coverage. However, several publications point to desired or even essential features that need to be incorporated into the basic combinatorial testing model to make it more useful in practical software testing. Our research focuses on incorporating these requirements into the models enabling rigorous studies and providing efficient algorithms for the construction of effective test suites for a variety of applications.The proposed research will produce substantial advances on the theory and algorithms for the construction of efficient combinatorial test suites that are flexible enough to be used on an increasing variety of testing situations. This research is based on combinatorial theory and design and analysis of algorithms for the construction of generalized covering arrays, which forms the basis for combinatorial testing. We also propose to make these methods more accessible by developing tools and collaborating on applications in software and network testing. In the long term, this approach is likely to benefit many areas such as software testing, network security, network vulnerability testing, configuration testing and interaction testing.""480225,""Mourez, Michael"
"478384"	"Nascimento, Mario"	"Exploring context in spatio-temporal data"	"Humans and animals often present movement behavior that can modeled as patterns.  The long-term objective of this research is concerned with such movement patterns, i.e., trajectory patterns that can observed, grouped, summarized and indexed for further analysis.  Each of those tasks pose interesting short- to medium-term research challenges. To give but a simple example application,  assume that customary routes taken by a set of drivers are known and consider the event of an accident or road construction.  Being able to determine in an efficient way which drivers would be affected and therefore proactively advising them of the potential impact of that events has a clear practical value. Note that checking every stored trajectory against each event of potential interest is clearly not practical.  As well, handling only the spatial component of trajectories is not enough, e.g., if an accident happens in the morning it is very unlikely to affect those who drive through that location every evening.  A different application domain where knowing trajectory patterns is useful is that of routing queries on-demand through mobile objects.  For instance, in many application domains, such as farming, having the objects broadcast their data continuously is either too expensive (in terms of energy cost) or unfeasible (due to distances between objects and a central location).  We envision a model of query dissemination, where objects are used as the transportation means to take queries from source to destination and back.  In this case being able to take advantage of trajectory patterns is clearly an advantage in terms of query routing optimization.  Finally, both application scenarios above become much more complex if probabilistic trajectories are considered instead of deterministic ones.  We plan to start our research program by considering deterministic trajectories and move naturally towards probabilistic trajectories.""481531,""Nash, Joanne"
"480540"	"Neville, Stephen"	"Component-level virtual machines: A paradigm for achieving controllability in cloud-deployed dynamic distributed systems"	"Component-based software engineering (CSBE) has become standard, e.g., Microsoft's COM, Sun's EJB, and CORBA CCM. Modern societies have become reliant on CSBE systems for core services across many diverse domains, e.g., entertainment and social networking, banking and finance, healthcare, eGovernment, and critical infrastructure. But, ensuring that real-world CSBE systems are robust to failure, meet their quality of service (QoS) guarantees, etc., has remained an open problem, in part due to the lack of predictability of server-level behaviours. Queuing theory provide abstracted models of CSBE system behaviours that tend to become suspect towards a system's performance, capacity, and fault boundaries and neglect system dynamics and transients. Cloud-deployed dynamic distributed systems (DDS) offers a paradigm to address some of these issues by enabling distributed systems that grow/shrink in real-time in response to workload demand. But, DDS introduce closed-loop control theory issues into software systems given DDS use feed back control loops, i.e., stability, transients, potential oscillatory behaviours, etc., all come into play.This research program will address the problem of controllability in cloud-deployed DDS systems through introducing component-level virtual machines (CVMs) as a pragmatic approach that wraps each CSBE component its own minimally scoped VM, with full CSBE systems then composed via CVM-to-CVM message passing. This places component behaviours into hard VM-enforced shells thereby allowing: a) CSBE system predictability, b) tractable control theory analyzes, c) a practical method for applying formal method constructs via CVM-enforced message policies, and d) potentially, significantly improving cloud resource utilization over their current 20% range, a core facet to improving carbon footprints within clouds.  Overall, the research will significantly advance the state of the art in building at-scale software systems that meet QoS guarantees, in a manner that is usable by real-world programmers.""478759,""Nevins, Monica"
"478699"	"Ngom, Alioune"	"High-order/variable-order dynamic Bayesian networks and dynamic qualitative probabilistic networks --- new models of gene regulatory networks"	"Network Systems Biology (NSB) is an emerging area focusing on the study of various types of biomolecular networks, such as Gene Regulatory Networks (GRNs), Transcriptional Regulatory Networks (TRNs), Drug Gene Regulatory Networks (DGNs), Protein Interaction Networks (PPIs and DDIs), Metabolic Networks, and Signaling Pathways Networks. The goal of NSB research is to gain biological knowledge and obtain an understanding of the mechanisms of cellular systems and processes, through analyzing and mining the different molecular interactions, and on the basis of the networked data generated from high-throughput technologies. Sophisticated computational methods are required to reconstruct GRNs from gene expression time-series data or to analyze large biomolecular networks, for instance. The task of NSB research is made difficult by the highly diverse set of biomolecules and the complexity of their interactions, the large size and high-dimensionality of high-throughput data sets often containing imprecision, uncertainty or noise in measurements, or small number of samples or time-points.High-Order and Variable-Order Dynamic Bayesian Networks (HO- and VO-DBNs) and Dynamic Qualitative Probabilistic Networks (DQPNs) are recent models of GRNs, which are capable to predict the time-delayed interactions between different genes.  DQPNs are qualitative abstractions of Dynamic Bayesian Networks as well as temporal extensions of Qualitative Probabilistic Networks, describing how variables in a complex dynamic system influence each other and which are very robust to the imprecision and uncertainty in the data. The long-term goals of this research proposal are to: (1) Devise efficient High-Order DBN (HO-DBN), Variable-Order DBN (VO-DBN), and DQPN approaches for identifying accurate GRNs from (multiple) time-series microarray data, which are scalable and capable of dealing with noise, uncertainty and imprecision in the data, and using various biological knowledge; and (2) Devise efficient HO-/VO-DBN and DQPN methods for the revision, modeling, and simulation of given GRNs with time-series microarray data.""476460,""Ngsee, Johnny"
"474749"	"Nickerson, Bradford"	"Spatial data structures"	"Extremely large amounts of spatial data (e.g. LiDAR data for digital elevation models, mulitbeam sonar data measuring sea floor bathymetry, high resolution aerial and satellite imagery) are being collected at an increasing rate.  Society is increasingly data driven, and this trend is likely to continue and even accelerate in the coming years.  The Large Hadron Collider, for example, generates 37 terabytes (37,000 gigabytes) of useful data per day.  Data is so large that traditional computational models (e.g. the RAM and pointer machine models) are being replaced by models such as the I/O-model that consider disk I/Os as the only meaningful performance measure of an algorithm.  The research program  proposed here investigates new I/O-efficient data structures and algorithms to efficiently answer queries and provide updates (additions and deletions) for such massive spatial data.  These algorithms and data structures form the basis for geographical information systems and for other software systems (e.g. databases, on-line mapping and image services) that make massive spatial data useful for businesses, scientists, government and the general public.     These new spatial data structures will play an important role for operations performed on spatial data, such as isosurface extraction (e.g. for use in flood prediction), volume computations (e.g. for dredging operations) or dynamic path planning for navigation.  We will also explore data structures supporting efficient search to detect features among massive amounts of video and colour imagery (some with audio tracks or associated text), which is valuable for ongoing maintenance of civil infrastructure and security.  This research will contribute directly to improving computer software products that use spatial data.  Canadian companies that sell systems or services incorporating spatial data (or multi-dimensional data in general) can directly use the results of this research to improve the performance or increase the functionality of their software.  Canadian companies will also benefit by hiring the highly qualified personnel (eight graduate students, three postdoctoral fellows) proposed to be funded under this research program.""493736,""Nickerson, Hannah"
"481668"	"Osgood, Nathaniel"	"Stocking Hygeia's toolbox: methodological innovation in support of computational epidemiology"	"Despite great past achievements, Canada faces pressing threats to public health.  While designing appropriate public health policies requires judicious choice of intervention strategies, such choice is complicated by the complex relationships between interventions and their effects  -- raising the risk that even well-intended policies may prove ineffective or cause worse problems than they solve.  To enable them to better grapple with these challenges, public health decision makers increasingly turn to agent-based simulation models (ABMs) for insight.  By letting decision makers simulate the effects of policies in ""virtual populations"", such models can help inform the design of health interventions that are high leverage, robust, and cost-effective, facilitate quicker reaction to an infectious disease outbreak, aid in understanding of health trends, help prioritize data collection, and assist in communication with diverse stakeholders.  Unfortunately, such models are frequently needlessly challenging to build, hard to understand, computationally expensive, and are difficult to share and collaboratively explore in a team environment.  Frequently there is  also insufficient observed data on individual behavior to be confident about the fidelity of the virtual populations to the real-world population.        Within our proposed work, we seek to facilitate the design and construction of ABMs that  inform understanding of population health trends and health policy tradeoffs.  Specifically, we seek to make such models easier to use and faster to simulate, thereby opening up more time to carefully investigate policy impacts. We will also aid the construction of models that are better designed and more widely shared and understood by building software to improve team access to models and model results, and by supporting clearer ways of describing models.   We will provide smartphone applications to collect rich data to inform models' representation of peoples' behaviour, and to permit cross-checking model results against observed data.  Finally, to help improve the ABM accuracy, we will provide ways to automatically correct inaccurate models.""473401,""OShaughnessy, Douglas"
"481842"	"Pan, Jianping"	"Protocol design and performance evaluation for network science and engineering"	"The Internet has achieved great success in the last half century. The networking research community has realized that we are now at the doorstep of designing and evaluating next-generation network protocols for the next half century. There are many efforts already initiated, such as FIND/GENI/NetSE (US), FIRE (EU), JGN (Japan), etc, driven by the emerging of cloud computing, smart grids, cyber-physical systems, online social networks, etc, collectively under the name of network science and engineering. Besides the traditional demand on networking research, such as performance, reliability, security and testability, these new networking paradigms have created many new challenges such as energy-efficiency, ultra-scalability and heterogeneity, and so on, among which the network topology plays an increasingly important role and affects all upper-layer protocols. The proposed research program, together with other academic and industry research programs in Canada, will keep Canadian society and economy on the leading edge of this international competition. Particularly, traditional network protocols follow the ""store-and-forward"" principle. With the advance of physical-layer technologies such as cognitive radio, compressive sensing, cooperative communication and white-space networking, as well as the upper-layer technologies such as in-network processing and overlay networking, the design principle is expanded to ""store-carry-process-forward"" in our research program, to fully utilize predictable, controlled, or user mobility to carry information and to process data flows inside the network to achieve the maximum capacity. This is the first time that such principle is proposed in the research community, although the literature has been examining carry and process separately in last few years. Due to the time/location-varying topology in many emerging, complex systems such as vehicular ad hoc networks, wireless sensor networks, mobile social networks and peer-to-peer networks, we believe such a thorough examination of store, carry, process and forward is very necessary to design and evaluate new protocols for the critical infrastructures that we have to rely on.""494627,""Pan, Kang"
"480583"	"Papadopoulos, Christo"	"Nanoscale electronic networks: materials to applications"	"The extraordinary success of the electronics industry over the past 50 years has led to increasingly powerful and complex silicon integrated circuits whose features now have dimensions less than 100 nm; a regime referred to as the nanoscale.  As these feature sizes are reduced further, continued progress in computers, consumer electronics and telecommunications becomes increasingly challenging.  Industry forecasts predict that by the end of this decade performance and manufacturing challenges when features sizes reach below 20 nm will make it necessary to augment silicon by introducing new materials and devices into integrated circuits.  Carbon-based nanostructures have been identified as prominent candidates to meet these future requirements.This research seeks to create materials and devices to enable future electronics based on carbon nanostructures.  The objectives are to achieve the controlled assembly of carbon nanomaterials and develop these nanoscale assemblies into viable electronic devices with reliable properties and capable of high performance.  The anticipated outcome of this work is the creation of new materials, methods and devices based on carbon nanostructures that can meet the difficult challenges being faced by the semiconductor electronics industry.  Directed self-assembly will be used in conjunction with high-resolution microscopy and precise electronic measurements to develop carbon-based nanoscale network devices in a manner that is compatible with current electronics manufacturing.  By assembling carbon nanomaterials into nanoscale networks their excellent electrical properties can be harnessed to provide tremendous improvements to the performance of electronics.This new approach to materials and devices at the nanoscale will enable innovative electronic and information technologies that bring social and economic benefits to Canada.  Extensive training of highly qualified personnel in the proposed research will be provided by a broad nanoscale research environment.""495970,""Papangelakis, Vladimiros"
"478625"	"Paquet, Joey"	"Context-awareness and demand-driven evaluation in heterogeneous distributed systems"	"This research proposal aims at the definition of a theoretically sound framework to introduce the notion ofcontext-aware system interoperability protocol, context-aware middleware, as well as the associated definition and implementation of a programming language semantics based on a demand-driven evaluation method. We propose to integrate our past theoretical and practical results in this framework, which include: (1) a sound theory of context calculus; (2) a demand-driven evaluation semantics, including a language-independent type system necessary for such an implementation; (3) validation of the approach vs scalability and ease of use as to improve existing methodologies in the specific application and middleware domains, e.g. SOA, and cloud, grid, and mobile computing which are increasingly context-aware.Context awareness and its corresponding demand-driven computation model can be applied to theservice-oriented architecture (SOA) model of distributed computing, enabling service-oriented clients to gather their own context and make service-oriented server demands based on their own context, and the services to be defined according to the context of the service demands. On another spectrum, new computing models such as grid computing and cloud computing are now generally relying on the notion of context in order to constraint or direct the computational resources being used by processing demands. Again, the notion of context-aware middleware, coupled with a corresponding demand-driven evaluation model fits exactly this current need. We will re-use and expand our previous best theoretical results and their practical component-based realization in the above mentioned domains of SOA, cloud grid, and mobile computing and their applications, to unify the approach, test and validate the scalability and usability of various context-aware paradigms in a uniform research evaluation platform.""482244,""Paquet, Marc"
"473836"	"Parker, Philip"	"Simultaneous and proportional myoelectric control"	"Proposal SummaryThe current software and hardware in myoelectric control for powered limb prostheses are limited to sequential and, typically, on/off function selection.  This is due, in part, to the nature of the available prosthesis terminal devices in that they are limited in number of functions and dexterity.  This is rapidly changing due to increasing numbers of amputees and demand for much improved dexterity.  Multifunction prostheses with increased capability are becoming available, and are expected to make marked and rapid improvements.  This change must be accompanied by the development of improved controllers to take advantage of the functionality. Myoelectric control has a major role to play in this regard and to this end our research has recently been directed to the development of myoelectric controllers for simultaneous and proportional control of multifunction prostheses. Considerable progress has been made, however there is a need to extend the results to allow dynamic muscle contractions on behalf of the prosthesis user, to decrease the number of electrodes required, to increase the control performance, and to reduce control performance degradation due to crosstalk between myoelectric channels.  These improvements will make the prosthesis controller more user friendly, and the prosthesis easier to fabricate.""476369,""Parker, Wayne"
"481171"	"Pei, Jian"	"Towards context-aware data mining"	"Communicating context of user requests to data mining applications remains a huge bottleneck.  On the one hand, since data mining tasks are often complicated, it is very difficult for a user to completely express her sophisticated intent. On the other hand, data mining applications are often not designed well to understand context of user requests proactively and automatically.  Fortunately, more and more online information retrieval and data mining applications, such as web search engines, online data analysis and business intelligence services, and online social networking sites, have accumulated user interaction history, for example, in various kinds of logs.  Such user data may provide useful hints to understand context of user requests. In this project, we will investigate the principles and applications of context-aware data mining.  By being context-aware, a data mining application should  understand the context associated with a user request, such as possible constraints, implicit information need, tasks, and ad hoc, query instance dependent user preferences.  We will focus on data mining applications and mining context from user data, such as user logs and user generated content (e.g., interaction with an application, links and tags).  The project will focus on three aspects. To model various context information for data mining tasks, the context categorization and modeling sub-project will investigate different types of context information and the corresponding modeling techniques.  To build a framework of context-aware data mining, the context mining sub-project will develop effective, efficient and scalable algorithms for mining context information from user data.  To build several successful context-aware data mining applications, the context-aware data mining application sub-project will construct practical case studies.The project contains a training that will involve a good number of HQPs all the time. The project also aims at strengthening the collaboration with industry and transferring new techniques to industry.""484102,""Peiman, Kathryn"
"475994"	"Plant, David"	"Scaling fiber optic and micro/millimeter wave communication systems in the next decade and beyond"	"Connectivity drives the way we work, live, play and learn, and the long term objective of the proposed research program is to realize higher capacities and bandwidths in both fiber transmission systems and dispersion engineered micro/millimeter-wave systems. In this context, we are proposing research on two interrelated topics: Theme 1: Terabit Fiber Optic Transmission Systems; and Theme 2: Dispersion Engineered Micro/Millimeter-Wave Signal Processing Systems. The past decade has seen profound changes not only in the way we communicate using the Internet but also in our expectations of what the Internet will deliver. The exponentially growing demand created by human generated traffic and machine generated traffic will push fiber optic networks to their fundamental capacity limits. The long term objective of Theme 1 is to provide innovative and original solutions to the problem of capacity and bandwidth exhaust in fiber optic transmission systems. Our proposed Theme 1 research will significantly impact both system design and technology deployment in future fiber optic metro networks and wide area networks. As use of Internet capacity grows, so too does consumption of the local wireless frequency spectrum. Specifically, GHz-range Ultra-WideBand (UWB) signals will enable new short range high data rate (multi-Gbps) communication systems, as well as being applied in areas such as penetrating radar for medical, rescue and security applications. The long term objective of Theme 2 is to develop and demonstrate tools of unprecedented flexibility in the time-and-frequency domain manipulation of UWB signals for such wireless communications and sensing systems. Successful implementation of Theme 2 research will significantly impact how UWB signals in the wireless world are processed. The ICT sector is central to Canada's economic development and this sector has grown at twice the annual growth rate of the overall Canadian economy since 2002. Furthermore, R&D spending in the ICT sector is 38% of the total Canadian private sector R&D expenditures. Ideas developed will be embraced by Canada's innovative ICT industry, an industry with which I have extensive collaborations.""482587,""Plant, David"
"476917"	"Plataniotis, Konstantinos"	"Intelligent signal processing for future digital media systems"	"The objective of this proposal, entitled `Intelligent signal processing for future DIgital MEdia Systems', (I-DIMES) is to research promising ideas in the design, development and application of intelligent signal processing solutions that provide cognitive processing of digital content, model and analyze complex human behaviour, and facilitate autonomous recognition of context in a process-centric digital environment. This program attempts to provide answers to three interlaced problems, namely: (a) attention problem - how to extract the relevant digital information; (b) interface problem - what is the best way to present the information when the conditions have changed; and (c) application problem - what are the best uses for the extracted information. Thus, the long term research objectives of this proposal are to: (i) provide a comprehensive and systematic study of the links between digital content, context learning, situation awareness, activity recognition, and adaptation; (ii) devise a generalized processing framework for activity and behaviour recognition as part of a digital media environment. The short term objectives are to: (i) personalize access to visual content using end-users' preferences and profiles; (ii) organize content in order to enhance processing, retrieval, coding, and transmission of visual content; (iii) empower users through content and context fusion; (iv) introduce an integrity preservation and privacy enhancing  framework; and (v) exploit novel applications of the developed solutions. In addition to having significant theoretical implications, the proposed research program has potential for social impact through the development of technological solutions for enhancing the life of individuals. To this end, an application scenario has been chosen to test the applicability of the proposed methodologies. The scenario comes from the field of EEG-based brain-computer interfaces and it will be used to test user-intention inference and cognitive processing recognition procedures.""493508,""Plater, Samantha"
"474923"	"Popowich, Frederick"	"Acceleration of real time natural language processing"	"My research into natural language processing (NLP) has over the years dealt with issues related to the design, implementation and evaluation of advanced systems in the area of information retrieval, summarization and machine translation of electronic documents. The current proposal will build on this research and investigate issues in real time NLP, in applications where users need to dynamically interact with potentially large volumes of text and domain knowledge. This large amount of textual data is often generated dynamically, in a very wide range of different languages, and for different purposes. The research will investigate a range of parallel processing techniques to accelerate system performance, in conjunction with the use and extension of existing knowledge resources associated with NLP.The research will contribute to human language technology by providing open-source tools incorporated into one of the largest research and development platforms, and training students in the application of these tools and research. The need for rapid processing of text is considerable when dealing with large or real time textual data streams that are frequently encountered in security applications, which includes information management, business intelligence, and bioinformatics.Canada has a strong international reputation in the area of language translation tools and technologies, and the area has been recognized as a key international market by the recently established Canadian language industry association (AILIA). ""485733,""Poppenk, Jordan"
"487982"	"Quimper, ClaudeGuy"	"Scheduling using constraint programming"	"Scheduling is the process of allocating tasks to a time slot and a resource. Resources should not encounter overloaded periods and tasks are often subject to deadlines. Many additional constraints make the problem intractable. Solvers are computer programs that take the problem parameters as input, explore the solution space of the problem, and produce a solution.This research program aims at designing more efficient solvers that are able to produce solutions in a shorter period of time. Reaching this long-term goal will impact the industry in many different ways. Industries that schedule their operations in a time critical environment will greatly beneficiate from this research since they will obtain better schedules in a shorter period of time.We see three ways that are promising for reaching this goal. First, we want to improve key algorithms that are currently used in schedulers. Some algorithms, whose complexity has been identified as suboptimal, can be greatly improved by using simple and efficient data structures.Second, we want to explore stronger relaxations and new global constraints that can be used in a solver. Relaxations, and their associated global constraints, limit the scope of the search and let the solver focus on the most promising candidate solutions.Finally, we propose a new way of modeling scheduling problems that will improve the computation time. With new implicit models, solvers will ignore multiple details of a solution and rather focus on solving the hard part of a problem. An implicit model leads to a partial solution that specialized algorithms can efficiently complete.""480241,""Quine, Brendan"
"473742"	"Rackoff, Charles"	"Topics in cryptography and computational complexity"	"This proposal is concerned with a number of issues in cryptography and computational complexity.The most basic issues in computational complexity involve trying to show that certain computational problems require enormous, impractical amounts of computer time.  In the setting of cryptography, in fact, we actually want it to be the case that an adversary cannot break the system in a reasonable amount of time. In this project we study a number of specific questions related to cryptography. What does it mean for two people to share a session-key, and what are good ways of doing this in various settings, such as a public key infrastructure? How can we protect our algorithms against insecure hardware they might run on? What are the justifications for constructions of ""block ciphers"" such as AES (Advanced Encryption System)?""483866,""Rad, Ahmad"
"474125"	"Reilly, James"	"Application of machine learning in neuroscience"	"Major depressive disorder (MDD) is a serious mental condition that often completely debilitates a client.  MDD is typically treated with one of several currently available antidepressant medications.  However, the response rate to any of these medications is only about 30%. Unfortunately, there are currently no means for a priori assessment of whether a specific person will respond to a particular medication. Thus, in prescribing a treatment for MDD, the psychiatrist must by necessity resort to a trial-and-error procedure.  This can result in long delays before remission and significant stress on the health care system.    In conjunction with collaborating psychiatrists, the applicant has developed a preliminary EEG-based machine learning (ML) methodology that can predict the response of a person to an SSRI medication  (which is one of the classes of anti-depressant treatment)  before the therapy begins.   It is clear that such a capability, when fully developed, will vastly improve the treatment of MDD.  However, before this system can be exploited in clinical applications, significant further development of the ML methodology is required.  The objective of the proposed research is therefore to develop new ML methods that can reliably  predict response, not just to the SSRI class as is currently the case, but to a wider variety of  pharmacological therapies.  This requires development of new high-performance ML methods that are specific to this application.      To improve performance, better classifiers and better features are required.  We propose using EEG-based brain-source localization methods in conjunction with information theoretic criteria to identify brain sources that are directly associated with MDD. Extracting features from these sources should result in features with increased salience.  We further  intend to investigate improved classifier structures which can optimally exploit the nonstationarity and nonlinearity inherent in the underlying prediction model.""475466,""Reimchen, Thomas"
"480947"	"Robillard, Martin"	"Automated support for framework documentation"	"Software development organizations rely on application frameworks to build applications. Frameworks provide implemented and tested solutions to recurring requirements, and allow organizations to invest their efforts in innovative, value-adding development through the reuse of software infrastructure. Large frameworks must be supported by extensive technical documentation, which can include reference manuals, tutorials, and code examples. Despite the fact that developing and maintaining framework documentation now requires a major investment, very little is known about the impact of the documentation system on the framework's learnability, and very little automation exists to help organizations manage framework documentation. The goal of this research program is to investigate how automatic techniques can be used to support the development and maintenance of framework documentation. In the short term, we target the development of automated techniques to a) analyze existing documentation and usage data to recommend improvements and updates; b) dynamically adapt documentation systems to users needs based on user modeling techniques; c) automatically create tutorial examples based on software version history mining. For the longer term, we plan to build on our understanding of framework documentation to expand the research to address other pertinent issues in framework usability, and in particular the problem of efficiently querying framework documentation. The research methods for all three objectives will involve a combination of exploratory empirical studies, algorithm and prototype tool development, and validation studies using both active participants and archived project artifacts. The proposed work has the potential to significantly impact how organizations produce and utilize framework documentation and may help organizations shift from a handcrafting approach to a semi-automated documentation creation and management approach supported by advanced software analyses. The research will also involve extensive re-thinking of existing software artifact analysis techniques to account for the unstructured and pedagogical aspects of software documentation.""473279,""Robillard, Pierre"
"478626"	"Ruuth, Steven"	"Algorithms for approximating continuum processes on surfaces"	"Continuous processes are ubiquitous throughout the applied and natural sciences. Because analytical solutions are rarely possible, the practical importance of accurate and efficient algorithms for the corresponding partial differential equation (PDE) models cannot be overemphasized. There has been a great effort made to develop numerical methods for many important classes of PDEs. These efforts focus on finding solutions in one, two or three spatial dimensions. But problems involving general differential equations also arise on two-dimensional curved surfaces or one-dimensional curved filaments. For example, PDEs on surfaces can be used to place a texture on a computer generated surface, or to enhance or restore a damaged pattern on a scanned surface.  In machine recognition of objects, the solution of  surface PDEs can be used to characterize the shapes of objects.  In material science such equations have been used to examine phase change of a material on a curved surface, while in theoretical biology,  PDEs on surfaces arise as part of the modeling bone pathologies such as osteoporosis.  Despite the widespread occurrence of PDEs on curved surfaces there is still a need for a systematic approach for effectively computing the solution of general PDEs on general surfaces. Our research will continue the development and analysis of algorithms for solving PDEs on surfaces. The focus is on methods which (1) apply to general surfaces, (2) apply to general PDEs, (3) are accurate and efficient, and (4) are simple in the sense that they treat different PDEs in uniform fashion by making use of standard, existing techniques and software. It is anticipated that the methods obtained under this research grant will enable researchers and end-users to numerically investigate realistic models of general surface processes with great accuracy.""477339,""Ruwanpura, Janaka"
"479141"	"Saavedra, Carlos"	"Advanced frequency converters for the radio front-end of wireless communications transceivers"	"Frequency converters, also known as mixers, are nonlinear circuits that are critical for sending and receiving information over communications links, such as mobile phone networks.  In addition, mixers are needed for the modulation and demodulation of information-carrying signals. Mixer performance is evaluated using several metrics that quantify things such as the conversion gain, the distortion level, or the amount of noise produced by the circuit.  The metrics are usually interdependent, which makes it difficult to keep the mixer's metrics at their optimal level when there are changes in the circuit's operating conditions such as the input signal strength levels, or due to temperature variations, or other factors.     To address that situation we will first investigate new mixer circuits with multiple control pins so that different performance metrics can be varied after the circuit has been fabricated.  This work will position us for the long term objective of the research program, which is to explore how to adjust a mixer's performance, in real-time, to adapt to changes in the circuit's operating conditions. To this end, the mixer will be monitored continuously, the collected data will be analyzed using digital chips and, subsequently, the mixer's control inputs will be adjusted to obtain the best performance from the circuit. Since digital technology will be used to significantly improve the performance of mixer circuits operating at gigahertz frequencies, these advanced mixers can be accurately described as ""digitally-assisted RF circuits,""  a class of circuits that are poised to dominate RF chip design throughout this decade and beyond. The outcomes of our research program will be directly relevant to the Canadian telecommunications industry, including: mobile phone companies, smart-phone manufacturers and chip companies because it will enable them to better control the RF performance of the mobile phones in real-time.  Ultimately, this research will benefit the Canadian public because when they use their smart-phones they will experience better voice and video quality, undergo less signal outages, and their batteries will last longer because their phones will consume less DC power.""487909,""Sabatini, Peter"
"479582"	"Sander, Jörg"	"Projected and semi-supervised clustering for high-dimensional data"	"Clustering is one of the major unsupervised data mining methods facing severe challenges when applied to today's high-dimensional data sets, which are collected on a large scale by automatic equipment (e.g. microarray chips, sensors, logging devices). The sparsity of the data, the small variance of distances in the full-dimensional space, and the inclusion of a large number of ""irrelevant"" or ""random"" dimensions make it typically impossible to detect a meaningful clustering structure using standard clustering algorithms with common full-dimensional (dis-)similarity measures. Meaningful structure can rather be detected by either considering lower-dimensional subspaces, or, by taking into account background knowledge if it is available (often as ""must-link"" or ""cannot-link"" constraints for a small subset of data points) to guide an algorithm to a certain clustering structure that is consistent with this information - overriding to some extent the information derived in the full-dimensional space.   Main objectives of the proposed research program:1) Advancement of the theoretical understanding of clustering methods applied to today's very high-dimensional data sets, particularly for the following relatively recent approaches ""projected (or subspace-) clustering"" and ""semi-supervised clustering"".2) Development of novel and improved algorithms for projected and semi-supervised clustering, overcoming some of their current limitations, extending their applicability, and also combining the concepts of both for a wider range of application areas where such clustering methods can be useful.3) Demonstration of the usefulness of the proposed methods on some real world data sets including  gene expression data, text data, and medical image plus clinical data.""488803,""Sanders, Ashley"
"474407"	"Scott, Philip"	"Polarized logics, geometry of interaction, and the dynamics of resource sensitive computation"	"In this proposal we look at proofs as dynamical systems, interacting by the cut-elimination process.  We study the dynamics of proofs, their complexity, and the dynamical invariants of normalization, via an algebraic framework.This is part of the general theory of logic in computer science, in which we think of proofs as programs. More generally, this is part of a larger program to understand the topological and dynamical behaviour of the kinds of graphs and structures arising from interpreting logic in this manner.  For example, in programming languages developed in this style of resource sensitivity, to know one has constructed a program of a given type correctly is to be assured it will meet some strict complexity restrictions.  In general, we study algebraic and dynamical structures which arise when one has networks with feedback, which are ubiquitous, from analog computing to systems biology.""488259,""Scott, PierreAndre"
"481833"	"Shahbazpanahi, Shahram"	"Power- and bandwith-efficient relay-assisted communications"	"As estimated by the European Commission, the total annual energy consumed by broadband technologies (to provide access to homes) will soon approach 50,000,000,000 KWh (or 50 TWh), out of which 70% is consumed in homes, twice as much energy as spent in servers and across the networks [1]. Taking into account that network data rates are expected to almost double every one and a half years, the power consumption for broadband access technologies and the associated costs are rising exponentially. Driven by these facts, we expect significant changes to occur in the way we communicate today: we are moving from wire-line to wireless access at an exponential rate. As information and communication technologies are driving engines for our society, the profound impacts of such changes are inevitable. Currently, our wireless infrastructure has a rather rigid architecture where communication resources, such as transmit power and radio spectrum at a certain location, are left unused if the user devices are not utilizing these resources to their fullest extent. As such, our wireless networks appear highly inefficient due to its non-dynamic resource allocation strategy. Thus, it becomes very expensive to keep up with the ever increasing demand of ``communicating anywhere anytime''.Alternative network architecture is based on collaboration between different user devices in the network. In co-operative networks, the user devices participate in a collaborative communication strategy to forward/route a message originated from one of these devices and deliver it to the intended destination. Such networks are often called ad-hoc relay networks. While the design of relay networks is still in its infancy, it is essential to consider resource (power and radio spectrum) restrictions at this early stage of the network design to ensure that future relay networks will make highly-efficient use of the available resources, thus providing sustainable communication services to our society. The time to develop ``green networks'' of the future is now and that is what this research focuses on.""487701,""Shahdi, SeyedAli"
"481869"	"Shaw, Christopher"	"Highly interactive visual analytics systems"	"Visual Analytics (VA) is defined as the science of analytical reasoning facilitated by interactive visual interfaces. In contrast to computation alone, visualization can harness the human mind's innate visual intelligence to gain novel insights into situations characterized by complex data. While methods for creating graphical visualizations are understood, visual analytics (VA) directly addresses the ultimate goal of this process: human reasoning and analysis about complex situations.In this proposal, I aim to develop a new architectural model of Visual Analytics software that more directly takes into account the various cognitive activities that analysts engage in during the analysis of large collections of documents. Currently, established practice for examining large data collections is to use information retrieval tools to analyze a document set for trends and to search for evidence that supports hypotheses that may arise from examining the documents. Information visualization tools such as maps are plotted with time-coded events to visualize the movement of people across a city, for example. The strength of these visual tools is that they supply a means of external cognition: they augment internalcognitive processes with concrete spatial representations of relationships of interest that can be referred to later.The drawback of current visual tools is that they do not themselves offer much direct support for more ""high level"" cognitive activity. Many significant analysis tasks take place over a long period of time and must be shared among many people. In my discussions with intelligence analysts, there has been a frequent call for systems that are capable of recapitulating key points of past analyses, and for the re-use of analytical steps on new or updated data. This proposal focuses on the design and development of new models and software to aid in the analysis process. The first new element of this work is to provide an editable and re-playable history navigation channel in addition to multiple visual representations of document collections and the entities within them.""479408,""Shaw, Cliff"
"480532"	"Sima, Mihai"	"Shift-enabled reconfigurable architectures"	"It is increasingly important to compensate for non-linear effects and multiplicative noise with the advent of wireless communications, pattern recognition, speech and image processing. Signal processing in these domains typically employs the calculation of transcendental functions, for which the common Taylor or Chebyshev series expansions require a sequence of multiplications and additions. The word-length required for a given precision increases linearly with the number of consecutive multiplications in the series' expansions. This is problematic on the embedded platforms of greatest interest as the computations are performed using fixed-point arithmetic with reduced precision. Thus, other solutions are needed. Sequential algorithms that calculate transcendental functions using simple hardware are the COordinate Rotation DIgital Computer (CORDIC) and Convergence Computing Method (CCM) that employ Shift-and-Add operations. Since these algorithms are sequential, software implementations are inherently slow, even on parallel processors. Conversely, implementing these algorithms in custom hardware in order to speed-up the computation requires the barrel shifter to be optimized at transistor level. The resulting circuit lacks flexibility and wastes silicon area when transcendental function evaluation is not required. A good trade-off to that problem is the use of a reconfigurable array. The research program aims to investigate the appropriateness of using Pass-Transistor Logic (PTL) to implement a Shift-Enabled Embedded Reconfigurable Array (ShEERA), which provides architectural support for shift operations within its interconnection network. The PTL is known to provide a number of advantages over standard logic at the expense of dynamic power consumption. We will use an unfolded multiplexor (UMUX), which has been proved effective in mitigating this drawback. The research program will also investigate the incorporation of ShEERA into a host processor; such a hybrid would give the programmer in the wireless communications and/or signal processing domains a powerful computing platform for which the execution of transcendental functions is no longer the bottleneck.""484094,""Simaika, John"
"479572"	"Singh, Karan"	"Sculpt and sketch interfaces for 3D modeling"	"Sketching and sculpting have been the medium of visual communication for millenia. Since the early days of computing, as far back as Ivan Sutherlannd's seminal Sketchpad system (1963), interfaces based on a sketch or sculpt metaphor have been touted as ""natural"" approaches to 3D modeling. The last two decades have seen a number of promising sketch or sculpt based 3D modeling systems but the central goal of transforming a visual idea into a digital 3D model continues to be a fertile area of ongoing research. In the past 5 years my research has culminated in successful 3D modeling systems: ILoveSketch and Analytic 3D drawing (sketch-based), Shapeshop3D and MeshMixer (sculpt-based). This and similar research in computer graphics has been largely oblivious of an important body of research in human visual perception that studies the various biases inherent limitations in human perception and understanding of 3D shape. This proposal thus intends to explore sculpting and sketching for 3D shape modeling with greater rigour by first attempting to computationally model the various biases we have in our perception of 3D shape. For example, it is well-known that when asked to draw a cube from an oblique view, humans elongate it in depth significantly. Without a model for this bias, a computer would erroneously infer the drawing to be an elongated cuboid. Conversely, a digital cube, if displayed without correcting for our visual bias, would appear to us as a squashed cuboid. A significant aspect of the proposed research will thus build upon prior research in human perception, shape understanding, abstraction and aesthetics in art and test and model various hypotheses relating to biases in visual perception. Despite the existence of 3D input devices and displays, the bulk of input devices and displays in use today continue to be 2D. Even with a good model of the biases in human perecption the ambiguities in inferring 3D shape from 2D input is what makes digital 3D modeling a uniquely challenging problem. There are two ways to address this fundamental problem: by building better inference engines that use context and likelyhood models of 3D shape to resolve ambiguities (eg. Analytic 3D drawing) or by building better interfaces and interactive workflows that allow users to augment their 2D input and explicitly express their intended 3D shape (eg. ILoveSketch). This research will continue to explore both these research directions.Finally, advances in the digital acquisition of real-world data now make it possible for artists to use traditional media as part of the digital creative process. The challenge is thus no longer to digitally capture the affordances of traditional sketching or sculpting but instead to let users work in a medium of their choice (physical or digital) and reconcile any real-world data with a working digital model. This shall be yet another facet of the proposed research.""485362,""Singh, Mohit"
"481850"	"Sirois, Frédéric"	"Improvement of efficiency and robustness of power superconducting devices through high performance simulation tools and extended resistivity models"	"In industrialized countries, the expansion of power systems is increasingly complex. High temperature superconductors (HTS) is considered as one of the enabling technologies for solving a number of currently challenging issues. In particular, superconducting equipments are about 50% smaller and lighter than their conventional counterpart, and withstand more easily temporary overload, which is critical to peak load management. HTS materials also have the unique property of being natural fault current limiters, a property with no classical counterpart and which opens new doors for operating power systems.The ultimate goal of this research program is to ""enable a new generating of compact, lightweight and energy efficient power devices for solving major bottleneck and security of supply issues on power systems"". Over the next 5 years, the following objectives will be addressed:A1) Characterize the resistivity of commercial HTS wires over their whole current and temperature range of operation, in order to allow full CAD design of fault current limiters (or other applications)A2) Propose empirical mathematical models of the resistivity of superconducting materials valid over the whole range of practical environmental parameters (data sheet-like)B1) Identify the best electromagnetic formulations for solving low frequency Maxwell equations in presence of i) highly non-linear resistivity, and ii) interfacial resistancesB2) Find ways to handle very large problems, such as 3-D geometries (e.g. cables) or 2-D geometries in which hundreds of conductors interact together (e.g. coils, which implies hundreds of constraints)C1) Derive a rigorous explanation of the current transfer mechanisms between the different layers of HTS coated conductors and propose efficient ways to improve their thermal stabilityC2) Improve our understanding of the interaction between HTS and ferromagnetic materials in coil and cable applications, in order to further reduce AC losses ""476296,""Sirois, Jean"
"473972"	"Skillicorn, David"	"Adversarial knowledge discovery"	"Adversarial knowledge discovery builds models from data in settings where the interests of those doing the modelling are not aligned with the interests of some of those being modelled. This includes law enforcement, counterterrorism, border control, fraud, anti-money-laundering and, increasingly, mainstream domains such as customer relationship management. Criminal activity costs Canada perhaps as much as 6% of GDP, billions of dollars per year; and some crime and terrorism also has costs in property damage, injuries, and lives lost.At present, the process of understanding the actions of adversaries from the traces they leave in data is driven by analysts who interrogate the data for patterns that they think may be significant. This is laborious and requires analysts to be skilled, experienced, and creative. Their results can be improved substantially by adding an inductive component -- constructing plausible models algorithmically from the data, and asking analysts to assess them.This is commonplace in mainstream knowledge discovery, but more difficult in adversarial settings because likely patterns are harder to discover: because adversaries are actively trying to vary their activities, because they are trying to conceal themselves from analysis, and because they may be trying to manipulate the analysis process. The next stage of my work in this area tackles four problems: (1) finding algorithms to rank data records without assumptions, based on 'interestingness' so that analyst attention can be focused on the critical few; (2) extracting information from text in a deeper way,taking ideas about how we, as humans, generate language and making them algorithmic; (3) extracting richer information from graph-structured data (for example, relational connections among people based on their interactions; and (4) algorithmically discovering potential meanings of clusters and other structures in data.Progress in solving these problems will make Canada safer, will reduce the risks to our population from criminal and terrorist violence, and reduce the substantial economic cost of crime and fraud.""483310,""Skinner, Brian"
"479107"	"Sodan, Angela"	"Time- and space-adaptive parallel job sceduling for HPC servers, grids, and clouds"	"Recent important new challenges in system software for High Performance Computing are the omni-presence of multi-/many-core CPUs and the emergence of cloud computing. The proposed work focuses on solutions to these challenges in scheduling parallel HPC jobs on cluster/SMP servers, grids, and clouds. The purpose is to develop approaches with ideas which are feasible for practical application in Supercomputing Centers.The main goals of the work are predictability and quality-of-service (differentiated service) in job scheduling. Thus, the job scheduler will employ some form of preemption. Resource allocations can be flexibly defined as resource needs which may be met in either the time dimension (time sharing) or the space dimension (space partitioning) or a combination of both. Additionally, the resource shares may be adjusted in dependence on the job context. In this sense, the job scheduler is highly adaptive.The scheduling algorithms are accompanied by performance estimation. Response times strongly depend on the allocated resource shares. It is therefore important to know the performance resulting from certain resource-share allocations. This permits informed decisions. The performance estimation employs a combination of a model and machine learning of parameters. Therefore, the work will significantly support future server, grid, and cloud computing for parallel HPC jobs.""486219,""Soderman, Stefan"
"479389"	"SoltysKulinicz, Michael"	"Proof complexity of matrix algebra"	"Proof Complexity (PC) is an area of theoretical computer science that deals with the computational complexity of formal reasoning.  PC is intimately connected to major open problems in complexity theory (notably the famous ""P vs NP"" problem, which lately received a high-degree of notoriety in the press; see, for example, the Globe and Mail article by Kate Allen from August 20, 2010), and to the field of automated theorem proving.    My research is particularly concerned with the complexity of reasoning required for matrix algebra.  The language of matrices has become universal in computer science; matrices are used to express all kinds of incidence relationships (such as, for example, connectivity in a network), and formalizing reasoning about matrices, in low complexity, would extend greatly the fields of automated deduction and reverse mathematics.  The Canadian scientific milieu is especially well positioned to work in the field of proof complexity, since the field's motivating question, the aforementioned ""P vs NP"" problem, was posed in 1971 by Prof. Stephen Cook at the University of Toronto.  Thus, Canada has traditionally been a hub of activity in this area.  The high degree of attention is justified by both the practical importance of the ""P vs NP"" problem, and its status as a ""Millennium Problem"" with a one million dollar prize offered for a solution by the Clay Mathematical institute.  My short-term objective is to apply the techniques of the proof complexity of matrix algebra to lattice-based cryptography, and to give a low circuit-complexity proof of standard matrix identities, employing ideas from finite fields and combinatorial constructions such as clow sequences.  My long-term objective is to build a strong case for the security of lattice based cryptography and a low circuit-complexity framework for matrix algebra.  All this falls under my program of reverse mathematics of linear algebra, a sub-program of Cook's research thrust.   Finally, I have developed contacts with the industry, and I am training two graduate students, Tim Paterson (Toronto Centre for Forensic Sciences) and Mohamed Sabry (Pitney Bowes Canada) in cryptography.""493372,""Solymosi, David"
"475398"	"Sousa, Elvino"	"Autonomous infrastructure wireless networks based on LTE femtocells"	"There is currently a great interest in 4G wireless networks as a means to cope with the vastly increasing demands for broadband wireless services. The ITU is currently undertaking a process to specify the next generation wireless networks known as IMT-Advanced. These networks will incorporate newer technologies such as MIMO transmission and the transmission of signals from multiple coordinated base stations/access points. At the same time there has been a great interest in the deployment of very small cells known as femtocells which in the end provides the ultimate path to achieve truly high capacity networks. The wireless lab at the University of Toronto was the original proponent of autonomous infrastructure wireless networks which generalized version of femtocells which preeceded femtocells. One of the major technology candidates for IMT-Advanced  is LTE-Advanced which has been submitted by the wireless operator alliance - 3GPP. However the introduction of autonomous concepts into the 3GPP proposals has lagged and much research remains to answer basic questions and determine the main algorithms. The goal of this proposal is to focus on the evolving LTE air interface and make contributions on coordinated multipoint and autonomic aspects of the network. The autonmoic features of the air interface will allow greatly simplified deployment by a mixture of network operators and users thus paving the way for organic growth of the network infrastructure and ultimately the only path to achieve truly large data rates for a large number of users.""485264,""SoussiGounni, Abdelilah"
"481687"	"Stepanova, Maria"	"Nanofabrication and characterization platform for novel bioNEMS architectures"	"Instrumentation for bioelectrical engineering is a rapidly advancing interdisciplinary field of research and innovation. New emerging technologies integrating proteins and other biologically functional molecules with solid state electronic devices are expected to revolutionize biodiagnostics, biomedical instrumentation, bioengineering, and bioelectronics, as well as empower further basic research in structural molecular biology, biochemistry, biophysics, and bioinformatics. The impact of integrated bio-nanoelectromechanical systems (bioNEMS) on the society of the 21st century has been predicted to be comparable with that of integrated circuits in the 20th. The objective of my research program is to develop a novel nanofabrication and characterization platform for integrated bio-MEMS architectures coupling proteins molecules and artificial membrane assemblies with fabricated solid state structures and devices. My activities will comprise fabrication of biocompatible chips that will carry supported or tethered lipid membranes with embedded or anchored protein molecules; design of chip architectures allowing for label-free characterization employing localized surface plasmon resonance (LSPR), surface-enhanced Raman spectroscopy (SERS), and terahertz frequency sensing; as well as attempts to combine the spectroscopy characterization with addressable electrical manipulation of biomolecules. These activities will build upon the unique nanofabrication infrastructure at the Department of Electrical and Computer Engineering of the University of Alberta and the complementary capabilities in chemical analysis, molecular biology, and  general biochemistry at the NRC National Institute for Nanotechnology. I will also rely on achievements of my other projects, including high-end processes of lithographic  nanonabrication, as well as on breakthroughs in understanding and numerical modeling of protein conformations and morphological organization of lipid membranes. Implementation of this research would provide next-generation bioNEMS with a powerfully enhanced capacity to monitor and direct intermolecular binding, conformational changes, and signal transduction in biopolymers.  ""480557,""Stepanova, Natalia"
"477327"	"Stergiopoulos, Stergios"	"4D ultrasound image processing for computer aided diagnostic applications"	"Thousands of Canadians die each year from non-visible internal injuries resulting from automobile & sports accidents, and - especially of late - military and peacekeeping engagement. Pinpointing the location and nature of these internal injuries as rapidly and precisely as possible dramatically improves patient outcomes.  However, rapid diagnosis of internal injuries in an austere and possibly hostile front-line environment remains a challenge for medical and search and rescue personnel. Moreover, portable and easy-to-use 4D (i.e. real time 3D) non-invasive medical imaging systems are not yet commercially available, primarily because of unresolved major technological and engineering challenges.  To address these challenges, it is proposed to develop a 4D ultrasound computer aided diagnostic imaging capability that will have minimal requirements for training, thus allowing medical practitioners to freely use the new 4D ultrasound concept in Emergency and Trauma diagnostic operations. This proposal addresses the recommendation of the NSERC Review Committee - 1510, relevant with the previous application, 203828-2010. The proposed investigations under this NSERC Discovery Grant will support only Graduate Students to be involved in algorithmic development of (1) adaptive 3D beaforming algorithms with volumetric visualization, which will extend the applicant's previous developments in the same area,  (2) computer aided organ segmentation integrated with decision support processing for providing automated diagnostic capabilities for non-invasively detecting internal injuries and facilitating image guided surgery, in intensive care & emergency units and in remote areas with minimum requirements for training on ultrasound imaging. The two graduate (MASc, PhD) students that will be trained in the interdisciplinary fusion of 3D ultrasound imaging and advanced 3D beamforming processing will benefit also from over $10 million dollars in state-of-the-art equipment already in place. Ultimately, the project will lead to improvement of care for Canadians, and will also train a new generation of researchers and engineers who can further advance such life-critical technologies.""485627,""Sterling, Shannon"
"474521"	"Stewart, Lorna"	"Graph classes: Structure, algorithms, and complexity"	"Many graph problems are known to be NP-hard, which means that they likely cannot be solved in a reasonable amount of time even by a fast computer. However, it is sometimes possible to construct an efficient algorithm for such a problem if something is known in advance about the kinds of graphs that need to be handled. In my research, I consider graph classes that arise in applications or that give insight into a problem's solution, and make use of their properties to design polynomial time algorithms, or to prove that such an algorithm is unlikely to exist. The goal is to understand the interplay between problems and graph properties, and to identify relationships between graph classes and problems that lead to efficient algorithms.""492538,""Stewart, Lucas"
"474347"	"Szafron, Duane"	"Using computer poker as a testbed for solving multiagent decision problems"	"Games are used to advance multi-agent decision-making techniques. Many games are deterministic (no chance) with perfect information (all info visible to all agents) like Chess and Checkers. However, many real-world scenarios with competing agents are non-deterministic with imperfect information. For two-player zero-sum perfect recall games, a recent technique called Counterfactual Regret Minimization (CFR) computes strategies that are provably convergent to an epsilon-Nash equilibrium. A Nash equilibrium strategy is useful in two-player games since it maximizes its utility against a worst-case opponent.We study two-agent non-deterministic imperfect information decision problems that have a many-valued parameter at many decision points. One example is no-limit poker where in addition to selecting a probability of taking each action, the bet and raise actions have an associated amount. Although this parameter is not continuous (measured in chips), there are so many values that using all valid discrete values generates a huge game tree. Other examples include a farmer deciding when to sell grain and how much to sell and a stock-trader deciding when to buy or sell and how much. We will create a new variation of CFR to solve this problem. We can already compute the optimal continuous parameter value under certain conditions for simple no-limit poker variations.For multiplayer (three or more player) games, although we lose all theoretical guarantees, we used CFR to generate agents that won the 3-player events in the AAAI Annual Computer Poker Competition in 2009 and 2010. We will determine what characteristics of CFR-generated agents contribute to ""good play"". We believe that in 3-player games, computing Nash equilibria is unnecessary. We think that algorithms should focus on removing dominated strategies and our goal is to characterize CFR with respect to its ability to do this. After all dominated strategies have been eliminated, the algorithm must filter the remaining strategies to limit exploitability and exploit other agents. We will show that CFR (and some variations) can provide insights.""494065,""Szczechura, Adam"
"487985"	"Talhi, Chamseddine"	"Security enforcement for resource-constrained systems"	"Context: The majority of efforts devoted to improve security enforcement architectures have been characterized by sacrificing system resources. However, such approach is no longer appropriate for information systems dedicated to mobile devices (e.g., mobile phones and smartphones), which are characterized by strict memory and power limitations.Objectives: The long-term objective is the exploration of new approaches to allow efficient enforcement of security policies on resource constrained systems. The short-term objectives are (1) analyzing the security of mobile systems to evaluate the vulnerabilities and assess the underlying risks, understand security policies that should be enforced, and identify opportunities for efficient enforcement, (2) formally characterizing security enforcement to investigate enforceable security policies and constrained security enforcement, and (3) defining domain-specific languages for specifying security policies and providing the appropriate mechanisms required for enforcing them efficiently.Scientific Approach: This research program targets the investigation of important issues related to the security of resource-constrained systems. One important issue targeted by the research is analyzing the security of these systems. The objective is to reach the most complete vision of (1) the security attacks threatening mobile systems, (2) the vulnerabilities facilitating their achievement, and (3) the underlying risks. The results of such analysis will be leveraged to generate a classification of security policies to be enforced. The target enforcement mechanisms will be built on top of formal foundations allowing the evaluation of required resources as well as identification of optimization opportunities. In addition, a bridge will be built between the proposed formal models and high-level security policies specification languages.  Appropriate languages will be defined to allow specifying security policies and their efficient enforcement on mobile systems. Generic enforcement mechanisms will be generated and optimized when embedded inside mobile applications.""474224,""Tall, Franklin"
"478727"	"Tayebi, Abdelhamid"	"Control of vertical take-off and landing unmanned aerial vehicles"	"This project deals with nonlinear state observers and control design for a class of under-actuatednonlinear mechanical systems. We will focus our attention on small-size and low cost vertical take-off andlanding (VTOL) unmanned aerial vehicles (UAVs) commonly known as ""miniature Drones"".In fact, miniature drones are very important when it comes to performing a desired task in a hazardous and/or inaccessible environment. The design of efficient and reliable VTOL-UAVs with a certain level of autonomy, is an important contribution to the field of aerial robotics since potential civil and military applications are tremendous (e.g., high buildingsand monuments investigation, rescue missions, surveillance, etc).Throughout this project, we will contribute to the development of efficient and reliable controllers, ready-to-use in aerospace applications. Furthermore, the expected high-quality research, supported by practical applications, will increase the state of the knowledge in modern control and aerospace engineering, and will provide opportunities for excellent training of high-qualified professionals in a challenging and promising research field.""488258,""Taylor, Adam"
"479542"	"Tellambura, Chintha"	"Multiple antenna two-way relay networks"	"Traditional mobile/wireless systems that use single-antennas terminals (ie handsets andbase-stations etc) cannot keep up with the user demand for increased data rates. Consequently, multiple-inputmultiple-output (MIMO) or ""multiple-antenna technology"" has been developed, which achieves higher datarates and better link reliability than conventional single-antenna systems. It has already been deployed in the third generation broadband wireless networks, wireless local area networks (802.11n) and will also be anintegral technology for fourth generation networks (e.g. long-term evolution (LTE)).However, small phone handsets cannot support multiple-antennas owing to constraints on physical size, batterypower. This constraint has led to the idea of cooperative relay networks. The promise of cooperative relaying has already led to evolving standards (e.g. IEEE 802.16j). Such relaying can be further extended to multiple-antenna relay networks. This approach is feasible for most broadband terminals, wireless laptops andbroadband wireless switches, for instance, that can accommodate multiple antennas.Thisproject will focus more on communication technologies for multiple-antenna two-way  relaying systems. Forinstance, channel and frequency estimation methods, signaling methods, and practicalcoding and decoding techniques will be developed. The MIMO two-way relay technology is the solution for the next generation energy, bandwidth efficient wireless networks. The proposed research thus aims to develop related MIMO, OFDM, CR and relay technologies, contributing to the advancement of 4G wireless standards such as IMT-A. ""474289,""Teller, James"
"481093"	"Thomo, Alex"	"Extracting intelligence from an interconnected world of data"	"Data is generated at an exponential rate from scientific experiments, measurement devices, business information systems, social networks, applications extracting facts from unstructured text, and so on. The imperative is to make sense of these data, to explore and link them, to extract intelligence from them, and to integrate them with data from other sources. The common difficulty today in handling data is the existence of many different representations among applications, which need to share, exchange, and link data. Graph-structured data offers an elegant solution for achieving flexibility and seamless evolution. Still, coming from different applications and data-sources, graph-structured data is typically heterogeneous, imprecise, uncertain, and/or incomplete.  How can we query and navigate heterogeneous graph-structured data? How can we make sense of the results of these queries when the data stored is imprecise and/or uncertain? How can we reason about uncertain data under conditions of incomplete information? These and other related questions will be our main focus in this research.The novelty of this research is in proposing new powerful mechanisms for managing graph-structured data under the above real-life assumptions. Our research strives to provide methods and techniques to make full use of such data and has the potential for great impact in a multitude of data analysis activities in many diverse areas of science, engineering, and business.""479178,""Thompson, Alison"
"488087"	"Thorpe, Julie"	"Improving computer security through the analysis of human factors"	"Computer security and privacy problems can arise for a wide variety of reasons that often involve the interaction of people with computer applications. Unfortunately, many computer applications are designed and evaluated as though they are separate from the people who interact with them (e.g., end-users and developers of the application itself ). This lack of incorporating human factors (e.g., human behaviour, mental models, and usability) in the design and evaluation of computer applications can result in security problems in practice. Such security problems motivate the proposed research program. Two directions will be examined in this research: one on human factors related to end-users (i.e., user-centered human factors) and another on human factors related to software developers (i.e., developer-centered human factors). The goal of this research is to improve computer security and privacy by incorporating user- and developer-centered human factors into the design, creation, and evaluation of new security-focused software tools.     The user-centered direction is motivated by security problems in user authentication applications that are theoretically secure by mathematical arguments, but not in practice as users tend to choose similar passwords. This research direction aims to improve security for the general population by designing novel methods for stronger passwords. The motivation for considering developer-centered human factors is that many high-impact security problems, such as denial-of service or code injection attacks, are caused by programmatic or design errors in software. It is people - software developers and designers - who create faulty software, which can lead to security vulnerabilities. Thus, this research direction aims to design novel software development tools (i.e., security-comprehension tools) to help developers identify hidden security problems in their code.         This research program will train graduate students in human-factors-based analyses of existing user authentication applications and software development tools (to identify areas for improvement), and designing/evaluating new user authentication applications and security-comprehension tools.""475617,""Thorpe, Steven"
"473860"	"Tsotsos, John"	"Re-visiting Ullman's visual routines"	"The dream of useful robots, such as companion robots for the elderly or autonomous automobiles, requires major advances in the computational realization of visual cognition. I and my trainees have previously presented theory, algorithms and systems that successfully demonstrate a wide variety of the component capabilities of such visual cognition, but we have not yet put the pieces together into a whole functioning system. An old but influential concept, the visual routine  suggested by S. Ullman in 1984 seems to present the right foundation for this integration. Ullman suggested that a process he called a visual routine, composed of sequences of elemental operations could be responsible for computing qualities such as shape or spatial relations. Such qualities are important for visual cognition in general. Many have looked at this idea in the past but with limited success. I feel the idea continues to have merit and that perhaps the lack of progress is due to the fact that no one has yet updated the visual routines idea given a current understanding of vision and visual attention; a vast amount has changed in our understanding since 1984. The proposed research will re-visit the visual routines idea in the context of a current understanding of vision and visual attention, focussing on how our past work may be extended to define a new conceptualization of visual routines.The role of visual perception, for both humans and robots, is not to simply categorize single images. It is to enable an understanding of the physical world and to guide action and manipulation within that world. Without the ability to visually reason about what is seen, this goal cannot be realized. The proposed research will contribute to this goal and will be tested on my lab's visually-guided robots. Not only would advances in this area bring closer the dream of companion robots for the elderly or autonomous automobiles, but they would also contribute to more useful visual surveillance systems or to robots that may safely interact with humans in manufacturing settings, among other applications.""478161,""Tsotsos, John"
"479640"	"Turcotte, Marcel"	"Development of bioinformatics tools for RNomics research"	"New high-throughput projects and techniques have shown that a significant fraction of the genome of higher organisms is represented in primary transcripts. Furthermore, comparative sequence analysis revealed that many regions are evolutionarily conserved, which raises the important question: what is all that non-coding RNA used for? The proposed research program is for the development of bioinformatics tools to assist the identification and annotation of functional RNA elements. In recent years, our group has developed tools for the simultaneous alignment and structure prediction of RNA sequences, as well as for the discovery of structure motifs using suffix arrays algorithms.  With our life science collaborators, we studied the IRES motifs in the UTRs of mammalian genes, we explored the landscape of RNA secondary structures in subviral RNA pathogens, and developed computational approaches to investigate the trans-splicing activity in the mitochondrial genome of diplonemids. The new research program described here extends these approaches and comprises two themes: 1) Identification of RNA-RNA interaction motifs: discoveries in the mitochondrial genome of Euglenozoa, 2) Breaking the genome's regulatory code: a logical and relational learning approach.  RNA elements are playing critical roles in the cell, the dysregulation of the associated processes is often associated with disease state, consequently progress in understanding them will therefore have a direct impact on human health, agriculture, and on our understanding of cellular biology in general.""494333,""Turcotte, Nicolas"
"479592"	"Tzerpos, Vassilios"	"Software pattern detection"	"A large number of science graduates are employed in software development projects all across Canada. Such industrial software systems are invariably large and complex making it impossible to possess a complete understanding of the whole system. Experienced developers are often looking for particular patterns that will help them navigate this large space more effectively.    However, such seasoned developers can be hard to come by and often exhibit high turnover. Important knowledge about the system departs when experienced members of the developing team move to other projects or companies.  This creates significant productivity losses, especially when the time comes to fix bugs, add new features, or migrate the software system to a new hardware platform.    The research outlined in this proposal aims to help increase the productivity of software companies in Canada by providing an infrastructure that will allow the efficient detection of interesting patterns in information extracted from the system's source code, execution logs, and repositories.  This will be accomplished by:    a) developing a data model that will combine the three distinct types of information above at various levels of granularity and abstraction.    b) identifying design pattern instances in the system design, thereby providing clues about the design rationale.    c) identifying security vulnerabilities by detecting patterns in the extracted information, and suggesting fixes that can be applied before the software is released.    We believe that the software industry stands to benefit in increased productivity and system quality from the development of techniques such as the above.""475368,""Tzoganakis, Costas"
"475349"	"Urquhart, Alasdair"	"Logic and computational complexity"	"The principal aim of my research is to understand the limitations of computer programs in solving certain important types of problems.  These problems are ones where there are a certain number of conditions to be satisfied; assuming a given collection of conditions, it is easy to check whether or not a proposed solution is in fact correct, but it appears to be difficult to search for a solution.  An everyday example of this kind of problem is provided by puzzles, such as jigsaw puzzles, or the problem of solving Rubik's Cube.  These type of problems are in the category NP.  Of these, the best known is the Satisfiability problem from logic, which is the problem of whether a formula of propositional logic has a satisfying assignment.The main goal is to show that such problems, and in particular the Satisfiability problem, are hard to solve in general.  In the case where a satisfiability problem has no solution, we can demonstrate this by providing a proof in a logical system.  If we can show that in certain cases, where the problems are complicated, that the proofs of unsatisfiability must be exponentially long, then this shows that certain algorithms for the problem must take an exponentially long time in the worst case.  This has already been shown for the best known and most successful algorithm for satisfiability, the resolution method.  I hope in my research to extend these results to more powerful logical systems, and hence to broader classes of algorithms.""485400,""Urquhart, Bradley"
"478263"	"Vassileva, Julita"	"Designing social infrastructures encouraging behaviour change and participation through personalized incentives"	"Social media will revolutionize how business, politics, and education work. It has the potential to influence people's behaviour to do good things for themselves and for their communities. To unlock this potential, one needs to answer profound research questions about human motivation and solve the technological challenges in building infrastructures that support successful communities. This research aims to develop an infrastructure that motivates people to pursue and accomplish goals that benefit them and their communities. The research will focus on developing:   - a user-modeling and personalization infrastructure that allows to share user data among internet applications, social networks and apps on mobile phones. This will ensure the capability of personalizing the motivational interventions and incentive mechanisms to the needs, goals, context and social relationships of the user.   - a persuasive and incentive mechanism that encourages learners to explore the vast resources on the internet through self-directed learning.  - a set of persuasive applications that motivate people to eat healthier, exercise, be more connected (for elderly people), as well as social incentive applications to encourage youth involvement in volunteer groups and charities. As a result, software systems will be developed motivating people to learn, live a healthy lifestyle, act to the benefit of community. ""484506,""Vassileva, Svetla"
"485408"	"Wachowiak, Mark"	"High-performance global optimization and signal processing on multicore, GPU, and hybrid architectures"	"Computation is now considered as one of the three pillars of contemporary science, along with theory and experimentation.  The proposed research will contribute to interdisciplinary computational science by exploiting the potential of graphics processing units (GPUs), multicore computer systems, and hybrid GPU/multicore architectures to solve complex problems in the related areas of global optimization and signal processing.  Many investigators in diverse fields are exploring GPUs, originally developed to speed simple arithmetic operations for computer graphics, for general purpose computation. This research will concentrate on identifying and exploiting data- and task-level parallelism for optimization and signal processing problems that require the processing of large amounts of data.  Optimization will be applied to multidimensional scaling, an important component in information visualization and analysis for geospatial problems, and to particle swarm techniques, which will be used to calibrate large mathematical models, such as those that are common in spatial econometrics.  Global optimization will also be used in blind source separation, which has implications for the development of noise-cancelling hearing aids.   Furthermore, signal processing on new architectures can facilitate geospatial research by enabling efficient analysis of streaming data from sensor networks and weather stations. These computational areas have great societal and economic importance, as improved high-dimensional global optimization approaches will facilitate high-fidelity simulations and models needed to solve important scientific and engineering problems.  Signal processing algorithms utilizing the new architectures allow more efficient solutions to problems in biomedicine and real-time applications. The goal of the proposed research is to investigate and analyze the capabilities and limitations of multicore/GPU algorithms that can be incorporated into applications in simulation and modeling, biomedical signal processing, and in geospatial data analysis.  As most desktop and laptop computers are now available with both multicore chips and GPUs, exploitation of these capabilities is increasingly relevant.""487720,""Waddell, Thomas"
"487965"	"Wakkary, Ron"	"Improving design strategies for customization and appropriation"	"The objective of this research is to significantly improve end-user customization by the development of new design processes and technical models within human-computer interaction (HCI) and interaction design. Modular design strategies underpin almost all customization approaches today and there is a need to push beyond and revise these strategies or develop new ones. We propose to leverage existing practices like modularity and end-user development (EUD), as well as emerging end-user practices like toolkits, do-it-yourself (DIY), and appropriation to overcome current limits. The promise of this research is that end-user customization can be significantly improved upon by the development of new design processes and technical models. The anticipated path to this contribution is a substantial revision or replacement of modular design as a design strategy. Improvements in end-user customization and appropriation will lead to optimized and situated use of interaction design artifacts and systems that will have the capacity to adapt over time addressing changing needs and contexts as directed by end-users. The work will establish the degree to which modularity is indeed a limiter to appropriation and specify what aspects in particular are problematic. The work will also establish more systematically the benefits and value of emergent end-user practices of DIY, toolkits and appropriation for designers and developers. The contributions will include design methods and technical framework guidance for interaction design and HCI professionals in designing for appropriation, plus digital technology prototypes that allow for extended customizations that have the potential to be appropriated by end-users. ""490990,""Walbridge, Scott"
"481688"	"Walus, Konrad"	"Exploring coherent dynamics and field clocking of quantum-dot cellular automata circuits"	"Advancements in silicon technology and the computer design tools have enabled the production of chips that have over a billion transistors. However, it has become clear that fundamentally different ideas are necessary in order to push the ultimate limits of scaling. As a result of the investment by both industry and governments, technology is available for manipulating matter at the nano and atomic scale with incredible precision allowing us to consider molecular and atomic devices as potential successors to the conventional transistor. Quantum-dot cellular automata (QCA), a computing paradigm based on the field interaction of dynamic arrays of bi-stable molecular and atomic nanostructures has been shown to implement general purpose computing, as well as contribute to significant reductions in power dissipation, a major challenge with scaling conventional technology. Exciting recent experiments demonstrated a functional QCA device at the atomic scale, providing strong evidence that QCA can indeed reach the ultimate limits of scaling. QCADesigner, the most widely used numerical simulation tool for QCA research has enabled a large number of fundamental circuit and system level studies of this emerging technology. Analytical and numerical research using this tool has highlighted the critical issue of coherent dynamics in QCA and its impact on the ability of the numerical tools to identify the correct system ground state for an arbitrary layout and input vector. The purpose of this fundamental research is to provide entirely new theoretical and numerical tools for QCA by developing and integrating a model for coherent dynamics and the necessary tools to simulate field clocked molecular QCA arrays. These tools will enable a broad range of fundamentally new research studies of the behaviour of QCA circuits and systems including critical studies of the effect of coherent dynamics on the clocking networks, power dissipation, circuit density, and the potential of QCA as a scalable quantum computing technology. These studies will facilitate an improved assessment of the commercial potential of QCA even before all the technical challenges associated with its implementation are fully solved.""491397,""Walzak, Timothy"
"475193"	"Wang, David"	"Issues in the implementation of haptic compression"	"The growth of the Internet can be attributed to the ability to compress information such as pictures (jpg), videos (mpg) and music (mp3). There is growing research into adding the sense of touch, or haptics, into computer applications. Haptics is created by mechatronic devices that exert forces on the human user, with the goal of increasing realism in immersive environments for applications as varied as computer games and medical training. Haptic models that are currently used are quite complicated.       A complex object is usually broken down into a sufficient number of smaller elements such as triangular polygons. Touch is added by giving each of these polygons a haptic effect. Thus, a vast amount of data needs to be stored.  As well, detecting which polygons are in contact with the human user is extremely computationally intensive. Two approaches have been studied by the research team to compress haptic data and the goal of this proposal is to further expand on these studies:1. It is well known that the ability to sense touch, much like vision and sound, is greatly impacted by human perception.  We propose to continue to study the influence of speed, force and other factors in haptics as it relates to the level of detail that can be perceived by the human user, so that imperceptible haptic details can be removed.  This will involve human testing with haptic devices.2. The current practice of creating complex virtual objects using simple polygons is inefficient. We propose the use of analytical solutions that, although complex, describe larger areas of the surface, creating a much more efficient methodology for haptic interactions. In this proposal, this work will be extended to include objects with both stiff and deformable surfaces.      We propose the use of analytical solutions that, although complex, describe larger areas of the surface, creating a much more efficient methodology for haptic interactions. In this proposal, this work will be extended to include objects with both stiff and deformable surfaces.  ""484051,""Wang, Dennis"
"481260"	"Wang, Hai"	"Data summarization on probabilistic databases"	"Data summarization techniques have been used in many database management systems to provide quick approximate answers to queries when exact answers are not required or when early feedbacks are helpful.  For example, the query optimizers in database management systems employ data summarization techniques to quickly estimate the sizes of intermediate results and the costs of different query execution plans.  Many on-line analytical processing, data mining, and data visualization applications also rely on data summarization techniques to compute fast approximate answers to queries.Currently, data summarization techniques are generally applied to databases in which all data values are precise and accurate.  However, in many databases, the data values are often associated with uncertainty and follow a probabilistic distribution.  The uncertainty is possibly originated from inherent measurement inaccuracies, sampling errors, or network latencies.  Such databases are referred to as the probabilistic databases in the literature.  Probabilistic databases have been recently studied in a number of domains, including database management, sensor networks, consumer behavior modeling, and bioinformatics.The primary objective of this project is to develop new data summarization techniques on probabilistic databases.  These techniques will be evaluated through experiments on synthetic and real data sets.  Results from this project will significantly contribute toward probabilistic database management.""482087,""Wang, Haijiang"
"479110"	"Wei, Ruizhong"	"Combinatorial methods in network security and communication codes"	"This project is a continuation of my previous research work that is supported by an NSERC Discovery Grant. In this project, we will apply combinatorial mathematics to several important research topics in computer networks and communications. We will provide some new security schemes for wireless sensor networks that have a lot of real applications, such as military sensor networks, sensor networks to detect and characterize Chemical, Biological, Radiological, Nuclear and explosive attacks and material, sensor networks to detect and monitor environmental changes in plains, forests, oceans,  etc.  We will also propose more efficient scheduling methods for sensor networks to prolong the lifetime of a sensor network and increase its reliability.  For communications, we will give new constructions of optical orthogonal codes, which will  provide better communicating performance and support large number of simultaneous users in optical networks. One of the distinctive characteristics of this proposal is that  combinatorial methods are the main tools to approach the destination.  This proposal is dedicated to promoting the combinatorial methods in network security and communication codes by investigating  important topics in these fields and by training highly qualified young researchers in this area. It is also expected that new theories and efficient protocols are invented to solve important problems in these fields. And theories of combinatorial mathematics are further developed as well.""492527,""Wei, Ryan"
"488043"	"Wigdor, Daniel"	"User interface feedforward and feedback supporting and enabling body tracking technologies"	"Recent advances in computer vision have made it possible for users to provide input to systems in far more natural ways than before. Camera-based tracking of a user's body allows systems to recognize hand gestures, body posture changes, and movements. While these are impressive capabilities, such systems have failed to make the leap to extensive industrial application. Previously, this lack of use could be attributed to the limitations of the technology. However, significant improvements have been made, and such systems are now capable of achieving recognition rates approaching those of traditional input devices. Therefore, we attribute the limited commercial use to shortfalls in two general areas of knowledge: (1) the lack of understanding of the true utility of these sensors with respect to human capabilities and needs; and (2) the composition of user interfaces required to facilitate their use. This research project is designed to broaden the current understanding of these two areas with the specific goal of providing fundamentally new user interfaces capable of supporting interaction using body-tracking technology. To achieve an understanding of human capabilities, we will apply empirical evaluation and ethnographic techniques to: 1) test and determine the limits of human abilities; 2) identify those capabilities that can best be mapped to the capabilities of the sensors; and 3) understand the contexts under which these capabilities are best leveraged. This will yield a more comprehensive understanding of two vital issues, i.e., where and when each input modality is best suited and what means of input is best suited to that context. From this understanding, we will construct user interfaces suitable for those contexts, allowing the user to provide input to devices in fundamentally new ways. The result will be a far greater understanding of the potential of these technologies, of human needs and capabilities, and ultimately of the utility and methods for the application of these technologies to industrial uses. ""491913,""Wight, Andrew"
"474229"	"Wonham, Walter"	"Supervisory control of discrete-event systems"	"Our proposed research lies in the branch of engineering and applied mathematics called ""automatic control"" or sometimes ""cybernetics"", namely the science and technology of feedback systems.  Such systems are designed to maintain some critical variable or variables within an acceptable range.  For instance, an aircraft's automatic pilot corrects for wind disturbances which might otherwise drive the plane off course, perhaps disastrously.  Of specific interest to us are systems, like traffic systems, which must respond to sudden ""events"", like the arrival in a service queue of a vehicle; and manufacturing systems, which must respond to a new part order or machine breakdown.  Another example is the pushbutton system in a modern automobile which, although ""randomly"" activated by the motorist, should be designed never to commit a logical error like ""hangup"" or blocking.Such ""discrete-event"" systems and their control call for new and specialized techniques at the interface of control theory and computer science.  A major challenge is that these systems can be very large and complex.  For instance, the automobile pushbutton system may potentially enter many millions of different internal ""states"" or logical configurations.  This may lead to almost overwhelming requirements on the computing power for control engineering design.We are developing ways of decomposing such systems into components or logical ""layers"", like the levels of management in a business or government organization, so that the design of controls can be rendered tractable.  We are also exploring how such architectural principles can be combined with advanced computing techniques. The expected payoff from this research is powerful systematic design procedures which guarantee that the controlled system will behave as desired.  Such guarantees translate ultimately into higher levels of protection against malfunction, and therefore into improved reliability and economic performance.""474752,""Woo, Carson"
"476054"	"Xiang, Yang"	"Graphical models: Inference, decision and acquisition"	"My research concerns intelligent systems, known as agents, that function in uncertain or constraint-based environments, either individually or cooperatively. It focuses on graphical models for knowledge representation and covers issues on knowledge acquisition, inference, and decision making.  My previous research has established several classes of multiagent graphical models, known as MSBNs for probabilistic reasoning, DMSBNs for forecasting, and CDNs for design decision making. Agent communication in MSBNs and DMSBNs is initiated by one agent (the root), and its election incurs overhead. To improve flexibility, feasibility of an unrooted regime will be investigated.  Multiagent planning will also be studied focusing on online planning, rather than commonly pursued offline policy making, to gain efficiency.My previous research on multiagent constraint graphical models, known as MSCNs, confirms that lessons learned from multiagent probabilistic graphical models can be usefully extended into distributed constraint satisfaction (DCSP) and optimization (DCOP).  The proposed research will explore the structure embedded in lower level runtime representation of MSCNs for more efficient constraint reasoning.  Motivated by certain unique, desirable computational properties of CDNs and MSCNs (relative to existing frameworks for DCOP), a multiagent graphical model for DCOP will be developed by generalizing CDNs and MSCNs. Previous research developed causal models, NIN-AND trees, for efficient acquisition of conditional probability tables (CPTs) in constructing Bayesian networks.  These models extend the expressive power of existing models from reinforcing interaction to undermining and mixture of the two.  The proposed research will investigate approximating an arbitrary CPT as an NIN-AND tree, algorithms to acquire these models by data mining, and direct incorporation of NIN-AND trees into inference to improve efficiency.""476281,""Xiao, Chijin"
"481876"	"Xie, LiangLiang"	"Some fundamental problems in the development of wireless relay networks"	"Interference, fading and signal attenuation are the major difficulties in wireless communications. To build highly efficient wireless networks, relay nodes can be used to shorten communication range, improve spectrum reuse, increase spatial diversity and fully exploit available signals. However, in order to achieve these goals, more advanced coding schemes and cooperation strategies have to be used. With more nodes involved, more signals transmitted, more correlations introduced, the wireless networking problem becomes more challenging. Decode-and-forward is a relay strategy where a relay node needs to decode the message before forwarding it. This has the advantage of eliminating noise from the signal received, and noise will not be accumulated even after many relays. There have been many studies of decode-and-forward relay schemes in the literature, and satisfying results have been obtained in the case of a single source. However, the problem is much more challenging in the case of multiple sources due to their mutual interference. We intend to develop an optimal decode-and-forward relay framework in the case of multiple sources. Compress-and-forward is another relay strategy, where, decoding at the relay is not needed. Instead, the relay simply forwards the received signal after quantization. Although without decoding, noise in the received signal will also be forwarded, no constraints on the communication rate will be exerted by the relay. This relay strategy can significantly outperform decode-and-forward for some networking problems. However, the optimal compress-and-forward scheme has not been determined yet. We made some progress on this topic recently, and intend to further investigate this fundamental problem and determine the optimal compress-and-forward relay scheme for general wireless networks. Finally, it is of great interest to compare different relay strategies, and determine the best choice for different networking problems. Although each relay strategy has been studied extensively, a general guidance on how to optimally select from them, or any combinations of them is still missing. We intend to develop such a general guidance, which will be very important for the design of practical wireless relay networks.  ""492920,""Xie, Lin"
"481830"	"Ye, Qiang"	"QoS provisioning in heterogeneous networks: A perspective from the edge"	"The Internet is a revolutionary technology that changes our life dramatically. Both business and individuals have benefited much from various Internet applications, including email, www, etc. The Internet has evolved dramatically over the past decades. Nowadays, it is moving forward in several aspects. In terms of Internet applications, more and more time-sensitive applications are emerging. For example, Internet telephony and video conferencing have been widely used. Many newer applications, such as online interactive television, are stepping into our life gradually but firmly. For these applications, Quality of Service (QoS) mechanisms that guarantee a certain level of performance to data flows are required. In terms of access technology, although wired access is still very popular, more and more Internet users are adopting wireless technologies, such as WiFi (i.e. IEEE 802.11 networks). As a result, the Internet is becoming a heterogeneous network that is composed of wired core networks and wired/wireless access networks. Over the past years, the bandwidth of WiFi networks has increased significantly. However, the bandwidth commonly offered by Internet Service Providers (ISPs) has not matched up with the increase. In addition, the uplink bandwidth offered by ISPs is often much lower than the downlink rate. As a result, when multiple Internet users access the Internet via a WiFi network, the connection from the WiFi network to the ISP tends to be the bottleneck. In this case, if QoS provisioning is not in place, time-sensitive applications, such as VoIP, will suffer seriously from the bottleneck. In this project, we plan to explore QoS provisioning in the heterogeneous Internet when WiFi is used as the access technology. Specifically, we focus on the scenarios in which the connection from the WiFi network to the ISP is congested. We attempt to provide a solution to the QoS problem from the perspective of the edge. Our vision is that cross-layer QoS provisioning mechanisms will solve the QoS problem efficiently. ""485496,""Ye, Winnie"
"473768"	"Yongacoglu, Abbas"	"Cognitive radio networks enabling techniques"	"It appears that with the introduction of each new service at highly desirable frequency bands (e.g. 1 to 3 GHz), bandwidth scarcity becomes more acute. For decades, most of the demand for efficient utilization of spectrum has been addressed by improvements in modulation, coding and multiple access techniques. Further efficiency improvements by these conventional approaches seem to be limited. More importantly, it has been determined that often there is no real scarcity of spectrum to meet the increased demand. Scarcity is due to static and permanent assignment of the bands only to license holders. Considering space-time distribution of spectrum, the desirable frequency bands are severely underutilized at many locations for most of the time. Once the conventional model is modified to allow unlicensed users to share the spectrum, the scarcity can disappear. Hence the critical issue is to find ways to dynamically utilize the available bandwidth. Cognitive Radio (CR) allows unlicensed secondary users to share the spectrum that is licensed to primary users. This sharing may take place if secondary users transmit when primary user is not active, or by transmitting at a level not harmful to the license holder. The proposed research is mostly within the physical layer area of cognitive radio networks. We view our research efforts on CR in two directions. The first one is concerned with estimating the power distribution in space and frequency (i.e. interference map) of the region and then utilizing the least occupied bands with a dynamic spectrum access.  For this purpose we propose new approaches which involve wavelets and partial band sensing using t-designs in cooperative wideband spectrum sensing. The second direction deals with interference modeling and analysis and designing cognitive transceivers that can easily adapt their parameters to the environment.""493275,""Yoo, Jaeyun"
"488113"	"Young, James"	"Leveraging people's everyday skill sets for interaction with robots"	"As advancing robotic technology begins to enter the everyday world of the general public, for example as utility or health-care robots, it is important to consider how people and robots will communicate, and how people will issue commands or extract data. Unfortunately, interacting with robots can be very difficult and generally requires training and expertise. As many robot users may not be robotics experts, I believe that it will be important to make interacting with robots as easy as interacting with other people.In this work we move away from robot-centric command consoles, and built robots that enable people to accomplish complex robot interaction tasks using their existing everyday (non-robot oriented) abilities and work practices (e.g., ability to teach others and give directions, or skills working with animals). We further aim to develop a stronger understanding of which existing social skills and structures robots can leverage in their designs, how they impact environments they integrate into, and how impact can be influenced by design.As one component of this work we will study which everyday tasks robots could assist with, and which existing interaction structures a robot could integrate into. Second, we will develop new robotic implementations which serve as proofs-of-concepts for incorporating existing human interaction techniques into robotic behaviors and designs. Finally, we will place our new technologies into real-world environments and observe people working with them as a means to test our designs and to further inform future interfaces.The resulting new human-robot interfaces will be among the first practical robot implementations that are designed to integrate naturally into existing everyday scenarios and social structures, and the in-context evaluations will provide new insight into how people interact with robots in real-world settings. ""491492,""Young, Jeff"
"475256"	"Zhang, ChangNian"	"Fault tolerant cryptography and advanced multilayer access control"	"The proposed research program is comprised of the following two research projects.1.    )Fault Tolerant CryptographyMost cryptographic algorithms and protocols involve massive computations and communications. Errors may occur from time to time. When there is an error, for an example, in the digital signature verification process, it is hard to tell whether this is a fake digital signature or there is a computation and/or communication error. The algorithm based fault tolerant (ABFT) approach achieves fault tolerance by tailoring the fault tolerance scheme to the algorithm to be performed. ABFT approach requires less overhead, uses the same type of computation as the targeted algorithm and the throughput will not be affected until an error is detected. The limitation of the ABFT approach is that it only applies to certain types of algorithms. In this research we develop an extended ABFS (EABFT) approach that can be applied to all well known cryptographic algorithms, including stream ciphers, AES block cipher, RSA and ECC public-key algorithms. In addition, we will design and implement a byte-wise 2-D fault tolerant AES systolic array for applications where high speed and high reliability are required.                        2.    )Advanced Multilayer Access Control ModelingA document in a digital library could contain different types of content with different levels of privacy and/or restrictive information. A user should be able to access certain parts of the document according to his/her access rights and needs. In this project, we are going to develop an advanced multilayer access control model such that access to different types of content and different layers of security information can be done efficiently and effectively. The proposed advanced multilayer access control model can be integrated with other well known access control models, including role-based, mandatory and discretional access control models. It can also be implemented as a part of an operating system. The applications of the proposed access control model include Web based multimedia applications, digital libraries, e-education and e-health.  In addition, we will investigate a method to convert the access control right rules for an application into a set of security criteria. The set of security criteria will be used as the inputs of the system design. As a result, it will significantly reduce the cost during system design and implementation due to any changes to the access control rules.""475365,""Zhang, Chao"
"474951"	"Zhang, Hong"	"Scalable appearance-based robot navigation"	"""As I come down the hallway, heads start popping out of the cubicles and offices, all eyes turning in my direction. Some of my colleagues laugh, some frown. One looks terrified and flees.  That's what happens, I suppose, when you show up at the office as a robot"".  So began a recent article in IEEE Spectrum (September 2010) by Erico Guizzo,  to describe his experience of coming to work in his robot avatar.  The context of the story defines an intuitive scenario for where and why this research is important and relevant. After all, if one can create a robot that ""shows up"" for you, a great number of opportunities will open up, including providing companionship and assistance to the elderly or physically handicapped in a household or hospital. But wait, you don't want to be bogged down by your avatar with every tedious and repetitive move, do you? At the heart of this type of autonomous mobile robot is the research of robot SLAM (simultaneous localization and mapping).  The research described in this proposal is concerned with a promising approach to SLAM, called appearance-based SLAM (aSLAM).  In aSLAM, the robot uses a camera to acquire sensor data and describes places of interest not in terms of 3D landmarks as is done traditionally, but in terms of pictures taken at these places, resulting in simple control algorithms with improved performance.   The proposed aSLAM framework departs from the current method of image description based on the so-called bag-of-words (BoW) or vector-quantized visual features, but instead uses carefully selected raw visual features, to address the perceptual aliasing problem of BoW. In addition, our aSLAM framework will be online, through the use of hashing techniques for matching images, rather than tree structures which require offline construction.  The research will be conducted in both indoor and outdoor environments, on wheeled robots and a humanoid robot called NAO.   With any luck, we may just produce a robot avatar that can work as your electromechanical proxy in the near future!""480902,""Zhang, Hong"
"489464"	"Zhang, Xuehua"	"Relaying strategies for Network-coded cooperative systems"	"wireless Communication, cooperative networks, network coding, relaying strategies, pair selection, relay selction, error propagation, two-way relay channels""488554,""Zhang, Xuekui"
"488000"	"Zouaq, Amal"	"A learning-by-reading framework for ontology learning"	"The learning-by-reading challenge refers to a number of natural language understanding tasks dedicated to the extraction of a formal conceptual model from free texts and to the filtering and integration of this knowledge. In the context of the Semantic Web, this challenge often refers to the ability of learning a domain ontology from free text. This task is particularly challenging when the adopted approach is an open (domain independent) and unsupervised (without any guidance) knowledge extraction. Such a domain independent approach is crucial to the interoperability and reuse of ontology learning methodologies across various applications and domains. This research aims at developing a domain independent methodology and a set of tools for learning the ontology layers (terms, taxonomy, conceptual relations, relation hierarchies, axioms and axiom schemata) from unstructured knowledge sources. In particular, we aim at automatically learning the three last layers, which has been done very rarely in the state of the art.The research will also explore filtering techniques that identify important information and integration techniques that adequately combine knowledge coming from various sources. The interest of the availability of a complete ontology (with all its layers) is that it is essential for agents to reason over semantic data and this will provide effective knowledge retrieval capabilities. The long-term objective of this research is to build learning-by-reading systems by exploring the integration of deep semantic analysis, filtering and knowledge integration techniques. The short-term objective is to build a proof of concept by extracting a domain ontology using a combination of some of these techniques and by focusing on layers that require deep semantic analysis. The major contribution of our proposal is the extraction of domain conceptual models from unstructured sources enabling the indexing of texts and reasoning as well as the exploitation of such conceptual models in various areas such as the Semantic Web and the corporate world.""484895,""Zouqi, Mehrnaz"
