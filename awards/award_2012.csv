"id"	"researcher_name"	"application_title"	"application_summary"
"508122"	"Abdulrazak, Bessam"	"Adaptable Smart Environments for Elderly People  [Environnement Intelligent Adaptable pour les Personnes Agées]"	".                        *** Adaptable Smart Environments for Elderly People *** An imminent and sharp increase of elderly population is forecasted around the world, inducing human, social and economical impacts. Progressive degradation of vision, hearing, mobility and cognitive skills deprives elderly people from the possibility of independently performing Activities of Daily Living and engaging in social interaction, and often leads to reliance on services in special care facilities. Although several technological solutions could assist them, existing solutions do not fulfill the sporadic needs of the different elderly people and their continuously changing environments. My goal is to provide adapted assistive services in adaptive Smart Environments that can play a significant role in maintaining decent Quality of Life (QoL) for elderly people.     I have applied Context-awareness to adapt and provision assistive services according to user profile and surrounding environments. The next agenda in my research is to provide continuity of assistive services that adapt to changing user conditions and environments while ensuring successful system execution. Therefore, the following challenges will be addressed in my program: how to design systems that enable continuity of service, how to model environment Context for such system, how to guarantee successful system execution, and how to adapt services according to aging process.      My program will have significant impacts on the way healthcare is delivered and on socioeconomic development. The anticipated output of the program is the opening of new avenues for elderly assistance, promoting active aging and improving QoL of elderly people. My work, in close collaboration with caregivers and domain experts, will lead to concrete solutions that a) enable seniors to remain longer in their own communities, b) relieve caregivers and allow them to provide quality care, and 3) decrease the costs and pressures on the health system.""498942,""Abedi, Jalal"
"509411"	"Agbossou, Kodjo"	"Intégration des énergies renouvelables et systèmes de stockage à la gestion de la demande locale d'énergie"	"Le développement des énergies renouvelables (ER) et sources de production d'électricité décentralisées, est souvent freiné par une inadaptation entre les moyens de production et le fonctionnement du réseau actuel. Pour un développement énergétique durable, il devient nécessaire d'une part d'optimiser la gestion énergétique aussi bien au niveau du réseau de transport que localement près du réseau du client, et d'autre part de faciliter l'intégration des sources d'énergie renouvelable pour augmenter leur utilisation et ainsi réduire les émissions de gaz à effet de serre.  Dans ce contexte, nous avons développé et testé avec succès à l'Institut de Recherche sur l'Hydrogène (IRH), un système à énergie renouvelable (éolienne, panneaux solaires, pile à combustible) autonome basé sur la production d'hydrogène pour le stockage à long terme d'excédents d'énergie. Une approche basée sur les réseaux électriques intelligents (Smart grid) devrait permettre d'intégrer les ER, de les autogérer et de renseigner sur l'utilisation de l'énergie. Ce projet vise de manière spécifique l'étude de la gestion de la demande locale via l'intégration des ER et systèmes de stockage à la gestion intelligente de la demande locale. Les objectifs spécifiques de ce projet dans le cadre d'un réseau décentralisé sont :- l'analyse de l'intégration des ER et des systèmes de stockage sur la gestion de la demande dans le contexte d'un bâtiment résidentiel ou institutionnel typique du Canada;- le développement de nouvelles approches d'intégration de systèmes de stockage (incluant le stockage sous forme d'hydrogène) à la production décentralisée locale du bâtiment;- l'étude des différentes topologies de connexion intelligente des sources d'ER  incluant le développement des interfaces pour l'interconnexion efficace et sécuritaire,- l'analyse de l'impact de la connexion de véhicule électrique sur le profil énergétique du bâtiment, le développement de systèmes embarqués pour la gestion locale de l'énergie.""504154,""Agellon, Luis"
"509678"	"Aghdam, Amir"	"Design of Robust Distributed Control Schemes with Applications to Multi-Agent Systems"	"The problem of controlling an interconnected large-scale system by means of structurally constrained controllers is considered in this proposal. In many physical systems such as sensor networks, power systems, coordinated motion of automated vehicles and traffic networks, the interacting subsystems are geographically dispersed. Usually, traditional centralized control for this type of system is not as efficient, due to the distributed nature of the network and also the complexity of the required on-line computations. Furthermore, continuous transmission of data from the subsystems to the controller and vice versa is typically expensive. Another obvious disadvantage of such structure is that the resultant closed-loop system has a single point of failure. In a distributed control structure, on the other hand, one can generate the control input of each subsystem by only concentrating on those outputs which have a relatively high impact on the corresponding subsystem (through interconnections). This reduces the complexity of the computations, and at the same time improves the efficiency of the control topology.It is known that inexact modeling, actuator limitations, sensor constraints, measurement error and time delay, if not taken into account in the controller design, can lead to performance degradation or even instability of the interconnected system. Due to the complexity of the control analysis and synthesis with the above-mentioned practical considerations, they are often neglected in the problem formulation. In this work, effective approaches will be used to tackle the problem in the most practical setting, and sufficient conditions will be derived to guarantee robust performance. In particular, the effects of these practical issues on connectivity preservation, consensus, containment, formation control and tracking problems in a multi-agent system will be investigated. Such considerations in control design and analysis will make the results of this research more applicable to real-world multi-agent networked control systems.""510092,""Aghili, Farhad"
"508068"	"Alhajj, Reda"	"Employing Data Mining and Machine Learning Techniques for Social Network Modeling and Analysis"	"Collecting digital data today is much easier and more comprehensive than ever before, e.g., social media (SM), news, blogs, postings, event logs, network and email traffic, web logs, gene expression data, protein drug interaction, publications and citations, etc. It is possible and affordable to keep a log by tracing users of digital media, including stationary computers and hand-held mobile devices. The increased amount of collected data necessitates a shift from manual to automated Social network (SN) construction and analysis.        Existing automated SN construction and analysis techniques, though excellently adapted SN analysis metrics, are not capable of replacing traditional human observers who can successfully handle small scale networks. Thus, there is an urgent need to develop revolutionary paradigms and new technologies towards next generation SN and SM analysis. Fortunately, whatever is satisfied by traditional means could be extracted from digital data by data mining (DM) and machine learning (ML) techniques.        The main objective of this proposal is to research innovative technologies with the aim to build the world's first automated model capable of replacing human observers and scales well for large and dynamic networks, a scope impossible for human observers to cover. This will be achieved by employing DM and ML techniques to comprehensively investigate the behavior of individuals by digging out implicit relationships instead of concentrating solely on explicit relationships as it is the case with most of the SN techniques described in the literature. Graduate and undergraduate students will be involved in this research program to work on various parts of the methodology from developing the necessary theory and algorithms to implementing and testing them in NetDriller, which is a SN software under development by my research group at University of Calgary. It is available as open source. The outcome will serve various applications, e.g., stock market analysis, study of the interactions between molecules inside the cell, collaboration, recommendation and team work, homeland security and early warning, etc.""519662,""Alhamami, Mays"
"509335"	"Alirezaie, Javad"	"Novel methods and applications in Computer Aided Medical Diagnosis"	"Computer aided medical diagnosis (mCAD) is a rapidly developing field of investigation offering improveddiagnosis and quantitative image analysis solutions for clinicians, helping to increase specificity and diagnosticconfidence. mCAD systems can be applied to all imaging modalities for accurate lesion detection, treatmentplanning and serial monitoring of the patients. Feature extraction and analysis is a core element of signal andimage processing systems encountered in mCAD. The key to successful design of such a system for human-machine interaction depends on a robust feature extraction and classification systems which is the main topics of our investigation. Our goal is to develop a computer aided medical decision support system fordisease identification and classification in pediatric patients. Pediatric patients often have a variety of usual and uncommon disorders not encountered in adults such as metabolic brain diseases (MBD). Compared to the adult patient population, pediatric patients also represent unique challenges for diagnostic imaging due to theirsmaller size, anatomical variability and altered motion.We propose a novel computer aided methodology to assist radiologists in early and accurate diagnosis ofpediatric metabolic brain diseases. To our knowledge, our approach introduces a new era in medicaldiagnostics and computer aided support systems by infusing image and signal information obtained from twodifferent imaging modalities for enhanced diagnosis accuracy, where each modality by itself may not always be sufficient. Additionally, while our proposed approach focuses on metabolic brain disease for the purpose of this grant application, the resultant methodologies will significantly contribute to the advancement of biomedical signal and image analysis and pattern recognition extensible to other disorders. Our proposed mCAD scheme could also have immense potential in the intelligent systems and information technology sectors. We are collaborating with the Department of Diagnostic Imaging at SickKids in Toronto. The MSc and PhD trainees will be provided with a unique opportunity to collaborate in this multidisciplinary environment.""501718,""Alizadeh, Maryam"
"507992"	"Arnold, Dirk"	"Constraint handling in evolutionary algorithms"	"Optimisation problems are abundant in all areas of science and engineering.  Solving an optimisation problem amounts to choosing values for a set of decision variables that result in the best solution possible.  Often, optimisation problems are constrained in that there are restrictions on the values that the decision variables can take on.Evolutionary algorithms (EAs) are optimisation strategies that see increasing use in many areas of application.  They iteratively improve the quality of populations of candidate solutions by subjecting them to variation and selection.  Their robustness in the face of non-differentiable or noisy objectives, along with the relative ease with which they can be adapted to poorly understood problems, often make EAs the method of choice where other approaches are not applicable or prone to failure.A multitude of techniques for handling constraints in EAs have been proposed and are in common use.  However, knowledge with regard to their respective capabilities and shortcomings is limited.  Most crucially, the interaction between adaptive variation operators and constraint handling techniques is poorly understood.  I will achieve an understanding of scaling properties of EAs for constrained optimisation by analysing their behaviour for sets of carefully selected test problems.  The results obtained will complement, extend, and help explain the large body of empirical knowledge generated on large function testbeds that is available today.  I will then use the insights gained to develop more capable EAs for constrained optimisation and systematically compare their capabilities with those of other direct search strategies.""510485,""Arnold, Mandy"
"508348"	"Atlee, Joanne"	"Modelling and Analysis of Feature-Oriented Software"	"Software systems are growing in size and complexity. The ""scale"" of large-scale systems no longer refers simply to large codebases but also to extreme variability, as characterized by features. Features are particularly relevant in software product lines, in which a family of related software products (e.g., smart phones, automotive software) shares a common set of mandatory features and products are differentiated by their variable (optional or alternative). When deriving a product from a selection of features, software developers must consider how the features interact. Some interactions are planned, some are innocuous, and others are harmful. For example, the software controllers for the braking features on the 2010 Toyota Prius interacted badly, reducing drivers' ability to brake and leading to 62 reported crashes and 12 injuries. To be safe, software developers analyze the consequences of all possible feature interactions, in order to find and fix the bad interactions. The Feature Interaction Problem is that the number of interactions to consider is exponential in the number of features. As a result, a software team finds that its work in developing new features is dominated by the tasks to detect, analyze, and verify interactions. A second problem is that software developers do not realize the full potential of feature-oriented development because verification is typically performed on derived products and it is too costly to verify all of the possible products. Instead, a small fraction of products are derived, verified and offered to customers.The proposed research aims to enable developers to *predict in advance* the effects and quality of software products that are derived from features. To that end, the project activities focus on (1) fundamental problems in feature modularity and granularity, (2) feature-composition operators that coordinate features so that they interact properly, and (3) analysis tools for exploring valid configurations of feature sets. The expected outcomes of the proposed work are vital in deriving higher-quality software products from feature sets.""505582,""Atoofian, Ehsan"
"507905"	"Aubanel, Eric"	"Parallel Heterogeneous Algorithms for Computational Science"	"High performance computing (HPC) resources are becoming more readily available at a reasonable cost, particularly with the availability of programmable graphics processing units (GPUs). The parallelism is challenging to exploit in practice, and the rapidly evolving hardware can lead to increased maintenance cost of software. This proposal addresses the challenges associated with developing dynamic architecture-aware parallel scientific computing applications, to deal with the rapidly changing nature of HPC resources. GPUs presently contain hundreds of cores, which when combined with the growth of the number of cores per CPU is resulting in a dramatic increase in the overall degree of parallelism required of applications. Three levels of parallelism must be considered to fully use such systems: coarse grained thread-level parallelism for CPUs, massive fine grained data parallelism for GPUs, and message passing for clusters of CPUs/GPUs. The balance between these three types of parallelism also needs to be matched to the computing system. Therefore static approaches to algorithm design and implementation may hinder portability of performance across different types of parallel computers. This proposal focuses on new algorithmic approaches to finding sufficient multi-level parallelism and on identification and scheduling of tasks, taking into account the performance of each computing platform component and the communication time between components. This will lead to applications that are able to adapt to different platforms by scheduling tasks to the heterogeneous components at runtime through efficient scheduling algorithms and selection of optimal values of performance-critical parameters. Work will focus initially on two areas: dynamic programming and solution of time dependent partial differential equations. In both areas new algorithmic approaches combined with scheduling on heterogeneous platforms will lead to a range of flexible high performance scientific computing applications in areas such as simulation of physical and biological processes and in bioinformatics.""504260,""Aubé, Mélanie"
"509339"	"Bahoura, Mohammed"	"Développement et implantation temps-réel de techniques avancées de traitement des signaux"	"Le domaine du traitement des signaux a suscité l'intérêt d'un très grand nombre de chercheurs appartenant à des disciplines variées, tels que l'audio (parole/musique), le génie biomédical, l'acoustique sous-marine, la télécommunication, etc. Dans ce projet, nous proposons le développement et l'implémentation temps-réel de techniques avancées de traitement des sons acoustiques et leur application à la classification des sons respiratoires et la reconnaissance des vocalises des baleines (rorqual bleu, rorqual commun et béluga). La démarche consiste à développer de nouvelles architectures de traitement des signaux acoustiques implantables sur FPGA. Ces architectures seront optimisées en termes de temps de calcul et de ressources matérielles afin de réaliser des systèmes intégrés fonctionnant en temps-réel. Les techniques proposées se basent essentiellement sur l'analyse temps-fréquence des sons acoustiques. Elles  présentent l'avantage de caractériser efficacement les différents sons acoustiques afin d'améliorer leur reconnaissance automatique même dans les conditions du bruit intense.Le développement d'un système d'analyse des signaux acoustiques respiratoires permet de réaliser : 1) Un outil d'aide au diagnostic des différentes maladies respiratoires robuste et non-invasif, 2) L'évaluation de l'efficacité des traitements prescrits aux malades, 3) Possibilité de suivi à domicile des patients.Le développement d'un système passif de monitorage acoustique des baleines permet de  : 1) Étudier l'occupation du territoire par ces mammifères, 2) Réduire les impacts dus aux collisions et à la pollution sonore afin de protéger ces espèces. ""511346,""Bahrami, Majid"
"514538"	"Balakrishnan, Ravin"	"User Interfaces and Visualization for Variable Format Displays."	"This research is focused on exploring new user interaction and visualization techniques for a broad range ofnext generation input and display technologies. These will range from very light and cheap paper like displays,to pico-projectors, to tabets, to much larger wall size displays. The key objective here is to develop a unifiedapproach to visualization and interaction across these myriad displays, each possibly with a different inputscheme. The approach will be one of building creative new exploratory interfaces that will be used as probes in user deployments, followed by empirical evaluation of the interfaces, with the results being used to refine the designs. The resulting system artifacts will significantly deviate from the interfaces of today, and hence has the potential to alter the way we consume and interact with data on future computing devices.""512452,""Balakrishnan, Ravin"
"508102"	"Baniasadi, Amirali"	"Power-Aware Multicore Processing"	"The goal of this proposal is to develop techniques to reduce power dissipation in multicore processors. Multicore processors, also referred to as chip multiprocessors (or simply CMPs), are employed in a wide range of computing systems from data centers to desktops and embedded processors. The energy consumed by such computing systems accounts for a considerable share of our daily energy consumption. Therefore, developing power efficient multicore processors is a critical aspect of green computing. CMPs achieve high performance by using multiple processor cores on a single chip.  While this approach has proven to provide effective means for current technologies, there are several studies that suggest that future CMPs will become power limited. Consequently, providing enough power for the chip will require turning off some sections. One way to reduce the proportion of these unused sections (also referred to as dark silicon) is to develop low-power CMPs. We will investigate solutions for enhancing power efficiency in future CMPs.  To achieve this goal we suggest developing alternative power-aware designs.  We will investigate a number of issues influencing the overall power dissipation in CMPs. We will study power scalability and the parameters contributing to the excessive power dissipation growth in large-scale CMPs. We will also analyze resource utilization in different CMP designs and identify resource utilization inefficiencies. We will use our findings to redesign conventional CMPs for better power efficiency.    Our proposed research makes important contributions to Canadian society. First, the proposed research program aims at developing ""greener"" computing infrastructures where power hungry processors are replaced with low-power alternatives.  Second, the resulting knowledge belongs to an area critical to Canada's future leadership in advanced technologies. Many Canadian companies can benefit from the findings of the proposed research and the resulting HQP training.   ""503530,""Banihashemi, Amir"
"508366"	"Banzhaf, Wolfgang"	"From Genetic Programming to Computational Evolution"	"The goal of the research proposed here is to move from earlier very abstract and simplified EC models, closer to realistic models of natural evolution implemented in computational systems, in other words, in the direction of ""computational evolution"". To this end, a number of basic assumptions of established evolutionary algorithms will be subjected to critical revision. In addition, new concepts of molecular and evolutionary biology not known at the time of the design of these algorithms will be incorporated. While the set of those concepts is virtually endless, my focus for the next five years is on the following topics:     (i) epigenetic interactions that allow a genome to reorganize in response to cues from its environment. This is an extremely important concept, notably with respect to the reinforced connection between evolution and development in biology. The inclusion of epigenetic mechanisms in algorithms will happen through the addition of a further layer of control signals for artificial regulatory networks. This constitutes a new way of allowing interactions between an algorithmic genome and its environment.     (ii) The evolution of cooperation and its contribution to evolutionary transitions. While earlier models studied two-level systems, in this research we shall study multi-level selection and the benefits to be gained from adapting the size of groups and hierarchies to the problem that is to be solved by evolution. Substantial efficiency gains are expected from these approaches, since they address one of the key problems evolutionary algorithms, and Genetic Programming in particular: scalability.     (iii) The emergence of novelty in both biological and computational environments. Novelty and the associated phenomenon of innovation is a central tenet in Darwinian thought. Without the invention of new devices neither nature nor computing reap the full benefits of evolution. Here, we shall examine the mechanisms by which novelty is produced in natural settings, for example by gene duplication in genomes, and seek to transfer recipes into computational systems.""510849,""Bao, Xiaoyi"
"507919"	"Bartram, Linda(Lyn)"	"Ambient and Ubiquitous Visualization for Sustainability"	"Distributed sensing and data collection mechanisms are making a wealth of data about resource consumption available to residents. Visualizing this information in ways that are meaningful and contextually appropriate will help to bridge the gap between data and informed decisions about how to sustainably use resources. This research program will examine how different visualization approaches, from typical information displays to more embedded and ambient representations, can be used to help people understand and manage their energy use in non-desktop contexts: in the home, at school and in public environments. It supports conservation activities, and particularly reducing energy use, requires a range of visualization techniques to aid informed decision-making.However, typical information visualization techniques are not necessarily applicable. Numerous researchers have pointed out that we cannot just import established visualization techniques to non-work environments. The process of designing the user interface systems for these environments means considering a range of user attention from ambient to focused, appropriate placement of devices to support daily activities, and balancing aesthetic appeal and usefulness. This research program will use both empirical and scientific approaches to establish basic legibility and perceptibility constraints while integrating more applied knowledge from design and architecture to explore the impact of placement, operation and utility in different tasks and environments. It will investigate a gamut of approaches from traditional information visualizations of data to the ambient, distributed and ubiquitous representations advocated by recent research. It will improve the appearance, communicative scope, usability and efficacy of a range of computer-generated techniques for visualization of energy use and make it easy for Canadians make informed energy conservation decisions.  ""506384,""Bartz, Jennifer"
"509668"	"Basir, Otman"	"Multimodal Mobility Modeling and Traffic Profiling in Cyber-Physical Systems"	"The main objective of this research work is to investigate the use of mobile devices along with cellular wireless networks and other sensing technology to estimate mobility and to build robust tempro-spatial mobility models. Transportation systems will be considered as a target application. Extracting a mobility model for individuals enables the profiling of traffic conditions and hence can facilitate a wide range of transportation applications: vehicle routing, optimized traffic lights signalling, road maintenance planning, and congestion management. Soft-Computing inferencing and filtering methods offer a range of capabilities for modeling mobility.  However, since mobility is generally a non-linear dynamic process, a small non-linearity can lead to difficulties to represent the posterior knowledge. To investigate such challenges, Bayesian filtering and sequential Monte-Carlo methods such as particle filters will be employed. The Bayesian methods will be employed to tackle two open issues: a) wireless localization based on signal profiling (e.g, fingerprints and RSS; b) simultaneous localization and mapping. Comprehensive Sensing methods are explored to address the sparse signals issue present in such systems. A diverse range of data sources, such as cellular networks, cell-phones, road cameras, on-board GPS devices, loop detectors, and mobile augmented reality models, will be studied as complementary mobility cues. To exploit multi-modality to its utmost potential, the proposed research work will investigate several data fusion techniques for inferencing and estimating mobility and traffic trends. Here, the proposed research project will adopt a soft-computing as reasoning approach, where probabilistic techniques are used to capture trends from mobility data and fuzzy and Dempster-Shafer techniques offer linguistic paradigms for reasoning about uncertainty in the captured mobility and traffic data. Particles, Kalman, GMF, and HMM filters will be employed at various levels of the proposed modeling scheme; to address sparsity in the measurements Compressing Sensing techniques will be investigated.""502951,""Basith, IftekharIbne"
"508016"	"Beaudry, Eric"	"Algorithmes de planification appliqués aux jeux sérieux"	"Ce programme de recherche s'inscrit dans le domaine de la planification en intelligence artificielle (IA). Il s'agit d'une capacité fondamentale en IA permettant d'automatiser la prise de décisions pour des agents et des systèmes intelligents et autonomes. La planification en IA a de multiples applications dans divers domaines dont les jeux, les systèmes tutoriels intelligents, la robotique mobile, l'exploration spatiale et la défense. Ce programme de recherche comporte deux volets. Le premier volet (fondamental) vise à ajouter des capacités fondamentales aux planificateurs existants afin de les rendre davantage applicables à des applications réelles. Ces améliorations permettront à rendre les systèmes et agents intelligents plus performants et mieux adaptés à des situations complexes. Le deuxième volet (appliqué) vise à développer et à intégrer des algorithmes de planification dans des jeux sérieux. Les jeux sérieux dédiés à l'apprentissage sont tout particulièrement ciblés. Ces derniers représentent un complément fort intéressant aux méthodes d'enseignement traditionnelles. Toutefois, leur conception pose de nombreux défis puisqu'ils doivent concilier des objectifs sérieux, comme l'apprentissage, avec des aspects de divertissement. Dans le cadre de ce programme de recherche, nous proposons une approche novatrice pour concilier automatiquement ces deux aspects. À l'aide de modèles éprouvés du domaine des systèmes de tutoriels intelligents, des algorithmes de planification sélectionneront automatiquement des actions dans le jeu qui maximisent l'apprentissage du joueur-apprenant tout en s'assurant que les scénarios présentés lui procurent du plaisir et le motive à poursuivre le jeu. De nouvelles méthodes seront développées pour automatiser certaines parties du jeu, comme pour le contrôle des personnages non jouables. Les techniques développées pourront être adaptées par l'industrie du jeu vidéo, très présente au Canada, pour simplifier le développement des jeux. Des avancés dans le domaine des jeux sérieux pourront être mise à contribution pour capter l'intérêt des jeunes afin de réduire le taux de décrochage scolaire.""505651,""Beaudry, Francis"
"509519"	"Beauvais, Jacques"	"Lithographically induced self-assembly techniques for advanced electronic device fabrication"	"The semiconductor industry has achieved a steady and tremendous growth in performance for more than 40 years, leading to the pervasiveness of electronics in our daily lives. But this industry is now facing three key challenges: 1) the tremendous amount of heat that is generated by electronic chips now sets serious limits to their continued miniaturization; 2) the choice of materials for pursuing the miniaturization trend leads to major issues with material properties, for example brittleness and heat dissipation issues; 3) the microprocessor architecture is now evolving towards the integration of several cores on a single silicon chip, thus requiring very challenging connections between segments of a microprocessor and between the microprocessors and other components of a computer. This led to the development of new materials and of innovative 3-dimensional and/or optoelectronic connections. Over the next 5 to 10 years, the industry needs innovative low power devices capable of extremely high speeds and these devices must be compatible with the current CMOS technology in order to capitalize on the massive manufacturing capacity currently available. Single electron transistors are very promising for addressing these issues, but they need highly advanced fabrication techniques for manufacturing; these techniques are now seriously challenged by physical limits at the scale of the nanometre. Self-assembled nanostructures such as carbon nanotubes that could be used in these and other nanoelectronic devices suffer from different problems, many related to the need to accurately position the nanotubes in an electronic circuit, and others related to the challenge of taking the fabrication processes out of the laboratory and into the manufacturing environment. This research program will explore hybrid approaches which leverage sophisticated nanolithography with simpler self-assembly techniques for producing nanostructures such as carbon nanotubes and metallic dots and accurately controlling their location in devices and circuits, in order to exploit their exceptional electrical, mechanical and thermal properties, and eventually to increase the performance of advanced electronic circuits.""518769,""Beauvais, MariePierre"
"508254"	"Bener, Ayse"	"Decision Making under Uncertainty in Software Engineering: Release Readiness"	"This research investigates the detection of software defects. The aim of the research is to address the question of ""when to stop testing the software and release it?'. Under the constant pressure of ""Deliver Now!' due time and budget constraints, software managers in real life do not observe only one factor, such as defect rates. Managers combine their prior knowledge with historical data about factors representing different lifecycle processes and their effects on the final reliability of software product. Furthermore, product reliability is strongly connected with software development life cycle (SDLC) activities. Each phase in the SDLC has to be modeled by considering the relations of product, process, and people related factors, as well as the relationships between these phases and their impacts on the final software product reliability. Previous studies in release planning well addressed how SDLC processes can be prioritized based on stakeholder preferences. However, there is also a need to model casual relations between SDLC processes to decide if the release is ready in terms of its reliability. This research aims to understand cause - effect relationships of SDLC activities leading decisions to stop testing activity. This research program is based on empirical analyses and aims to develop a model to quantify release readiness of software product. Other researchers in the software engineering domain can also use this model. My objective is to develop a Bayesian model that uses both quantitative and qualitative data based on data availability and quality. Such a hybrid model would give flexibility by combining local data extracted from organizational systems with expert judgments collected through questionnaires. I also plan to extend my existing software metrics collection tool, Prest, to include more metrics, and capabilities for analysis, defect and reliability prediction. The proposed research program is highly relevant to the Canadian software development industry. Software development companies in Canada to assist their managers to make decisions under uncertainty can use the tool.""505603,""Benesty, Jacob"
"508250"	"Bentahar, Jamal"	"""Intelligent, Strategic and Verifiable Agent-based Communities of Web Services: Theoretical Foundations and Automation"""	"Web services are hailed for their role as implementation technology of service computing. The widespread development and use of this technology for Business-to-Customer (B2C) and Business-to-Business (B2B) have recently contributed in identifying an evolutionary way of organizing and deploying web services by gathering the ones having similar functionalities in the same ""virtual"" space, called Communities of Web Services (CWSs). CWSs permit to create pockets of expertise and settings of both cooperation and competition to provide the users with the best services. To achieve the full potential of this emerging concept, web services are perceived to be equipped, through agents, with semantic information and rich interaction,  reasoning, and decision making capabilities so that business contracts and community joining benefits can be negotiated. However, despite recent and numerous initiatives around specifying, engineering, and securing CWSs, several fundamental issues and problems remain unresolved and yet to be addressed, mainly because of lack of rigorous theoretical foundations. To make CWSs the technology of choice for B2C, B2B and e-applications, there is an urgent need to provide researchers and engineers with strong theoretical foundations to support and analyze flexible and efficient interactions, decision making, verification, and security, which are core to the emerging applications. The first aim of this project is to raise the state of the art in service computing from a level where services are simply reactive entities  and deployed individually, to a level where they are empowered with semantics and advanced reasoning and strategic decision making capabilities and deployed within communities. The second aim is to advance the research into CWSs from a level where web services and communities are designed in an impromptu way to a level where theoretical foundations are established allowing sound engineering using formal methods and techniques, namely formal economic models, game theory, computational logic, and model checking. The ultimate objective of this project is to contribute in the social and economic development of Canada by contributing in the growth of its service industry.""498487,""Bentley, Laurence"
"514541"	"Berenbrink, Petra"	"Randomized Algorithms for Distributed Systems"	"The proposed research concerns itself with the development and analysis of randomised  algorithms for distributed computational resources. The distributed resources can be  networks of closely or loosely connected computers, ad-hoc networks for disaster response, sensor networks, or simply a set of communicating robots that jointly perform a task, e.g., exploring some terrain.Whilst distributed computing has been around for a very long time it and a few of its relatives have more recently become fashionable also in the public eye.  As the main driving forces are to be consideredoff-site storage of data (e.g., Google docs or Dropbox for very mainstream applications), or the now ubiquitous Cloud computing.  At present there exist commercial large-scale data centres, large-scalewith respect to storage capacity as well processing power.  However, whilst in the olden days many aspects of parallel code execution and data communication in networks, including primitive operationslike load balancing, scheduling and routing, were hand-optimised, under the new paradigms this is clearly not feasible any more.The proposal suggests research in three different areas of distributed computing. The first  area is  load balancing and here the proposed work is in the area of balls-into-bins games and diffusion load balancing.  The second area is  random walks. Random walks can be used as a mathematical model for terrain exploration. Here the terrain is modelled by a graph and the robots (random walks) are allowed to movealong the edges of the graph.  The third area is communication in networks, and here the proposed research is concerned with  information spreading.""511043,""Berezovski, Maxim"
"507857"	"Bhavsar, Virendrakumar"	"Parallel and Distributed Intelligent Systems"	"PARALLEL AND DISTRIBUTED INTELLIGENT SYSTEMSThe long-term objective of our work has been to develop efficient models, algorithms and implementations of parallel and distributed intelligent systems. Our goal has been not only to develop and experiment with new algorithms, but to develop operational software systems that can be employed in the real world as well. Whenever opportunities arise, the outcomes of our research will be commercialized and integrated into products.We propose to generalize the concept of the weighted tree similarity proposed earlier by us, to the weighted graph similarity and exploit it to carry out semantic matching, matchmaking and semantic search. Graph algorithms are more complex than tree algorithms. Therefore, in order to handle large data sets, we will develop parallel and distributed algorithms suitable for multi-core processors, clusters, grids and clouds to drastically reduce computing time. Our proposed work, which generalizes our previous work to provide more precise matching, has even greater potential for real world applications. The applications in e-Business would result in much more precise matching than simple keyword matches used in Kijiji and similar systems. In e-Health domain, the proposed approach promises better social networking among patients and more precise results from querying patient record histories. Finally, the proposed approach will give highly relevant results to user queries compared to simple keyword-based searches used in search engines like Google.      ""497167,""Bherer, Louis"
"508216"	"Bonakdarpour, Borzoo"	"Automated Construction of Dependable Component-based Cyber-physical Models"	"A cyber-physical system (CPS) is a system in which there is a tight conjoining between computational and physical resources. Examples of such a system include autonomous collision avoidance systems, medical devices, autonomous search and rescue devices, and avionics systems. It is expected that the role of these systems will expand significantly in the near future. To realize this potential, future CPSs will need to be correct and significantly more efficient and dependable. These objectives introduce numerous challenges. For example, we currently lack formal methods for specifying and reasoning about the correctness of CPSs. Moreover, it is unclear how constraints in a CPS's environment affect widely understood notions such as correctness, fault-tolerance, maintenance, and separation of concerns.In this proposal, I advocate the approach of developing model-based methods that automatically build CPSs that are correct by construction. Model-based application development aims at increasing the integrity by using models employed in clearly defined transformation steps leading to correct-by-construction artifacts. In particular, I would argue that CPSs are inherently component-based, as physical and computational processes are normally stand-alone entities communicating with each other. Thus, this research program will focus on model-based development of fault-tolerant component-based CPSs. Specifically, the outcome of this project will be (1) a simple, intuitive, and yet expressive framework for formal modeling of component-based CPSs, (2) rigorous complexity analysis of automated formal methods for adding fault-tolerance to a fault-intolerant component-based model of a CPS, (3) transformation methods that separate fault-tolerance from functional concerns and sufficient conditions for compositional synthesis of CPSs, and (4) a powerful tool chain that realizes the theoretical results in practice.""508711,""Bonato, Anthony"
"507973"	"Bonner, Anthony"	"Computational Methods for Detecting Multi-gene Relationships in Gene Expression Data"	"Gene coexpression has emerged over the last several years as a powerful bioinformatics tool.  Coexpressed genes may be involved in the same biological process, and thus coexpression networks are often used to investigate gene function.  Methods for detecting, analyzing and clustering pairs of coexpressed genes are now well developed.  Pairwise coexpression, however, is clearly too simplistic to describe the complex relationships between genes, since these relationships may involve multiple genes and can vary depending on the biological context.  To address this limitation, we propose to develop computational methods for detecting and analyzing multi-gene relationships in gene expression data, and to use these methods to solve problems in molecular biology. These problems will include the reconstruction of genetic regulatory networks, the detection of modulators of gene regulation, and improved tools for gene function prediction.One of the main difficulties in detecting multi-gene relationships is the vast number of possible relationships which need to be considered.  This is especially true in organisms with a large number of genes, such as plants and mammals.  In such cases, separating the true relationships from the possible relationships poses challenging computational and statistical problems. To address these problems, we shall first develop methods of low computational complexity for detecting relatively simple multi-gene relationships. To avoid overfitting the data, we will develop methods for accurately estimating false positive rates.  We will also develop heuristics for speeding up the computations while missing as few significant relationships as possible. To validate the methods, we will test them on both real and simulated gene expression data. Building upon these results, we will develop methods for detecting more-complex relationships involving larger numbers of genes.""519947,""Bonsma, Madeleine"
"513284"	"Borodin, Allan"	"""Design, analysis and theory of algorithms"""	"Algorithm design and analysis has seen significant progress in the sophistication of the algorithms being developed as well as in the expanding set of application areas where recent algorithms have become essential. However, algorithm design still remains mainly an art. The goal of this research project is to make algorithm design somewhat more of a science than an art. Namely, we wish to help develop a theory of algorithms that would allow us to better understand the benefits and limitations of general algorithmic approaches as applied to various problem domains. This general project is admittedly both too vague and too ambitious. So in more pragmatic terms, I am studying the power and limitations of well known ""conceptually simple meta algorithms"" such as greedy algorithms, dynamic programming, primal dual/local ratio algorithms and local search algorithms. To do so, we propose precise models that capture particular instances of such algorithms in relation to a wide variety of problem domains. We proceed to both design and analyze algorithms within a particular framework and as well to establish impossibility results (e.g. inapproximination bounds) with respect to the model.  This approach stands in contrast to the fundamental limitations one tries to establish vis a vis complexity bounds (e.g. what can and cannot be computed in polynomial time or logarithmic space). Rather we try to establish limitations independent of complexity issues but instead establish such limitations based on the restricted design of the algorithm. In particular, we prove results that hold whether or not P = NP.""519669,""Boronka, Laura"
"507878"	"Boufama, Boubakeur"	"Semantic Structure from Multiple Uncalibrated Images"	"Images contain considerable amount of information that is usually immediately deciphered by humans. Despite the remarkable progress made so far, image understanding by artificial vision systems remains limited. Hence, making machines see as humans do is still an ultimate goal for computer vision research.This research proposal aims at advancing current vision systems to make them capable of inferring 2D and/or 3D high level semantic information from one or multiple uncalibrated images.  In particular, the following problems will be investigated. (1) The automatic detection of geometric information such as parallelism and perpendicularity. These geometric relationships can play a central role in image understanding as they allow to infer other higher level information. We have recently achieved some success on the automatic detection of parallel planes from multiple non-calibrated images and we plan to target other geometric information.  (2) The relationship between semantic geometric information and camera auto-calibration for inferring full 3D metric information of the observed scene using multiple images. We aim at developing a framework where different high level geometric relationships can be used in a flexible way as constraints on the camera parameters and/or the whole 3D structure of the observed scene. This is especially important when images have been taken by different unknown cameras. (3) The semantic segmentation of the scene into objects, a problem if solved will make the machine vision system one step closer to the human one. Different clues coming from different sources can be used to help this segmentation. In addition to the color segmentation, the 3D metric information and the high level geometric relationships could used together for a successful semantic segmentation.""511468,""Bouferguene, Ahmed"
"507981"	"Boulanger, Pierre"	"TEMIS-Training Environments for Minimally Invasive Surgery"	"Minimally Invasive Surgery (MIS) has revolutionized general surgery because of its many advantages of minimal postoperative pain, few postoperative adhesions, minimized blood loss, low risk of surgical complications, short hospital stay, and early return to normal activities. In MIS, access to the surgical site is obtained via small incisions, visualization of the internal organs is performed using a small video camera within an endoscope, and actual surgical operations are performed using a few long, slender instruments. Common MIS procedures include cholecystectomy, appendectomy, and hernia repair. Although MIS has several advantages over traditional open surgery, MIS is a complex procedure involving independent visual and manual skills. In most cases, skills learned during open surgery are not easily transferable to laparoscopic procedures. The combination of the physical, precision, safety and visuo-motor constraints make minimally invasive surgery a very difficult procedure to learn and to master.The proposed research program is to conduct a comprehensive study of the visuo-haptic information requirements in performing MIS, followed by the design and development of new training tools that incorporate effective visuo-haptic guidance between a teacher and a trainee to improve the acquisition of MIS skills. These training tools (e.g., real and/or virtual reality part-task simulators with 3D visualization and haptic feedback) are aimed at facilitating the transfer of skills including explicit and implicit knowledge, from experts to novices, and to develop expertise in trainees in an efficient and effective training program. The long-term goal of TEMIS is to enhance performance in MIS training through innovative design of training tools using novel techniques in Augmented Reality (AR), haptics, and Human Computer Interface (HCI).""499697,""BoulangerLewandowski, Nicolas"
"514741"	"Bowling, Michael"	"Human-Scale Game Theory in Imperfect Information Settings"	"Decision making in the presence of other agents is a critical challenge for deploying artificially intelligent agents in real-world settings. Real world settings -- ranging from negotiation to security to interactive entertainment -- also typically involve imperfect information, where agents are missing key pieces of information to make their decisions.  Game-theoretic techniques for decision-making under such circumstances have recently seen considerable improvement. However, they are still not adequate for human-scale problems. For example, advancements in general-purpose equilibrium algorithms for sequential games have made it possible to solve games with trillions of game states (up from millions only 5 years ago), yet the smallest variants of well known games played by humans (e.g., poker) are still far much larger. While my research group has had some recent success at playing complex games at a human level, the techniques employed necessarily give up theoretical guarantees when applied to such large domains. Without guarantees, though, it is hard to predict the success of the approaches for other applications, or guide a practitioner in how best to exploit improvements in computing resources.This research will develop new algorithmic approaches to human-scale multiagent scenarios with imperfect information, while preserving theoretical guarantees. The research aims at developing a toolbox of techniques for automating decisions in such domains, with the theoretical guarantees enabling predictable results for the practitioner. The research focuses on both tools for computing robust static strategies, when no knowledge of the other participating agents is available, and learning strategies that adapt to the other participants' behavior. In addition to tools for decision-making, the research also examines the problem of identifying illicit collusion in complex multiagent interactions. The toolbox aims to make the currently complex domain-dependent game-theoretic algorithms more practical and off-the-shelf usable for domain experts.""506818,""Bowman, Jeff"
"514749"	"Boykov, Yuri"	"Combinatorial Optimization Methods for Computer Vision and Bio-medical Image Analysis"	"Automatic computer/robot vision for manufacturing, health care, security, and multi-media is still largely""work in progress"" and relatively few methods produce consistently reliable results on real data. Sheer size of high-resolution volumetric images in stereo-vision, motion analysis, computer-assisted diagnosis (e.g. MRI, CT), and other applications demands very high level of efficiency from computer vision algorithms. Despite significant progress in the last 10-20 years, the vision community is still widely researching new theories and mathematical concepts that could lead to computationally feasible methods for image segmentation, shape representation, model fitting, stereo, and many other low-level problems forming the base for all computer vision systems. My research concentrates on practical computationally-efficient and mathematically solid models for low-level vision. This topic offers an exciting ground for creative interdisciplinary research linking optimization, statistical physics, information theory, learning, applied differential geometry, and other mathematical sciences. Most low-level vision problems can be formulated as optimization problems using either information-based methodology (e.g. minimum description length principle), or concepts from statistical physics (e.g. posterior energy), or differential geometry (minimum surface).  The focus of my proposed research are mathematically solid models in low-level vision and the corresponding fast optimization methods computing either their global minimum or some guaranteed-quality approximation. Such optimization problems are a challenge for the state of the art in discrete combinatorial algorithms and continuous variational techniques. Firstly, it was shown that many problems in vision are intrinsically difficult (NP-hard), thus effective approximations must be explored. Secondly, computational efficiency and scalability of the proposed optimization algorithms is crucial when a solution is sought on huge 3D or 4D image volumes common in vision and medical imaging. ""507444,""Boyle, Latham"
"508225"	"Brecht, Timothy"	"Evolving the HTTP Ecosystem to Efficiently Support Video and other Large Files"	"Studies report that in North America video now accounts for 40-66% of all traffic on the Internet and it will rapidly increase to 90-95%. Internet video services are now using the same methods to deliver video to consumers as are used to deliver data from web searching, banking, on-line shopping, social networking and other popular services.  The problem is that the existing systems used to deliver that content have not been designed and optimized to deliver video.  The result is often a less than satisfying video viewing experience, with videos being interrupted or paused while the video is fetched from the server.  This can be especially noticeable when trying to view high-definition videos or during times when others are using the same service or network.  More importantly, there is evidence that shows that as the demand for video increases, it will make the existing services we have come to rely on slower and slower.Our research will devise new methods to improve the efficiency of the underlying systems software and networks used to deliver content to consumers.  We will experimentally evaluate, identify bottlenecks in, and improve the performance of existing software systems.  This will reduce the number of machines required by providers of Internet services, thereby reducing the power they consume and lowering their costs.  For consumers this should reduce costs paid for these services.  In addition, this will enable the smooth delivery of high-quality video to their desktop PC, laptop, netbook, tablet or smart phone whether they are at work, home, or on the road.  Most importantly, we will make these improvements while ensuring that existing services that have become critical to our lives not only continue to function but function well.""506898,""Breden, Felix"
"507985"	"Buro, Michael"	"""Search, Opponent Modelling, Cooperation, and State Inference in Complex Imperfect Information Domains."""	"Artificial intelligence (AI) research applied to games has a long tradition that reaches back atleast 75 years with Alan Turing's work on computer chess. The advantage of studying AI algorithmsin this area is that games are precisely defined, relatively small when compared to real-worlddecision domains, and yet sufficiently complex to pose tough research problems whose solutions canhelp us create machines of human-level intelligence. AI research has had its successes in gameslike chess, backgammon, and checkers - where machines now play on par with or better than the besthuman players. However, in more complex domains machines are still trailing behind humanexperts. The main differences to the games in which AI research has been very successful are thatgame state information is hidden from players or the number of move choices is very large. Bothproperties render complete enumeration, which is a cornerstone of many high-performance AI systems,less effective.    The objective of the research proposed here is to create systems that will reach or surpass theperformance of human experts in real-time decision domains that feature uncertainty, imperfectinformation, or complex state and action spaces. The benchmark applications we will be working onare trick-based card games and real-time strategy video games. Improving the state of the artrequires us to develop new algorithms that can model opponents, infer hidden game states, cooperatewith partners, and look-ahead in abstracted search spaces. In the long term, the results of thisproject will increase our understanding of fundamental AI problems that need to be solved in theprocess of creating human-level AI systems. In the short term, we anticipate the computer gamesindustry benefiting from our research, because it is in need of credible computer controlled agentsin the domains we study.""512879,""Burrell, Robert"
"514291"	"Carpendale, Sheelagh"	"Interactive Visualization"	"Modern society demands that people manage, communicate and interact with digital information at an ever-increasing pace. While information is a crucial part of people's everyday lives, many people find today's technologies awkward, stressful to use, and overly intrusive in their lives. The problem is not with the information itself, but rather with its volume and the unwieldy ways currently provided for interacting with digital content. My long range goal is to design, develop and evaluate interactive information visualizations so that they support people's everyday work and social practices as they interact with information. My primary motivation is to promote information comprehension by creating appropriate tools that can help people negotiate the transformation of vast amounts of information into knowledge in people's everyday lives.My long term objective is to design, develop and evaluate interactive visualizations of information that begin to address people's challenges as part of an information society and to enhance their cognitive and communicative abilities. Visualization can be created to help us see the invisible, to comprehend vast information spaces, to manipulate abstract concepts, to appreciate the beauty of information structure, and to support decision making and collaborative processes. Interactive visualizations are successful when they can help people interpret and understand information, steps which are integral in our processes of developing knowledge. My more immediate goals are to (1) improve our understanding how people naturally use sketches, diagrams and visualizations to support their cognitive processes; (2) to further explore how visualizations can support the many and varied ways that people access and discover information in the daily lives; and (3) to extend the information understanding that visualization is starting to offer in the laboratory and workplace to personal information needs. My research will work towards developing a basis for creating effective interactions that fit seamlessly in our everyday work and social practices, allowing us information manipulations that better support our decision processes and our ability to handle our ever-increasing information spaces.""499222,""Carpendale, Sheelagh"
"509399"	"Castle, Peter"	"Electrostatic Charge Limits for Small Particles"	"ObjectivesTo clarify some limitations involved in the electrostatic charging of small solid particles and liquid droplets commonly used in industrial applications.    Statement of ProblemElectrostatic forces are widely used in many industrial applications that involve the movement and deposition of small particles. The particles are typically in the size range from submicron to the order of one hundred microns. The effectiveness of these forces is directly dependent upon the magnitude of charge that can be applied to the particles. Since electrostatic charge resides on the surface, it is commonly assumed that the total charge is proportional to surface area giving rise to the prediction that the charge to mass ratio (Q/M) is proportional to 1/r where r is radius. However common practice has shown that there are a number of situations where this is not true. In particular, non spherical solid particles cannot simply be assumed to be equivalent to those of a spherical particle of the same effective area. Particle shape and surface roughness can dramatically affect the charge. In the case of liquid droplets, although the particles are usually spherical, the values of Q/M have been reported to be inversely proportional to radius to the power x, where x may range from 1 to 2.   Expected SignificanceIt is believed that this work will result in a clearer understanding of the effect of shape, size and surface roughness of solid particles used in various industrial processes enabling optimization of the deposition and adhesion forces. In the case of liquid droplets, the study of the effect of the mode of droplet formation and the difference between the induction and corona charging cases should lead to further understanding of applications where both are present such as in a commercial liquid paint spraying system.""502315,""Castonguay, Patrice"
"509661"	"Champagne, Benoit"	"Innovative Signal Processing Techniques and Algorithms for Cooperative Wireless Communications"	"IMT-Advanced has set forth an impressive array of performance targets for the 4G of wireless cellular systems. Major improvements in capacity, quality-of-service (QoS) and deployment flexibility are sought, including much higher peak data rates and the ability to provide these over larger portions of the cell area. To attain, and eventually surpass these exacting requirements, significant improvements in interference rejection and bandwidth extension will be needed as compared to current 3G systems. To this end, 4G and beyond will favor the use of innovative radio technologies, including: spectrum aggregation, cooperative message relaying and coordinated multipoint (CoMP) transmissions. Within this framework, the long-term objective of the proposed research program is to develop and investigate new cooperative signal processing techniques and algorithms, needed to successfully access additional spectrum and combat radio interference in future generations of wireless networks. With a focus on distributed space-time processing, the short-term goals of the research are articulated around three complementary topics: (1) wideband spectrum sensing; (2) non-regenerative relaying; and (3) distributed CoMP beamforming. We will investigate new cooperative detector structures and spectrum sensing techniques that are robust to uncertainties in channel state information and noise background statistics, allowing multiple terminals to collaborate in the reliable detection of spectrum holes. Our work on non-regenerative relaying will focus on optimal cooperative design of multiple (i.e. parallel) relay transformations and transceiver architectures, and adaptive extensions thereof to allow practical message relaying to/from mobile terminals. Our proposed work on CoMP transmissions aims to investigate new decentralized blind adaptive beamforming strategies for synchronized downlink transmission from multiple base stations to mobile users located near the cell boundaries. This cutting-edge program of research will provide numerous opportunities for HQP training; the main findings will be presented at international conferences and published in highly selective journals.""499494,""Champagne, Émilie"
"508094"	"Chang, XiaoWen"	"""Algorithms, Analysis, Applications and Software of Numerical Linear Algebra"""	"Numerical linear algebra is at the core of most scientific computing. Many scientific, engineering, and other problems are most effectively represented and then solved on computers, as matrix problems, or linear algebra problems. One of the important applied areas of numerical linear algebra is estimation. This research aims to develop numerically reliable and efficient algorithms and software for solving general and specific estimation problems which may arise in practical areas such as communications, signal processing, global navigation satellite systems, and statistics etc. These problems can be either small scale or large scale. This research also carries out the analyses of  the algorithms to show their reliability and sensitivity analyses of the problems to show what effects changes in data caused finite precision computation or uncertainty in the data will have on the final results. The outcome of this research will be valuable not only to the study of numerical linear algebra but also to industry, particularly to the communications technology companies and manufactures of global navigation satellite systems receivers.""503135,""Chang, YuLing"
"509344"	"Chen, ChihHung"	"""Physics-Based Stochastic Noise Characterization and Modeling of Nanoscale Field-Effect-Transistors (FETs) for Designs of Low-Noise, Low-Power Integrated Circuits (IC)"""	"The research proposal presented in this application is to develop a first-rate research and training program for the investigation of high-frequency (HF) noise in semiconductor transistors, especially for nanoscale silicon-based Metal-Oxide-Semiconductor Field-Effect Transistors (MOSFETs), from device characterization to low-noise, low-power integrated circuit (IC) designs. This program consists of four major research areas - namely design and characterization of nanoscale MOSFETs using technology computer-aided-design (TCAD) simulators, high-frequency noise measurement techniques, simulation-oriented noise modeling, and low-noise, low-power IC designs. The emphasis of this research program is primarily on the development of (1) the infrastructure to study the characteristics of nanoscale MOSFETs using TCAD simulators; (2) fast noise measurement techniques and a novel four-port HF noise measurement system, (3) accurate and physics-based corner models to take care of the impacts from process variations, channel engineering, device architectures, and gate leakage current; and (4) novel HF circuits for low-voltage, low-noise wireless applications. All efforts will be taken to fully exploit the research results commercially, generate economic benefits and therefore fuel the Canadian economy. Potential users of these research results include manufacturers associated with the semiconductor manufacturing, microelectronics, and telecommunication industries. In addition, the state-of-the-art infrastructure developed in this research program will be used extensively in collaboration with industrial partners, with the objective of empowering them with the competitive edge that is vital to excelling in the present economic scenario. Finally, the proposed research program will create an essential research and training environment that is appropriate to impart requisite skills to highly qualified personnel (HQP) that are in line with the needs of academia or Canadian microelectronics and telecommunication industries in the area of high-frequency device characterization and circuit-level testing. The program and innovative infrastructure will attract talented and experienced researchers and HQP in the world and retain them in Canada.""501335,""Chen, ChihYu"
"509337"	"Chen, Chunhong"	"Towards ultra-low power digital circuit design with single-electron tunneling technology"	"As today's electronic devices are pushed toward nanometer scale, single-electron tunneling technology (SET) plays an increasingly important role in reducing both power consumption and physical size of future electronic systems. In SET-based circuits, individual electrons can be transported in a controlled manner for both analog and digital signal processing. While the opportunities created by SET devices for technical breakthroughs in electronics are clear, circuit designers are facing a lot of practical design challenges ahead.This research proposal is an attempt to explore the approaches to analyzing and designing SET-based digital systems with improved performance (such as low power consumption, small delay and high reliability). Since electron transports within SET devices are stochastic in nature, new models are required to evaluate the power, delay and reliability in different ways. One of the unique characteristics with SET devices is the Coulomb blockade oscillation. This allows the designers to develop novel structures in order to achieve high-performance implementations. SET circuits are also subject to the background charge fluctuation, a serious deficiency which could entirely suppress the Coulomb blockade. Therefore, there is a strong demand for techniques at different design abstraction levels to increase the circuit's immunity against these noises. The proposed research work aims to (a) develop the modeling for power, delay and reliability with SET circuits, (b) adopt new architectures (such as feedback and hybrid CMOS-SET architectures) for cost-effective implementations of digital systems, and (c) address the SET background charge effect through a variety of methodologies, such as Boltzmann machines and redundancy strategies methods. The ultimate goal is to overcome the technical hurdles with SET technology for more practical applications in the post-CMOS era.""501747,""Chen, Chunlin"
"509428"	"Chen, Li"	"Study Single Event Effects in Microelectronics"	"Silicon technologies have been the dominant platform for the majority of the electronic devices in industry. The silicon technology scaling has significantly improved the performance of the integrated circuits (ICs). However, the scaling also brought a number of reliability issues that have previously been less of a concern. Due to the small device dimension and low operating voltages, nanoscale ICs have become highly sensitive to operational disturbances. These disturbances, especially those caused by single event effects (SEEs) due to energetic particles in ICs, can introduce transient pulses in logic circuit nodes or upset data in storage cells. Their impact can range from a single data corruption to a severe system crash. For example, Sun Microsystems had to recall their flagship servers in 2000 due to sudden and mysterious crashes which were caused by cosmic particles. Even for ICs in terrestrial environments, the error rates caused by SEEs can be 100 times higher than those from hard failures such as device wear out. The objectives of the proposed research program are to study the SEEs in microelectronics, hence advise cost-effective mitigation solutions to achieve reliable operations for electronic circuits and systems. Digital and analog circuits with/without SEE-hardening techniques will be designed and fabricated in test chips using advanced silicon technologies. Ion beams (protons and heavy ions) will be used to evaluate their performance in term of SEE tolerance. An on-campus pulsed laser facility has been established and will be used as an investigation tool to characterize SEEs in the test chips, since it can precisely induce SEEs in the ICs at a designated location and time. In addition, device- and circuit-level simulation tools will be used to model and characterize the performance of the test circuits. The simulation results will be used to correlate with the laser and ion radiation results, which will provide insight understanding and information of SEEs in ICs. The proposed program will develop SEE-tolerant technologies for Canadian industry to improve reliability of microelectronics. Through the research program, a number of highly skilled personnel will be trained in this highly demand field.""517923,""Chen, Li"
"509423"	"Cheng, Julian"	"Next Generation Free-Space Optical Communication Systems"	"Free-space optical communication, also known as wireless optical communication, is an important technology in future heterogeneous wireless networks. The current radio frequency based wireless networks are carrying increasing data traffic, and free-space optical communication emerges as a viable solution to the spectrum shortage and congestion problems. Free-space optical communication is a cost-effective, secure, interference-free, and license-free technology capable of delivering high speed data at gigabits per second without use of fiber optic cable. While some current generation commercial free-space optical communication systems have been deployed, they are incapable of overcoming the impairments caused by turbulence-induced fading in a power efficient manner. The communication theory of wireless optical communication is far from mature. The challenge remains to develop low-cost and low-power free-space optical communication systems capable of reliable high-speed data communication.Under our proposed research program, we will identify core technology for the next generation free-space optical communication systems. We will perform mathematical analysis and modeling of these systems in the presence of realistic channel and system impairments. We will invent new algorithms that will facilitate the implementation of our new free-space optical communication systems. We anticipate to make contributions to the advancement of modern wireless optical communication theory. Our development of novel synchronization algorithms for free-space optical subcarrier modulation will solve the outstanding challenge and become a major technology breakthrough for wide adoption of next generation free-space optical communication systems. Our proposed research program will likely generate new inventions leading to technology that can be patented. Our new free-space optical communication technology will keep Canada stay competitive in wireless communication sector. ""503377,""Cheng, LinOi(Irene)"
"508124"	"Chiasson, Sonia"	"Universal Usable Security"	"Computer security and privacy affect every aspect of computing and are a critical matter of concern for allusers. While low-level system and network security remain important, there is an increasing need for end-users to become active participants in maintaining their computer's security, guarding their credentials for important accounts, and making privacy and security decisions online. Security now involves having non-security experts, or even novice users, regularly making important security decisions while their main focus is on other primary tasks. We must find ways to design security systems appropriate for this new environment.  I focus on designing usable security for children and mobile users initially because both groups are increasingly using technology and are targets for attacks.  In fact, I argue that there is a critical and urgent need for better usable security for both children and mobile users.The theoretical impact is to make available in the literature well-tested design principles and design patterns of usable security that can serve as foundation for development of security applications. Through practical designs of security software, I expect direct and measurable improvements in security for both groups of users. For children, warnings and security cues tailored to their mental models of security will enable them to make better security decisions online, and authentication schemes tailored to their aptitudes and abilities will allow for stronger passwords. A security game will provide a safe environment for children to explore different security choices and provide at testbed for proposed security designs. For mobile users, determining the best methods for user authentication and CAPTCHAs will address a common complaint among users, while improved permission models will give users more control over installed applications.""512575,""Chiasson, Sonia"
"509539"	"Chodavarapu, Vamsy"	"Integrated Microsensors and Microdevices for Diagnostics and Prosthetics"	"The proposed research program aims to develop sensors and microdevices for real-time biological and chemical monitoring for point-of-care diagnostics, chronic diseases, and traumatic medical conditions. These sensors and microdevices take advantage of several established and emerging integrative technologies including Complimentary Metal-Oxide Semiconductor (CMOS) microelectronics and photonics for signal detection and processing, nanostructured materials for high sensitivity detection, bio-/chemical- recognition elements for high specificity detection, micro-/nano- fabrication for sensor miniaturization, and microfluidics for world-to-sensor interfaces. The proposed research consists of three research themes: (1) integrated biological and chemical sensor microsystems, (2) biomedical microdevices and silicon carbide neural prosthesis, and (3) porous nanostructured materials enabled devices. In the proposed research, we will target sensors towards important issues being faced by large patient populations such as monitoring of metabolism for obesity, blood glucose level for diabetes, blood oxygen demand for hypoxia conditions, metabolic compounds related to aging and genetic disorders and infectious diseases due to pathogenic bacteria. We will research biomedical microdevices to measure various physiological conditions including blood pressure for stroke, ocular pressure for glaucoma, strain sensor to measure bladder volume in patients with urinary dysfunction, and acceleration sensor for cardiac pacing. We aim to develop ultra-long reinforced silicon carbide based neural electrode arrays as front-ends of Brain Machine Interfaces to help people suffering with paralysis to lead normal daily activities. The developed biological and chemical sensors and biomedical microdevices have the potential to play an important role in our personal wellbeing and social and economic growth and development. Further, the requested funding will allow train the next generation of bioengineers, well rooted and adept in design and production of bio-/chemical- sensors, biomedical microdevices, and nanostructured materials, needed to tackle complex challenges in health sciences.""515065,""Choi, ByoungChul"
"514122"	"Chow, Paul"	"Computing Architectures and Programming Methodologies for Heterogeneous High-Performance Computing and Embedded Applications"	"Modern compilers and programming tools have made computers accessible to the experts in many domains so that they can develop their own applications.  However, the need to use parallelism to get performance today adds significant programming challenges that are not easily understood by the average programmer. Moreover, programming is becoming even more difficult as we move to specialized architectures such as GPGPUs (General-Purpose Graphics Processing Units) and FPGAs (Field-Programmable Gate Arrays) to find more performance and better energy efficiency.  Most domain experts do not have the expertise to make effective use of such computing technologies.   The plan in this proposal is to work with several domain experts who must have significant computational requirements of a grand challenge nature -- large problems with significant impact.  Modeling the brain is one example.  The key element of the proposed work is that a computer engineering (CE) graduate student will be ""embedded"" with the collaborating group that needs help developing an application.  This will allow the CE student to gain intimate knowledge about the requirements of the domain experts while helping them with their application.  The expectation is that this collaboration will inspire new approaches to computing that will result in novel changes to computer architectures and systems.    Most computer architects address very specific low-level aspects of systems, such as the memory, the interconnect and the organization of the processors.  This proposal recognizes that working applications are the tools of the domain-specific users.  Building working applications on real systems will better inform the computer architect of the requirements for next-generation computing systems.  At the same time, this approach to research provides a significant benefit to the domain-specific users by helping them to implement the most efficient and highest-performing applications that they can use to advance their own scientific research.  This has the potential to create more immediate impact than the computer architecture study!""505713,""Chow, Robert"
"509551"	"Cloutier, Sylvain"	"Hybrid Polymer-Nanocrystal Multilayered Heterostructures for Low-Cost Optoelectronics"	"My long-term objective is to build a world-class research program studying the fundamental interrelationsbetween the synthesis & processing, structural and physical properties of hybrid nanoscale-engineeredoptoelectronic materials and devices. More specifically, this program will focus on polyfluorene-based systems combined with semiconductor nanocrystals to form novel hybrid multilayered heterostructures. In the long-term, this emerging class of hybrid polymer-based heterostructures has the potential of transforming the field of optoelectronics by providing low-cost and high-performance semiconductor-based nanocomposite materials and devices for key applications such as light sources, biomedical & lab-on-a-chip devices and solar-energy harvesting platforms.Compared to more common conjugated polymers, polyfluorenes tend to be easier to process and less sensitive to photo-chemical degradation, while still offering very decent optoelectronic properties. This combination of facile processing and durability makes polyfluorenes an ideal host system to provide a fundamental understanding of all polymer-based heterostructures and their limitations and to investigate how semiconductor quantum dots can be used to (1) add new functionality and (2) improve their performances.Yet, this hybrid integration of conjugated polymers and colloidal quantum dots also raises many importantfundamental questions and crucial technical challenges before reaching viable low-cost hybrid optoelectronic devices with superior performances. For one, recent reports have shown the urgency of better understanding the consequences of the incorporation of semiconductor nanocrystals on the structural and optoelectronic properties of the polyfluorene host itself. In turn, this improved understanding of the guest-host interactions involved will allow us to design and fabricate better hybrid materials and optoelectronic device architectures.""512153,""Cloutier, SylvainG"
"508020"	"Corbo, Jacomo"	"Information Network Flows and Social and Economic Outcomes: Models, Prediction, and Regulation"	"This research project is a three-part program that examines how the structure and dynamic of information flows-as well as the information technologies that facilitate and modify these processes-impact the productivity, performance, and more broadly, the behaviour, of individuals and organizations. First, the research will map the flow of information in real organizations over time and will combine economic theories of production with social network theory to estimate the effects of information propagation on the performance and productivity of individuals and organizations. Second, the project will develop general models of information diffusion and of behaviour in large-scale networked settings using a number of data sets relating to real organizations as well as online social networks. Third, the research will develop methods for improving or otherwise regulating information networks. This work will advance economic theory about information worker production, operations theory relating to resource allocation methods in organizations, sociological theory concerning the propagation of information in social networks, indirect mechanism design in large-scale environments, and statistical methods to model and forecast outcomes in information networks. The central goals of the project are: 1) to estimate and enhance the productivity of information workers, 2) to model, measure and improve organizational performance and our ability to forecast organizational behaviours, and 3) to improve performance-related intervention strategies in information-networked environments. The proposed activities are designed to have broad impact for science, engineering, education, and society. Our understanding of information worker productivity is increasingly paramount to economic growth. Developing robust, practical intervention strategies for regulating the behaviour of networked systems is important in domains ranging from organizational design to epidemiology to viral marketing. The project's emphasis on modeling and forecasting organizational performance will have important implications for how enterprises are organized and how resources within them are allocated.""512515,""Corbo, Jacomo"
"513352"	"Cordy, James"	"Software Analysis and Transformation Systems"	"Structural source transformation is a recent and increasingly popular computer programming paradigm with a wide range of applications in academia and industry. We have designed the TXL programming language to explicitly and conveniently express problem solutions using this new paradigm. In this work we explore and extend techniques for expressing and implementing structural source transformation systems (such as TXL) in the context of their application to computer software analysis and maintenance, document recognition and analysis, and model-driven engineering.The global goal of this work is the pursuit of efficient, cost-effective and correct methods for the development and maintenance of computer software systems.  Our past work has been adapted into the products and services offered by IBM Canada and other companies in the Canadian software and financial industries, and by other researchers worldwide.  In this proposal we concentrate primarily on languages and tools for software analysis and maintenance, which accounts for over 70% of industrial software costs.This work involves the collaboration and training of more than 20 PhD and MSc thesis students over the next five years.  These students will be applying state-of-the-art tools and techniques to design and develop the transformation-based software analysis and maintenance methods of the next generation of industrial software development.""505477,""Cordy, James"
"509438"	"Dalton, Colin"	"Non-mechanical continuous flow micropumps for biomedical applications"	"This research will focus on investigating non-mechanical micropumping methods for making small devices to pump fluid in bio-medical devices. The fluid will be pumped using electricity through small microneedles. The chosen area of research is specifically aimed at improving patient care by removing the need for hypodermic syringes and providing the path into automated drug delivery methods.  Specifically, AC electrokinetic techniques will be investigated for pumping fluids through custom made silicon microneedle arrays. Microneedle arrays penetrate the upper part of skin, avoiding the nerves, allowing for practically painless skin penetration. The investigation will focus on the need to create small, low power, reliable devices. Traditional methods of fabricating the devices will be used initially, such as photolithography, coupled with computer simulations to design optimal devices. During the later stages of the research, fabrication will transition to alternate technologies, such as commercial silicon chip manufacturing processes and laser micromachining. To achieve these goals, a series of experimental testing, simulation and design optimization of AC electrokinetic micropumps will be conducted. The experimental data will be compared with simulation data and the simulation models modified accordingly, iteratively leading to new designs of micropumps. The research methodology will involve investigating several electrokinetic micropumping factors, such as voltages and currents applied; temperature issues such as damage to the fluid solution, fluid properties within microfluidic channels and also the geometry of the electrode arrays and the microfludics channels themselves. These devices  will form an integral part of a biomedical engineering solution for healthcare.""515275,""Dalton, John"
"514659"	"Darcie, Thomas"	"""Terahertz Photonics - Passive Components, Active Devices and Systems"""	"Our research program over the past five years has targeted the advancement of terahertz technology. This work has been facilitated through several major grants and partnerships with industry leading to considerable expertise and numerous collaborations. With this new Discovery Grant application, we seek to continue research in several promising new directions that have been identified in our prior activities. This work is centered on three main areas: terahertz waveguide technology, terahertz source and detector technology, and terahertz systems. A major objective is to develop components and systems in which terahertz waves are entirely confined within waveguide structures, rather than radiating as free-space beams as is typically done in today's systems. It is anticipated that this work will result in terahertz-based tools and instruments that will offer higher dynamic range, longer sample interaction lengths, lower cost, and more widespread utility than today's free-space terahertz technology.""512825,""Darcie, ThomasE(Ted)"
"509333"	"Davison, Daniel"	"""Decentralized Regulation, Tracking, and Disturbance Rejection for Multi-agent Systems: A Targeting Approach"""	"This research deals with the control of multi-agent systems, that is, systems that are composed of multiple interacting subsystems.  We focus on problems in which control is introduced through separate control agents that influence the system agents.  Examples of socio-economic systems of this type include police officers trying to control a crowd (where the system agents are people in the crowd and the control agents are officers), military leaders directing a field of vehicles (where the system agents are vehicles and the control agents are leaders), and environmentalists trying to entice companies to adopt ""green"" technology (where the system agents are companies and the control agents are activists).  Examples of technological systems of this type include distributed energy systems, in which thousands of small sources of electricity, rather than a few large ones, are connected to the power grid (where the system agents are generators and the control agents are voltage/phase/power controllers) and automated traffic systems (where the system agents are traffic lights and the control agents are traffic controllers).  All of these systems can, in principle, be modelled mathematically and control rules for the control agents can be devised.  Due to the complexity of the systems in question, a decentralized framework is appealing.  However, even decentralized controllers can quickly become overwhelmingly complicated to design and, in practice, to deploy.  We aim to deal with this complexity by using a new approach we have developed called targeting, wherein each of the control agents focuses attention on a single, carefully chosen, system agent.  The major contribution of this research is that rigorous theory, based on both existing knowledge and the knowledge created through this proposal, is used to determine how many control agents should be used, where they should be placed, what their respective targets should be, and what control signals should be used to successfully control all of the system agents.  At present, such information is often unavailable scientifically, leading to dependence on intuition or experience alone.  A total of 10 graduate students will be partially supported through this funding.""505557,""Davison, Edward"
"507982"	"Dean, Thomas"	"Language Based Software Engineering for Large Web Based Systems"	"My proposed research has two thrusts. The first is the evolution of online applications. The recent introduction of HTML 5 is more than just a simple extension of existing AJAX technologies. The enhancement of Javascript and the introduction of dynamic elements in the browser creates a portable platform for computation. For example, Kobo has very recently suggested that they will be implementing a mobile book reader entirely within the HTML 5 space. The explosive growth of mobile devices also has interesting and important research possibilities. Thus online systems must deal not only with desktop browsers, but also mobile browsers and also dedicated mobile applications. These systems must be treated as a coherent whole, not as isolated and independent applications. My proposed research will leverage my existing research in web applications (both client and server), and in software analysis and transformation to investigate issues in these large web systems that span server, desktop and mobile devices. The goal is to ensure consistency between multiple clients and to reduce risk by consolidating source code for multiple client platforms.The second thrust of my proposed research continues to address the security and robustness of network applications. Our approach is unique in that we consider the network protocol as a programming language and can draw on twenty-five years of existing program comprehension research which we can apply to the security testing problem. In particular, we have started to apply a combination of javascript analysis, code normalization and clone analysis in hopes of creating a realtime detector of malware in adobe acrobat files(i.e., pdf files) and shockwave flash applications (i.e., online games). I have also started some research in counter-intelligence operations, assisting in tracking back malware infections to the source.""500898,""Deane, Petra"
"513007"	"Delgrande, James"	"""Belief change in rational agents (with application to action formalisms, ontologies, and inferentially weak systems)"""	"In Knowledge Representation (KR) in Artificial Intelligence, an agent's knowledge will generally be incomplete, possibly inaccurate, and evolving.  This will be the case whether the agent is a robot in a physical domain, or a software agent on the web, or a reasoning agent dealing with a corpus of medical information.  A crucial problem in KR is to manage such changing information, whether change comes from being told a new fact, or as a result of some executed action, or from sensing the environment. The focus of the proposed research will be on managing belief change.  While much work has been carried out in the areas of reasoning about action and belief revision, there are crucial issues that have not been fully addressed or that are not fully understood.  For example, it is not clear that the models that we have for belief revision fully characterize all aspects of belief change; as well, research has only recently addressed change in special-purpose KR languages, such as those involving Horn theories or ontologies.  Work in the previous grant period focused on the foundations of belief change, with emphasis on iterated change operators and change in inferentially weak systems, along with work in the related areas of preferences and nonmonotonic reasoning. For the coming grant period I will continue working on the foundations of belief change, but also will address three specific interrelated projects concerned with the application of belief change in realistic settings.  The first project (with colleagues from SFU, Greece, and Australia) will examine belief change in description logics, specifically with logics used for representing medical ontologies on the one hand, and the so-called semantic web of the other.  The second project (with a colleague from Toronto) will be to provide a full high-level specification of an agent able to incorporate information about known action effects, being informed of something, and sensor information, with the goal of being able to revise its knowledge, whether the incorrect knowledge came from incorrect information, fallible sensors, or failed actions.  The third project (with a colleague from SFU) will address situational awareness in large-scale (maritime) and local (household assisted living) settings.""502142,""DElia, Francesco"
"509422"	"Depalle, Philippe"	"Sound synthesis and sound processing for audio and musical applications"	"This research project aims at improving the accuracy and the adequacy of audio signal representations and modelling by benefiting from recent advances in signal processing techniques. More precisely the goal is to better address the dual nature of sound signals, which are often a mixture of temporal transients, and modulated frequency components. It will result in a better quality of sound processing, sound synthesis, and will nurture applied projects such as high quality sound source separation or audio morphing.This project is part and parcel of the so-called analysis/synthesis approach for sound synthesis and sound processing, an often used approach in musical applications, but also in sound design, audio coding and sound recording. The basic goal of analysis/synthesis is to conceive relevant models to represent acoustical signals by a set of temporal functions (the control parameters) that controls a synthesis structure. These temporal functions can be generated in abstracto or extracted from a pre-recorded sound during the analysis process. Re-synthesis produces a signal that sounds perceptually identical to the original sound. By modifying and substituting control parameters, this analysis-synthesis scheme can provide for very refined and precise processing of sound material. For example, it may allow one to produce a family of synthetic sound signals derived from a single original one or to carry out a morphing between two key sounds. It can also be used to make decision on source separation by a proper sorting and grouping of temporal functions into coherent subsets. In this context the quality of the representation of audio signals is essential.The proposed research program is two-fold: at a fundamental level the research work consists in strengthening the core foundation of time-frequency representation and parametric modelling of sound signals. At a practical level the research is focused on the audio applications which could benefit from these new signal processing tools.""504168,""DePassillé, AnneMarie"
"508234"	"Desharnais, Josée"	"""Duality, approximations and distances for probabilistic processes"""	"The general goal of the research project is to establish formal methods for reasoning about concurrent  probabilistic processes. In particular, I am interested in processes that have a continuous state-space, but are observed in discrete time steps: a well-known model for these systems are Labelled Markov Processes (LMPs).Possible application areas include communication protocols and process control software.  For such systems we would like to reason about correctness, performance and safety critical properties.I want to develop the field along  theoretic and practical sides.  I work with LMPs, but also with infLMPs, which are a generalisation of LMPs where the usual requirement for measures to be additive is dropped. This model subsumes the usual ones: probabilistic transition systems, LMPs, probabilistic automata. It allows to model underspecified systems, for example systems for which we only have incomplete specification.  ""508220,""Desharnais, Jules"
"508346"	"Desmarais, Michel"	"Algorithms and Applications of Learned Student Models for Adaptive Learning Systems"	"In the last few years, a number of innovative applications have emerged to guide a user through a study process that can lead to an adequate preparation for an exam, a diploma, a new job, or simply a self-paced, self-motivated learning agenda.  Such applications are designed to guide the user into a large body of learning material, according to specific learning objectives, and to the user's personal needs.  They take full advantage of the one-on-one, personalized tutoring that a computer can offer.These applications rely on a detailed cognitive skills assessment that determines what the user has mastered and what topics should be the focus over the next hours, or the next days of study.  The current research program's primary objective is to develop probabilistic models for skills assessment that are automatically built from test data.  We aim to construct skill assessment models solely from small data sets, yet with fine grained representations of skills.  The approach we advocate, item-to-item models, was shown to outperform current approaches in terms of predicting a user's success to unanswered items.  However, it also raises issues such as model update and the mapping of items to skills. This project addresses these issues, in addition to looking at new means to improve the predictive performance of existing models.""516019,""Desmeules, Sarah"
"509318"	"Despins, Charles"	"Wireless access strategies for carbon-neutral networks"	"The broad and uninterrupted growth, during the last few decades, of the products and services of the information and communications technologies (ICT) industry as well as their omnipresence throughout all sectors of human activity, has lead to significant consequences with respect to the ICT industry's global carbon emission footprint. While this footprint is currently under 4% of global emissions, it is doubling every four years. On the other hand, the ICT industry offers a largely undevelopped capability to generate energy efficiency gains of 800 billion US dollars per year as well as to eliminate 15% of global greenhouse gas emissions, which is equivalent to all emissions of the USA or of China thus creating a huge economic opportunity in the 21st century, carbon-constrained economy. The research program will target the application to telecommunications networks, in the context of the availability of renewable energy resources, of cloud computing and virtualization principles in order to reduce the carbon footprint of these ICT infrastructures while at the same time improving their economic viability. Virtualization of wireless access will be the specific focus as it has received relatively little attention in the open literature compared to virtualization of network backbones and applications. It is notably proposed to apply cognitive radio principles to virtualization of radio access in order to adequately spectral resources and to maximize the use of such resources when they are available. Radio environment maps will also be examined in order to support cognitive functions in a virtualized radio access environment. This comprehensive research program is expected to contribute to groundbreaking outcomes for Canada's telecommunications future and to the training of highly qualified personnel with the technical competence and the vision to use such Green ICT principles as the basis for the 21st century, digital and carbon-constrained economy. ""509511,""Despins, Charles"
"508107"	"Dick, Scott"	"Novel developments in computational intelligence with applications to data stream mining"	"Complex fuzzy logic is a recent generalization of the traditional fuzzy logic. Complex fuzzy truth values are vectors from the complex plane with a magnitude less than or equal to 1. There has been limited progress in elucidating the properties of complex fuzzy logic, and a few machine-learning approaches based on it have appeared (with our work prominent in both strands of research). However, it is believed that complex fuzzy logic is actually an infinite family of multivalued logics; and that the design space for learning algorithms based on it is correspondingly vast. Plainly, there is still an enormous amount of work to be done in understanding this area, and a clear opportunity for our research to make a lasting mark on the field. Our proposed program of research for the coming five years will continue our theoretical and applied research in this field, tackling key open questions such as: what are the classes of operators that form complex fuzzy conjunctions, disjunctions, and implications? What are their properties? What functions form useful complex fuzzy sets? How do we linguistically interpret complex fuzzy sets? How are complex fuzzy sets and logic most usefully realized in machine-learning algorithms, and what classes of problems are they best-suited to solve?   To focus our program of research, we will pursue one class of applications. Based on current research results, we believe that complex fuzzy logic can be effective in mining data streams. Our ANCFIS learning architecture was very accurate in time-series forecasting (an instance of stream data), and we now seek to generalize this result. Accomplishing this goal will require us to answer the theoretical questions we have raised above, as they all directly relate to data stream mining. Our learning algorithms (using the identified operators) will extract synopses of the data stream in the form of complex fuzzy rules, which must then be interpreted to be actionable. Our initial stream mining problems are a pair of sensor-data applications: air-quality monitoring in Alberta's Wood Buffalo region (site of the oilsands), and livestock disease surveillance based on animal-mounted sensor platforms; both are of significant economic importance.""510369,""Dickerson, Clark"
"514593"	"Dong, Xiaodai"	"Wireless Machine-to-Machine Communication Networks"	"With the rapid penetration of Internet into people's daily life, ubiquitous connectivity is finally dawning with the introduction of machine-to-machine (M2M) communications where thousands of embedded devices are connected wirelessly and function in a cooperative and autonomous manner.  When a massive number of devices are networked, they provide significant amounts of status information of the physical world in which they are embedded and further build intelligence into a broad range of infrastructures.  Applications such as smart grid, smart home, intelligent transportation, e-health, industrial automation, tracking and logistics, will be enabled by machine-to-machine communication networks.  M2M research incubates new services and applications that open a mass market and business opportunities, whilst transforming the way communication systems are defined and operate. All major telecommunications and networking companies are actively involved in M2M research, product development and standardization.  Despite of the promising picture projected by M2M, mass market adoption is conditional on the fact that critical challenges of M2M networks are resolved.  Energy efficiency and security are the two most compelling issues to address.  From environmental perspectives, the energy savings brought by the M2M network need outweigh the energy consumed by the vast number of wireless devices to justify their usage.  Green communications is essential to the deployment of M2M.  Moreover, proper operation of any network needs security and the ubiquitous M2M network poses significant challenges on security.      In this proposal, we approach the design of key enabling technologies for M2M networks from the following aspects: 1) Wireless propagation measurements and modeling; 2) Node cooperation, resource management and spectrum management; 3) Medium access control (MAC); 4) Heterogeneous network architecture and cross layer design; 5) Security; 6) Testbed development.  The expected outcome will provide novel solutions to the wireless M2M communications network.""512409,""Dong, Xiaodai"
"508356"	"Drew, Mark"	"""Physics-based colour models in computer vision, colour science, graphics and multimedia"""	"A.) Computational Photography: (i) I have recently generated re-integrated images starting with an image's desired gradient field pair, a longstanding problem, with greatly reduce artifacts. (ii) In a 2011 patent I show how to transfer the contrast from hi-D image data down to lower dimensional images, e.g. transfer the contrast from RGB+infrared down to colour images. This work uses the patent in (i); however, the solution found is global over the image. A local mapping should in principle be much more effective, and I propose a variational-equation solution. (iii) Then other problems could be attacked, such as image sharpening without halo artifacts, and anisotropic smoothing (""wrinkle remover""). B.) Graphics/Visualization: (i) In work on multiple-lighting image sets, lighting directions are required. But for ""in the wild"" objects directions are inaccurate, so instead I wish to discover them, along with finding lights' visibility subspaces. (ii) The above could be used to discover damage to artworks lent out by museums. (iii) In hi-D medical image visualization, I have generated perceptually meaningful colour outputs. However, clipping often occurs. If gamut-fitting is included into the optimization this problem could be solved whilst generating the visualization. C.) Computer Vision: (i) Shadow-free intrinsic (illumination invariant) images utilize the entropy in a feature space as a crucial step; I propose using video information, accumulating evidence over time. (ii) Another direction would apply the intrinsic image feature to dermoscopy images: the intrinsic image can be used to eliminate light-colour and intensity changes. I also propose removing skin colour, for a colour-image lesion atlas independent of skin colour. D.) Multimedia: If actors in large theatres were crowd-tracked in 2-D, audio depth plus a panning could be generated. E.) Colour: (i) A substantial proportion of papers use colour incorrectly. I propose a publication and reference codeset for a hierarchy of camera calibrations, from complete to simple, for guiding researchers in how to proceed. (ii) Colour Balance is critical for camera data. I recently presented a new algorithm to the iPhone Camera team at Apple and propose further investigating this.""509951,""Drew, Robin"
"507924"	"Dutchyn, Christopher"	"Rigorous Modularity: formalizing and verifying software constructions"	"Software development is undergoing another paradigm shift: certified programming. Previously, small changes to significant code bases would require time-consuming and unreliable testing before deployment. For example, altering the space shuttle software to enable missions to fly over New Year's day was estimated at millions of dollars, and hence not implemented. But, similar systems are being deployed to control airliners, automobiles, and life-support systems.Over the last five years, tools such as Coq from INRIA have empowered software developers to achieve the long-sought goal of certified software. These are programs which are not just verified by checking at individual points in the input space, but which are certified with mathematical precision. Theorems certify the correct behaviour of the program are proven using the calculus of constructions, yielding rock-solid validity of the program at reasonable cost.  The illustrative example is LeRoy's CompCert compiler: in 18 months, he and four graduate students produced a production-grade C-compiler (the code it produces has 7% overhead compared to gcc), and a proof of its correctness. In a recent study from the University of Utah, CompCert showed zero bugs.My research is to adopt this new approach, and combine it with my research on software modularity. Coq is a powerful tool, but it is limited to pure functional programming.  I believe that object- and aspect-oriented modularity has a basis in this theorem-proving software environment. Meyers and others have already given us glimpses of this in the contracts construction in Eiffel. Their insights, hampered by more limited logic and constrained by less computational power, can be combined to give a logically-sound statement of modularity. For example, each abstract method needs to be accompanied by a formal logic statement about its action, and any concrete implementation must prove a theorem at least as strong as the logic statement. Every class must include a proof that encapsulation of private methods and fields is not violated. Every re-implementation of a class must satisfy the same theorems: although it may have stronger theorems about space and time efficiency.""508573,""Dutilleul, Pierre"
"507997"	"ElKhatib, Khalil"	"Securing the Next-Generation Smart Electric Grid"	"Recent advances in electronic and communication technologies have enabled the development of miniature computing nodes with wireless communication capability. These nodes can communicate with each other over wireless channels, and in the absence of an existing infrastructure, autonomously organize themselves to form small- or large-scale wireless networks. One of the domains that directly benefits from advances in wireless communication networks is the electric grid: old meters are being replaced with smart meters that allow energy providers to continuously and remotely collect, monitor, and provision energy consumption and distribution.  Building the smart grid infrastructure with two-way communication of data consumption, monitoring and control, requires ultimate consideration of many security and privacy issues. As the infrastructure is expected to be very complex, with multiple entry points, and to connect an extremely large number of communication devices with various communication, computation, and storage capabilities, there is definitely a high risk that an attack on the infrastructure can have a drastic effect on the safety and well being of large number of people. Additionally, an ongoing monitoring of energy consumption by utility providers can be highly intrusive and can reveal information about the activities of consumers inside their homes. The objective of this research program is to build secure communication networks for the smart grid with trusted and authentic data and devices, and which allows the utility providers to reliably provision the grid without jeopardizing the privacy of the consumers.""517952,""Elkin, Lisa"
"513504"	"Ellis, Randy"	"Computational Models For Surgical Navigation"	"This research program can be summarized simply as the pursuit of answer to the question: ""How can computer science improve a surgeon's understanding of the human body?''Mathematical modeling and virtual reality can provide tools for modeling organisms, predicting outcomes of proposed changes, and guiding a surgeon in making these changes; in orthopedics we find that images, motion and manufactured components are routinely combined in advanced surgical procedures.  This work will use the applicant's expertise in computing, and considerable experience in surgery, to give new theoretical results and implementations that will directly improve the health of Canadians.  This will be done, first, by integrating computer technology into North America's first operating room to have both 3D X-ray scanners and 3D trackers for surgical instruments. The goal of this part of the research is to provide surgeons with fast, extremely accurate ways of operating on anatomical regions that are normally invisible and/or inaccessible using current surgical techniques.The second part of this research will develop computational models to describe complex shapes, mainly of skeletal bones.  This will bring mathematical descriptions, from areas such as General Relativity in physics, to body parts such as the hip joint. These models will improve the ways that surgeons understand the human body, plan surgical procedures to correct highly prevalent diseases such as joint arthritis, and improve preoperative and postoperative treatments such as physiotherapy. If the research results are achieved then Canadians will enjoy greater quality of life from individualized analysis and treatment of joint diseases that affect a large proportion of our aging population.""501043,""Ellis, Stephanie"
"509673"	"ElSaadany, Ehab"	"Operation and Control of Smart Distribution Systems"	"Reliable, efficient, low-cost, and high-quality electricity supply is a major factor in improving and maintaining a high standard of living in Canada. Currently, electricity supplies face numerous challenges from different avenues such as environmental pollution, decline of indigenous energy supplies, and aged energy infrastructure. Although it is generally accepted that centralized electric power plants will remain the major source of electric power supply for the future, it is evident that simply stringing together additional central plants and wires can lead to a more vulnerable power system. Recent restructuring of the energy system has created a competitive landscape for power generation and opened a new era for the utilization of decentralized energy sources or what is known as Distributed Generation (DG). The potential lower cost, higher service reliability, high power quality, increased energy efficiency, and energy independence and security are all reasons for interest in DG. Moreover, since most of the DG technologies are sustainable such as wind, photovoltaic, geothermal or hydroelectric, or clean energy sources such as fuel cells, significant environmental benefits can be achieved. Increased deployment of DG technologies can have a beneficial impact on Canadian Government energy policy goals which are reducing emissions, ensuring reliable energy supply, and promoting competitive markets. Further, the energy sector is moving toward implimenting the smart grid. The natural evolution of this vision is the transformation of conventional distribution systems into Smart Distribution Systems (SDSs), which host distributed and renewable generation units, energy storage devices, load management controllers, grid-automation devices and algorithms with seamless integration characteristics. The proposed research work aims to develop of advanced models, operation and control algorithms that will facilitate timely and safe integration of SDSs devices into existing networks. This research will help the utilities sector eliminate the current barriers impeding deployment of sustainable and other clean sources, intelligent automation devices and control algorithms into the utility grid. ""512307,""ElSaadany, Ehab"
"509366"	"ElSankary, Kamal"	"Development of low-voltage and low-power mixed-signal ICs using nano-scale multiple gate field-effect transistors"	"The low-power and cost-effective Complementary Metal-Oxide-Semiconductor (CMOS) devices have forced the analog designers to develop integrated circuits (ICs) using digital CMOS technologies. Conventional nano-scale planar CMOS devices suffer from undesirable short-channel effects, such as high drain induced barrier lowering, substantial leakage currents due to threshold voltage roll-off, lower intrinsic gain, lower output impedance, lower dynamic range and poorer device matching. These non-idealities eventually degrade the performance of CMOS devices and pose a serious threat to ICs' performance. The Multiple gate field-effect transistors, such as Fin-shaped Field Effect Transistors (FinFETs), are emerging as the most promising replacement for planar CMOS in sub-32nm technologies. In fact, FinFET technology offers interesting features such as tight channel control to mitigate short channel effects, and steeper sub-threshold slope. Despite the advantages of multi-gate technologies, new design challenges arise particularly for analog, mixed-signal and radio frequency (RF) circuits. FinFETs suffer from larger parasitic compared to planar CMOS transistors. The narrow fin width increases the source and drain resistances and that result in degrading the high frequency and noise performance of FinFETs. Also the empty region between the gate and the source/drain causes a large increase in the fringe capacitors of the transistor and consequently reduces the speed of RF applications. FinFET devices also show new undesirable effects that have to be taking into consideration while designing analog and mixed-signal circuits; namely self-heating and hysteresis effect. Moreover, accurate physical characterization of FinFET devices represents a tremendous challenge for analog designer and new optimized layout techniques are also necessary to fully benefit from this advanced technology.As a result to enjoy the merit of multi-gate field-effect transistors, the objective of this proposal is to develop novel design methodologies for high performance analog, RF and mixed signal circuits using FinFETs.""501793,""ElSawy, HeshamMahmoud"
"508364"	"Ester, Martin"	"Probabilistic Graphical Models for Data Mining and Recommendation in Social Media"	"Social media allow the creation and exchange of user-generated content and support various forms of interactions among content producers and consumers, based on the technological foundations of Web 2.0. Compared to traditional media such as newspapers and TV, social media have a much larger and more diverse group of producers, leading to many short and noisy posts, allow users to comment on content and to communicate with each other, and evolve dynamically. Social media have the potential to provide valuable feedback to producers and to allow consumers to tap into the ""wisdom of the crowds"" as aid in their decision making. The long-term objectives of our research program are to develop (1) Data mining methods to model the complex dynamics of collective action in social media, enabling a better understanding of these media and the prediction of future events. (2) Recommendation methods that recommend trust-worthy and relevant content, specific to a given user, and enable social media sites to increase the level of user participation. To model the complex effects in social media, we will explore probabilistic graphical models, which are very informative and can naturally integrate available background knowledge. As short-term objectives, we will focus on the following key issues:(a) Modeling social influence and recommendation: How to learn the strength of social influence from observed user actions? Such a model is crucial for recommendation and for other tasks.(b) Topic modeling and opinion mining: How to model the textual content of posts, users or user groups at a semantic level? In particular we will investigate this for aspect-oriented opinion mining.(c) Community discovery: How to model and discover closely connected communities with similar attributes? Communities are common structures in social networks.(d) Spatio-temporal models: How to model the spatio-temporal aspects of social media? This is important, since social media, as well as their users, ""live"" in a world with spatio-temporal references.""518591,""Estrin, Ron"
"509521"	"Fapojuwo, AbrahamOlatunji"	"Energy Efficient Architectures and Protocols for Beyond-4G Mobile Cellular Networks"	"The increasing pent-up demand by users for new and emerging wireless services is driving the need for future generation broadband (high capacity) wireless networks, such as the Beyond-Fourth Generation (Beyond-4G) mobile cellular networks. However, the energy consumption of such broadband wireless networks is becoming a serious concern, not only because of increased operational expenditure to the network operators (due to the escalating cost of energy nowadays) but also in terms of the associated environmental impacts arising from carbon dioxide (CO2) emissions, as the electricity used to power the wireless network infrastructure is generated by fossil fuels. The main goal of the proposed research is to make the future generation broadband mobile cellular networks energy efficient, to be environmentally friendly, and economically sustainable. I propose to design, analyze and evaluate original and innovative network architectures and protocols to achieve the above goal. Specifically, I shall investigate both the heterogeneous base station network and cooperative relay network architectures. My research will propose and discover energy efficient protocols and algorithms for data transmission, multiple access and resource management, and also exploit the characteristics of wireless applications and services to achieve energy efficiency. I shall study the interactions amongst the protocols proposed at different layers to further achieve increased energy efficiency. Novel theoretical techniques will be developed for performance evaluation, to gain valuable insights. My research will offer a solution to one of society's needs: achieving a green environment, through a reduction in the carbon footprint induced by future generation broadband wireless networks. Furthermore, my proposed research program will provide technical training in energy efficient broadband wireless networking to graduate students whose highly desired skills are vital for continued growth of the Canadian wireless industry.""509700,""Far, Behrouz"
"508121"	"Ganjali, Yashar"	"Control and Management for Next Generation Networks"	"The structure of today's Internet imposes several limitations: it cannot deliver the potential of emerging technologies such as wireless communications; it does not provide any guarantees on availability and reliability; it is intrinsically unsecure; identifying the root causes of problems is extremely difficult in the current Internet; economic incentives for peering are not easy to manage, and create; and in order to keep up with the growing demands, it needs more and more power, and becomes more costly over time. Recently, there has been a tremendous amount of interest in a clean slate design for the Internet to address these problems. One promising idea is to use Software-Defined Networks (SDNs) that has attracted a lot of attention both from academia and industry. Switching to software-defined networks poses several challenges and creates many opportunities. The existence of a centralized controller that has a global view of the entire network, and the simplicity of creating new applications and protocols are luxuries that we can use to provide new services, protocols, and functionalities that are prohibitively difficult in today's Internet. Globally optimized traffic engineering, congestion control, access control, and identity-based security are only a few of such possibilities. On the other hand, relying on a centralized control brings about various scalability, reliability, and potentially security challenges, that we need to address.This project aims at (i) design and implementation of new tools, and methods for a highly scalable and reliable control and management framework for next generation networks; (ii) design and development of a high level network management framework for home and enterprise software-defined networks; and (iii) creating globally optimized protocols that take advantage of the global network view available in software-defined networks.""503679,""Gannon, Terry"
"508129"	"Garcia, Ronald"	"Enhancing Support for Metaprogramming"	"Given the ever-increasing importance of software, it is imperative that software developers be provided with techniques, theories, and tools that can help support these continuing advances. One programming methodology with immense potential in this regard is metaprogramming, the process of writing programs that read, process, and generate other programs.  Metaprogramming has clear potential to significantly improve software by reducing development time, increasing software quality, and increasing software flexibility.     Unfortunately, metaprogramming has simply been too complex for most mainstream software developers to master. To address this, my long term research goal is to expand the applicability of metaprogramming by making metaprograms easier to write, optimize, and maintain.     This research will advance the understanding, methodology, and technology of metaprogramming, and produce highly qualified graduates who have mastery of state-of-the-art software development techniques and are poised to lead ongoing advances in research and application of this vital and high-impact research area.     My students and I will make metaprogramming easier to use for high-performance software by developing new analysis techniques that help optimizing compilers speed up metaprograms.  We will also improve the quality of metaprograms by extending safety checking theories to support metaprograms that use re-assignable variables.  Finally, we will increase the flexibility and accessibility of metaprograms by extending the C++ language with developer-friendly features to generate custom-tailored data structures at compile-time.     This research will contribute to more intelligent programming practices, invite a more efficient means of building bug-free code, and ultimately accelerate the science behind the software revolution that is transforming society.  Two of the three projects will produce new programming tools that generalist developers can immediately use to improve their software development. The Canadian software development industry will benefit from the techniques, technology, and highly-qualified personnel produced by this research.""517705,""GarciaPoulin, Mikel"
"509419"	"Gaudet, Vincent"	"Energy-Efficient Computation and its Application to Baseband Signal Processing Systems"	"The proposed research program investigates energy-efficient integrated circuit realizations of basebandcommunications and signal processing algorithms, such as those used in recent wireless and wireline systems. To achieve near-capacity performance, modern communication standards use forward error correction such as Turbo or low-density parity-check (LDPC) coding, with high-speed (e.g. IEEE 802.3an for 10 Gb/s Ethernet) or low power consumption requirements (e.g. IEEE 802.11n, IEEE 802.16m, and 4th-generation (4G) wireless (e.g. Long-Term Evolution, LTE)). The proposed research has the potential to make decoding feasible for high-speed or energy-constrained emerging areas such as 100+ Gb/s Ethernet and body-area networks. Due to their numerically intensive nature, decoders can consume significant power, especially for high-speed implementations. In recent years, very impressive LDPC and Turbo decoders have been demonstrated in complementary metal oxide semiconductor (CMOS) technologies, with some decoders consuming as little as a few tens of picojoules per transmitted bit. Despite these results, there remain many open scientific issues. (1) Most of the techniques require parallel processing and are only applicable to short-to-moderate block length codes (i.e. up to a few 1000's of bits). Longer codes have better error-correcting capabilities but parallelism cannot be fully exploited due to area limitations. (2) To adapt to time-varying channel characteristics, standards often define several code rates. The most efficient decoder implementations often realize only one of the code rates (e.g. R = 1/2). Satisfying multiple rates requires configurability, which comes at the cost of energy efficiency. (3) Since the design space for LDPC and Turbo decoders is immense, decoders can yet be made to be far more energy efficient, for instance by aggressively scaling supply voltages. The proposed research program will investigate the issues above. The techniques will first be verified using high-level simulations, and will ultimately be demonstrated through extensive tests and power measurements from proof-of-concept integrated circuits designed by the HQP as part of their research and training program. ""501282,""Gaudin, Catherine"
"509420"	"Genov, Roman"	"Ubiquitous Biomedical Sensory Microsystems"	"The long-term objective of the research program proposed in this application for renewed funding is to develop analog and digital VLSI circuits, architectures and algorithms to enable novel implementations of disposable, wearable and implantable microsystems in order to address key unmet ubiquitous sensing needs in biology and medicine. Our short-term goal is to design, prototype, experimentally characterize and validate in high-impact applications the following two such sensory microsystems:     Fully-implantable wireless brain interface for treatment of neurological disorders     Disposable wireless electrochemical DNA analysis microsystemThe reason for the ubiquity of the proposed biomedical sensory systems is two-fold. First, they are miniature, inexpensive, versatile and autonomously operating, which enables their widespread utilization, both within and outside of clinical settings. Second, a collection of such sensors can be used to monitor a number of vital signs, and perform various types of automated medical diagnostics and, in some cases, automated treatment. In order to facilitate their use, and to eliminate expensive wiring and packaging, each sensor has a fully wireless interface.This research program is cross-disciplinary and has a collaborative nature. The focus of this application for funding is on the novel functionality and performance optimization of integrated circuits which serve as the sensory and computational core of these ubiquitous biomedical sensory microsystems.""502225,""Genovese, Matthew"
"509405"	"Girard, André"	"Models and approximations for the fast calculation of packet delay variation"	"The information transmitted over the Internet, say from a web server to a user, needs some time to reach the user. Anybody who has watched video on the Internet has noticed that it takes a little while before the picture actually appears on the screen. It also turns out that this delay is not always the same but varies around some average value. This is called the ``jitter'' of the transmission and it can be the cause of poor service, say for instance for a two-way video conference, where it could make the image and sound freeze or even disappear completely. Operators therefore need to design their networks to take this jitter into account and try to control it as much as possible. This in turn means that their design tools must be able to actually compute the value of this jitter whenever a new configuration is proposed. This calculation has to be done a large number of times in a given planning so that one needs techniques that are both fast and accurate. The objective of this project  is to find calculation methods that meet these two requirements. They will then be included into network design tools which will give operators an efficient mean to plan networks with better quality of service and lower cost. This will also allow us to extend our research on the  design of various kinds of networks to take jitter into account thereby increasing their accuracy and effectiveness. In the longer term, this research will  improve the position of canadian companies in the world telecommunication market and re-inforce the role of Canada as a leader in the field.""514115,""Girard, Denis"
"509504"	"Gleason, Scott"	"Remote Sensing of the Environment Using Reflected GNSS Signals"	"The goal of this research proposal is to develop algorithms to make environmental observations by processing Earth reflected Global Navigation Satellite System (GNSS) signals. Initially, theoretical analysis and experiments will focus on sensing the Earth's cryosphere and ocean wind and waves.  Subsequently, analysis will be performed which will define the requirements for a new satellite mission capable of validating the remote sensing algorithms developed.  In addition to sensing ice and ocean wind and waves, additional applications will be explored using the most promising methods developed from the early stages of this research, including the possibility of sensing soil moisture, snow and other environmental parameters. Finally, the use of non-GNSS signals of opportunity will be studied to further expand the measurements achievable using this technique.  These research tasks are designed to accomplish the long term goals of, a) developing and validating robust remote sensing estimation algorithms for sea ice and ocean winds and waves using GNSS reflections and b) integrating these new measurements into global and regional climate models for improving our understanding of the environment and climate.This research has the potential to significantly increase the number of observations of the Earth's environment available for climate change study and modeling.  It will enable a new category of remote sensing applications which will greatly increase the measurements available to environmental scientists, industry and the general public.  This research will enable the development of remote sensing algorithms which have the potential to contribute to Canada's need to observe and understand its natural resources, particularly in the Arctic regions.  During this research 2 Ph.D.'s and 3 M.A.Sc's will graduate with training in the areas of GNSS, remote sensing and satellite mission and instrument design.""498250,""Gleeson, Sarah"
"508112"	"Goel, Ashvin"	"End-to-End Data Reliability with Runtime Verification"	"Several studies have shown that failures due to software bugs account for 25-35% of the total system downtime and cost $60 billion annually in the US alone. A particularly severe type of failure occurs when a software bug causes corruption of data on disk. Unlike transient memory failures, data corruption cannot be resolved by restarting the system, or by using hardware reliability techniques, such as storage redundancy. Instead, complex recovery procedures are needed, and they tend to be error-prone. For instance, restoring data from a backup is time-taking and risks loss of recent work.The goal of the proposed research is to prevent data corruption on disk in the face of arbitrary application bugs and vulnerabilities, thereby minimizing the need for expensive and error-prone disaster recovery solutions. We plan to address this challenging problem by using a technique based on runtime verification. The key idea is to detect faults by observing disk input/output behavior, and then verifying that disk updates satisfy an application's consistency requirements. Unlike traditional formal verification techniques, such as model checking, that statically verify a simplified model of the target system, runtime verification is performed continuously on the real system, thereby providing assurance that the implementation matches its specification. We expect that this research will have a significant impact on the design of reliable software systems, especially storage-based applications.""506774,""Goel, Vinod"
"508255"	"Golab, Lukasz"	"Characterizing and Improving Data Quality in Very Large Databases"	"Data mining and data analysis have become standard practice in business and government organizations.  However, analysis results are only as good as the input data. Unfortunately, the size and complexity of modern databases and information systems make it difficult to ensure data quality. In particular, inadequate understanding of the data semantics (leading to poor database design), data evolution over time (e.g., due to integrating new data sources) and error-prone collection may lead to data that are inconsistent, incorrect and incomplete.  Understanding and monitoring data quality to avoid the ""garbage-in-garbage-out"" problem is an important and challenging issue.An additional challenge is that business-critical applications often collect continuous streams of data, which must be processed ""on the fly"" to support real-time decision making.  Several data management technologies have recently been proposed to handle streaming data, including data stream management systems, stream data warehouses and event-processing systems.  This proposal establishes a new research direction into methodologies and tools for streaming data quality.  This includes models that describe the semantics of streaming data, and algorithms that incrementally detect and resolve data quality issues as new data arrive. Improving data stream quality will make stream processing systems more usable.  Thus, the proposed research is of interest to businesses and government organizations that can gain a competitive advantage by making data-driven decisions in real time (e.g., healthcare, financial institutions, telecommunications companies, Web-based companies such as Google, Yahoo!, Facebook and Twitter, law enforcement, smart power grid management, and highway traffic management), and to database and information systems vendors (e.g., IBM, Oracle, Microsoft, Teradata, SAP, StreamBase Systems).""502462,""Golbabaie, Mahsa"
"508109"	"Gong, Minglun"	"Computer vision algorithms for live video processing using programmable graphics hardware"	"Today's technology is increasingly powerful and affordable.  For example, an 800 dollar graphics card today can process one Trillion FLOPs in double precision.  Merely a decade ago, such processing power would come with a one million dollar price tag.  Similarly, digital video cameras at the time were only available to movie producers, whereas today they are in the hands of billions of users, as well as integrated into phones, vehicles, and game consoles.  The availability of low cost processing power and digital video capturing devices allow computer vision techniques to affect and benefit our day to day lives.  They are making our phones smarter, our vehicles safer, and our game consoles much more fun to interact with.     This research investigates how to perform challenging computer vision tasks on live video at real-time speed.  The tasks include inferring depth from video sequences captured from different viewpoints (stereo vision), detecting moving foreground objects from dynamic backgrounds (foreground segmentation), and extracting objects with fuzzy boundaries for seamlessly blending with new backgrounds (video matting).  Being able to perform these tasks for live video has a widely range of applications in our daily life.  For example, real-time stereo vision can be employed for sensing the 3D environment and foreground segmentation for detecting pedestrians; both are key components of the future driverless cars.  Foreground segmentation and video matting can be used to extract video conference participants from captured video and place them into the same virtual environment, providing better sense of presence and better protection of privacy.     The goal of the research is to develop novel algorithms that are not only fast enough for handling live video, but also having better or comparable performance to the state-of-the-art offline algorithms.  The algorithms to be developed therefore need to be both effective and efficient.  In addition, to harvest the processing power of modern graphics hardware, these algorithms will be designed with parallel execution in mind.  Some preliminary work along this research direction has yielded very promising results.""505789,""Gong, SiewGing"
"507999"	"Gooch, Amy"	"Framework for Preserving Salience through Perceptual Differences in Spectral Image Creation and Manipulation: From Scene to Display"	"Vast amounts of informative images pervade our society. Assimilating, understanding, and managing this information is a daily challenge.  The first avenue of my research creates an abstract representation of the data conveyed in images, carefully mapping data representation to data presentation. Image data that is too subtle blurs into the background; data that is too intrusive turns into noise, which can easily be ignored. My research will examine image processing algorithms that consider pre-attentive processing from cognitive psychology and addressing questions of image representation and creation by examining, recording, and representing the electromagnetic spectrum, including infrared, ultraviolet, visible spectrum and the capture of polarized light in indoor and outdoor scenes.  The second avenue of my research is motivated by a Chinese proverb, that states: ""Tell me and I will forget. Show me and I may remember. Involve me and I will understand."" Images are capable of conveying a very large amount of data, putting content worth a `thousand words' in a single image. However, without interactivity, such as an exploratory simulation or an educational game-like interaction, the information may not be retained. With the ""tell me, show me, involve me"" metaphor in mind, my research aims to explore image and data representation and  establishing methods of interaction, evaluation, and deployment, such as museum exhibits.  Initial specific avenues of exploration include aesthetic and informative ambient visualizations of energy usage to demonstrate energy usage in a sensor-enriched house and aesthetic music visualizations for the hearing impaired.Computers are increasingly used as multimedia devices rather than computation machines. The research in this proposal will train graduate students and postdoctoral fellows, and co-op undergraduate students in areas of information technology including computer graphics, image representation, gamut mapping, multimedia systems, signal processing, human-computer interaction, and software development.""498938,""Good, Allen"
"508093"	"Graham, TCNicholas"	"Software Engineering of Networked Digital Games"	"Digital games have become an increasingly important form of entertainment. Their appeal includes their engaging interactive nature, the social opportunities they afford through networked play, and their wide availability on platforms ranging over the web, mobile phones, specialized consoles, and traditional PCs. Game developers face relentless pressure to deliver better graphics, more realistic physics, more natural forms of interaction, and support for larger networked groups. Thus, games are becoming more expensive to produce and require ever more sophisticated expertise to create. At the same time, prices of game are dropping (particularly around mobile and social platforms.) Games are also proving valuable in specialized markets such as education, advertising and physical therapy, where the large teams and budgets used by traditional studios are not available.To help address these problems, we will research novel techniques aiding in the software engineering of networked games. Our goal is to raise the level of programming of networked games, providing highly-performant support for dealing with the hard problems of network communication, persistence, and consistency maintenance in presence of lag. This work will result in new programming abstractions, reified in experimental toolkits. Our new techniques will be applied to two demonstration projects, in exercise video games for children with Cerebral Palsy, and in orchestration of military simulations.""513242,""Graham, Terry"
"509349"	"Grami, Ali"	"Next-Generation Multi-Band Satellite Systems"	"Making the information infrastructure accessible to all Canadians is a major principle advanced by the government to make Canada the most connected nation in the world. Canada's vast territory, low population density, and harsh northern terrain pose economic and logistical barriers to the expansion of terrestrial broadband networks. The only affordable means of providing universal access to areas un-served or under-served by terrestrial networks is via satellites. This research on the next-generation of advanced multi-band multi-beam high-capacity GEO satellite systems presents many challenges, as it is a huge, multi-dimensional non-linear optimization problem. It is virtually impossible to fully meet all relevant requirements and constraints, for some are clearly in conflict with others. In order to develop a novel, comprehensive methodology of optimally designing advanced satellite systems, an extensive research will be carried out by a PhD student to assess the impact of the following parameters on one another and on overall system: satellite resources and requirements (buses, launchers, power, mass, real estate, orbital slots, payload architectures, beam configurations), frequency bands (30/20 GHz Ka-band, 50/40 GHz V-band, and 80/70 GHz W-band), service considerations (connectivity, capacity, coverage, cost), terminal features (antenna, power amplifier, uplink power control, access, speed, modulation, coding), interference components (inter-modulation, adjacent satellite, adjacent carrier, adjacent channel, cross-polarization, inter-beam, de-polarization), and performance parameters (bit error rate, delay, link availability). Also, in parallel, the following three major research projects, which can have profound impacts on satellite resources and link characteristics and in turn on the optimum design methodology, will be carried out by three MASc students: i) asymmetric apportioning of link availability between uplink and downlink, ii) uneven apportioning of link degradations: thermal noise versus a multitude of interferences, and iii) unequal allotments of spectrum between forward link and return link.""503869,""Grandbois, Michel"
"509407"	"Grégoire, JeanCharles"	"User-Centered Services at Internet Edge"	"How is the Internet evolving? Several visions of the future collide: Internet of things, Mobile, Content-oriented or Cloud-based; yet it is clear that the evolution from today's Internet demands a convergence between such, rather radical, perspectives. Meanwhile, key issues in the use of the Internet, such as performance in access, quality of service and security remain critical issues, further encumbered by the emergence of IPv6 which raises the challenge of the transition from the current technology base to a new infrastructure, with a deep impact for the general public.In this context, much work remains to be done on finding the most suitable ways to orchestrate the different usages of the Internet into a coordinated, efficient, pleasant, convenient - and secure! - experience for the general public. Some applications remain end-to-end while others are cloud-based; some information should remain at the home while other can be moved into the cloud. A simple server-to-cloud migration is not satisfactory for applications where latency and quality remain key factors, especially multimedia. Yet even as technology and architectures evolve rapidly, the specific concerns of users and the quality of their experience of the Internet tends to be neglected.This research will investigate a new paradigm in the experience of the Internet, which extends the work I have already laid out with the ""edge cloud"", the deployment of cloud-like functions at the periphery of the Internet, in partnership with or under the control of the Internet Service Provider (ISP) controlling the access network, thus leading to new business opportunities.The research objectives are multiple, and focus mostly on the security and performance impact of the break up of the traditional desktop and client-server functions in a three tier home-edge-core model, and the study of the benefits of this break up for a variety of user-oriented services based at the edge, with a strong emphasis on multimedia, which has been identified as the strongest growth sector of the Internet.""519224,""Gregoire, Melanie"
"509354"	"Gregori, Stefano"	"ON-CHIP POWER MICROCONVERTERS"	"This research program advances the development of new power microconverters integrated in mainstream digital CMOS fabrication processes with no off-chip devices.   As the power requirements of modern microsystems grow, efficient power converters are becoming increasingly critical to the new generation of integrated circuit designs.  The limitations imposed by power constraints are already sharply curbing the performance designers might otherwise expect.  To overcome this challenge, distributed power processing based on new microconverters is needed to jointly optimize power, energy, and operating performance.  Such optimization will lead to significantly improved system reliability, efficiency, and battery lifetime.  From both circuit-level and system-level perspectives, this program investigates key design issues, control schemes, circuit architectures and future research directions which might be pursued in the development of application-aware and variable-output power microconverters.   The integration of microconverters via standard digital processes will not only avoid the fabrication complexity of high-voltage devices, it will also reduce the number of heterogeneous chips and discrete components required for a given design.  As a result, weight, volume and parasitic losses of interconnects will be smaller.  Microconverters will be the enabling hardware platform for on-chip power processing which will, along with traditional signal processing, rise to become a key factor in next-generation microsystems.   Ultimately, this research will contribute to reducing energy consumption and improving functionality of many electronic systems around us, from battery-powered mobile multimedia devices to biomedical monitoring systems and wireless sensor networks.""506190,""Gregory, Diane"
"508242"	"Greif, Chen"	"Iterative Solvers for Saddle-Point Systems"	"This proposal concerns the investigation, analysis and implementation of numerical solution methods for a family of linear systems that arise from problems that can be generally posed as minimization problems with constraints. Such problems are extremely important, and they frequently appear in a surprisingly large scope of scientific models. Examples of relevant applications include computer graphics, data mining, image processing, medical imaging, fluid flow, electromagnetics, and many more instances.The proposal puts forward a list of objectives that include the design of new solvers for constrained optimization problems and constrained differential equations, the derivation of new algorithms that combine various solution methodologies into one optimized solver, and the investigation of questions pertaining to the best way to solve the problem, with focus on the question to what extent various reformulations affect the robustness and reliability of solution procedures. Long-term objectives include the pursuit of a unified framework that sheds light on connections among different solution methodologies and among the different applications that lead to the systems that are considered.The issues addressed in this proposal are not just of theoretical interest but also of a high practical value, including the implementation of various solvers and the involvement of highly qualified personnel.The timeliness of this proposal is driven by the continual development of new disciplines and problems that involve the solution of large-scale linear algebra systems, and the new frontiers that processing speed, parallel computing architectures and high performance computing have reached, pushing the envelope in terms of the size of problems that can be tackled.""501526,""Greig, Michael"
"508000"	"Gromala, Diane"	"VR Systems for Body Image and Body Schema"	"Recent research in the emerging field of embodied cognition demonstrates the mutual importance of understanding the extent to which technologies affect our physical/physiological factors, and how our assumptions about our bodies affect the ways we design technology. Thus, it is increasingly important to study the fundamental processes of body schemas and body images. A body schema comprises the collection of processes that are responsible for typically non-conscious processes such as posture and movement. A body image refers to attitudes and beliefs we have about our body, which change over time, or when we face disability or disease. Body schema and body image function as continuous, mutually influential processes that provide important information about a user's psychophysiological state.  This proposal aims to contribute to increasingly important research on fundamental human processes of body schema and body image, particularly as they change over time. We propose to: (i) investigate computation of body schema and body image by abstracting and modeling relevant correlates of each, (ii) identify relationships of body schema and image that together indicate variable psychophysiological user states, (ii) develop visualizations that correlate to changing user states, initially in VR avatars for range of movement studies in seniors, (iii) develop an interface that enables user annotations of body image data, (iv) develop methods for accurate repeatability of capturing user state data, (v) visualize aggregation of variable timeframes for later analysis of longitudinal data of users' psychophysiological states and (vi) develop methods of empirical assessment. The long-term objective of this research is two-fold: 1. To identify the theoretical underpinnings and formalisms necessary to devise computational models of variable body schema and image data. 2. To develop methods of assessment to provide empirical evidence of effectiveness. ""517298,""Gromko, Nicholas"
"507980"	"Guenin, Bertrand"	"Structural problems and minimax relations in graphs and matroids"	"In this research proposal I plan to investigate the structure of certain minor closed classes of graphs and binary matroids. The particular classes considered arise naturally in the context of combinatorial optimization. The motivation is fourfold: to provide the building blocks for understanding more complex classes of objects; to find common generalizations to flow and coloring results in graphs; to design efficient algorithms to recognize these classes of graphs and matroids; and to show that certain NP-complete problems become polynomial when restricted to these classes of objects. In particular I will investigate the following problems:1) Tutte characterized which matroids arise from graphs. His work had important theoretical implications; it led for instance, to Seymour's decomposition theorem for regular matroids. We want to generalize this result by characterizing two classes of matroids that arise from signed graphs.2) A quintessential minimax relation is the Max-Flow Min-Cut theorem, which states that the largest amount of flow that can be sent between a pair of vertices in a graph is equal to the capacity of the smallest bottleneck separating these vertices. A famous 1977 conjecture by Seymour predicts that the same minimax relation holds for binary matroids that do not either of three special obstructions.3) Graphs that do not contain the complete graph as odd-minors play a pivotal role in the study of multi-commodity flows and graph coloring. A decomposition theorem for this class of graphs remains elusive, however. We wish to attack this problem by reducing it to a special case of the first problem.Each of these problems is widely regarded as important and challenging. An original feature of this proposal is that it highlights a common strategy to attack all three problems.""511755,""Guenther, Axel"
"514095"	"Gutwin, Carl"	"Improving Quality of Interaction in Distributed Real-Time Groupware"	"Real-time distributed groupware systems are becoming common, but the types of collaboration supported in groupware are still limited. In particular, real-time interaction is poorly supported, even though in the real world, people are experts at this kind of shared work: in face-to-face settings, people can coordinate fast turn-taking actions smoothly and efficiently. In distributed groupware, which lacks the rich information of a face-to-face setting, this 'interactional expertise' becomes difficult to maintain, and groupware still traps users in a 'beginner mode' of interaction where shared activity is stilted, awkward, and slow.      The problem addressed in this proposal is that current groupware systems do not provide adequate support for true real-time interaction. Without this support, distributed groupware systems cannot provide the rich and subtle environments that are needed for expert performance in many tasks. The long-term goal of this research is to make distributed collaboration as expressive and flexible as it is in face-to-face settings. To achieve this aim, two areas of research are needed: first, groupware must support faster interaction that happens at a smaller time scale; second, groupware must support more expressive interaction to provide the rich set of cues and signals that people use to coordinate real-time interaction in the real world. The proposal contains eight separate projects in these two themes, and includes work on determining the effects of delay, toolkits and protocols for improved network performance, new techniques for expressive gesturing and movement, and better support for prediction and awareness.     There is  considerable potential for economic and social benefit in this research. We will produce several enabling technologies that make new kinds of interaction possible at a distance; as groupware systems become more common, systems that  are able to  support true real-time interaction will clearly differentiate themselves from competitors. In addition, reducing frustration, effort, and errors in real-time collaboration will have a substantial impact on the quality of everyday work life for those who collaborate at a distance.""505243,""Guy, Robert"
"507968"	"Hahn, Gena"	"Graph searching and applications"	"Cops-and-robbers games are useful in modelling various graph widths, which in turn are useful in the design of efficient algorithms. They are also useful in robotics by modelling different problems that robots used in practical situation may encounter. This proposal is concerned with the mathematics of the latter applications. In this case, a cop and a robber play on a given graph, by alternating moves from a vertex to one of  its neighbours, until the cop occupies the same vertex as the robber. Thus a robot can attempt to locate an intruder in a network or in a maze. In particular, we investigate, for graphs on which the cop has a winning strategy, the length of games in which each player moves intelligently, that is, the cop tries to catch the robber as fast as possible while the robber tries to survive as long as possible.  We also consider a new and more general game, in which a number of cops play on one graph G, trying to capture a number of robbers on another graph H. The capture relation between G and H governs the game. This can model, for example, a robot moving in one room, while his position is translated to a situation elsewhere (a discrete version of a mouse on a pad one uses with a computer, except of course the robot is not controlled by a human). As an extension, we look at the games on infinite graphs since their gaming properties are quite different. While this is a purely mathematical endeavour, some of the results could be applicable to the web graph as some people like to model it by an in infinite one. Note that the numerous people that work on graph searching are ready for industrial collaboration.""508441,""Hahn, Heekyoung"
"507974"	"Haque, WaqarU"	"Distributed Real-Time Transaction Processing"	"Many real-world applications such as airline reservations and stock markets require real-time response while interacting with large, geographically distributed, databases. Transactions in such a system have deadlines, but there are no catastrophic events if a deadline is missed; however, a penalty must be paid for late completion (same concept as paying your taxes after the April 30 deadline). The primary goal is to meet the deadlines, but for transactions that miss it, the delay must be minimized. For instance, a typical requirement may be that 95% of transactions complete in less than 2 seconds. Prior to building large/expensive networks and distributed systems, efficient protocols must be designed and tested to demonstrate their optimal performance under a variety of conditions. To do this cost-effectively, we propose a modular approach for development and testing in a simulated environment. Among other things, the simulator should be capable of simulating low, moderate and high load conditions. An interactive graphical user interface would allow a combination of input parameters to define various system configurations and data visualization.     The sheer volume of data that is generated makes data analysis a very tedious task. We propose to apply business intelligence tools to scientific data for gaining insight into the results. For instance, predictive analytics may generate an early warning of possible congestion that could lead to poor system performance. This information can then be used to prescribe remedial actions to avoid performance degradation. This concept is similar to predicting traffic congestion several minutes before it is going to happen and then taking corrective action to avoid the imminent congestion. Business intelligence concepts have never been applied to data of this nature.    The proposed research has potential applications in areas such as automated manufacturing, multimedia databases, real-time process control, banking, airline reservations, and stock exchanges. The analytics and data visualization modules can also be used in other applications which generate large volumes of data.""508791,""Harada, Megumi"
"508017"	"Hardy, Simon"	"Computational modeling and analysis of the dynamics of control mechanisms in molecular interaction networks of neurons"	"The recent technical innovations in measurement of cellular phenomena provide the means to collect data required for quantitative modeling of cell biology systems. Nowadays, a growing number of biologists adopt a systems biology approach and integrate computational modeling and simulation with experiments in their scientific endeavor. In such setting, modeling can be a tool to generate testable hypotheses and to facilitate understanding. In the cell, biochemical signals are rarely propagated by simple, direct paths, but rather through a network of molecular interactions. With modeling, it is possible to understand the complex dynamics of this network. From a theoretical perspective, to achieve this understanding, one needs more realistic models and analysis methods of simulation data that highlight systemic properties.This research program aims at developing new and innovative computational methods and models to investigate the intracellular mechanisms of information processing in neurons. One promising method is the dynamic graph, which produces a network representation of a dynamical system. This method helps to understand how the behavior of a system arises from temporal and spatial interactions of functional modules known as regulatory motifs. Another axis of research that will improve modeling capabilities is the coupling within a single modeling approach of all the mechanisms (e.g. electrical, biochemical, shape modification) regulating the cellular activity of neurons. These computational tools can then be applied to the analysis of dynamic imaging data from cells to understand their behavior.Reaching this program long-term goal will impact the biomedical academic and industrial communities since both are increasingly using computational modeling to achieve a more precise mechanistic understanding of health and disease.""497588,""Hare, James"
"508127"	"He, Meng"	"Succinct Data Structures with Applications to Large Data Sets"	"The problem of efficiently storing and retrieving information is an essential topic in computer science. During the past decades, various techniques have been developed to index data so that the useful information can be retrieved almost instantaneously by performing queries for keywords or phrases. In recent years, as the size of the data has grown rapidly, many techniques that were useful for small, older systems have become infeasible for large, modern applications because they occupy too much storage. Most of this space is not the raw data, but structural information added to improve search efficiency. Succinct data structures were proposed to address this problem, so that the information in large systems can be retrieved quickly, but the space requirement is little more than that of the raw data. In order to provide theoretical and practical solutions to modern systems that process large data sets such as web search engines, geographic information systems and bioinformatics applications, this program will extend the research on succinct data structures, and start new research directions on this subject. The proposed research will use succinct data structures to develop new solutions to fundamental problems in algorithms and computational geometry, such as text search and range search. It will start a new research direction that uses cache-oblivious model to improve the I/O efficiency of succinct data structures for applications that deal with large data sets stored in external memory. It will also start a new line of research by designing succinct data structures for bioinformatics applications and text databases by addressing useful types of searches performed in these systems, such as approximate search. In addition, algorithm engineering will be performed to study the efficiency of our solutions in practice, and code will be contributed to software libraries that deal with succinct data structures, to make them more complete and hence more useful for software development.""508915,""He, QiMing"
"507895"	"Hepting, Daryl"	"Personalization of interaction based on discovered constructs"	"Eyewitness identification is often highly regarded by the legal system, yet it is not always accurate. An  identification may be compromised if the words used to describe the target face impair the witness' memory of that face (verbal overshadowing) or if the witness sees so many photos that he or she may identify a photo as the target face when it is actually similar to one of the early photos viewed (inaccurate source attribution). Faces of a race different than the witness may cause additional difficulties (cross-race effect).In order to improve the accuracy of eyewitness identification, these sources of inaccuracies must be mitigated. Verbal overshadowing may be addressed by using only non-verbal means to ask for recognition, rather than recall, of the target face. Inaccurate source attribution may be ameliorated by reducing the number of candidate photos presented to a witness. In order to meaningfully achieve this reduction, it is necessary to understand how a particular witness judges similarity between photos so that a computer-based presentation can be personalized accordingly. My research to date indicates that people employ one of at least two different strategies when judging facial similarities. I seek to democratize accurate identification of faces by: understanding the different strategies that people may use to judge facial similarity; developing a concise means of testing which strategy a witness is likely to apply; developing a computer interface that makes use of a witness' identified strategy to allow accurate identification of the target face, while viewing as few candidate photos as possible; and developing an extensible system to manage and store candidate photos for use with the interface.   My research to date has used a set of 356 photos (178 Caucasian and 178 First Nations to allow for examination of cross-race effect), which participants have sorted based on how they each judge similarity.  This work will contribute to eyewitness identification by providing better tools for witnesses and by providing a basis for understanding which faces may be more or less easy to identify.  The same techniques can be used to study interaction with fractal images, which has applications for democratization of multimedia composition.""515002,""Hepworth, Shelley"
"509397"	"Ho, Paul"	"Physical Layer Design Issues in Cooperative and Relay Communications"	"Broadband wireless technologies such as smart phones, tablets, 3G/4G networks are perpetually changing the landscape of the communications and entertainment industries and greatly enrich our life. According to figures compiled by the Canadian Radio-television and Telecommunications Commission (CRTC), wireless revenues in 2009 leapt to $16.9 billion from $16 billion the year before, or by 5.3 per cent. The revenue of the overall telecommunication industry, on the other hand, only grew by a modest 1.8 per cent over the same period. It is evident that the wireless sector presents the greatest opportunities for growth. Traditionally, Canada has a vibrant wireless industry and in fact some Canadian wireless companies are major players at the international level. To maintain and increase the competitiveness of our wireless industry, Canada needs the cutting edge research in the fast evolving field of networked anywhere and anytime high data rate wireless services. This project is in the general area of Cooperative Communications - a wireless communication technology that promises better network coverage, higher data rate, and better signal quality. The concept can be used in future generation cellular networks, as well as in inter-vehicular communication networks, and wireless sensor networks. The focus of the project is on the physical layer aspect of cooperative communications. In particular, we will investigate the signal design and processing issues that enable a cooperative communication network to transmit data accurately and efficiently under adverse conditions such as non-linear distortion, rapid signal fading, multi-path, and interference. Our ultimate goals are (1) to create technical knowledge that is of relevance to the Canadian wireless industry, and (2) to train highly qualified personnel ready for employment and create growth in the industry.""509642,""Ho, PinHan"
"509476"	"Hossain, MdJahangir"	"Advanced Technologies for Energy-Efficient Cooperative Wireless Communication Systems"	"The wireless industry has experienced a tremendous growth in recent years and this is expected to continue in the years to come. As such the overall energy consumption of wireless communication systems cannot be neglected any longer.   Additionally, mobile devices such as smart phones and iPads support  applications like Youtube and video conferencing over wireless networks that require high transmission rate in the order of Megabits per second.  To date, wireless systems have been mostly optimized in terms of bandwidth efficiency and reliability of communication.  However, there is an increasing need to improve energy efficiency of modern communication systems. One main reason is to reduce the net carbon footprint of communication networks. The other reason is to prolong the battery life for mobile devices. Whilst the manufacturers' and operators' objective is to reduce overall power consumption, the supported services are demanding higher and faster data rates, lower latency and increased service coverage. These competing needs pose a significant challenge in designing and deploying modern wireless communication systems.  The objective of the proposed research is to design innovative technologies that can improve energy efficiency for future generation wireless communication systems that rely on so-called ""cooperation"" mechanism among the communicating users/nodes in the network.  In particular, using a holistic design approach, we plan to develop novel wireless transmitter/receiver pair and radio resource allocation schemes for cooperative communication systems that can save transmission signal energy significantly.  The outcome of this research program is expected to advance knowledge and deliver technologies to communication industries in order to implement energy-efficient wireless networks. This will not only prolong the battery life of mobile devices but will also save the Canadian environment by reducing energy consumption as well as electromagnetic radiation.""503358,""Hossain, Shahadat"
"509436"	"Huang, Weimin"	"Ocean Remote Sensing Using High-frequency and Microwave Radars"	"High frequency surface wave radar (HFSWR) and microwave nautical radar (MNR) are widely employed in ocean remote sensing. HFSWR provides sea surface current, wind, wave and target information ""over the horizon"" due to the interaction of HF radio and ocean gravity waves. Traditionally, HFSWR systems have been land-based; compact ship-borne HFSWR has also been used but is subject to distortion of the Doppler spectra due to platform motion. Most HFSWR systems in use rely on a monostatic configuration in which the transmitting and receiving components are at the same location. In order to reduce the cost and potentially improve coverage, research on operating HF radar bistatically - i.e. transmission and reception occur at different sites - is necessary. Sea surface information can also be retrieved from the MNR images that contain the spatial and temporal structure of the ocean surface. MNR provides sea surface information with better range resolution but has limited coverage. In order to provide real-time information for applications in areas such as search and rescue, oil spill control and marine navigation, MNR needs to be further investigated.     A long-term goal of this research program is to integrate HFSWR and MNR into a comprehensive coastal ocean observation network in Eastern Canada. To help achieve this, we will address issues associated with HFSWR antenna motion and the extraction of sea surface information from MNR data. The research will seek to (1) gain an improved understanding of HF radio wave scattering from the ocean surface and to further develop theoretical models and application algorithms for HFSWR ocean remote sensing; and (2) explore the methods of sea surface current fields, wave information extraction and target surveillance from MNR images for possible data fusion with HFSWR results. Our research will add knowledge to key theoretical and applied areas in ocean remote sensing. It will lead to an improvement in marine safety and provide ocean surface data products that do not presently exist in Canada. In doing so, we will meet the ongoing needs of the Department of National Defence, Fisheries and Oceans Canada, the Canadian Coast Guard and other marine stakeholders.""513552,""Huang, WeiPing"
"507991"	"Inkpen, Diana"	"Natural Language Generation and Analysis"	"Recently, there was a lot of progress in the research area of Natural Language Processing, leading to many commercial applications. Tools such as grammar correctors, machine translators, and information extraction systems can now assist people and help saving time and costs. They are taking us one step closer to the goals of Artificial Intelligence. Tools such as automatic speech recognizers and intelligent tutoring systems also lead to progress in Human-Computer Interaction. The sub-area of Natural Language Generation has seen a slower progress, but automatic text generation started to be used in applications, including generating instructions, health care reports, text messages for interactive simulations (for training purposes or for entertainment), etc.           The long term objective of the proposed research program is to facilitate further advances in these directions and to fill in the gap in the area of Natural Language Generation. The proposed short-term objectives are to develop new methods for automatic text generation and for text analysis and classification, by using the results of the analysis stage to guide the generation stage. The novelty of the proposed approach consists in the ways the results and the insights from the text analysis stage will be used in order to develop new methods for text generation.          I will focus on several applications for English and French, for which I did some preliminary work in current and past projects: classification of texts by the emotion expressed (such as happiness, anger, fear, surprise, disgust, and sadness) and generation of texts that express a desired emotion; classification of texts by style and generation of texts with a particular style (such as formal or informal); classification of medical texts and generation of texts for medical reports; intelligent text retrieval and database access using keywords or natural language sentences, and generation of natural language descriptions for existing database reports. I anticipate that the new methods and the proposed applications will advance the state-of-the-art in research, while also allowing for technology transfer into industrial applications.""500276,""Inman, Derek"
"509338"	"Iqbal, MohammadTariq"	"Control of Small Renewable Energy Conversion Systems"	"Renewable energy conversion systems are a growing energy technology in the world. Canada has vast untapped renewable energy resources that can be exploited to meet the increasing energy demand without additional CO2 emissions. Canada is lagging on research and deployment of renewable energy systems. The objectives of the proposed research are to design control systems and components of small renewable energy conversion systems and in this process produce a number of highly qualified personnel. The proposed research program will contribute the following: (1) a new controller design and an optimum control strategy for small doubly fed induction generator based wind turbines; (2) a new optimal sizing and control method of adding pumped hydro energy storage to hybrid power systems for remote isolated sites; (3) a novel design of a magnetic harmonic gear system with an integrated permanent magnet generator for small hydrokinetic energy conversion system; (4) a new d-q load flow based control method for  micro-grid system with energy storage; and, (5) a novel control scheme and design of a cost effective supervisory controller and data logger system for small remote hybrid power systems. The proposed research program will lead to new knowledge and innovative designs of small renewable energy systems. It is expected that the developed designs will be used in Canada. Hence this research will help enhance the application of renewable energy in Canada. This research will also help train five graduate students on renewable energy conversion systems.""518961,""Iqbal, NaqshE"
"509664"	"Jayaram, Sheshakamal"	"Insulation Condition Assessment of Power System Components in the Future Smart Electricity Grid"	"Renewable energy distributed generation has been identified as the key feature of the future smart grid which will make the power system network more efficient, reliable and sustainable.  To this end, power electronic devices are being increasingly added to the existing grid. These devices, due to their mode of operation, inject substantial harmonics into the grid giving rise to distortions in the voltage and current waveforms, which, in turn, leads to increased voltage stresses in transformers, machines, cables and cable accessories, and causes faster degradation of the electrical insulation, shortening the life of the equipment.    )""A rush to install wind turbines has outstripped the usual developmental learning curve, one in which new technologies mature by trial and error, and define equipment that is well suited for the job at hand. In this 21st century land rush to cash-in on wind energy, developers are often trading low initial costs for higher total costs of ownership which eventually is shouldered by wind farm owners and operators. Nowhere is this more evident than with wind-turbine generator (WTG) step-up transformers"". This statement, appeared in Wind Power News - June 9, 2010, is, in fact, substantiated by the alarming rate of failures reported of transformers and cable terminations connected to wind and solar farms, which is the cornerstone of the proposed research. The research will begin by examining the harmonic based stresses in power system equipment, which will lead to mitigation techniques and/or improvements to insulation design or materials, as well as to the development of diagnostic methods for the early detection of potential problems.    )The knowledge and technology outcomes of this research are of significant importance to the development of the Canadian smart grid, and the applied research will enhance the teaching and training of engineers that will contribute to Canadian manufacturers of power system equipment and Canadian consulting industries as technology leaders.""509800,""Jayaraman, Raghavan"
"508218"	"Jepson, Allan"	"Probabilistic Models for Image and Video Parsing"	"A long-standing problem in computer vision is to be able to parse an image or a video into chunks which plausibly correspond to individual objects in the scene or, at least, parts of objects.  For example, in 1983 two  computer vision researchers, A. Witkin and J. Tenenbaum, explained how humans seem to impose an organization on visual scenes (images or video), thereby betting on which image parts go with other parts, even when they have no idea what they are looking at.  The surprising thing is that this pre-imposed, somewhat blind, organization of the data often turns out to be useful for interpreting the contents of the scene.  In order to build  increasingly flexible and adaptable computer vision systems we must design them to first impose a similar organization on the visual data, before they begin to understand what they are looking at. Previous relevant research efforts include the development of algorithms for image segmentation, boundary probability, contour grouping, and, in the case of video data, motion segmentation.  The objective of the proposed research will be to significantly increase the descriptive power of the extracted image chunks, and to study algorithms for their assembly.   A test is to determine whether the extracted image chunks form a useful basis for class-specific object recognition.   This can be measured on standard benchmarks which measure artificial vision systems ability to locate and label cows, cars, sheep, bicycles, and so on, in typical snapshots.  ""509494,""Jeremic, Aleksandar"
"509355"	"Joseph, Dileepan"	"Electronic Imaging: Digital Pixel Sensors and Systems"	"Microsystems exploit micro-level technology to deliver system-level performance. Digital cameras are a good example of this concept, as tiny circuits in each pixel of an image sensor, and their support systems, are responsible for photographic capabilities. Digital cameras also provide a motivation for this research program. Because image sensors add billions of dollars in value to consumer, defence, industrial, and medical applications, research progress in pixel design could lead to a multi-million dollar impact.     CCD and CMOS image sensors have enjoyed tremendous successes. Boyle and Smith shared a Nobel Prize for inventing the CCD technology and NASA invented the CMOS active pixel sensor (APS) technology that is now in every mobile phone. However, the dark limit with such image sensors is orders of magnitude worse than with the colour vision of the human retina, which is why digital cameras may need a flash when human eyes see clearly without one. If not impossible, it is very difficult to realize low dark limit with CCD or CMOS APS technology without sacrificing dynamic range, image quality, and/or frame rate. Consequently, a long-term objective of this research program is to break these barriers with new technology.     Conventional CCD and CMOS image sensors use analog pixels. Because digital technology is robust to the noise and distortion that is inherent to analog technology, digital pixels are game changers. The main advantage of analog pixels is they are smaller but wider trends suggest this issue will become less significant. Vertically integrated CMOS technology, which is explored through an NSERC Partnership Grant, enables 3D stacking of two or more semiconductor tiers to achieve a more complex integrated circuit in the same area.     By funding this Discovery Grant, NSERC supports the creativity and innovation that is at the heart of this research program on electronic imaging. In the short term, the spatial and bit resolution, power consumption, and colour matching of novel digital pixel sensor (DPS) technology will be improved, while training highly qualified personnel to design, build, and test microsystems of significant complexity, i.e., digital cameras.""498060,""Joseph, Jamie"
"507969"	"Jurgensen, Helmut"	"""Formal Languages, Codes, Automata"""	"The ongoing theme of my research is foundations of computing. I study models of computation and information processing and also their applicability to limited real-live scenarios. My work addresses fundamental issues concerning the representation of information, structure of information, formal languages, codes, grammars and automata.Within the realm of these theories, I study how to identify changes in a sequence of events, how events are synchronized and which kinds of computational devices are needed and natural to afford the necessary communication.Formal languages and automata provide the combinatorial and computational framework; codes formalize the treatment of communication and synchronization. As results I expect fundamental insights into the information, synchronization and communication structure of formal languages and the capabilities of special kinds of codes when applied to synchronization and fault-tolerance issues.""519795,""Jurisica, Igor"
"507976"	"Jutla, Dawn"	"Accessibility in identity and privacy protection mechanisms for Web applications"	"This research program's objectives are to develop methods and technologies that address scientific problems in providing accessibility in identity and data protection mechanisms for Web-based applications, and to contribute new international technical standards in the space. Briefly, accessibility, in the context of the Web, means that people with disabilities can perceive, understand, authenticate, navigate, interact, state preferences, give consent, and successfully transact with online applications. Today, most existing identity and privacy mechanisms fare poorly on accessibility; picture the difficulty that a user with a visual disability has in deciphering the common Web CAPTCHA where they are asked to re-type the distorted and misaligned characters shown to them in a text box. Moreover, robust identity mechanisms, such as two-factor authentication, are becoming increasingly required in practice. This trend means that the process for a user with disabilities to identify her/himself to a system is becoming more complex, thereby making some systems more inaccessible. Major technical problems in providing accessible identity and data protection mechanisms include challenges (1)  to provide high levels of dynamic and robust customization within security and privacy mechanisms for the user with a disability, (2) to keep user accessibility preferences private and secure when users interact with such mechanisms, and (3) to scale integrated accessibility/privacy and accessibility/identity solutions. Further, organizations tend to adopt cost-effective technologies, and so a further difficult requirement in the complex Web domain is (4) to ensure our technologies are cost-aware. To tackle these problems, my research intends to use the flexible, integrative power of the Web Services and service oriented architecture paradigms, declarative rules-driven approaches, and new statistical optimization approaches to offer viable and needed solutions. Our results can potentially provide the millions of people with disabilities worldwide with new technologies for vastly easier but yet secure and private access to their data and organizations' data, and will enable organizations to provide more inclusive online services and workplaces.""505598,""Kabal, Peter"
"507908"	"Kahl, Wolfram"	"Pushing the Frontier with Dependently Typed Programming in High-Level Structures"	"The goal of this research is to leverage mathematical abstractions to improve software reuse, and to increase software developers' productivity and confidence in the development of software involving complex transformations of deeply nested structures, as they are needed for example in code generation for high-performance computing.    It is well-known that in many application areas, in particular where networks of any kind are involved, concise relation-algebraic specifications are available for many tasks. This research will open up new ways to combine specification and programming against these relation-algebraic interfaces in a way that ensures compositionality of correctness properties. This will be achieved by using the novel paradigm of dependently-typed programming, and employing it in a way that directly leverages its main strength of providing a natural and precise way to express rigorous mathematical definitions. With this, we will able to specify, in a modular way, novel transformation concepts that involve moving components across several levels of abstraction in deeply nested structures representing for example combined control- and data-flow graphs of concurrent multi-core programs. The associated verified implementation building blocks will individually be accessible to human understanding, which provides essential validation to the highly complex implementations that will be composed and derived in a certifiably safe manner to perform symbolic manipulations that would be almost impossible to confidently develop with conventional approaches.    This unified approach to specification and programming of complex optimisation techniques will make them available also to the increasingly many application areas where software is used in safety-critical environments, and therefore must be certified as correct. As a practical application, the nested code graph transformation capabilities produced by this research will target generation of mechanically verified high-performance code to be used in medical imaging.""515599,""Kahn, Amanda"
"507988"	"Karakostas, George"	"Algorithmic and game-theoretic analysis of network design"	"Is it possible for a network operator to lure network users into a certain behavior by lying to them for a very long time, especially when  they are automated? Can we extend the lifetime of a solar-renewable battery used to keep traveling cars connected to the Internet? What is the best way to lay highways so that we can use the roads already built to connect cities as soon as possible on average? These are some of the problems arising during the modelling, construction, operation, and use of networks. They are addressed in this proposal by using tools from algorithmic game theory and by designing efficient approximation algorithms when those problems are proven to be computationally hard.   The use of game-theoretical concepts to model environments with competitive entities has been the field of economists for many years. Algorithmic game theory is particularly suited for studying networks of selfish users and/or operators. The proposed work aspires to a paradigm shift in the use of algorithmic game theory from one-shot games to repeated games that now include a history of previous plays. We propose to do that by exploiting the constraints imposed on the players due to their automated (algorithmic) nature. Hence, the proposed work should reveal new connections between computer science and economics through the big volume of work already done by economists on repeated games and reputation effects.   On a more practical level, the proposal introduces a new set of problems in approximation algorithms that are related to energy conservation and infrastructure construction in networks. Although more limited in scope than the objectives above, they nevertheless present us with the opportunity of immediate impact on the environment (by, for example, the clever scheduling of energy dispensation used for data transmission) or the cost of network infrastructure (by, for example, the time-efficient scheduling of building new roads). Such problems are usually hard to solve exactly, and, therefore, we propose the development of algorithms that produce provably good approximate solutions.""515815,""Karamali, Mustafa"
"508354"	"Kari, Lila"	"Foundational Aspects of Biomolecular Information and Computation"	"In the same way we use the letters of the alphabet to write text, and the bits 0 and 1 to write computer machine code, the four basic DNA units (Adenine, Guanine, Cytosine, Thymine) are used by Nature to write genetic information as DNA strands. The possibility of encoding symbolic information on DNA, and the fact that biochemical processes such as cut-and-paste of DNA strands have been proved to be able to perform arithmetic and logic operations, led to the development of the field of DNA computing. The primary goal of my research is to analyze this new way of storing and manipulating data on DNA strands, as bioinformation and biocomputation are  fundamentally different from their electronic counterparts in several aspects. Most importantly, DNA-encoded information is not associated with a memory location but consists of infinitesimal free-floating DNA strands that interact with each other due to their Watson-Crick complementarity.This proposal  aims to develop  the foundations for a ""theory of bioinformation and biocomputation"", by  defining and  investigating formal-language-theoretic models of DNA-based information and bio-operations, that take into account the intrinsic characteristics of their biological substrates. Besides expanding the traditional notions of information and computation to  account  for computation taking place in nature, the results of this research could provide insights into the computational properties of biomolecular information and consequently have  potential implications for their practical applications in, e.g., multi-input RNA-based logic  circuits for medical diagnosis, or building DNA nanostructures for nanoelectronic and nano-optical devices.  Another aspect of the proposed research  is an exploration of the paradigm of nanocomputation by self-assembly by investigating the power and limitations of  models of computational self-assembly systems with DNA tiles. The results of this  research  could  have potential implications for the design, evaluation, and analysis of DNA architectures for  bionanotechnology.""503684,""Karigiannis, Spiro"
"509524"	"KazemMoussavi, Zahra"	"Novel Acoustic Technologies for Obstructive Sleep Disorders Diagnosis and Treatment Monitoring"	"The proposed research falls at the interface between engineering and medicine. Its focus is to develop new engineering approaches to signal processing and instrumentation in order to address the challenging medical problem of obstructive sleep apnea (OSA).  Specifically, we will develop non-invasive and effective technologies that can readily be applied in a clinical setting in order to better understand the mechanism and diagnosis of the disorder.  OSA is a common and serious respiratory disorder with major comorbidities such as memory loss, increased risk of accidents, and cardiovascular disease. It is characterized by recurrent episodes of partial or complete obstruction of the upper airway and is usually associated with drop of blood oxygen saturation and frequency arousals (waking up). OSA is highly prevalent in the Canadian adult population and also in children and is also a major but hidden cause of cardiovascular diseases.  Importantly, OSA is treatable but is massively underdiagnosed. To find and treat those who are impacted by OSA, a simple technology for risk assessment is urgently needed. The long term objective of this proposal is to fill that gap.  We propose a novel, non-invasive and cost-effective for use in clinical settings.  Based upon acoustic technology, we will monitor significant airway narrowing, diagnose OSA and its severity, and identify the site of obstruction and the response to treatment.  These goals will be achieved through non-invasive tracheal respiratory sound analysis during the experimental procedures in collaboration with the medical colleagues. Outcomes of this research are: to advance our understanding of the mechanism of upper airway obstruction and its associated disorders; to develop non-invasive, cost effective and quick diagnostic tools;  and therefore improve the quality of life of a large population affected by OSA.""512360,""KazemMoussavi, Zahra"
"509345"	"Kent, Kenneth"	"Computer Aided Design Tools for FPGA Exploration"	"Over the past 50 years, advances in Integrated Circuit fabrication technology have enabled society to progress faster than ever before. As integration continues its exponential progress, however, the complexity of the underlying fabrication technology, the expertise required to exploit it, and the financial risks associated with each deployment have also grown at an exponential rate. Field-Programmable Gate Arrays (FPGAs) have become the implementation medium of choice for many of these crucial applications of digital circuits. FPGAs can be configured to implement any circuit, allowing designers to immediately test designs without the cost, risk and delay of producing a fully-customized chip. They provide companies with large-scale integration without requiring access to a state-of-the-art chip fabrication plant. This is crucial; the cost of building an integrated circuit is rising dramatically, and is far out of reach for most small and medium sized Canadian electronics companies. Without FPGAs, these companies will be unable to economically produce equipment that can compete with their counterparts internationally.In order for FPGAs to find greater acceptance in the market place, new heterogeneous FPGA architectures need to be explored for greater suitability to various application domains. To effectively utilize heterogeneous architectures, the Computer-Aided Design (CAD) flow must be aware of the target FPGA architecture and perform high-level compilation, synthesis, placement and routing accordingly. I have completed preliminary research in this area through the front end (verilog compilation) of the Verilog-to-Routing (VTR) academic CAD flow. Based on this, in the current research I will investigate the CAD flow for new heterogeneous energy-efficient FPGA architectures and circuits, and methods for mapping applications to FPGAs with performance and energy as primary concerns.""517959,""Kenwell, Amy"
"507890"	"Kharma, Nawwaf"	"Evolutionary Computing Application to Biomedical Image Segmentation"	"Our objective is the development of a prototype of an evolutionary computing platform, which accepts asinput a random sample of images of a certain type, along with their corresponding ground truths. It wouldoutput a computer program that is capable of segmenting unseen images of the same type, with a high degreeof segmentation accuracy and computational efficiency. The platform itself could be described as a geneticprogramming method that evolves a population of feed-forward networks of nodes, corresponding to candidateimage segmentation programs. The particular form of representation is inspired by Cartesian GeneticProgramming (CGP). Each node is defined by its identification number (ID), image transformation,transformation parameters and the IDs of the nodes that provide its input(s). The transformation comes from anopen library of transformations, which include well-known image pre-processing, segmentation andpost-processing methods. The fitness of an individual will be measured from both a pixel-based and aregion-based perspective. The overall evolutionary cycle searches the space of all (small-sized) feed-forwardnetworks, of different structures and parameterizations, for those networks with highest relative fitness values.The ultimate value of our work lies in its attempt to emulate and exceed the manner in which highly-paidprofessionals develop distributed programs for image segmentation. Hence, we would consider our effort asuccess only whence a fully-functional and demonstrable prototype is produced.""519459,""Kharotia, Shikhil"
"507967"	"Kim, Kibum"	"Supporting Peripheral Participation on the Non-Planar Displays"	"Ubiquitous and pervasive computing means that people are sometimes working together in the presence of communication artifacts. Often they have shared visual context. That is, they are looking at the same devices and displays at the same time.  This allows them to engage in the processes that create and maintain common ground. However, sometimes they have partial, intermittent, or unpredictable access to shared information.  A substantial body of research in the past has focused on the role of shared visual context in focused interaction, especially as a pair and usually at a distance. But pervasive and ubiquitous computing means that computation is occurring in many kinds of public settings. These settings raise the importance of peripheral participation. In fact, different kinds of peripheral participation present related but different kinds of challenges. For example, latecomers may negotiate entry into the conversation, while side-assistants must monitor for enough information to know when to intervene. In this project, I will create a new user interaction with non-planar displays to support peripheral participants, supporting different properties of sharing. Three experiments will be conducted to test the effects on understanding and simple learning outcomes of technological support to different kinds of peripheral participants such as early side-participant and late overhearers. In general, the purpose of this work is to facilitate the acquisition of sufficient understanding for effective action by peripheral participants.""501972,""Kim, Michael"
"507986"	"Kiringa, Iluju"	"Processing Queries over Conceptual Integration Schemas"	"Business intelligence (BI) is a software industry term that denotes the use of information within organizations to allow deciders, usually executives, to make informed decisions, and to effectively run operations using known data. As a Computer Science research field, it encompasses several subfields of Computer Science, among which data integration (aka data warehousing) plays a pivotal role. Ideally, integration would happen almost automatically. One important aim of BI is to be end-user oriented: decision makers and business users need only to focus on modelling their data needs in (usually high-level ) business terms that they fully understand. A business user would specify what information is needed and in what form, and the integration system would satisfy the request by producing integrated data as automatically as possible. To achieve this, we must raise the level of abstraction significantly to accommodate business users. The latter would use conceptual-level schemas to model their integration needs. Once those schemas are in place, business users can pose queries expressed in a conceptual query language against those conceptual schemas. Building on previous work on conceptual integration schema modelling, the objectives of the present proposal are as follows: (1) address the problem of answering business queries over conceptual integration schemas; (2) identify and study tractable classes of queries that are of practical importance; (3) develop and implement efficient algorithms for some of those tractable classes.""503466,""Kirischian, Lev"
"509675"	"Kirubarajan, Thia"	"Computationally Efficient Adaptive Spline Filters for Nonlinear State Estimation"	"The problem of nonlinear/non-Gaussian filtering has generated significant interest in the literature due to the inherent nonlinearity in most practical systems. The nonlinearity in state estimation problems may arise due to its presence in the state-to-measurement equation or in the evolution of the state itself. The presence of multiple objects further complicates the problem by adding data association to the mix. The optimal nonlinear state estimator consists of the computation of the conditional (posterior) pdf of the multitarget state given all the measurements available up to the current time. Optimal multitarget nonlinear filtering is in general a non-tractable problem, not just because of computational complexity but also due to the multimodal nature of multitarget pdf.Under these circumstances, one needs an algorithm that is capable of automatically adapting itself by recognizing the spatio-temporal nonlinearity variations (over one target or across multiple ones). Our multitarget state propagation will be based on multidimensional spline representation. Splines have been used effectively to represent complex (and arbitrary) curves and surfaces in computer science, graphics, aerospace, automobile industry, statistics and mathematics using a finite set of knots. Our innovative approach is to use splines to represent any arbitrary multitarget pdf and then derive the equations for propagating the splines over time based on the standard prediction and update steps. Splines posses a number of desirable properties: they are continuous, can handle multiple models, inherently capable of measuring nonlinearity, do not suffer from degeneracy or need resampling, can incorporate road map-like constraints, are sensor-agnostic and can be adaptive by varying knots spatially and temporally. Significant theoretical extensions to the more realistic state estimation problems with multiple targets, false alarms, missed detections and constraints are proposed in this work.""512376,""Kirubarajan, Thia"
"515743"	"Kishk, Ahmed"	"Novel Low Loss Guiding Structure for High Frequency Applications"	"There is an urgent need for new transmission lines and measurements techniques in order to explore and exploit the frequency range from 30 GHz up to Terahertz (THz). At such frequencies, conventional transmission lines become either too lossy or too expensive to manufacture. The aim of the proposed research program is to investigate a new transmission line referred to as gap waveguide that will resolve the losses issue of transmission of millimeter wave frequencies to potentially THz. Efficient numerical methods tailored for these specific applications will be developed and used in the analysis and design of microwave circuits and radiators based on this technology.  The basic geometry of the new gap waveguide comprises two parallel conducting metal plates, which are separated by a small gap which is typically smaller than 0.25 guided wavelengths.  This gap can be filled with air or fully or partly filled with a dielectric.  Local waves appear in the gap between the two plates. These waves are controllable by surrounding the two metal plates by textures from both sides that forces the waves to decay rabidly away from the two metal plates. The proposed technology will adapt the concept of an artificial magnetic conductor and develop techniques to enhance its design process and its operating bandwidth. So far a 2:1 bandwidth can be achieved with simple structures and we aim to further improve and enhance this bandwidth. Another application for the concept of an artificial magnetic conductor is in packaging, where it would eliminate the need for lossy resistive sheets in the packaging cavity as it is currently practiced.  The proposed research program will have tremendous impact on the use of higher frequencies in radio astronomy, satellite communications, and imaging for medical as well as for security applications in terms of higher efficiency as compared with the currently achieved with conventional guiding structures.""512880,""Kishk, Ahmed"
"514154"	"Klymyshyn, David"	"Microfabrication of compact wireless devices in very thick materials"	"Wireless devices represent multi-billion dollar industries in Canada, and drive our lives and economy like never before with a desire for tiny communication and sensor devices and networks that are changing the way we interact with each other and gather data about the environment around us. The relentless pursuit of device miniaturization for such systems often comes at the price of compromised performance, at a time when the requirement is for increased functionality. One of the biggest obstacles to further miniaturization of wireless devices is the passive circuit elements responsible for signal conditioning, and also the radiating antenna element. These passive structures account for a large portion of the size. This project is an international collaboration, proposing a radically different approach to fabrication of compact wireless devices using non-traditional ultra-thick polymer-based materials, enabling improved performance and increased functionality for various emerging wireless communication devices, automotive radar systems, small satellites, RF identification, and sensors/biosensors. Specialized techniques for microfabrication of high performance wireless structures using synchrotron-based deep X-ray lithography will be used to fabricate very tall structures (up to millimetres), in thick metal, polymer, and polymer-dielectric composite layers, with high-aspect-ratios and exceptional structural features down to the micrometer range.  Lithography will be used to directly fabricate dielectric resonator antenna elements and combine these with tall embedded metal structures and circuits for the first time, to realize vertical integrated wireless devices with new capabilities and enhanced functional properties.""503100,""Klyuchnikova, Tatyana"
"507984"	"Kondrak, Grzegorz"	"Natural Language Processing at the Sub-Word Level"	"Processing language involves processing words. However, words are not indivisible abstract atoms -they are made of smaller units, such as morphemes, syllables, letters, and phonemes. In computationallinguistics, words are analyzed on distinct levels: phonetic, phonological, orthographic,morphological, etc. These levels of representation underlie such important tasks as morphologicalparsing, speech synthesis and recognition, spell-checking, and stemming. Since different levels arestrongly inter-related, the understanding of the interactions between them is crucial to advancing thestate of the art in word-oriented applications. My objective is to investigate such interactions, anddevelop algorithms for alignment and conversion between levels. In particular, I will focus on thetasks of grapheme-to-phoneme conversion and transliteration.My general approach to the problem will be to incorporate linguistic knowledge into advancedmachine-learning techniques, which, given sufficient training data, substantially outperformrule-based approaches. However, the latter often achieve impressive accuracy without any need fortraining data. They also tend to perform well across different domains. I will explore the ways ofcombining the two paradigms, by exploiting linguistic understanding for guiding and informing machinelearning approaches, as well as for pre- and post-processing their training data.The long term goals of this research programme are acquiring deeper understanding of the dependenciesbetween various representations, and leveraging the novel insights in order to advance the state ofthe art in natural language processing. I am confident that the implementation of the planned projectswill result in successful applications in other areas of natural language processing.""511791,""Könemann, Jochen"
"509361"	"Kordi, Behzad"	"Transmission Line Remote Condition Monitoring for Smart Grid Applications"	"Aging of the transmission and distribution systems of the electric power grid and the need to maximize the utilization of the system have resulted in operating the power grid closer to the edge of reliability. In addition to being overstressed, the existing electric power grid suffers from a lack of sufficient and effective condition monitoring and diagnostics of its electrical insulation system whose failure is the major cause of power outages and disturbances. Smart diagnostics and condition assessment of the electric insulation system are becoming an integral part of the development of modern electric power grids. Condition monitoring and diagnostics of the power distribution and transmission systems include measurement, collection, transmission and interpretation of data related to their operation and maintenance conditions. The electric insulation system can be affected by external factors such as rain, ice, and pollution, or internal factors such as degradation or damages in the insulating materials.In this research program, we will develop sensing equipment and techniques for online, distributed, low-cost, remote condition monitoring and diagnostics of the electrical insulation system and operational condition of the electric power grid. We will develop low-cost sensors that can remotely collect information regarding the insulation condition of the electric power apparatus without the need of having a close contact based on the radio frequency (RF) radiations. Techniques will be developed to interpret the collected data and extract information that represents the condition of the insulating system. This will enable a safe, reliable, and remote condition monitoring and diagnostics of the electrical insulation system. The collected information can be employed to enhance the quality and reliability of the electric power delivered. The outcomes of this research are also essential for the shift to a cost-effective, condition-based maintenance of the power system. Further, they will allow electric power asset managers to better manage the equipment and increase their utilization.""501826,""Kordzadeh, Atefeh"
"509321"	"Korenberg, Michael"	"""Nonlinear Systems Identification for Modelling and Analysis of Biological, Physical, and Industrial Processes"""	"Objectives of the proposed research are:(1) to develop techniques for nonlinear system identification and time-series analysis, creating methods capable of extracting useful information, e.g., significant frequency content, from short noisy experimental records.(2) to apply such methods to important problems in biology, and to physical and industrial systems.Scientific Approach:Regarding system identification, the proposed work involves representation of nonlinear systems byfunctional expansions and kernel estimation, parallel cascade identification (PCI) employing dynamic linear and static nonlinear elements, and use of other nonlinear models such as difference equations. Other means of representing nonlinear systems will continue to be actively explored.Regarding time-series analysis, proposed work includes estimation of sinusoidal, exponential, andexponentially-decaying sinusoidal series models from brief noisy time-series that may be unequally spaced.Regarding application of these methods in biology, the applicant intends to concentrate much activity on the interpretation of gene expression profiles and of tissue microarrays, such as for diagnosis and for prediction of treatment response. Physical and industrial applications will include reducing errors inherent in using (micro-electro-mechanical-system) MEMS-based inertial navigation systems (INS) and their fusion with a global positioning system (GPS).""513389,""Kornblatt, Jack"
"514197"	"Kouki, Ammar"	"""Integrated LTCC/MEMS Circuits for Small-Size, Light-Weight and Power-Efficient Intelligent RF Front-Ends"""	"In recent years, École de technolgie supérieure (ÉTS), with support from Economic Development Canada, invested significant amounts of money to build and equip two co-located clean room facilities that house a Low Temperature Cofired Ceramics (LTCC) fabrication process, in one, and a Micro-Electro-Mechanical Systems (MEMS) fabrication process, in the other. The LTCC facility, a first of its kind in Canada, includes a world class industrial grade fabrication chain while the MEMS facility houses state of the art microsystems fabrication equipment. LTCC allows the fabrication of circuits in a 3-dimensional fashion such that circuits can be made smaller and lighter. MEMS on the other hand can make circuits reconfigurable such that one circuit can serve multiple frequencies and multiple standards thus reducing the number or components needed in wireless terminals such as a smart phone.  The present proposal aims at carrying out research using these facilities to develop new innovative radiofrequency circuits and devices commonly used in wireless and satellite communications.  By combining LTCC and MEMS, we plan to develop new smarter, smaller, lighter weight and more power efficient circuits that can drive down cost while offering more performance. The personnel who will be trained under this program will be called upon to meet this objective by making a number of scientific and technological advancements and by developing a wide range of new skills in the design, fabrication and testing of these new circuits. A number of Canadian companies, federal laboratories and universities have expressed great interest in ÉTS's new facilities. The results of the proposed research program will help advance the existing prototyping processes techniques and develop new ones. This will in turn help meet industrial needs and will make collaborations with them easier and substantially more productive.""507426,""Koustov, Alexandre"
"514709"	"Könemann, Jochen"	"Flexible and Effective Techniques for the Design of Approximation Algorithms"	"Discrete optimization problems are abundant in everyday life, and arise whenever complex decisions about the efficient distribution of scarce resources have to be made. There is a plethora of such problems, including the timely scheduling of trains in a local transit network, and the optimal layout of wires in the VLSI design phase of a modern chip. In these two examples, and in general, relevant practical instances are often NP-hard, and thus intractable. This proposal focuses on the design of Approximation Algorithms that efficiently compute near-optimal solutions to given optimization problems. Emphasis will be placed on the systematic use of mathematical programming (MP) in the design of such algorithms.MP is an important tool in the development of approximation algorithms. Starting from a mathematical model of a problem, one typically first derives a strong and tractable relaxation, solves it, and ultimately rounds its solution into one that is feasible for the original problem. The performance ratio of such an algorithm chiefly depends on two main factors: the quality of the MP relaxation, and the rounding method. While several powerful and general purpose methods for the above rounding process have been developed in the last 30 years, the following meta question (paraphrased from Vazirani's book) remains: Given a strong MP relaxation, is there always a way of rounding it into a good solution for the underlying optimization problem?  Shedding light on this question is the long-term goal of my research. More explicitly, in my research program I intend to systematically study the use of strong MP relaxations in the design of approximation algorithms. I give three examples of ongoing projects together with several example short-term goals.  The proposed research objectives are central in Combinatorial Optimization and Theoretical Computer Science; progress made is anticipated to have significant direct impact in these fields, and in practical applications of MP. The proposed research program is a natural continuation of existing work, and it will continue to attract excellent students. ""514174,""Konermann, Lars"
"509369"	"Laforge, Paul"	"Adaptive Microwave Filters and Multiplexers for Communication Systems"	"The use of multi-band microwave systems is increasing, especially for cellular networks. Many cell phones today can transmit different types of coded signals over multiple frequency bands. Radio frequency filters are essential devices in the communication of these signals. With a proper multi-band filter design, a two terminal device can be made that performs the needed filtering for the multiple frequency bands used by a cell phone or by base stations. This reduces the amount and total size of the filters and eliminates the need for switches and multiple chip sets for each type of transmitted signal.The spectral efficiency of a microwave system can be improved by implementing adaptive filtering architectures. A switched filter bank has traditionally been used to achieve this type of architecture. Tunable multi-band filters allow for bandwidth to be assigned as needed by the microwave system. The design of adaptive duplexers and multiplexers will further offer flexibility into the microwave communication system. The filtering provided by tunable multi-band filters and tunable multiplexers can adapt correspondingly to satisfy the demand in bandwidth. The adaptive devices will be developed, designed, fabricated, and characterized. Their linearity performance will also be characterized while considering their loss and tuning range.  Computer aided design algorithms will be developed to control the tuning of these adaptive filters and multiplexers.The main goals of this research program are to develop, fabricate and characterize multi-band microwave filters, to develop, fabricate and characterize novel adaptive multi-band microwave filters, to develop, design, fabricate and characterize adaptive multiplexers, to characterize the linearity performance as a function of frequency, loss and tuning range for the adaptive filters and multiplexers, and to develop computer aided design  programs for determining the optimum tuning states for the fabricated filters and multiplexers.""498333,""Lafrance, Bruno"
"507865"	"Lam, Clement"	"Construction of Designs and Codes using computers"	"The long term objective of my research is to use computers to solve open problems related to the existence and enumeration of combinatorial objects.  One characteristic of this type of research is that it often requires a large amount of computing, something equivalent to using hundreds of computers over months or years.  Since the mid 1990's, computers found in typical university teaching laboratories provide the most cost-effective computing environment. One can just use these computers for research purposes when no students are using them, such as during the overnight hours. However, this fortunate situation is changing. As the price of computer drops, more and more students are buying their own laptops. Universities are starting to cut back on general computing facilities that students do not need, and moving towards a model of providing fewer computers but with higher-end models that students normally do not have access to, in order to complement their learning experience. The net result, for the researchers, is that we have access to fewer but more powerful computers. These high end computers typically have multi-core CPU's, and have powerful graphics processors. While it is easy to effectively use the multi-core CPU's, using the graphics processors effectively is not as simple.My research work divides naturally into two parts: developing tools, and using tools to solve problems. It is time to do some tool development. My short term objective is to develop algorithms for enumerating combinatorial objects that can be run effectively on massively parallel graphics processors. I will work on Nvidia graphics devices, which uses the SIMT (Single Instruction Multiple Thread) model. I will draw on my earlier experience of programing a CRAY (a vector computer) and a Connection Machine (a SIMD machine). After developing these algorithms, I will continue my long term objective and work on the many open questions related to the existence and enumeration of combinatorial objects.""504629,""Lam, Donald"
"507917"	"Lanson, Nathalie"	"Meshfree hybrid Lagrangian numerical methods for the simulation of cardiac flows."	"This research program will focus on applying and improving meshfree methods to the equations of fluiddynamics and solid mechanics in order to simulate cardiac flows. Meshfree methods, also referred to as particle methods, have shown great potential and robustness in handling difficult simulations, especially in the case of highly distorted and multimaterial systems. This is due to the fact that while the numerical points (or particles) move according to the transport field, they are not linked by a mesh, but rather interact with each other through the use of continuous basis functions centered at each particle. This enables the particles to move around freely, such that no re-meshing is necessary when high distortion occurs, and no front tracking is necessary. Despite the robustness of meshfree methods, there remain some difficulties related to their accuracy and their computational cost. For instance, the sets of interacting particles must be re-computed at each iteration. This is one of the most time costly aspects of a meshfree software. Another difficulty with using a meshfree code is related to the numerical viscosity needed to stabilize the scheme. Even though they belong to the class of Lagrangian methods, and consequently display less diffusivity than an Eulerian mesh based method, meshfree methods still present pathologies related to the amount of numerical viscosity used, especially when dealing with fluid flows. The objective of this programme is to develop new techniques that will alleviate the problems described above and will be adapted to build computational tools for simulating blood flow in the heart that are numerically robust, flexible, consistent and fast. The final goal of this research is to develop a more comprehensive multi-physics model of the human heart that will contribute to the understanding of the human heart, healthy or diseased.""518481,""Lant, Nicholas"
"508123"	"Larochelle, Hugo"	"Learning Algorithms for Deep Architecture Systems (Algorithmes d'Apprentissage pour Systèmes à Architecture Profonde)"	"More than 50 years after the founding of the field of artificial intelligence, we are still baffled by the capacity of humans for abstract thought. When faced with the daunting task of implementing such complex thinking in a machine, one notices the richness and variety of concepts the human mind can manipulate. Humans can look at the 3D array of pixels encoding the video of a scene, and produce from it several different high-level (abstract) representations with varying structures: the set of objects it contains, the network of relationships between these objects, a textual description of the semantics of the scene, etc. How could we recreate, in a computer, this ability that humans have to smoothly transition between these rich structures and preserve the core semantics of the real world they represent?   This research program will attempt to answer this question, by taking inspiration from the deeply layered organization of computations in the human brain. It proposes to investigate algorithms that can adapt the behavior of an artificial neural network, organized in a deep architecture, so that it can take as input an observation with some arbitrary structure and transform it so as to output a desired target with a possibly different structure. Deep learning systems have been developed for problems with data of limited structural complexity and have thus improved the state-of-the-art on such problems. This research program will aim to bring the same improvements to problems with structured data that do not have deep learning solutions at present. Such adaptive, predictive systems will find application in a number of fields, such as computer vision and natural language processing. More fundamentally, the results of this research will shed light on principles for building, from simulated experience, autonomous intelligent systems capable of manipulating high-level abstract concepts, and shrink the gap between the state-of-the-art in artificial intelligence and human intelligence.""505363,""LaRochelle, Sophie"
"508095"	"Lawford, Mark"	"Certification of Safety-Critical Real-Time Systems"	"Public safety is dependent upon the safe operation of computer control systems in cars, airplanes, medical devices and other systems. These systems rely upon the correct operation of their software to provide otherwise unattainable benefits. Therefore we need to ""certify"" that the software will operate correctly and not endanger the public. Unfortunately, construction, formal verification and resulting ""certification"" of safety critical real-time software is very costly, requiring significant time of highly trained personnel, both for the production of the software and for its certification. As a result, mathematical proofs of software correctness are typically only used by industry in the safety critical software when it is mandated by a regulatory body.  More typically, even in safety critical applications, a certification of the software is done by checking that a standards based software process was applied by developers. To make the construction and product-focused certification of real-time software less labour intensive and more cost effective, the proposed research will create tool supported methods and analysis techniques to create product focused evidence for certification as part of the forward software development process. A key feature of the work will be seamlessly integrating the formal methods with the work-flow and software development process used by practicing engineers by creating extensions to commonly used software development tools.  In particular we will target  model based design tools used in control systems such as Matlab/Simulink and MapleSim. This will be accomplished by the use of Domain Specific Languages to capture key features of the models and their properties and then allow their translation to multiple verification environments so that the most appropriate environment can be used to establish a property of a design. By allowing the designers to model in a familiar environment and then reason at a higher level, using the formal verification tools ""under the hood"", we intend to develop a library of control systems design templates that efficiently produce the evidence for certification of control systems properties at a more reasonable cost that results in more reliable software intensive systems.""516308,""LawKamCio, YannMeing"
"507995"	"Lawrence, Ramon"	"Efficient Data Management for Tiny Embedded Devices"	"The objective is to develop a comprehensive data management library for embedded devices that reduces their cost and complexity.  Embedded devices are used in a vast variety of areas and represent a multi-billion dollar annual industry.  These devices serve critical roles in our economy, industrial processes, environment, and personal lives.  Developing applications for embedded devices is challenging due to their limited resources which restricts the type of operations that can be performed and motivates customized solutions that are costly to develop.  A data management library that handles issues of data storage, consistency, transactions, and performance would greatly simplify embedded applications and make their development more economical.     The scientific approach is to develop database algorithms specifically optimized for embedded devices that use flash memory.  Our previous work constructed an optimized flash sorting algorithm, a low-memory flash translation layer and wear leveling scheme, and a complete sensor architecture that has been used for environmental monitoring and reducing water used in irrigation.  The work will develop additional algorithms for joins and data aggregation and investigate novel approaches to query processing and optimization.  A key goal is to integrate the database layer with the flash storage system layer as there is no holistic system that combines them.  This project will research the limits on how efficient data processing can be on devices with minimal resources.       The significance of the research is that constructing the data management library will provide an efficient data abstraction for embedded devices like SQL relational databases provided for computers. This will improve development costs and times and increase reliability.  The research will be driven by specific applications, including irrigation management where water savings can be as high as 50%, agricultural and vineyard monitoring to improve yield and quality, and environmental monitoring and scientific data collection. These existing research partnerships will improve the research while contributing to the application areas.""519564,""Lawrence, Vanessa"
"507896"	"Leduc, Ryan"	"Scalability and Implementation Issues for Discrete-Event Systems"	"The primary objective of this proposal is to address the two main concerns standing in the way of the adoption of DES theory by industry: scalability and implementation issues. In discrete-event systems (DES), a great deal of effort is currently being applied to overcome the state space combinatorial explosion problem, in order to allow supervisory control methods to scale to large systems (> 10^20 states). To deal with the complexity of large scale systems, the software engineering community has long advocated the decomposition of software into modules (components) that interact via well defined interfaces.  The applicant has  developed a similar approach, called hierarchical interface-based hierarchical supervisory control (HISC). This approach develops well defined interfaces between components to provide the structure to allow local checks to guarantee global properties such as controllability or nonblocking.  The proposed research will build upon this framework by extending and generalising it, adapting existing DES methodologies so that they can exploit the HISC architecture and developing tools that utilise the method. In particular, the research will adapt the HISC method to timed DES, DES fault-diagnosis, and decentralized supervisory control. Also, the HISC concept will be extended to other software verification methods, such as state-based specifications verified using theorem provers.   DES also needs a reliable method to transform a DES supervisor into an actual software or hardware implementation. In doing so, concurrency and timing delay issues must be resolved. This proposal will build upon the applicant's Sampled-Data (SD) Supervisory control work. First, a method to automatically generate software code or hardware design from SD supervisors will be developed. Next, SD control will be extended to the HISC platform. Then, a discrete-optimization method will be developed that respects the SD conditions. Finally, this method will be adapted to the HISC platform. The above tasks are key to making DES  ready for industry adoption. ""502688,""LeDue, Emily"
"509424"	"Lee, Daniel"	"Protocols and resource allocation for future cooperative wireless networks"	"Wireless communications and network services are penetrating our society at an explosive rate, while the engineering community is setting ambitious goals for rapidly advancing their capabilities. My research program stems from two challenges in wireless communication systems and networks: first, wireless systems should deal with time-varying channel conditions; and second, the capacity of wireless communication is fundamentally limited by the spectrum bandwidth and transmission power.  Therefore, efficient use of wireless resources is extremely critical. To that end, the main objective of my research program is to discover practical and near-optimal protocols and algorithms that can make timely decisions in response to changing environments in order to dynamically allocate resources of wireless networks.  In addition, this research program will investigate fundamental and practical issues arising from dynamic resource allocation mechanisms.      New paradigms of wireless networks will need new algorithms for dynamic resource allocation. The application areas of this research program will include emerging technologies, such as cognitive radio, cooperative communications, smart grid, and future technologies for extremely energy-efficient (green) communications.  ""502732,""Lee, Daniel"
"508113"	"Leung, CarsonKaiSang"	"Mining Interesting Useful Patterns"	"Frequent pattern mining is an important data mining task that finds sets of frequently co-occurring items. Many existing algorithms find frequent patterns from precise data (e.g., supermarket transactions), in which the contents of datasets are precisely known. However, there are real-life situations in which data are imprecise or uncertain (e.g., sensor data, medical test results) due to factors like inherited measurement inaccuracies or sampling frequency. Despite their uncertainty, these data contain a rich set of useful knowledge. Over the past few years, I have developed algorithms that use probabilistic approaches to find frequent patterns from uncertain data, in which items in each transaction are assumed to be independent. However, this assumption may not hold in many real-life situations. Hence, I propose a research program with an objective to build an exploratory, efficient, user-friendly, and powerful mining framework--which consists of systems that mine useful patterns that are interesting to users from data streams and/or uncertain data. Specifically, I plan to (i) explore non-probabilistic approaches in finding frequent patterns, (ii) relax the above assumption so as to handle more realistic situations where items in each uncertain transaction may be related, (iii) incorporate user preferences in the mining process so as to allow users to find other useful (frequent or infrequent) patterns that are interesting to users, (iv) further improve performance so as to provide users with real-time responses, (v) develop visual analytics tools so as to enable users to visualize and analyze static (or dynamic) datasets of precise (or uncertain) data. Consequently, for this proposed research program, I and my HQP would develop new data mining technology for mining interesting useful patterns. This, in turn, advances knowledge of researchers in the field. Moreover, I also plan to apply the proposed system to various real-life applications (e.g., mining Web data, telecommunication data, agro-meteorological data, and tweets from social networks) so as to demonstrate the effectiveness of the proposed systems in addressing scientific/business needs of the application users when mining interesting useful patterns.""499467,""Leung, Cecilia"
"509395"	"Leung, Cyril"	"Future Wireless Communication Networks"	"Wireless communication networks have revolutionized our modes of communications, information retrieval and distribution, entertainment and social interaction. To-day, the number of cellular telephone subscribers worldwide exceed five billion and industry revenues are in the one trillion dollars range. Rapid traffic growth in the coming years will be driven by new applications such as mobile web-browsing, video streaming, multi-player online e-games and social media. The design and operation of a network to support such a wide range of applications in a resource-efficient manner is a daunting challenge. There are also many other alternative/complementary wireless products and services. These include local area networks, personal area networks, metropolitan area networks and regional area networks. This wide choice caters to the individual needs of different applications and deployment scenarios. Wireless networks are envisioned to play important roles in many new exciting applications such as personalized health delivery, telemedicine and real-time location- and context-aware services.Future networks will need to support much higher user data rates and provide a high-quality user experience at an affordable price. This research program will investigate innovative approaches, models and techniques to make feasible this necessity. It will specifically focus on three areas which are widely regarded as key: resource scheduling, interference mitigation and wireless relaying. Fast resource scheduling algorithms will be designed to allocate network resources to users so as to achieve desired objectives such as meeting specified throughput, delay, packet drop rate or fairness targets. Interference mitigation techniques will be developed to reduce the deleterious effects of intercell interference, a major problem in broadband networks which can severely affect users near the cell edges. The use of relays as a cost-effective means for improving coverage, system capacity and handset battery lifetime will also be studied.""501433,""Leung, Danny"
"514731"	"LeytonBrown, Kevin"	"Computational Game-Theoretic Analysis: Methods and Applications"	"Game-theoretic systems are environments in which two or more agents interact, and agents' desires cannot be relied upon to coincide. Examples of such settings include the housing market, a game of poker, a morning commute, and the United Nations. A particularly important application area is markets, from familiar consumer platforms like eBay and eTrade to business-oriented exchanges addressing needs like supply chain management, procurement, or carbon trading. Such markets are lately both moving online and becoming more complex, and in the process encountering computational obstacles. Lately, computers have started to form game-theoretic systems, too: large-scale distributed systems are becoming ubiquitous, and the users of such systems cannot simply be expected to behave in the way a designer intends. In response to this changing landscape, game-theoretic systems have moved in the last decade from being mainly the province of theoretical microeconomists to being a major research area in computer science.Game-theoretic models are almost always studied analytically. The proposed work aims to change this. Specifically, it aims (1) to develop theoretically well-grounded new methods for analyzing these models computationally, and (2) to study the application of these methods to important, real-world problems. More specifically, it aims to develop answers to practical questions such as: What rules should a search engine use in advertising auctions to maximize revenue? How should an aid agency allocate scarce inspection resources to ensure that drug shipments are not diverted? How should a bidder bid in sequential auctions like those on eBay? How serious a problem is ""strategic voting"" across different voting rules? Overall, this line of work promises to open up a major new approach in the formal analysis of social systems, and hence to provide powerful new tools to decision makers both in the public sector and in industry.""507072,""Leznoff, Daniel"
"509537"	"Li, Cheng"	"Intelligent AUV-Assisted Heterogeneous Underwater Acoustic Sensor Networks"	"The UnderWater acoustic Sensor Network (UWSN) is an emerging networking platform that promises a wide range of potential applications, ranging from environmental monitoring, industrial instrumentation monitoring and control, to military surveillance and security monitoring. The advancement of UWSN study is further augmented by the recent increasing interest and engineering activity in the design and deployment of AUVs (Autonomous Unmanned Vehicles), which has required that UWSNs incorporate mobility properties. The use of AUV (or groups of AUVs) will facilitate network deployment strategy, maximize network coverage, and enable high-speed networking; however, it also raises new challenges for localization and synchronization, route planning, medium access control, coordinated estimation, and network connectivity due to the sparse deployment, low communication bandwidth, large propagation delay and delay variation, frequency dependent attenuation, strong multi-path and Doppler effects, and long reach of the acoustic signals of the UWSNs.      The objectives of the proposed research are (1) to study the delay/disruption-tolerant network architecture of the UWSNs; (2) to design and evaluate a delay/disruption-tolerant link and network layer protocols; (3) to utilize cooperative communication and networking to improve network functionalities; and (4) to apply a compressed sensing technique for better underwater channel estimation. The impact of AUVs and sensor mobility to algorithm design and network performance will be closely studied for these research objectives. In addition to novel algorithms and analytical models, a hardware platform for prototype design, implementation, and testing will be established through this research. The proposed research will be an ideal platform for highly qualified personnel (HQP) training, where the students will learn to conduct independent and creative research, as well as obtain valuable hands-on experience. This research will help solidify Canada's place in the forefront of underwater communications and networking technologies and provide valuable communication network support for  Canada's future natural resources exploitation, environment monitoring, and national security.""518006,""Li, Christine"
"509334"	"Lina, JeanMarc"	"Multiscale and wavelet based signal processing for electrophysiological and optical brain imaging"	"Medical imaging, and functional neuroimaging in particular, is one of the most dynamic of the scientific and technological fields in biomedical. Bioelectrical measurements acquired on the scalp of the head are vitally important in terms of responding to the challenges of neurological illness, as well as for designing technologies capable of decoding neural activity, control systems for the disabled, for example. Electrophysiology, the study of the electrical properties of biological organisms, has developed considerably since the first measurements of electrical potential were recorded by Richard Caton (1875), and some very useful tools for measuring neural activity have been devised as a result. Over the last thirty years, one of these technologies, magnetoencephalography (measurement of the magnetic field created by neural activity), has been combined with another tool, electroencephalography, to provide the 'bioelectromagmetic signature' of the activity of the brain. A third technology, based on the diffusion and absorption of infrared light by perfused brain tissues, makes it possible to measure the hemodynamic variations caused by neural activity. Such measurements are essential in evaluating a pathology in a clinical setting, like epilepsy, for example, or in the neurosciences and cognitive sciences, in which the brain functions are studied in particular situations, like during sleep or when the brain is engaged in tasks involving perception or memory, for example. This research program focuses on the digital tools that make it possible to process optical and bioelectomagnetic measurements on a computer, in order to visualize the regions of the cortex involved in the complex and dynamic networks that constitute brain activity. We will also study the intracranial electrophysiological measurements recorded in epileptic patients evaluated within the framework of a surgery, by studying the bursts of rapid oscillations that occur, and seem, some of them at least, to be related to the pathology. To characterize these signals from the point of view of their spectral complexity at the head surface is one of the challenges of this research program, which is aimed at developing new methods of analyzing the 'working human brain'.""500486,""Lincoln, Richard"
"508130"	"Lizotte, Daniel"	"Machine learning for non-myopic decision support and knowledge discovery"	"The goal of our proposed research is to develop methods for using data to help humans make good long-term decisions. As a motivating example, consider choosing a medical treatment for a patient with a chronic disorder: in practice, treatment decisions are made based on knowledge about the state of the patient, knowledge about the treatment options currently available, and knowledge about how future decisions will be influenced by the patient's progress over time. Another example is the control of a water reservoir: decisions about how much water to use to generate power or irrigate crops are made based on knowledge about current demand, the state of the reservoir, and about the potential useage decisions that might be made in the future. We say a decision is ""non-myopic"" if it is made based on knowledge of the potential for future decision-making.  In both of these examples, ultimately the final decision rests in human hands; however, there is enormous potential for that decision to be guided by sources of sequential data that are becoming more and more ubiquitous. For example, we are now seeing the development of databases that record how thousands or even millions of patients respond to different sequences of treatments over time, and these have the potential to inform non-myopic medical decision making more effectively than previous studies.  However, rigorous analysis methods for constructing non-myopic decision aids from data are still in their infancy. Analysis methods in reinforcement learning and machine learning have enormous potential, but in many ways are not suited to decision aid construction: current methods do not effectively account for user preferences, they do not provide appropriate measures of confidence in their recommendations, and they do not account for the cost of gathering new data. My research aims to develop new methods without these shortcomings that can be applied to produce useful decision aids from sequential data. In the long term, as the depth and breadth of sequential medical data increases, the methods will improve the delivery of health care in Canada by providing our medical doctors with new, high-quality evidence to aid them in choosing the best treatments for their patients.""502892,""Ljubojevic, Vladimir"
"509529"	"Lynch, Alan"	"Nonlinear Control and Observer Design for Unmanned Vehicle Systems"	"The proposed research investigates a number of fundamental control problems which arise in Unmanned Vehicle Systems (UVSs). The research problems defined are described by inherently nonlinear dynamics and objectives are met by both applying and extending the theory of nonlinear control. Based on its governing dynamics, this broad theoretical area of research studies the analysis, prediction, and influence of system behaviour. Conventional solutions to the proposed problems have either no theoretical basis or rely on linear system approximations and methods. However, such approximations can destroy useful system structure and lead to lost insight and reduced performance relative to nonlinear methods which directly account for nonlinearity without approximation. The research objectives aim to provide mathematically well-founded performance benefits and are complemented with experimental validation on custom developed UVS test stands which the researcher has developed. The empirical aspect of the discovery is important to guide the development of nonlinear control theory and ensure its real-world relevance. Research which combines the benefits of cutting edge theory and experiment is important in increasing student interest and providing relevant training for industry and academe to maintain Canada's leadership in high technology. It is well known that UVSs are an active and challenging application area with numerous real civilian and military uses. For example, UVSs enable inspection of critical infrastructure or provide environmental monitoring in a uniquely cost effective, accurate, and safe manner. The proposed research aims to develop nonlinear control for improving UVS motion control and navigation system performance which is a crucial element for increased adoption of UVS technology.""512423,""Lynch, Derek"
"509536"	"Majedi, AmirHamed"	"Integrated Quantum Photonics Based on Hybrid Superconductor-Semiconductor Nanostructures"	"Quantum photonic technology puts the quantum properties of light into practical use. Information encoded on individual photons not only offers new opportunities to enhance the security of communication but also radically new ways to perform sensing, imaging, computation and metrology. The best example of quantum photonic technology is a commercially available quantum key distribution system that potentially enhances the security of information over a point-to-point fiber optic communication link. It is anticipated that such systems will be extended to communication networks upon deployment of advanced photonic devices capable of generation, detection, transmission and processing of single-photons and entangled multi-photons.           This proposal explores a new integrated platform based on hybrid superconductor-semiconductor structures that integrate on-demand single and entangled photon sources, single photon detectors and quantum waveguides to make a quantum photonic transceiver. The rich optical and electronic properties of semiconductor nanostructures will be combined with the quantum coherency of superconductors to achieve the best overall efficiency and speed in processing of photons on a chip.           Integrated quantum photonic chips are typically comprise semiconductor quantum dots embedded in an optical waveguide with superconducting electrodes as a source of single and entangled photons, three different types of superconducting nanowire single photon detectors and ridge semiconductor optical waveguides that connect sources to detectors. The proposed program will address challenges to the design, simulation and fabrication of such structures and enhance the efficiency, indistinguishability, and speed of photon generation and detection in quantum transcievers.          Canada has identified quantum information technologies as strategic area for future development. It is highly expected that this program will deliver fully-integrated single and entangled photon generation and detection advances as well as several highly qualified personnel to this vital sector.        ""517909,""Majeed, Maliha"
"514604"	"Marcus, Brian"	"Entropy for Hidden Markov Processes and Markov Random Fields"	"Stationary Markov processes are ubiquitous as tractable models of random phenomena. The simplest setting isthat of finite-state, discrete-time stationary Markov chains, which model one-dimensional random sequences.These processes are fairly well understood. In this project, we focus on two generalizations of this simplesetting: 1) stationary hidden Markov processes (HMP) and 2) stationary Markov random fields (MRF). Theformer models sequences generated by a Markov chain and observed in the presence of noise. The lattermodels random arrays in higher dimensions, either on a square or cubic lattice or some other kind of graphstructure. Both are indispensable for modeling random phenomena in applications ranging from speechrecognition to network communications. For both classes, we focus on the finite-state, discrete-time andstationary cases.One of the the most fundamental properties of a stationary process is its entropy, sometimes called entropy rate. Computation of entropy is an important first step in the design of codes for data compression or error correction. While there is a simple, closed-form expression for the entropy of a Markov chain, it is rare that the entropy of an HMP or MRF can be computed exactly. The hard square model is a classical example, for which computation of entropy has eluded a solution for decades. The main goals of this project are to further develop estimates and asymptotics of entropy for HMP's and MRF's and to solve related structural problemsfor such processes.  We will extend our previous work on HMP's from discrete-valued to continuous-valued processes and will improve our earlier work on exponentially-fast approximations to entropyfor MRF's.       ""506115,""Marcus, Jeffrey"
"507977"	"Maurer, Frank"	"Agile software engineering for cloud-based applications"	"In the future, more and more software systems are expected to use multi-modal, multi-touch front ends like smartphones, tables, digital tables or wall-sized displays while the back end business logic will reside in the cloud. The proposed research program will focus on investigating processes, techniques and tools that support developing cloud-based software applications in small to medium enterprises. The research will look at these issues from an agile software development perspective to build on our group's results and research strengths.Concretely, we will focus on: 1. Test-driven development of cloud-based applications: investigate processes and tools that allow following test-driven development practices for building software applications for the cloud. Extend our existing rule-based exploratory testing tool to support testing of cloud based applications and develop comprehensive rule-bases for common defects of cloud-based applications.2. Utility-based software systems: Cloud-based software systems will provide software, infrastructure, and/or platforms as services. The corresponding shift towards an utility-based pricing model will likely impact software development economics as well as development processes and software design. The research program will investigate these issues in more detail with a focus on agile software processes for utility computing. 3. API usability: Study and evaluate the usability of software APIs for building cloud-based applications to determine what makes then easy to use for software developers. The focus on small and medium enterprises - as suggested in the proposed program - is appropriate in the Canadian context as the majority of Canadian software companies fall into this category. As a result, we expect that Canada has substantial receptor capacity to benefit from the proposed program.""498644,""Maurer, Frank"
"507897"	"Mayers, André"	"Intégration des connaissances conceptuelles aux systèmes tutoriels intelligents : théorie et réalisation"	"Les tuteurs par traçage de modèle (MTT) sont des logiciels qui aident les étudiants à acquérir des habiletés de résolution de problèmes. Ces logiciels interprètent les étapes de l'étudiant lors de sa résolution de problème et peuvent lui conseiller une prochaine étape, lui montrer comment exécuter certaines étapes et lui expliquer ses erreurs. L'efficacité des MTT a été démontrée, mais ils demeurent peu utilisés parce que leur coût de conception est élevé.La finalité de ce programme de recherche est le développement d'un système informatisé qui facilitera la conception des MTT. Pour atteindre cette finalité, nous développerons une théorie décrivant comment représenter l'expertise du domaine enseigné. Cette théorie permettra au concepteur d'un MTT d'encoder sa compréhension profonde du domaine, puisque c'est cette compréhension que nous voulons que l'étudiant acquière. La théorie minimisera aussi les coûts de conception d'un MTT. Plus particulièrement, les MTT développés à l'aide de notre système seront capables de générer automatiquement des rétroactions pédagogiques, tels que des messages d'aide, à partir des connaissances encodées. Cette dernière caractéristique sera une amélioration par rapport aux MTT actuels qui requièrent que la plupart de leurs rétroactions soient encodées par le concepteur du tuteur.Le système informatisé que nous développerons nous permettra de découvrir des stratégies pédagogiques qui maximisent l'apprentissage pour différents types d'étudiants. Puisque l'expérimentation avec des apprenants humains demande beaucoup d'organisation et de temps, nous proposons de créer des apprenants virtuels afin d'effectuer des simulations pour mettre au point nos stratégies pédagogiques. Cette première validation avec des apprenants virtuels n'élimine pas l'importance de valider avec des apprenants humains, mais en réduit de beaucoup le temps nécessaire.""510523,""MayerSmith, Jolie"
"514599"	"McGrenere, Joanna"	"The Design of Information Computing Technology for Older Adults"	"My long-term objective is to help establish the field of elder-computer interaction: putting older users (65+) at the center of interactive technology design, rather than being an afterthought, which is most often the case now.  The research program is to design interactive information and communication technologies (ICT) for emerging and current platforms such as smartphones, tablets, and desktop computers in accordance with the unique sensory, perceptual, cognitive, and motor abilities of older adults, as well as their preferences, computing expertise, and contexts of use. Elder-computer interaction is investigated within three parallel yet complementary threads. The first seeks to understand the nature of interruptions and multi-tasking experienced by ICT users, and how ICT designs should accommodate those interruptions. A specific focus is to understand how these needs change across the lifespan. The second thread examines the unique ICT learning needs and preferences of older adults to design novel, age-appropriate mobile and desktop interfaces. The third thread assesses and builds upon strengths and limitations of different input methods and their respective interaction techniques, including touch, for older users.     The research methodology includes iterative prototype design and development, formal lab experiments, and qualitative field evaluation. Investigations include participants across the lifespan, from young adult to age 65+, in order to identify age effects. The overarching design approach emphasizes personalization: designs that adapt or evolve as a user ages based on changing abilities, preferences, and contexts of use. The research program will produce novel interfaces and interaction techniques, design guidelines for existing and future platforms, and fundamental new knowledge about human interaction capabilities. All of the proposed research falls within Computer Science in the area of Human-Computer Interaction (HCI) and aligns with Universal Usability and User-Sensitive Inclusive Design.""503203,""McGuffin, Michael"
"509350"	"McGuire, Michael"	"Multi-core Signal Processing for Advanced Wireless Devices"	"The objective of this research is to investigate efficient signal processing algorithms for high data rate wireless communications with advanced wireless devices. Portable wireless devices need to support applications such as real-time high resolution video and mobile gaming with high data rate and computational requirements. Mobile computing devices now use multiple computing cores. These processors perform the same amount of computation per unit time as single core processors while requiring much less power. Making effective use of this new class of processors requires careful management of the information exchanged between the cores. Current wireless devices' signal processing algorithms consist of sequential calculations which do not map efficiently onto multiple cores. We will develop new signal processing algorithms with parallel calculations which execute efficiently on multiple cores. Trade-offs between calculation duration, parallelization, and final error performance will be investigated. The receiver trade-offs in terms of parallelization are not well explored in the current literature. The new signal processing algorithms will enable small low power advanced wireless devices to support highly reliable, high data rate communications applications. Several companies with significant research and development facilities operating in Canada such as Research In Motion, PMC Sierra, and Broadcom are developing hardware and software for wireless signal processing applications. My students will gain skills that are in very high demand with these companies. The students will gain experience in the design and implementation of real-time signal processing relevant for wireless communications and many other applications areas, such as video processing, radar systems, and vehicular control.""499555,""McGuire, Sarah"
"509474"	"Ménard, Michaël"	"Ultra-Compact Low-Power Photonic Network-on-Chip"	"As the computing capacity of microelectronic chips increases, thanks to continuous improvements in speed and transistor density, the flows of information they generate are becoming harder to handle.  As a result, the electrical interconnections between chips and even between logical units within a chip are becoming bottlenecks.   With aggregate bandwidths exceeding one Tb/s, optical networks have demonstrated the potential of light to transmit information, and now it can be used to resolve the challenges arising at the chip scale.  Silicon photonics has demonstrated that optical devices can be manufactured using CMOS compatible processes but there are still numerous challenges that must be addressed to efficiently combine optical and electronic circuits on the same chip.  For instance, the requirements on the silicon and buried oxide layers of silicon-on-insulator wafers differ greatly depending on whether they are used for optical or electronic applications.This research program investigates a novel approach to integrated optics that could enable the creation of ultra-compact and low-power optical devices on top of electronic circuits.  To achieve this, we will develop a fabrication process where silicon carbide (SiC) is used to form optical waveguides.  SiC offers numerous advantages for on-chip optical circuits, including a high refractive index, the possibility of being deposited at low temperatures, and it can exhibit the electro-optic effect.  Including optical components on CMOS dies represent a significant technological shift, and the knowledge developed during this research program will be valuable to Canadian electronics companies.  Moreover, it will train 2 PhD and 3 M.Eng. students in the design and fabrication of advanced photonic circuits.  The impact of integrated optical circuits based on SiC extends beyond on-chip optical telecommunications.  The mechanical strength of SiC can be used to increase the resonant frequency of optomechanical cavities, and since graphene can be grown on SiC, SiC waveguides will be well suited to optically interconnect graphene electronics.""502720,""Menchella, Jonathan"
"509430"	"Michailovich, Oleg"	"Distributed processing of medical diagnostic data for early detection and classification of brain-related disorders"	"The human brain is an organ of extraordinary complexity that determines our sensations, emotions, memory, intelligence, creativity, thought and action. Deep within the recesses of the human brain lie the answers to many of today's medical mysteries. Epilepsy, schizophrenia, Alzheimer's disease, depression, Parkinson's disease, stress, multiple sclerosis, learning disabilities and autism are examples of neurological disorders which affect millions in Canada and around the globe. Many of such disorders are nowadays known to cause damage to neural fibre tracts in the white matter of the brain. This damage, on the other hand, can be detected and assessed by means of diffusion-weighted magnetic resonance imaging (DW-MRI). This is what makes DW-MRI a unique tool of medical imaging, with a potential to revolutionize the field of neurological diagnosis. Unfortunately, collecting DW-MRI data is known to be a relatively lengthy process, which hinders the application of DW-MRI to many groups of neurological patients, including children and patients with dementia. As a result, the full diagnostic potential of DW-MRI still remains unrealized. To resolve this problem, in this project, we will develop methods for fast and accurate acquisition of DW-MRI data. Our methods will reduce the duration of DW-MRI scans, thereby improving their practical value. Subsequently, based on the acquired data, we will compute diagnostic biomarkers for early detection and classification of various neurological conditions, such as schizophrenia and post traumatic syndrome. If available, our technology will: 1) expand the applicability of DW-MRI diagnosis to wider groups of neurological patients, 2) provide practitioners with advanced methods for early detection of neurological disorders, and 3) improve the accessibility of MRI equipment to allow more Canadian patients to reap the advantages of this extraordinary tool of medical imaging.""497923,""Michal, Carl"
"509404"	"Miller, Daniel"	"Linear Periodic Controller Design"	"In systems control, the objective is to make a physical system (the plant) act in a desired manner through the use of an (automatic) controller, e.g. an autopilot (the controller) is used on an aircraft (the plant) to maintain speed, altitude and direction. The first step in control system design is to obtain a mathematical model of the plant, and then one designs a controller, described by a mathematical equation, which is typically implemented in software. This process is often difficult due to uncertainty in the plant model.Most controllers presently implemented are described by linear equations with fixed parameters, such as the well-known proportional-integral-differential (PID) compensator. As microprocessors become faster, it is becoming possible to implement (possibly complicated) nonlinear and/or time-varying controllers. The objective here is to develop advanced linear periodic controllers which provide enhanced tolerance to plant model uncertainty as well as good performance.In this proposal we consider two different contexts:(i) adaptive control, in which the plant parameters are unknown and possibly rapidly time-varying, and(ii) decentralized control, in which there is a constraint on information flow, perhaps arising from a geographical separation of sensors and actuators, such as in the control of a large power grid.This work is of a fundamental nature but has potential long-term benefits wherever control is used, from robotics to aerospace to the chemical processing industry.""498745,""Miller, David"
"514079"	"Mintchev, Martin"	"Advanced Instrumentation Methods for Biomedical and Oilfield Applications"	"The objective of the proposed research is to investigate, design, implement and test innovative instrumentation methods for biomedical and oilfield engineering applications. In the recent years the development of electronic instrumentation has influenced immensely the areas of biomedical and oilfield engineering. Although dramatically different from application point of view, the two areas have a lot in common from the perspective of electronic instrumentation. The infra-low frequency nature of the monitored signals and processes associated with biomedical and oilfield systems are among the most important factors linking them. In addition, recent advancements in electronic instrumentation related to the transition from macro-instrumentation to micro-instrumentation through the development of wireless microsystems and micro-electromechanical systems (MEMS), combined with real-time and multichannel signal processing, as well as advanced information handling, imply that integrated approach could be extremely beneficial. Specifically, the recent development of ""smart"" microsystems applicable both in the human body and downhole created new opportunities for synergy and fusion. My research will concentrate on the development of: (1) a miniature, sequentially-actuated, multi-cell, wireless, semi-invasive patch for pseudo-continuous monitoring of blood sugar levels for diabetics (""Electronic Mosquito"") and (2) wireless, downhole-mounted real-time inertial navigation system for azimuth monitoring in measurement-while-drilling oilfield applications. The first project, the Electronic Mosquito, is a skin-attachable microsystem for blood sampling and analysis. The overall vision for the apparatus is to resemble an array of real mosquitoes sequentially penetrating the skin each to sample a small blood droplet. The overall vision for the second project is to replace the conventional methods in horizontal drilling which incorporate magnetic surveying techniques for determining the position and the attitude of the bottom-hole assembly with modern, inertial navigation system (INS)-based microsystems utilizing our patented method, In-Drilling Alignment, for more precise downhole surveying.""517884,""Minty, Lauren"
"508116"	"Mishna, Marni"	"Applied Analytic Combinatorics"	"Combinatorial structures arising as discrete mathematical models of physical phenomena are increasingly found lurking at the interface of mathematics and other sciences, particularly physics, biology and computer science. The simplicity of the structures is deceptive, as they are often sufficiently rich to play a key role in our understanding of the underlying phenomena being studied.  Analytic combinatorics is a rapidly advancing area of theory which studies combinatorial families and their properties using complex analysis and algebra. A central feature of this theory is its algorithmic and generic nature.This research program considers several problems in the domain of statistical mechanics, genomics and theoretical computer science, and to each addresses questions well adapted to analytic combinatorics: solvability; enumeration and parameter distribution; and random generation. Three applications act as conduits of study: (I) lattice path enumeration; (II) tree models of permutations; and (III) RNA secondary structures.  The questions take the form of  ""Do we expect a particular  polymer to have this shape?"" ""What is a typical evolution  scenario between two mammals and their common ancestor?""  and ""How often should this folding structure occur in an RNA strand  of this length?"". Ideally, we then compare results to actual data in order to gauge the quality of the model.   Random generation allows us to form and test hypotheses, to determine the practical complexity of related algorithms, and to assess the quality of the data. The results for each application inform a larger study of combinatorial structures:we are interested in the meta classification of combinatorial classes by some notion of complexity. This augments the classical Chomsky Hierarchy of formal languages. ""506754,""Mishra, Ram"
"508111"	"Mitchell, Ian"	"""Nonlinear Algorithms for Verification, Control, Design and Analysis of Nonlinear Continuous and Hybrid Systems"""	"I propose to develop numerical algorithms and implementations for the verification, control and design of systems whose dynamics are best described by nonlinear continuous or hybrid (a mixture of continuous and discrete) models.  Such systems arise whenever a digital computer interacts with the external world, and they are hence becoming ever more widespread as computers are embedded into devices throughout our environment.  The foundation of this research program will be the study of a variety of reachability algorithms for continuous systems, including both grid and trajectory based with both parametric and nonparametric reach set representations.  Application of these techniques can reduce the likelihood of undesirable behaviours, but for nonlinear continuous and hybrid systems it has rarely been possible or it has required the intervention of reachability experts.  With the long-term goal of bringing these formal analysis methods into industrial practice, two distinct application domains will inform this program of research: safe collaborative control of robots, and design of robust analog and mixed-signal integrated circuits.  In order to solve real problems without the intervention of experts in formal methods, this program will also address two other key needs: construction of the mathematical model of the system and problem to be solved based on descriptions typically used by domain experts, and conversion of the verification results into a form which addresses those experts' concerns.  For the domain specific toolchains and, where necessary, for the core reachability algorithms I will create and release flexible, efficient, robust and well-documented implementations that will do for these techniques what the Toolbox of Level Set Methods has done for time-dependent Hamilton-Jacobi equations: permit other researchers in industry and academia to reproduce, utilize and extend published results.  Trainees will gain experience in numerical algorithms, robust software development practices, integrated circuits and/or robotics, all areas with positive job prospects.  Finally, the choice of distinct application areas will guide the extension of automated formal analysis into other domains exhibiting continuous and hybrid dynamics.""513899,""Mitchell, Jane"
"507926"	"Morgan, Yasser"	"Towards Hierarchical Autonomous Context-Aware Distributed Clustered Vehicular Ad-hoc Networks (VANET)"	"Future networks must serve human demands in ways never before thought possible. Networks will operate in dynamic environments, interact with users and other networks, respond to malicious attacks and yet, have properties of optimality and self-healing while keeping the user out of convoluted details. Future network architectures will be built on a foundation of autonomy, distribution, and intelligence. To achieve that, we must explore and examine new ways of looking at how we design, observe, and interact with future networks. Modern networks like Vehicular Ad-hoc Networks (VANET) offer a glimpse on future network apprehensions. VANET is a new network initiative vowing to improve transportation safety by applying a range of modern wireless technologies connecting vehicles together and to road infrastructure. VANET initiative aims at saving human lives and pledges economical, environmentally-friendly, and congestion-free highways. To put VANET's size into perspective, it is expected to form a continent-wide network with millions of cars joining, interacting, and leaving the network every second. Current and classical network architectures fail to serve the scale and dynamics of the expected VANET. The immediate goal of this research is to resolve the complex VANET scalability problem. This proposal presents the idea of dividing up VANET into smaller localized manageable clusters by dynamically extracting characteristics from its localized environment. Autonomy, distribution, and intelligence are the unique concepts behind the proposed clustering. This research introduces an autonomous clustering framework that mimics the essence of the localized transportation environments. Clustering VANET would not only play a role in decreasing road fatalities and reducing carbon emissions, it also presents a step towards the grand vision of autonomous intelligent networks.""509122,""Morgenstern, Norbert"
"508088"	"Muir, Paul"	"Software for Error Controlled Numerical Solution of Boundary Value Ordinary Differential Equations and Parabolic Partial Differential Equations"	"Along with theoretical and experimental science, computational modeling is now viewed as a third fundamental branch of scientific inquiry. A majority of computational models, arising in such areas as chemistry, physics, biology, and finance, involve complex systems of differential equations (DEs), for which approximate solutions are obtained through the use of sophisticated numerical software.  Because the numerical solutions are approximate, it is essential that the software also assess the quality of the computed solution. Two popular types of assessment are the global error (the difference between the approximate and exact solutions) and the defect (the amount by which the numerical solution fails to satisfy the equations). Good quality software will adapt a computation so that the numerical solution can be obtained efficiently and so that an estimate of the global error or defect satisfies a user-provided tolerance.         Our research will focus on numerical software for the efficient and accurate solution of types of DEs, known as boundary value ordinary DEs (BVODEs) and parabolic partial DEs (PDEs), that features adaptive control of efficiently computed estimates of the global error and/or defect. One major goal of our work is to develop software, for parabolic PDEs that depend on time and two spatial dimensions,  that will employ high accuracy methods in time and space to adaptively control accurate and efficiently computed estimates of the spatial and temporal errors. A second major goal is to investigate software for BVODEs that employs hybrid global error/defect control and to develop new algorithms and software to extend the problem class currently treatable by BVODE solvers to include, e.g., problems with periodic boundary conditions or with delay and advance terms. The significance of this work is that it will improve the ease-of-use, efficiency, robustness, and capability of software for the numerical solution of BVODEs and PDEs, thereby improving the tools available to help computational scientists efficiently treat sophisticated computational models arising in their areas of investigation.""516798,""Muise, Stacy"
"513315"	"Munro, Ian"	"Efficiency of Data Structures"	"This research lies at the heart of Data Structures and Algorithms, indeed at the heart of Computer Science itself. Most is motivated by applications on very large data sets; all is motivated by finding the ""best possible way"" to solve natural problems. The work is primarily mathematical in nature, developing data structures and proving lower bounds, but also includes some implementations. As demand for fast large information systems escalates, the manner in which data is organized and the algorithms used to manipulate information become increasingly critical. More time and space efficient methods must be created to allow large systems to better perform in response to sophisticated queries and updates. The principal focus of this research is to address several aspects of this issue. The first is that of succinct representations of data structures: the representation of structural information (search trees etc.) in the information theoretic minimum space while still permitting the required operations (navigation etc.) in constant time. A natural continuation of this is the application of succinct data structures to text indexing and to geometric problems. Another crucial issue is having the required data at the appropriate level in the memory hierarchy. This will be addressed through several approaches to I/O-efficient algorithms including cache-oblivious techniques (ones in which details of memory are not included and so apply simultaneously at all levels of the memory hierarchy). In tandem with this application-driven work is the fundamental notion of actually proving that the methods developed are indeed the best possible for the tasks at hand. A second line of research involves the computational complexity of comparison-based problems: issues such as sorting (or partially sorting) in the minimum number of comparisons. This latter work is primarily of theoretical interest and gives motivation for the development of various mathematical tools (such as graph entropy), but it also sheds light on the practical aspects of large scale computation. The work focuses on the intellectual development of the field, but also has real potential for immediate application and advancing Canada's software industry.""512760,""Munro, Ian"
"509510"	"Murari, Kartikeya"	"Optical Systems and Image Sensors for Imaging in Freely-Moving Animals"	"Imaging is a powerful tool for biomedical research offering non-contact and minimally or non-invasive means of investigating at multiple scales - from single molecules to large populations of cells. Imaging in awake, behaving animals is an emerging field that offers the additional advantage of being able to study physiological processes and structures in a more natural state than what is possible in tissue slices or even in anesthetized animals. It can provide important information on physiological as well as diseased states of living organisms.I propose the development of instrumentation that will allow long term imaging in freely-moving animals without any tethers. The two main aims of the proposal are to create new integrated circuit based image sensors that offer the performance or large benchtop cameras while offering considerable saving in weight, complexity and size. The other is to design miniature optical imaging systems that when integrated with the image sensors can create tiny, fingertip-sized, stand-alone devises that implement illumination, focusing and image capture. Miniaturized high-sensitivity, high-resolution imaging systems can also be used for portable, point-of-care medical imaging instruments, such as retinal imagers, that can be field-deployed in remote locations without much infrastructure. Ultimately, these instruments will be of great use for both biomedical research and clinical applications.""512431,""Murch, Susan"
"509332"	"Nandi, Subhasis"	"Fault Analysis and Detection in Electric Machinery"	"Condition based monitoring (CBM) of electric machines has become a very important research topic in recent years. Existing protective schemes such as over-current, over-voltage, earth-fault, etc. are sometimes inadequate and slow to detect and isolate faults developing in motors and generators. This may cause damage to the product line, interrupt very important processes, leading to heavy financial losses or under certain conditions even loss of life. Thus it is very important to diagnose faults on line and at their very inception, so that these faults do not propagate to major catastrophes.  The major faults of electrical machines can broadly be classified as: a) Bearing and gearbox failures, b)Stator winding faults, c) Broken rotor bar or end-rings, d) Static and /or dynamic air-gap eccentricities and f) Shorted rotor field winding.    Motivated and influenced by this background, our expertise and facilities, the current discovery grant proposal comprises five primary objectives: a) Estimation and mitigation of bearing deterioration and damage due to the presence of eccentricity (asymmetric air-gap) in Electric Machines ; b) Eccentricity fault diagnosis of slip-ring induction machines (SRIM) used mainly as wind energy generators, c) Fault detection under transient condition in squirrel cage induction motors (SCIM) which is the most common of all industrial motors, d) A PC based harmonic analyzer to detect positive and negative sequence quantities for fault diagnosis of electric motor drives and power systems using space vectors and e) Detection of winding inter-turn faults in three phase transformers wound on a single core.The impact of this research will be safer,more reliable and efficient  electric machinery and avoidance of catastrophic failures and consequent economic loss, which will benefit numerous Canadian industries and utilities.The interdisciplinary, practical yet sophisticated nature of the research will train HQP for industry as well as academic positions.""516801,""Nandiwada, Shiva"
"514283"	"Nikolova, Natalia"	"Near-field Microwave Imaging of Complex Objects"	"THE OBJECTIVE of this proposal is the development of methods for the imaging of objects which are optically opaque but are penetrable by microwaves. There is a great variety of such materials including living tissues, wood, ceramics, plastics, clothing, concrete, soil, etc. The focus of this proposal is on the near-field microwave imaging of objects with complex internal structure. This line of research has important applications in biomedical imaging, concealed-weapon detection and underground detection. In theory, near-field measurements, as opposed to far-field measurements, offer the potential for higher image quality due to the possibility of capturing much finer shape details. In practice, this potential has remained largely untapped.   Recently, our team has proposed two novel image reconstruction algorithms: model-based holography and sensitivity-based imaging. One important advantage of both techniques is that they perform in quasi-real time; in other words, producing the images from the measured data takes negligible time compared to the measurement itself. More importantly, they can take full advantage of the wealth of information contained in the near-field scattered microwave signals and achieve fine-resolution images where shape details as small as one-hundredth of a wavelength can be distinguished. Since the two techniques exploit conceptually different reconstruction principles, they are complementary in the sense that they provide independent diagnostic or detection results. Thus, they can be combined synergistically in a methodology the outcome of which is based on the results of the two underlying techniques but is superior to any one of them taken separately.  The current proposal aims at the development of the synergistic methodology described above and its integration with an experimental setup for near-field microwave data acquisition. The software and hardware development will target tools for tissue sensing with applications in early stage breast-cancer detection, which is of great societal importance. At the same time, the developed approaches will have a much wider range of applications with possible extensions into concealed-weapon and buried-object detection.""512196,""Nikolova, Natalia"
"508096"	"Nkambou, Roger"	"Domain Knowledge Modelling for Educational Purposes: Methods and Tools"	"The idea of having a system that can teach a given subject with the effectiveness of the best human teacher (i.e., Intelligent Tutoring Systems - or ITS), has been at the fore front of many applications of artificial intelligence since its birth. These systems have tremendous potential and important economic and social benefits in many areas. They convey many challenging problems of AI. One such problem is the acquisition and representation of domain knowledge (that is, knowledge about the topic being taught), as well as reasoning about this knowledge. The main goal of this research proposal is to develop methods and tools to enhance ITSs with automated facilities capable of building, improving and maintaining an effective semantic and procedural domain knowledge model. First, I will develop new methods for refining, completing, and improving the domain ontology within an ITS. Those methods stand to contribute to the knowledge engineering aspects of ITSs, as well as other knowledge-based or knowledge management systems. The provision of tools that can facilitate domain ontology creation and maintenance, will likely produce further ITSs with ontology-based domain knowledge modules, and thereby make it easier to share domain knowledge between different ITSs. Second, I will elaborate and test effective data mining algorithms that can yield useful procedural knowledge from educational problem-solving data, particularly in ill-defined domains. Such procedural knowledge will be used to provide useful tutoring services to learners in domain where a clear task model is difficult to set up. We expect our findings to contribute to current educational data mining efforts, by offering valuable alternatives and complementary opportunities for the development of ITS in ill-defined domains. This research program is bound to produce methods and tools, that will particularly enhance the performance of ITS and e-learning software in dynamic, evolving domains, unburdened by traditional concerns for capturing semantic and procedural domain knowledge. Such results will offer Canadian e-learning providers software and service enhancement options, and help other Canadian companies to improve their knowledge management platforms.""498476,""Nkongolo, Kabwe"
"507901"	"Ormandjieva, Olga"	"WEB-based Dynamic Social Networks (WDSN) Framework for Measurement-informed Decisionmaking under Uncertainty"	"My research proposal brings together web-based dynamic social networks (WDSN) analysis and representational theory of measurement within category theory and Markov decision processes. WDSN differ from traditional social networks in that they are cellular, distributed, web-based thus larger, dynamic, and may contain varying levels of uncertainty. The long term goals of my research deal with the following issues:  (1) how to best model WEB-based dynamic social network so that its essential properties, such as evolution laws, time progress, response strategies and self-healing are precisely captured in the model?  (2) what are the measurement goals that need to be achieved throughout WDSN evolution? A set of candidate metrics for describing the WDSN goals have to be defined. Two kinds of uncertainties - measurement errors and goals' fuzziness, can be differentiated in the measurement data. How to assess their combined effect on the measurement process and to assess the plausible effects on subsequent decision-making rules? (3) what are the rules for measurement-based decision-making under risk of uncertainty due to assumptions (that is, lack of information)? (4) what are the appropriate method and tools for an integrated validation of WDSN evolution, measurement-based decision making and self-healing?  The novelty and significance of my research lies in the development of new formal framework that enables modeling and analysis of WEB-based dynamic social networks (WDSN) with emphasis on time and self-healing. It will explicitly make use of measurement-informed probabilistic reasoning and decision-making about the evolving structure of social interactions and interaction patterns.  ""511088,""Ormeci, Banu"
"507860"	"Osborn, Sylvia"	"Database Access Control and Privacy"	"Access control is the ability to say who can do what operations on what data; e.g., the shipper in an on-line book store can read a customer address but not update it.  Access control takes over after user authentication, deciding what that user is allowed to access, read, write and update (e.g. the shipper should not be inserting new inventory).  Customers may be allowed to say that their address can be used for shipping but not for advertising; this is what is commonly referred to as data privacy.  Shipping and advertising are examples of privacy purposes.  Data concerning the company's inventory is controlled by access control.  Customer data, on the other hand, is also subject to privacy concerns.  Implementing these privacy requirements, and the tools to express what should be private, are aspects which concern Computer Scientists.   Our access control research is based on the role-based access control (RBAC) model where permissions are collected in roles and assigned to users in one operation.  RBAC simplifies the management of access control in complex environments, helping the security system designer to do a better job.  Some metrics exist to say when a role design is ""good"" (e.g. fewest possible roles); we will search for others.  We will study updates to an RBAC design, and tests for desired properties.  We have recently studied RBAC with privacy labels on some of the data.  We propose to further this research, examining the storage of data with these labels, and the run-time tradeoffs of different physical partitions of the privacy-labeled data, while querying and updating it.   The scientific approach involves developing a model, proving properties of the model, showing its applicability to real-world situations, developing prototypes and tools, and performance analysis comparisons.  The contributions of the research will be a better understanding of preserving privacy, preserving privacy in the presence of an access control mechanism, maintaining these systems under change, measuring when we have a good design, and more efficiently implementing access control and privacy- preserving systems. ""514617,""Oser, Scott"
"513706"	"Pai, Dinesh"	"Computational Models of Humans: Movement and Contact"	"My long term goal is to create useful computational models of how humans move and physically interact with their environment.  The focus is not on black-box modeling of movement data. Rather, it is on modeling the underlying system itself, as a biological machine: the motors (muscles), sensors (esp. mechanoreceptors in muscle and skin), structures (skeleton and connective tissues), and software (neural control). By modeling the underlying sensorimotor system we could achieve better generalization to novel situations and gain fundamental insights into biological principles.   In the short term my goals are (1) to create new simulation software for physically based modeling of the sensorimotor periphery (2) to construct realistic models of the human eye and hand, and of a biomimetic robot (3) estimation and validation of these models, so that their predictions are credible.  I focus on the eye and the hand because they provide complementary insights and are useful in character animation.   Models of human movement are required in an enormous range of applications.  A central goal of computer animation is the synthesis of human characters interacting with their environment, but current practice relies on significant user effort and ad hoc techniques. Our work could transform it into human simulation.  Realistic models of human movement are also important in biomechanics, ergonomics, neuroscience, and medicine and could lead to new robot designs based on biological principles.   Students and postdoctoral trainees are involved in all aspects of this research, and will acquire a broad range of skills in an interdisciplinary environment, including development of advanced simulation software systems, measurement and analysis of human performance with world class instrumentation, computer graphics, biomechanics, and robotics.""512634,""Pai, Dinesh"
"508003"	"Paige, Christopher"	"Numerical linear algebra: large sparse matrix computations."	"In order to solve many scientific, engineering, business and other problems using the computer, we often convert the problems to subproblems involving matrices --- rectangular arrays of numbers. Because of this, matrix computations and its theory (often called numerical linear algebra) lies at the heart of most scientific computing.  The key to solving many smaller matrix problems is to find some appropriate factorizations of matrices --- so for example a given matrix can be described as the product of two or more such factors.  However many problems are large or complicated, and the resulting matrix problems are extremely large --- matrices having millions of rows and millions of columns are not at all unusual.  And such large matrices are nearly always very sparse --- the vast majority of elements being zero.  Factorization methods can then become far too costly, and iterative methods may be necessary for solving such problems.  This research develops numerically reliable and efficient algorithms for iteratively solving such large sparse matrix problems.  It carries out analyses of such algorithms to prove their efficiency and reliability, and sensitivity analyses to show what effects changes in the data (for example caused by finite precision computation, or uncertainty in the data) will have on the final results.  These analyses lead to greater understanding of individual problems and their computed answers, and improved general purpose and specific area algorithms.The results of the research are useful for most practitioners in scientific computation.  Some application areas of particular importance in Canada that use these techniques are the aerospace industry, climate change and weather prediction, power generation and distribution, and resource discovery and extraction.""511535,""Paige, Matthew"
"509352"	"Papadopoulos, Christo"	"Carbon-based nanoelectronic networks"	"The extraordinary advances in electronics over the past 50 years have led to increasingly powerful and complex silicon integrated circuits whose features now have dimensions less than 100 nm; a regime referred to as the nanoscale. As these feature sizes are reduced further, continued progress in computers, consumer electronics and telecommunications becomes increasingly challenging. Industry forecasts predict that by the end of this decade performance and manufacturing challenges when features sizes reach below 20 nm will make it necessary to augment silicon by introducing new materials and devices into integrated circuits.  Carbon-based nanostructures in particular have been identified as prominent candidates to meet these challenges. This research program seeks to create materials and devices based on carbon nanostructures to enable future electronics. Its objectives are to achieve the controlled assembly of carbon nanomaterials and develop these nanoscale assemblies into viable electronic devices with reliable properties and capable of high performance. Directed self-assembly will be used in conjunction with high-resolution microscopy and precise electronic measurements to develop carbon-based nanoelectronic network devices in a manner that is compatible with current electronics manufacturing.The anticipated outcomes of this work are new materials, methods and devices based on carbon nanostructures that can meet the future requirements of the electronics industry.  By assembling carbon nanomaterials into nanoscale networks their excellent properties can be harnessed and provide tremendous improvements to the performance of electronics. This new approach to materials and devices will enable innovative electronic and information technologies that bring social and economic benefits to Canada. Extensive training of highly qualified personnel in the proposed research will be provided by a broad nanoscale research environment.""515746,""Papadopoulou, Barbara"
"507873"	"Parizeau, Marc"	"Apprentissage incrémental pour les réseaux de capteurs sans fils intelligents"	"Les réseaux de capteurs sont de plus en plus utilisés pour la surveillance et la protection de lieux physiques. Ils sont constitués de dispositifs sensoriels compacts, peu chers et capables de communiquer sans fil. Leur principal intérêt est de permettre le déploiement rapide d'un grand nombre de capteurs près des zones d'intérêt; leurs principaux défauts sont une autonomie énergétique relativement restreinte ainsi qu'une incapacité à s'adapter dynamiquement à leur environnement.Dans ce projet, nous proposons de développer de nouveaux algorithmes pour maximiser la performance d'un réseau de capteurs. Nous proposons de doter les classifieurs internes aux capteurs d'une capacité d'apprentissage incrémental pour que, après leur déploiement sur le terrain, ils puissent accroitre dynamiquement leur performance en s'adaptant aux conditions spécifiques de perception de leur nouvel environnement.Pour ce faire, nous construirons un simulateur à base de GIS («geographical information system»), dans lequel des avatars de formes se déplaceront pour à la fois mettre à l'épreuve le réseau et procéder à son apprentissage incrémental""515682,""Park, Chul"
"513039"	"Patel, Rajnikant"	"Design and Control of Robotic Systems and Devices for Medical Applications"	"The proposed research is concerned with the development of a framework for the design and control of advanced robotic and other mechatronic systems and devices for catheter-based medical interventions. These interventions provide a viable and often a preferred approach over surgical treatment because they can be performed with reduced trauma, less post-operative pain, and result in faster recovery. However, the complexity of the interventions places limitations on the reliability and effectiveness of the procedures. For example, in catheter-based cardiac ablation for the treatment of Atrial Fibrillation (a type of arrhythmia where faulty electrical signals cause the atria to fibrillate), the catheter tip must be positioned accurately at designated locations inside the heart under image guidance (primarily X-ray fluoroscopy) in order to perform ablation using radio-frequency energy. The imaging modality used and the inherent flexibility of the catheter further complicate the intervention. In the proposed research, we will focus on use of catheters in cardiac ablation for Atrial Fibrillation. The research will involve the development of appropriate theory with prototype designs and experimental validation to address a number of issues arising from the use of teleoperated (master-slave) systems for catheter-based interventions. A key feature of these systems is the use of haptics (sense of touch) in the master-slave operation.  This is particularly important because it has been noted in several studies that the success of a cardiac ablation procedure depends not only on accurate positioning of the catheter's ablation tip but also on control of the force of contact. Therefore the proposed research has a strong focus on developing both the theory and the technology to reflect this contact force to the clinician via a haptics-enabled teleoperated system. The resulting technology will also allow us to develop a system for training and skills assessment for catheter-based cardiac ablation based on a haptics-enabled master-slave system. The proposed research builds on the strengths of the applicant in the areas of advanced robotics and control, mechatronic system design, and teleoperation and haptics.""505467,""Patel, Rajnikant"
"509503"	"Peng, Hao"	"A simultaneous PET/MR system for whole body imaging"	"We plan to develop a high performance whole body Positron Emission Tomography (PET) system that can be inserted into an Magnetic Resonance (MR) system for simultaneous PET/MR imaging without performance degradation for both systems. The PET system will be configured as a ring of approximately 52 cm inner diameter and 20 cm axial dimension that fits within a whole body 3T MR system of 60 cm diameter bore. The NSERC financial support will help us build the necessary infrastructures and a 1/3rd of the full system for proof-of-principle. There are significant technology differences between the proposed whole body PET/MR and previous designs. First, the PET detectors are based upon silicon photomultipliers (SiPMs) and exhibit superior performances. Second, an electro-optically coupling scheme using laser diodes is deployed to minimize potential interferences between two systems. If successful, this proposal will lead to a truly simultaneous PET/MR hybrid system for whole body imaging. In addition to the benefits of PET/CT, the multiplicity of MR's anatomical information and PET's functional/molecular information will enable new paradigms for diseases diagnosis, prognosis and treatment in cancer screening/staging, cardiac imaging, therapy monitoring, and characterization of Alzheimer's disease. The proposed system will help physicians to detect diseases in the early stage with increase specificity and manage treatment plans more effectively. We believe that a simultaneous PET/MR system could ultimately make great impacts for advancing healthcare technology and improving quality of life in Canada. Economic benefits from commercialization and technology licensing through collaboration with industry partners are also expected.""504976,""Peng, Hongxuan(Heather)"
"508231"	"Pesant, Gilles"	"""Counting-Based Search in Constraint Programming: Ongoing Design, Analysis, and Application to Solve Practical Combinatorial Satisfaction and Optimization Problems"""	"Many planning and scheduling problems arising in industry and public services are challenging to solve satisfactorily, especially if we have limited time to solve them. The solutions actually  implemented could typically be improved to achieve better services, higher efficiency, or increased profit. This research is about one computational approach to solve such problems, called Constraint Programming. By improving the way we search for solutions, we may reach better solutions in reasonable time.""514165,""Peslherbe, Gilles"
"508103"	"Pestov, Vladimir"	"New set-theoretic tools for statistical learning"	"Statistical machine learning theory is a major research direction in computer science, providing a concentual framework for such applications as pattern recognition, classification, and data mining. The theory is using a wide variety of mathematical tools, primarily probability theory and statistics, as well as modern functional analysis, combinatorics, and geometry. In this proposal, we aim to address some open problems of statistical learning using tools of set theory, logic, and set-theoretic topology that have not been previously applied. Here are some of the problems we want to focus on. The question about the existence of finite sample compression schemes for every concept class of finite VC dimension remains open for a quarter of a century. It is a challenging combinatorial problem where, we believe, the methods of Ramsey theory of Fraïssé structures could be applied profitably. Another old open question is that of existence of a universally consistent learning algorithm that is at the same time ""smart"", that is, whose average performance improves with the sample size. Again, the problem seems to be essentially combinatorial in nature, and we believe the answer is in the negative due to a Ramsey-type argument. We will apply methods of logic and model theory in order to develop an ""automatic"" way of translating any known result involving an assumption of independent identically distributed random variables to a more general result involving a weaker assumption of exchangeable random variables in the sense of de Finetti. An open problem by Vidyasagar calls for determining the maximal discrepancy of a learning algorithm in a case where it is not zero (which is perhaps more realistic for applications); we will address the problem using the techniques of Talagrand (""witness of irregularity"") and descriptive set theory. Finally, much effort will go to the problem of dimensionality reduction of data to lower dimensions using a rather revolutionary idea of Borel isomorphisms between the domains, as opposed to traditionally used ""nicer"" functions (mostly, Lipschitz).""510792,""Peter, YvesAlain"
"507975"	"Petrenko, Alexandre"	"Distributed Systems Testing"	"The proposed research focuses on a model-based testing framework, called ""Assume-Guarantee-Infer"". As usual for model-based testing, in the framework, the starting point is a model of the expected behaviour of a system under test (SUT). However, differently from the most of the existing approaches, the goal of this research is to consider models which are not necessarily complete and precise. Incompleteness of a model is implied by partiality of knowledge about the expected behaviour of a system under test; while impreciseness may come from optional behaviours or over-abstraction of the actual behaviour which includes infeasible executions and is usually nondeterministic. The tester assumptions about implementation faults in an SUT at hand are then formalized in the form of some fault model. Test execution results may lead the tester to modify either the initial SUT model or the SUT itself if faults are detected. Once the rectified SUT passes all the executed tests the latter can help refine the model of the SUT, i.e., infer a more complete and precise model. A refined model is needed to better document the tested system and to generate if required additional tests increasing the level of the confidence in the quality of the system. This step completes the Assume-Guarantee-Infer testing framework.The significance of the proposed research lies in an increased level of automation of testing activities which can be achieved based on the Assume-Guarantee-Infer testing framework and in a wider acceptance of formal approaches to testing, since the main hurdle in their application, the availability of a complete and precise SUT model can significantly be reduced by allowing incomplete and imprecise models.""515486,""Petric, Anthony"
"508244"	"Pientka, Brigitte"	"Proofware: establishing trustworthy computing through programming with proofs"	"Software systems are an integral part of our infrastructure and our society more and more depends on them: Software monitors medical devices, manages our financial assets, and controls power plants. But as consumers take more and more advantage of online services, they are also becoming more concerned about whether their personal information, such as financial and medical records, are kept safe. If computing is to become truly ubiquitous, we will have to make software systems and services sufficiently trustworthy that people  do not worry about its fallibility or unreliability the way they do today. Proof-carrying architectures for trustworthy computing advocate establishing trust by verifying compliance of the software with a formal safety policy.  However, existing programming environments are inadequate to track and verify complex safety properties  about  programs. This is a major obstacle for this paradigm to become mainstream.  The Proofware project aims to change the way we develop and implement software systems by extending a general  purpose programming language with the ability to directly represent, generate, and manipulate proof certificates.  The objectives are: Design a foundation for certifying programs based on dependent types, build a proof-of-concept programming environment where proofs are seamlessly integrated into programs, and evaluate its effectiveness in two main areas, certified meta-programming and meta-reasoning.  Our goal is twofold: 1) to make it routine work for the  programmer to specify and mechanically verify complex behavioral properties of their programs and ensure that these properties are preserved during compilation. 2) to make it common practice to communicate, exchange, and verify proofs to establish trust and guarantee reliability and safety of software systems and services. ""510071,""Pieper, Jeff"
"514870"	"Pineau, Joelle"	"Regularized Reinforcement Learning"	"One of the key challenges of artificial intelligence is the sequential decision-making problem, whereby anagent must select actions, to maximize a selected outcome. Reinforcement learning provides a richmathematical and algorithmic framework for tackling this problem, yet the deployment of reinforcementlearning methods in practice continues to lag beyond that of other machine learning paradigms. At thescientific level, one of the limiting factors is the lack of attention paid to the problem of overfitting inreinforcement learning. We propose to tackle this problem by developing a rigorous treatment ofregularization methods for reinforcement learning. The usefulness of the methods developed will be investigated in the context of practical sequential decision problems in the fields of health-care and assistive robotics.  The contributions are expected to have a major impact on our ability to deploy reinforcement learning solutions in practical applications, by providing principled tools for effective generalization.""507023,""PinelAlloul, Bernadette"
"514326"	"Pitassi, Toniann"	"New Directions in Proof Complexity and Communication Complexity"	"The central problem in computational complexity is to understand which computational problems can be solved efficiently, and to develop the most efficient algorithms for such problems. The question of whether the NP-complete problems have efficient solutions, known as the P versus NP question, is the driving force behind my research. It is a beautiful and deep question, and its solution would have profound consequences.       One promising direction aimed at ultimately resolving the P versus NP question is propositional proof complexity. The central problem in propositional proof complexity is to understand which tautologies have efficient proofs in standard proof systems. Cook and Reckhow observed in 1974 that the question of whether there is a proof system giving rise to short proofs of all tautologies is equivalent to whether NP equals coNP, and therefore is closely connected to the P versus NP question. Proof complexity also gives a methodology for understanding natural classes of algorithms for solving NP-hard problems.         I propose to develop methods for proving lower bounds for various proof systems and classes of algorithms, and to use these methods to obtain new insight and new lower bounds and inapproximability results for realistic models of computation. A fundamental tool in this endeavor is communication complexity; thus new communication models and techniques are a key aspect of this proposal.""506899,""Pitcher, Tony"
"509421"	"Psaromiligkos, Ioannis"	"Intelligent Signal Processing Algorithms for Future Wireless Communications Systems"	"Wireless communication technologies are at a pivotal point in their development as they move from supporting person-to-person voice communications to enabling person-to-person and machine-to-machine high rate data communications. As a result, the future will see a proliferation of wireless networks of varying architectures that support a multitude of services in a wide range of operating environments. This will be a particularly challenging environment for wireless devices to operate in. The radio spectrum will be flooded with transmissions that each device will have to sift in order to isolate the data intended for it, under strict power and complexity constraints imposed by the small size and low cost of wireless devices. The goal of the proposed research program is to support wireless communications in challenging environments through advances in physical layer design. More specifically, our objective is to develop a framework for the design, analysis and implementation of adaptive receiver algorithms that:1.    )Addresses explicitly the fact that realistic wireless environments are non-stationary due to rapid changes in their physical characteristics and user population and, therefore, severely limit the amount of data available for receiver adaptation. 2.    )Permits low-complexity implementations that satisfy the strict size and power-consumption limitations of mobile devices.""500671,""Psutka, Katie"
"508105"	"Quintero, JoseAlejandro"	"Gestion de l'itinérance des réseaux mobiles hétérogènes de prochaine génération"	"Les technologies de communication sans fil connaissent aujourd'hui un essor remarquable. La quantité de terminaux mobiles et d'équipements de réseautage sans fil disponibles sur le marché témoigne de l'importance de ces technologies dans la vie de tous les jours. Les technologies sans fil répondent à un besoin réel : les usagers veulent être en mesure d'accéder à des informations, des données, à partir de diverses plateformes électroniques, n'importe où et n'importe quand, avec une qualité de service acceptable. Ainsi, les réseaux informatiques actuels, désormais axés vers l'ubiquité, permettent une intégration de différentes technologies d'accès radio avec un réseau dorsal basé sur IP. Les réseaux mobiles hétérogènes de prochaines générations (RMHPG) essaient de répondre à ce besoin, en permettant à l'usager d'accéder à un ensemble riche de services réseau. L'accès continu aux réseaux hétérogènes supportant la connectivité entre entités communicantes de plus en plus intelligentes engendre des impératifs pour  supporter de façon transparente l'intégration de diverses technologies de communication, ainsi que l'accès aux services, indépendamment du périphérique dont dispose l'usager et de sa localisation. Parmi ces problèmes, la gestion de la localisation est un des processus les plus  importants et un des plus complexes dans la gestion de la mobilité, qui permet au système de connaître en tout temps la position courante de l'unité mobile dans un réseau hétérogène. Il faut donc un mécanisme permettant de localiser l'abonné sous la couverture du réseau. L'objectif principal de ce programme de recherche est de développer des modèles pour la planification, le dimensionnement et l'exploitation des réseaux mobiles hétérogènes de prochaines générations, en tenant compte des schémas de gestion de l'itinérance.  Ainsi, nous nous intéresserons aux  modèles pour la conception et planification, aux modèles de gestion de l'itinérance et aux modèles pour la gestion efficace des ressources du réseau dans les systèmes mobiles de prochaines générations. Ces modèles demeurent sujets à des contraintes de qualité de service, de fiabilité, d'interopérabilité et de performance entre autres.""516025,""Quintero, Laura"
"509685"	"Rabbat, Michael"	"Network-Centric Methods for Distributed Machine Learning and Optimization"	"This research proposal aims to develop methods for large-scale distributed machine learning and optimization. Society currently generates tremendous amounts of data across all sectors, from tools enabling scientific discovery (e.g., large hadron collider, Sloan digital sky survey) to online social networks (e.g., Twitter and Facebook). Machine learning, data mining, and pattern recognition methods developed over the past few decades are now used to process such data and build useful models. These methods have already had significant impacts and are being used to detect and mitigate fraudulent financial activity, to search for effective new drugs and pharmaceuticals, to filter spam email, and to target advertising based on user preferences. In order to continue processing data at the accelerated rate it is being gathered, new methods are needed to exploit distributed computing resources such as those available in the cloud. The short-term objectives of this proposal target network-centric issues arising in this setting: how do delays incurred when information is transmitted between nodes in a compute cluster effect performance, how to determine which computers should exchange information to accomplish training as fast as possible, and what information should be exchanged. These objectives will be realized both through theoretical analysis and through the development, implementation, and evaluation of a computational framework based on decentralized asynchronous gossip algorithms. Our scientific approach combines techniques from communication networks, distributed signal processing, networked control, and information theory, together with mathematical tools from optimization theory, graph theory, probability and stochastic processes. The long-term goals of this proposal are to understand and characterize fundamental limits and tradeoffs arising in distributed machine learning and optimization; namely, how fast can training machine learning models be trained on a given dataset and how can machine learning optimally benefit from the use of distributed processing.""504828,""Rabbath, CamilleAlain"
"507879"	"Ramanna, Sheela"	"Discovery of Patterns in Associated Sets: Foundations and Applications"	"The proposed research introduces a computational framework for measuring resemblance between visual information granules. The utilization of associated sets and nearness distance functionals of feature vectors make it possible to design perception-based vision systems that support approximate reasoning (feature vectors provide descriptions of set members). This approach leads to information granule visualisation because such sets facilitate description- as well as location-based event detection in the design of artificial vision systems as well as broad-based technology transfer. The scientific approach of the proposed research is to develop the foundations and applications of associated sets in discerning affinities between descriptions of members of disjoint sets of visual objects. The proposed nearness quantification approach is accomplished via an associated set framework in discerning non-empty sets that are descriptively similar (or dissimilar) relative to the distances between the sets using various distance functions. In other words, we are seeking answers to the question: Is a pair of sets sufficiently near (sufficiently far apart) to be considered similar (or dissimilar)? A direct result of this research is its utility in a number of application areas such as visual surveillance, image authentication, trustworthiness of multimedia websites, product association displays in retailing,  in education, and change detection and tile clustering in remotely sensed images from satellite and airborne camera systems. The novelty of the proposed research is in (i) finding patterns based on nearness (or apartness) of members of associated sets that are compared descriptively, (ii) using various metrics that measure the distance between sets to determine the degree of nearness or apartness of visual sets with high acuity and (iii) introducing feature vector-based distance functions to discern fine-grained nearness to (apartness from) members of associated sets.""506339,""Ramassamy, Charles"
"513409"	"Reed, Bruce"	"The Combinatorics of Complex Networks"	"Transportation Networks, Communications Networks, and Social Networks have all played a crucial role in humanity's progress. While quite different, these various networks  are all sets of objects connected by links. Extracting this common feature leads  to an abstract model which allows us to study the properties these and other  networks share. A  graph consists of a set of objects called  vertices joined by a set of links called edges (formally, each edge is an unordered pair of vertices). If the links have an associated directions (e.g. a web hyperlink leading from one web page to another),  we obtain a directed graph .The simplicity of this model of connectivity makes it widely applicable. When modeling  transportation networks, the nodes may be cities and the edges roads, railway lines, or airline routes. Molecular chemists consider graphs whose nodes are atoms and whose edges are molecular bonds. Self organizing biological networks are also   modeled using graphs. Statisitical physicists use graphs to model crystal formation. In fact  the connections of any physical or conceptual network  can be modelled using a graph. The size of the networks modelled by graphs has grown exponentially. The first problem in  graph theory, concerned  seven bridges  and the four land masses they linked, Graph theorists now study the graph formed by the billions of devices linked by the web and help sequence  the billions of base pairs which occur in the human genome. The study of such large and complex networks is intimately linked to advances in computer science in two ways. Firstly, this area gives rise to many of the networks studied: the web, fibreoptic networks, the connections burned into a computer chip.  More importantly,  sophisticated computers are necessary to model, design, and solve problems with respect to the large and complex networks of the modern era.I develop and apply mathematical machinery which can  handle such large and complex networks. My principal focus is on using local analysis to obtain information about the global properties of the network. ""505780,""Reed, Bruce"
"509401"	"Rouat, Jean"	"Analyse et traitement objet des signaux pour des applications en interfaces homme/machine"	"Je propose de rechercher des représentations, des analyses et des traitements robustes de signaux sonores en tenant compte des connaissances à priori sur la nature des signaux. Je m'intéresse aux structures spécifiques des signaux que je considère comme étant composés d'objets.  Alors que la majorité des techniques actuelles de traitement des signaux sont basées sur un traitement systématique (par exemple, transformées en ondelettes, de Fourier, etc.) indépendant de la nature du signal, je propose d'intégrer dans le traitement, des connaissances à priori sur la structure de ceux-ci. Par cette démarche, il est alors possible de concevoir et réaliser des traitements  (analyse, codage, reconnaissance) qui soient robustes aux bruits, à l'occlusion (masquage), aux interférences sans avoir à connaître à priori la nature de ces bruits.Je propose aussi d'exploiter l'aptitude du cerveau à prédire sur la base de stimulations antérieures, puis à corriger sur la base des stimulations présentes (prédiction/correction).La prise en compte des connaissances à priori sur la nature des signaux permet de mettre en place l'analyse objet des signaux conjointement avec l'implémentation du processus de prédiction/correction, ce qui constitue l'originalité de ce programme de recherche.""501185,""Rouble, Andrew"
"514148"	"Sachdev, Manoj"	"Digital and Memory Circuits in nano-scale CMOS Technologies"	"Realization of robust integrated circuits in state of the art technologies is becoming increasingly difficult. The technology scaling makes transistors susceptible to process variations, and improper functioning of a single transistor may lead to the chip failure containing several million transistors. Often small as possible transistor sizes are used to reduce power, energy consumption, and to increase the packing density.  Unfortunately, transistors with smaller dimensions exhibit a higher susceptibility to process variation. In particular, low-voltage, low-power Static Random Access Memory (SRAM) circuits show higher degree of variation owing to smallest possible transistor dimensions. In this research, we will investigate variability-aware design of digital, SRAM circuits in power and voltage constrained environments. In microprocessors up to 70-80% of transistors are in SRAMs. As a consequence, various aspects of Systems on Chip (SoC) - power, energy, yield, quality, and reliability are influenced by SRAMs. The proposed research has two broad segments - (a) SRAM circuits, and (b) logic circuits. In the first segment, few aspects of SRAMs will be investigated. The key objectives of this research are (i) to lower SRAM power consumption through architectural and circuit innovation. We will design functional SRAMs working at sub-threshold voltages. (ii) To devise circuit techniques to alleviate the impact of process variations on important SRAM blocks such as the sense amplifier. We will investigate new architectures and circuits for the sense amplifier. (iii) Design soft error robust SRAM cells that can recover from single event upsets. In the second segment of the research, we will investigate high speed digital circuits working at the nominal voltage. Research on high speed circuits will entail investigation of high speed logic styles. This work builds on our recent research on Constant Delay (CD) logic style. Building blocks such as adder will be designed to demonstrate effectiveness of new logic families for high-speed applications.  ""507139,""Sacher, Edward"
"514750"	"Sahinalp, Cenk"	"Algorithmic challenges in population scale genomics"	"This proposal aims to address some of the algorithmic challenges offered by data explosion through high throughput sequencing technologies. We aim to bridge the gap between the capacity to generate data, which is growing faster than the Moore's Law, and the ability to manage, store, communicate and analyze it, through novel algorithmic approaches. Typically our algorithmic approaches rely on combinatorial optimization, data structures, approximation algorithms and techniques from combinatorial pattern matching. We will aim to develop novel methods aiming to simultaneously compress and index genome sequence data through re-ordering reads coming from high throughput sequencing platforms on which existing compression and mapping technologies will be applied. This boosting technique for is motivated by the Locally Consistent Parsing technique, which is applied the first time to genomics in this proposal. At the heart of the algorithmic challenges to be addressed in this proposal is the simultaneous discovery of structural variation among multiple related genomes problem. We will develop novel combinatorial formulations and heuristic techniques motivated by approximation algorithms literature for its solution. We will also focus on identifying biologically important structural variants through correlating them with other forms of high throughput biological data such as gene expression and interaction information represented as a network.""512295,""Sahinalp, Cenk"
"513404"	"Saif, Mehrdad"	"""Fundamentals and Applications of Health Monitoring, Diagnostics, and Self Healing in Complex and Networked Engineering Systems"""	"Many complex engineering systems need to perform reliably and efficiently for extended periods of time without any interruption or degredation in their performance. This requirement often stem from  safety, economics, or environmental reasons. Practical examples of such system are abundant. Aerospace, automotive, manufacturing, autonomous space and underwater vehicles and structures, (nuclear) power and communication; chemical process control and allied industries; energy storage and smart grids are example of systems that must perform uninterrupted and within the desired operating range for safety, economic, and environmental reasons. On the other hand, events (e.g. aging) leading to performance degradation, incipient or complete failure of instruments or component are inevitable. As such,  an automated ability to continously monitor the health of the system; detect and rank the severity of any possible fault, as well as isolate, and accommodate for it through self healing, would be highly desirable functions for all aforementioned systems. We propose to study the fundamental challenges in designing the next generation of smart systems that have health monitoring and self healing capabilities integrated along with the more traditional requirements for command, control, and communications.       The applicant has a solid track record of close to thirty years research and collaboration with industry, government, and other university organizations. NASA Lewis (now Glenn), NASA Ames, BC Hydro, Imperial Oil, GM R & D Center, Canadian Space Agency, Danfoss, International Submarine Engineering, University of Alberta, Northeastern University, Concordia University, Northumbria University (UK), ENSI Bourges (France) etc. are such examples. Additionally, many highly qualified students and research personnel have been trained in his lab and are successfully employed in Canada and all over the world. The proposed research meets the needs of many Canadian industries and has a high chance for commercialization, and as importantly, potential for  providing the industry with a supply of highly trained engineers.""517986,""Saikaley, Kyle"
"508350"	"Salem, Kenneth"	"Data Services for Cloud Applications"	"Cloud computing is already having a significant impact on the way that software is delivered and used, on electronic commerce, on web search and information retrieval, and in many other areas.  It promises economic benefits by allowing such services to be offered at lower cost.   It also spurs innovation by lowering barriers to the creation and deployment of new on-line services.Most cloud applications rely on a database.   My work seeks to build better database management for cloud applications, so that those applications can be simpler, faster, and more reliable.  The goal is to design database management services that are highly reliable, that can scale easily to accommodate growing databases and increasing workloads, and that provide a rich set of useful services that make it easy to build powerful and reliable applications.""517998,""Salerno, Anthony"
"509514"	"Salmon, John"	"High efficiency multi-functional power electronics for electric vehicles and grid connected industrial drive systems"	"Most industrial processes use power electronics to draw power from the electrical utility power grid. For instance, industrial electric motor-drive systems are a prominent means for many industrial processes to reduce energy consumption, hence lowering operating costs and contributing to efforts for more environmental friendly electric power systems. In addition, most of the prominent alternative and sustainable energy systems rely on power electronic technology to control and store electric power. The main technical goals of this program are to provide innovations in the design of power electronics used in two main application areas: electric vehicles and industrial electric motor-drives. The size, weight, cost and operational lifetime of electrical power conversion systems are affected by their power conversion efficiency. For instance, a more efficient electrical motor-drive system for electric vehicles can lower the weight and size of the motor and its cooling system: cheaper components and longer operational lifetimes can then result. Thus, a central theme of this research is the creation of new higher efficiency electric power conversion technology. More specific technical goals are to improve the design of the power electronics and their control, for example, innovations are expected in: (a) the systems used to store electrical energy in battery arrays such as used in electric vehicles; (b) motor-drive power electronic topology designs that better exploit the benefits that capacitors and coupled inductors can provide: capacitors are lightweight with a high energy storage density, coupled inductors have a high reliability and long lifetimes.This program will result in commercially feasible technology grounded in their practical application. Academic researchers and HQP will gain knowledge and experience in the design and the operation of industrial drive systems and electric vehicles. Canadian companies will benefit from the HQP and technical innovations generated. The program will contribute to the need of Canadians for a more environmental friendly industrial sector using cost effective energy efficient sustainable energy systems.""499870,""Salmon, Ricardo"
"508224"	"Sampalli, Srinivas"	"An Integrated Security and Reliability Framework for Resource-constrained Wireless Networks"	"There has been a phenomenal growth in wireless technologies and mobile communication systems in the last few years. In particular, two emerging wireless technologies, namely, radio frequency identification (RFID) systems and wireless body area networks have shown enormous potential in the areas of mobile commerce and healthcare, respectively. Convergence of RFID technology and the mobile phone has resulted in potential applications such as contact-less and wallet-less mobile commerce wherein secure payments can be made instantly. Combined with the existing cellular network infrastructure, the possible applications and services are vast. In a similar manner, wireless body area networks can offer tremendous benefits for remote health monitoring and continuous, real-time patient care by recording and transmitting vital physiological data from sensors placed on the human body to monitoring stations.      However, security and reliability are two critical design issues that need to be addressed before such networks can be successfully deployed in sensitive and time-critical applications. Severe resource and computational restrictions imposed by these networks make the use of sophisticated and advanced encryption and reliability algorithms infeasible. The primary objective of this research is to develop a framework for the integration of security and reliability for resource-constrained wireless networks. Towards the realization of this objective, the research will investigate and develop a set of lightweight mechanisms in RFID and body area networks to support wireless services. The proposal aims to unify and extend identity-based cryptography and zero-knowledge proof systems for developing an integrated framework. An end-to-end architecture that integrates an optimal set of security and reliability mechanisms will be developed and evaluated on an experimental test bed for mobile commerce and healthcare applications. The research will provide training for two Ph.D. and fifteen Masters students. In the long run, this research will be a significant step in the design of ubiquitous wireless networks, that are not only secure but also provide guaranteed reliability.""499285,""Sampath, Parthasarathy(Sam)"
"508133"	"Sanità, Laura"	"Routing algorithms and protocols in current and future telecommunication networks"	"Communication networks play a fundamental role in every-day life. In the last few decades, the world has seen a tremendous growth of telecommunication services and infrastructure and an additional increase is forecasted by major telecommunication companies in the coming years. Since modern life crucially relies on telecommunication networks able to route traffic demands quickly and properly, the design and the implementation of effective routing algorithms is a fundamental research area with a high technological and economic impact. How can we use novel algorithmic theory to provide effective and reliable routing of traffic in current and future networks? The investigation of this question, both from a theoretical and from an applied point of view, is the long-term goal of this project.The short-term goals are twofold. On the theoretical side, I plan to develop new complexity and approximation results for hot research topics such as routing problems under traffic demand uncertainty, or single-source unsplittable flow problems. On the applied side, I plan to focus on how to efficiently implement particular routings in networks ruled by fixed routing protocols of high relevance in the telecommunication industry.With this project, I plan to build up a highly visible team that contributes to this field, at the interface of applied mathematics, computer science and telecommunication engineering. Most of the questions and problems addressed in this proposal, although being of fundamental mathematical nature, are motivated by real-world routing problems in telecommunication. Therefore, I expect the results and methods developed in this project to have a direct impact on practical problem solving.""507521,""Sankey, Jack"
"514199"	"Sargent, Edward"	"Inorganic Colloidal Quantum Dot Solar Cells."	"In one hour, enough energy reaches Earth's surface from the sun to meet global energy demand for one year. Solar power is abundant, clean, and free. Today, it is not harvested in a manner that is simultaneously low in cost and high in efficiency.   Our overall research *program* seeks to address this crucial challenge using an innovative materials processing strategy. Colloidal quantum dots (CQDs) are semiconductor nanoparticles synthesized in, and processed from, the solution phase. They absorb the half of the sun's power that lies in the infrared. To date, the vast majority of work on CQD photovoltaics has employed organic ligands to achieve colloidal stability and surface passivation.  The research *project* proposed herein will strike out in a new direction. We have preliminary results showing that inorganic ligands offer great promise in solar cell realization. Indeed, our recent report of an all-inorganic CQD solar cell represents the highest documented CQD performance published to date. The goal of the project is a low-cost, high-efficiency, all-inorganic CQD PV device. We target devices exhibiting > 15% solar power conversion efficiency and operating stably in air.   The project will advance along three thrusts. We will develop models of inorganic chemical passivation of CQDs and of the inner workings of CQD devices. We will develop new materials processing strategies, guided by our models, that accelerate progress in high-mobility CQD films. We will build novel devices, such as multijunction photovoltaics and plasmonically-enhanced solar cells, that use architectural innovation for improved efficiency.   The project will generate scientific insights on novel nanostructured materials; and will lead to solar cell performance advances. It will support directly the training of 7 highly-qualified personnel. The training environment will be interdisciplinary and internationally collaborative and will exploit world-class research infrastructure.""512891,""Sargent, Edward"
"509431"	"Sarunic, Marinko"	"Rapid volumetric and functional imaging using low coherence interferometry"	"The purpose of this research proposal is to investigate novel optical techniques to advance the state of the art for imaging in highly scattering media such as tissue. Specifically, low coherence interferometry based techniques will be investigated for ophthalmic imaging through the following two objectives:Objective 1: Novel rapid volumetric retinal image acquisition: Optical Coherence Tomography (OCT) is a non-contact imaging modality for micrometer scale sub-surface imaging of biological tissue. Despite the increases in acquisition rate achievable with Fourier Domain (FD) detection, volumetric imaging with FDOCT is still hampered by motion artifact. Novel methods in Compressive Sampling will be investigated to decrease the volumetric acquisition time, and to improve interferometric signal acquisition in FDOCT with wavelength swept lasers. Objective 2: High-resolution three-dimensional molecular contrast imaging in vivo:A technological limitation of FDOCT is that the images are purely structural, and do not contain information on the molecular make-up of the sample. The inclusion of molecule specific detection capabilities with FDOCT is important to facilitate functional imaging in biological samples. Depth resolved imaging of fluorescent labels using Fluorescence Coherence Tomography (FCT) will be investigated as an in vivo imaging modality that is complementary to structural imaging with FDOCT.The potential impact of my research program in Biophotonics is to improve imaging technology for clinical ophthalmic diagnosis, and to create novel tools to accelerate vision research in the preclinical development of experimental therapies for vision robbing diseases.""518434,""Sarwal, Amara"
"513698"	"Sawan, Mohamad"	"Smart Brain Interfaces for Diagnostic and Therapeutic Applications : A Multidisciplinary Approach"	"Emerging medical microsystem technologies have the potential to create smart brain interfaces (SBIs) to enhance our understanding of intracortical organization and neural activity underlying cognitive functions and pathologies, and bring neural cell-based diagnostics and therapy closer to reality. An implantable SBI allows recording signals from a large number of cells in the cerebral cortex and monitoring neurotransmitters behavior and displacement among cells. Despite significant progresses in building such biointerfaces, further synergism of innovative techniques are needed for real-time assessment of neurobiological data, reliable diagnostics of dysfunctions and efficient treatment procedures. We focus in this research program on the design, and implementation of biodevices intended to interact with the neural cells, including harvesting energy methods to power up these implants, and other wireless links used to bidirectionally exchange data with specific external base stations. It includes also the microelectrode arrays used to interface the neural tissues. In addition, we are planning to pay special attention to the development of Lab-on-Chip (LoC) devices for in-vitro recording and stimulation of cultured neural cells. This type of biointerface is becoming a necessity avoiding calling upon animal and human to validate achieved SBIs.Priority of proposed biodevices will be given to two main applications: 1) primary visual cortex sensing and stimulation intended to recover vision for the blind, and 2) epileptic seizures onset detection and treatment. These contributions will also enable several medical challenges in restoring neural dysfunctions in aging western population. If successful, such challenging initiative should undoubtedly advance the state of the art in biomedical and neuroscience fields. Counting on this remarkable capacity to innovate, our program is devoted to generate social wealth and contribute to strengthen Canadian hi-tech industry.""512691,""Sawan, Mohamad"
"509508"	"Scardovi, Luca"	"Analysis and control of complex interconnected systems"	"We propose a research effort to analyze and control complex interconnected systems. Characteristics common to complex systems are the large number of interacting elements, the non-linearity of these interactions, and resulting aggregate or emergent phenomena observed within and across multiple scales. Analysis of complex networked systems has been an active field of research across multiple disciplines but unifying principles that enable modeling, prediction, and control of emergent behavior in complex systems are still at an embryonic stage. Our main goal is to develop principles and methodologies for understanding how emergent behavior arises from the interaction of coupled dynamical systems and how to control it. Future applications range from the domain of biological networks to the domain of complex man-made systems and include closed loop control of neuronal synchronization, analysis and control of synthetic biological circuits, coordination in autonomous sensing networks, control of smart grids amongst others.  ""506667,""Scarpella, Enrico"
"514580"	"Schober, Robert"	"Energy-efficient and Secure Wireless Communication"	"In the past, maximizing throughput (data rate) for a given radio frequency (RF) transmit power and minimizing RF transmit power for a given throughput were primarily adopted as design goals for wireless systems. However, recently it has become apparent that maximizing energy efficiency may be a more suitable design criterion and that existing security mechanisms have fundamental limitations, which can be overcome by incorporating security provisions already in the design of the physical layer (PHY) of the communication system. In response to these recent insights, the proposed research program focuses on the design and analysis of novel schemes and algorithms for energy-efficient and secure wireless data transmission. Energy efficient designs have many benefits. For example, they can extend the time of operation of battery-powered devices such as mobile phones or remote sensors. Furthermore, energy savings reduce the operation cost of wireless networks for service providers and the emission of CO2. On the other hand, although effective, traditional cryptographic security relies solely on computational complexity and can potentially be broken. Also, cryptographic methods do not take advantage of the unique properties of wireless channels such as fading and multi-path propagation. These limitations can be overcome by PHY-security schemes. However, energy-efficiency and PHY-security have only been considered in isolation so far and one of the major original contributions of the proposed work is to study both aspects and the associated trade-offs jointly. The long term goal of this research program is to provide theories and technologies that will have a lasting impact on the field and that will be used by other researchers and engineers. In the short term, it is our goal to publish our results in high quality journals and conferences to make them available to a broad audience, and wherever applicable, to transfer the developed technology to Canadian industry. ""512143,""Schober, Robert"
"507909"	"Selouani, SidAhmed"	"Speech recognition and understanding using cloud resources and soft computing in RFID deployment services and online playing games"	"Speech-enabled interfaces are especially useful in mobile devices due to the limited display and keyboard sizes. Despite the steady growth in the adoption of mobile devices in people's daily lives, and the prodigious progress of speech and language technologies, current systems are still a great distance away from providing satisfaction to users. In this program, new research directions are investigated in order to face this challenge. At low-level processing, cooperative approaches combining conventional and soft computing methods are proposed in various speech enhancement schemes to give speech recognition the ability to cope with adverse conditions. Soft computing is inspired by biological systems that are capable of dealing with uncertainty and imprecision to achieve robustness, tractability and optimal solutions. At high-level processing, current research still faces the challenge of giving dialogue systems the ability to perfectly understand the user intents within a few dialogue turns. The strategy consists of leveraging knowledge in distributed ontologies to reduce confusability and ambiguity and to provide a reliable semantic confidence measure through a new concept of ""ontological distance"". Both low-level and high-level processing are integrated in realistic applications by taking advantage of cloud computing and by providing versions for mobile operating systems. The first field of applications consists of allowing human operators to communicate verbally with Radio Frequency IDentification (RFID) devices and information systems. Hands-free speech systems combined with RFID technology are expected to improve the speed and safety of numerous business operations such as monitoring assets and shipping. The second field of applications will focus on online role playing games (RPGs), where an actor is mainly constructed through interactions with non-playing characters (NPCs). Our application will provide the option of interacting verbally with NPCs. Adding this feature will upgrade the gaming experience to a new level of intensity. The new frameworks will also be integrated in speech-enabled interfaces with the aim of enhancing the communication of both French and English-speaking users suffering from various speech disorders.""513832,""Selvadurai, Patrick"
"514704"	"Sheffer, Alla"	"Creation of Detailed Virtual Shapes: Foundations and Tools"	"We live in a three-dimensional world where the shape, or geometry, of objects is one of their major characteristics. Consequently, computer applications that deal with real-life objects, such as computer graphics, engineering, or medical visualization, require tools to create and process 3D shapes.  As the use of 3D data becomes increasingly ubiquitous, users in a variety of industries are becoming more interested in being able to create new personalized complex 3D content; however the geometry processing methods for creation of such content are lagging behind.The general objective of my research is to provide users the tools necessary to perform these tasks as easily and efficiently as possible by developing the underlying fundamental geometry analysis and processing technologies. Motivated by the increased user interest in content creation, in the framework of this discovery grant, I will focus on developing tools for constructing detailed believable virtual shapes and address the fundamental shape processing problems that arise in this context. Due to the diversity of the shapes around us generic shape creation methods are limited in their applicability as they can make very few assumptions about the expected output shapes. My work will therefore explore a more targeted approach, developing a number of content creation mechanisms for specific classed of shapes, utilizing appropriate domain knowledge and other priors to facilitate the process. I will also investigate the commonalities between the different domains leading to formulation and solution of new fundamental geometry analysis and processing problems, solving which will benefit content creation in general. ""501873,""Sheffield, Philip"
"514119"	"Shen, Xuemin(Sherman)"	"Cooperative Wireless Networking"	"With the rapid proliferation and penetration of various wireless networking technologies, future mobile terminals will be equipped with multiple radio interfaces for network access, e.g., popular smart phones already have built-in Wi-Fi interface in addition to regular cellular radio. Therefore, heterogeneous wireless networks should cooperate with each other to deliver broadband services seamlessly in an anywhere, anytime, and anyhow fashion. Wireless cooperative networking will enhance multimedia service experiences of mobile users, improve resource utilization of all the networks, and reduce network energy consumption for green communications. In this research program, we will investigate efficient cooperation strategies by proposing a cross-layer cooperative networking solution for heterogeneous wireless networks.  In specific, we will focus on four fundamental issues: 1) medium access control, which specifies how each user contends for and share available resources with its neighbouring users; 2)  cooperative resource allocation and multi-path routing, which design policies and protocols that will enable multimode devices to seamlessly roaming among different networks; 3) cooperative multi-radio and multi-homed transport layer protocol design to ensure end-to-end information delivery over multi-path routing; and 4) information security support, which enables the reliable  and secure operations of cooperative wireless networks. We will address significant technical challenges due to network heterogeneity, mixed user population, multihop end-to-end transmission, and dynamic network attachment. We will develop theoretical tools to guide the development of wireless cooperative networking and effective incentive mechanisms for cooperation. This cutting-edge research program is expected to generate new knowledge and create new ideas and approaches on how to effectively and efficiently provide ubiquitous, reliable, efficient, and secure multimedia services and applications to mobile users among heterogeneous wireless networks. The research outcomes will also be disseminated through publications in archival journals, premier conference proceedings, and invention disclosures and patents.  ""501202,""Shen, YaoQing"
"507899"	"ShiriVarnaamkhaasti, Nematollaah"	"Parametric Uncertain Database Management Systems"	"Available information in real-life is often incomplete, imprecise, uncertain, and/or possibly inconsistent. To model the real world, we need the ability to represent and manipulate such data with imperfection. This proposal focuses on uncertainty, i.e., data of limited reliability. Intuitively, a piece of data is uncertain if its truth is not established definitely. Uncertainty could modeled simply be a real value in the unit interval [0,1] or as more complex objects, i.e., a collection of values or symbols. Uncertainty has been studied extensively in artificial intelligence and database research, resulting in significant achievements. In the database self assessment reports in 1991 and 2005, uncertainty has been identified as an important challenge in database research: ""Further research [in uncertainty] is essential as we must learn not only to cope with data of limited reliability, but also do so efficiency, with massive amounts of data.""  Our proposal is motivated by this recognition, and stress the need for declarative programming and powerful tools and techniques to support many existing and emerging applications that deal with uncertain data. Our goal is to contribute to theoretical and practical foundations towards the development of a desired database management system (DBMS) with uncertainty, by using, adopting, and extending existing tools, techniques, and results. Recently we have witnessed increased research activities in for building such systems, e.g., Orion (Purdue) and Trio (Stanford), which treat uncertain data as a first-class citizen. What is stressed in our proposal, however, is that such a system has to be made aware of the nature and the kind of uncertainty modeled in the data, captured through the so-called parameters that are carefully selected by a user and includes uncertainty domain and uncertainty combination functions. These parameters will be considered by new query processing and optimization techniques, to be developed in this proposal, in order to carry out the required operations correctly and efficiently. We plan to build our parametric DBMS by extending PostgresSQL -- an object-oriented relational open-source database system. ""510920,""Shirmohammadi, Shervin"
"508126"	"Sled, John"	"Computational Methods for the Analysis of Cerebral Vasculature"	"Interpreting the genome, with the goal of identifying the roles and functions of all the individual genes and regulatory elements that encode the construction and development of the human body, is a major challenge in science that has the potential to completely reshape our understanding of both health and disease.  Experimental mice are playing a central role in the international effort to identify the functions of genes whereby the physical embodiment, in this case a mouse, that corresponds to a particular genetic sequence is created under well-controlled laboratory conditions.   Thousands of strains of mice have been created where the genetic code is minutely varied so as to identify the changes in the mouse that associate with these individual codes.   A particularly powerful way to assess the resulting changes in the mouse is based on three-dimensional images.  Here we propose to develop technology based on 3D imaging to detect subtle changes in the organization of the fine blood vessel network that permeates the brain.  Recent advances in microscopic imaging technology mean that the whole network of blood vessels for a given mouse's brain can be captured in a single very-large 3D image.  The organization of this complex vessel network is thought to play a critical role in how the brain functions.  A challenge for identifying organizational changes in these networks is that some amount of natural variation is observed, even when the genes are kept the same.  The major effort of the present proposal is therefore to develop computer methods that can systematically analyze the 3D images of the brains of different groups of mice so as to isolate those organizational changes that are associated genetic changes.  These tools will provide a completely new way to prise apart the construction plan for the cerebral blood circulation.""509127,""Sleep, Brent"
"508243"	"Stacho, Ladislav"	"Theoretical aspects and algorithms for architectural design and communication in current networks"	"The mathematical structure inherent in problems originated in design and administration of sensor networks has made them central to the theoretical computer science community as they utilize different techniques from areas like graph theory, computational geometry, design theory, and algebraic combinatorics. My goal is to build on my expertise and to answer some of the emerging questions in the area of ad-hoc and sensor networks: I propose to study and better understand the potential of geometric information in computation. The classical theory of graph algorithms was developed for offline models and is not sufficient for many modern applications where we are dealing with huge networks and we have only access to some local part of it at a time. It is more and more apparent that geometric information is useful to handle these settings.High connectivity is a fundamental criteria on communication network and it has been attacked in several research works where a planar subgraph of UDG with higher connectivity was build by adding/deleting edges to/from planar graph/UDG. However, existing results do not give complete understanding of tradeoffs here. Central question, which I would like to attack, is whether for some class of UDGs a similar technique as Gabriel Test or Relative Neighborhood Graph can result in a planar subgraph with higher (2-4) connectivity (vertex or edge). Today we know that for higher than 1-connectivity this cannot be the whole class of UDGs without extra conditions, and perhaps some topological requirements are necessary.Directional antennae are widely being used in wireless networks not only for reducing energy consumption and interference, but also for improving routing efficiency and security. The tradeoffs are studied between number of antennae and transmission range necessary to establish connected network. Existing results only partially address the issue of stretch factor. Also the effect of various technology limiting requirements such as allowed number of antennae at each node or allowed total number of antennae have to be better understood. I plan to attack some of these problems.""507302,""Stadnik, Zbigniew"
"509478"	"Standish, Beau"	"Combined multi-channel Doppler optical coherence tomography imaging and therapeutic delivery system"	"Both cancerous and non-cancerous conditions can vary dramatically from one patient to the next, limiting the overall success rates of treatments designed for the general population. If a biologically relevant feedback mechanism existed during these treatments, clinicians would be given a powerful tool to provide individualized patient care.      One such solution includes the novel imaging modality of optical coherence tomography (OCT), which produces 2D and 3D tissue and microvascular images at the cellular level.  Previous research by the applicant has demonstrated that this technology can in fact quantify treatments and predict therapeutic outcomes. However, the penetration depths of OCT images are limited (~1-3mm) to superficial anatomical locations, and existing single-channel systems can not image the large areas typically targeted as required for treatment. In addition, the matching of the treatment volume to the OCT data sets is of great importance for the accurate real-time quantification of therapies. Therefore, this proposal outlines an innovative approach to use simple cost-effective components to develop a multi-channel OCT system to image and monitor multiple anatomical areas at once. This research also details the creation of a next-generation OCT imaging catheter that permits light-based therapies (e.g., photodynamic therapy, laser thermal therapy) to be housed within the same OCT imaging fiber optic cable. New signal processing techniques will also be investigated to automate and quantify the structural and vascular response to therapy in an effort to predict treatment outcome.      The engineering and scientific study presented in this proposal has the potential to greatly increase the ability to provide accurate treatment dosimetry derived directly from the microstructural and microvascular response to therapies. This new system design and imaging method has widespread clinical potential for predicting treatment response in a multitude of pathologic conditions and anatomical locations such as the brain, liver, prostate, breast, heart and gastrointestinal tract.""519347,""Stanfel, Jennifer"
"508357"	"Stevenson, Suzanne"	"Probabilistic Approaches to Learning the Semantics and Syntax of Words"	"We have billions of words of online text available at our fingertips, yet tools for effectively processing it - going beyond merely finding documents, to truly understanding them - are sorely lacking. A primary obstacle is the inherent flexibility of words. People are continually extending the meaning of words and combining them productively in previously unseen ways. Current natural language processing (NLP) systems, however, rely largely on static lexical resources - electronic dictionaries and ontologies - that are unable to support the needed flexibility and adaptability for understanding text. This proposal focuses on computational methods for learning rich semantic and syntactic knowledge about words, using robust probabilistic representations that capture the flexibility of words and are adaptable to new usages.We approach this problem from two complementary perspectives. First, we ask how it is that very young children learn the complex information about words so effortlessly, and yet such understanding has proven elusive to NLP systems. We develop computational models of child word learning that help us to better understand this uniquely-human ability. Our models play an important role in the scientific study of cognition, by contributing precise, falsifiable theories of human language-learning mechanisms, and also contribute potential algorithms for use in NLP systems. Second, we develop novel techniques for automatically building and extending large-scale lexical resources for NLP. We adapt advanced statistical machine learning techniques to our tasks by incorporating linguistic and cognitive knowledge, enabling us to automatically infer richer information about words that can support more flexible and adaptable NLP tools than is currently possible. We also integrate linguistic and visual information in the processing of multimodal documents to contribute improved algorithms for automatic annotation of images with keywords to support more efficient image search.""516717,""Steward, Jeremy"
"508345"	"Stewart, James"	"Advances in Computer Assisted Mosaic Arthroplasty"	"Cartilage degeneration is a widespread problem which occurs predominantly in the knee, ankle, and shoulder.  For younger, active patients, early cartilage replacement can result in a much improved quality of life and can delay or eliminate the eventual need for prosthetic joint replacement, with substantial savings to the Canadian healthcare system.  Mosaic arthroplasty is a surgical procedure in which plugs of cartilage and bone are transplanted from non-load-bearing areas of the joint to the site of the cartilage defect.  We have shown in previous research that computer-assisted mosaic arthroplasty (CAMA) results in better clinical outcomes than conventional mosaic arthroplasty.  The proposed research, which will train four graduate and three undergraduate students, will develop computer algorithms and user interfaces to improve the delivery of CAMA in three areas:   1. Almost all computer-assisted operations involve a patient CT or MRI scan, followed by a pre-operative planning stage, followed by the operation. We will collapse the scanning and planning stages of CAMA into the operation itself: The surgeon will intraoperatively scan the cartilage surface with a tracked arthroscope probe, will use a planning algorithm to develop a surgical plan, and will execute the plan using computer tracking and guidance.  This can reduce waiting times and can reduce patient exposure to ionizing radiation.   2. For arthroscopic CAMA, we will provide the surgeon with an augmented reality guidance display, where the surgical plan is superimposed on the live arthroscope video, shown on a display beside the patient.  This will reduce the cognitive burden on the surgeon and may make the success of the operation more likely.   3. One method of CAMA tool guidance uses patient-specific guides, which are manufactured on a 3D printer; their bottom side fits tightly to the patient's cartilage surface and their top side contains holes to guide the surgical tools.  We will develop a computer algorithm that designs the guides automatically, encoding the operator's hidden heuristic knowledge and saving substantial operator time over the current manual method.""502131,""Stewart, James"
"508341"	"Suen, Ching"	"Error Reduction in Handwriting Recognition"	"The objective of this program is to conduct advanced research in computer recognition of handwritten characters, words, and symbols. Handwriting is one of the most challenging subjects in the field of pattern recognition due to the infinite varieties of character shapes and qualities. This research aims to increase the intelligence and capability of computers so that they can read, at high speed and with great accuracy, the information written on various types of documents. These documents include bank cheques, utility and payment slips, envelopes, income tax returns, customs declarations, archival documents, and other business forms where billions of dollars are being spent each week to manually enter the handwritten data from the documents into the computer. Based on many years of experience in (a) playing a lead role in this field and constantly interacting with prominent scientists and industrial researchers around the world, and (b) having guided many graduate students, post-doctoral fellows, and visiting scientists, the applicant proposes the following program:(1) Enhance the reliability of our advanced handwriting recognition systems by selecting dynamic sets of features for various stages of multi-stage recognition systems, and to discover additional distinctive features and vital parts of individual characters to produce the most reliable and the best system in theworld for recognizing handwritten data.(2) Make use of the above results to develop verifiers and cascades of complementary classifiers which can identify confusing character/word shapes and minimize costly substitution errors, so that the newlydeveloped system can be used in practice to save billions of dollars and a huge amount of manpower.The scientific merits include discoveries in handwriting recognition and improvement in techniques to optimize character/word recognition schemes. The long-term goal is to produce a new breed of recognition systems which will outperform or at least be comparable to humans.""519926,""Suen, PuiYan"
"509479"	"Suleiman, Wael"	"Toward a New Approach for Human-Robot Interaction"	"One remarkable event in the 21th century is the arise of service robotics. The service robots are designed and built for doing a specified work for humans. These robots include the autonomous, mobile or bipedal professional service robots, which are modern robotic platforms equipped with the most advanced sensors, actuators and processing capacities. This new class of robots counts as a new and promising technology that will work side by side with people, collaborating with employees on tasks.   MY MAIN OBJECTIVE in the current proposal is making the COOPERATIVE  human-robot interaction (HRI) tasks more safe and intuitive. The question of safety issue in HRI is one of the most challenging problems in robotics. Besides the capability of robots to understand and interpret human intentions is another difficulty which is still poorly understood. The HRI has been recognized as a multidisciplinary scientific field in the mid of the 20th century. Nevertheless, the cooperative tasks involving human beings and robots are still limited to few simple tasks. This is partially because of the lack of control algorithms and motion planning strategies that are able to achieve these cooperative tasks appropriately.  In the scope of my research program, I plan to develop a human safety model, and an interpreter of human intentions in the context cooperative HRI tasks. These models will be based on the modern motion planning approaches, advanced numerical optimization techniques and human practical reasoning theory. The ultimate goal is to execute the previous models on a real robotic platform interacting cooperatively and, importantly, safely with human beings.  The expected results will have valuable impact on HRI and service robotics fields, as well as on other scientific fields, e.g. cognitive science, neuroscience, human-interface design, etc. The future of ""co-worker robots"" in fields as diverse as the military, medical services, agriculture, rescue and health-care IS REALLY NEAR.""508687,""Sulem, Catherine"
"509531"	"Szczecinski, Leszek"	"HARQ protocols for cooperative networks"	"The objective of this research program is to increase our understanding of the fundamental principles, limitations, and trade-offs related to the deployment of the so-called HARQ protocols in the collaborative communications networks. The collaboration between the terminals in the network is seen as a way to increase the coverage, the rate, and the reliability of the communication in the wireless environment. It usually takes the form of relaying, where the terminals help each other to convey information to their respective destinations. In the wireless context, the transmission errors are unavoidable so it is necessary to consider retransmission via the so-called hybrid automatic repeat request (HARQ) protocol. While it is a well-known mechanism in point-to-point communications, deploying HARQ in the context of collaborative network is not trivial because the number of retransmissions possibilities is significantly increased due to the presence of many terminals. Another dimension to the problem is added since the relays may use the so-called network-coding and thus it is important to elucidate and analyze how to combine it efficiently with the relay-based HARQ. Our goal is to find the most convenient cooperative transmission schemes, which is possible only when we are able to optimize the rate and/or power of the HARQ retransmission rounds. We will also consider the HARQ parameters' adaptation strategies, which rely on the information being exchanged between the communication terminals. Since, in practice, this information may be subject to errors, we will analyze the effects caused by these errors and propose mechanisms to deal with these undesirable effects. Developing the methods to efficiently optimize and characterize the collaborative HARQ is crucial to use them and integrate these transmission strategies into high-level optimization of the network communications.""504270,""Szczepina, Monica"
"508114"	"Tahvildari, Ladan"	"A Context-Aware Software Adaptation Framework"	"There is an increasing demand for software systems that dynamically adapt their behavior at runtime in response to changes. Research into these systems has been conducted by researchers from two communities with two different emphases: self-adaptivity and context-awareness. On one hand, research in context-awareness is more concerned with how to model, process, and manage the context information but limited on how a system adapts itself in response to unanticipated changes in the context information. On the other hand, research in self-adaptivity is more about how to adapt the system's structure and/or behavior in response to requirements and/or context changes (possibly unanticipated changes), with less attention to how context is modeled, processed, and managed. In practice, the line between the two is rather blurred, and the engineering of a context-aware adaptive system needs to consider both aspects in a systematic manner. The objective of this research project is to investigate, design, and develop a novel framework to identify the requirements of considering context-awareness and self-adaptivity in an integrated manner. First, the framework provides a novel context modeling technique that captures all the system environment aspects and supports later adaptation and evolution during the software execution; second it maintains multiple system models (i.e. the structure, behavior and quality models) and their consistency at runtime, and enables system runtime evolution; and third it provides adaptation mechanisms that can cope with unanticipated environment changes using the system-context relationships.We believe that the benefit of this project to the targeted sectors of the economy is twofold. First, it provides a superior context-aware framework that will help to develop and implement sophisticated algorithms and methods for system runtime adaptivity. Second, we believe the particular modeling techniques we aim at developing for adaptive applications will help industrial companies to effectively employ methods so that the adaptive systems can be kept up-to-date during their evolution and maintenance.""506356,""Tai, TzeChun"
"508134"	"Tang, Anthony"	"Collaboration with personal devices in digital workrooms"	"Collaboration with personal devices in digital workrooms.Digital workrooms are multi-display ecologies comprising shared devices like interactive walls and tabletops, as well as personal devices such as tablets, laptops and PDAs. While rich with possibility, we do not yet know how people will work together collaboratively in these rooms. Collaborative work is generally comprised of both shared work (e.g. discussion) and independent activity, where an individual temporarily disengages with the group to complete a task. The problem is that if transitioning between independent and group tasks is cumbersome, it breaks the flow of activity. In traditional workrooms, with paper-based media, these transitions are achieved through the semantic division of labour across different surfaces and people. Yet, how will this occur in digital workrooms?  In the proposed research, I explore the integration of personal devices in digital workrooms as a mechanism for supporting these transitions. I will develop a theoretical design rationale for how interaction techniques can manage and coordinate these transitions through both spatial and semantic distribution of tasks. I will do this by pursuing three objectives: first, I will design, develop and evaluate mechanisms for moving information between devices that allow people to fluidly transition between shared and independent tasks in digital workrooms; second, I will explore how space can be exploited as a resource for coordinating activity across shared and personal devices in digital workrooms, and finally, I will study how digital workrooms are appropriated over long-term use as a means of providing design insight for the first objective.  This work is critical for near-future computing. Large-scale digital workrooms are rapidly becoming adopted in business, education and research. They are important, because they allow large groups with diverse expertise to tackle difficult problems (e.g. construction, data analysis, etc.) together. The research in this proposal will help inform the design of technologies to support meaningful, natural activity in these digital workrooms.""497415,""Tang, Boxin"
"509432"	"Tang, Shuo"	"""Multimodal Optical Imaging Endoscopy for Noninvasive """"Optical Biopsy"""" in Tissues"""	"Lung cancer is the foremost cause of cancer death in Canada. In 2011, lung cancer is estimated to account for 11,300 of cancer deaths in men and 9,300 in women. Early detection and treatment has the potential to greatly improve these statistics. The current gold standard of cancer diagnosis is tissue biopsy followed by histopathology. In this procedure, clinicians take multiple tissue biopsies from suspicious areas and perform histopathology examination. This procedure is invasive and time consuming. ""Optical biopsy"" is a new type of tool which uses optical imaging to examine subcellular tissue morphology in vivo. It can provide the same imaging resolution as histopathology but the procedure is noninvasive and in real-time. Therefore, ""optical biopsy"" can potentially have a huge impact on the diagnosis and treatment of cancer and other diseases. We propose to develop a multimodal optical imaging endoscopy for noninvasive ""optical biopsy"" of the lung. The endoscope will combined optical coherence tomography (OCT) and multiphoton microscopy (MPM). OCT provides cross-sectional anatomical imaging of layered tissue structures over a few millimeters deep and wide. It detects backscattered light from refractive index discontinuities. MPM images cells and extracellular matrix in turbid tissues with subcellular resolution. MPM provides functional information about biochemical specificity of collagen, elastin, NADH, and flavins. When developed into a single endoscope, MPM/OCT can provide multiple contrasts and multiscale field-of-views. The proposal will develop endoscopic MPM/OCT using optical fibers, miniature scanners, and micro-optics. The design will be optimized for simultaneous large field-of-view OCT and high resolution MPM imaging. The endoscope will be capable to quickly scan over a large tissue area to find abnormal regions with OCT, and perform a high-resolution zoom-in imaging on the regions of interest to identify the abnormalities with MPM. The development from this project will serve as an important tool for diagnosing and evaluating the effect of treatment of major respiratory diseases. ""518109,""Tang, SunnyHoKwong"
"507894"	"Trappenberg, Thomas"	"Integrated learning systems that anticipate"	"Machines are often inflexible since they can not learn. My research goal is to understand and to build learning systems. Learning systems that we want to understand better includes biological organisms, in particular humans, where learning is an essential part of cognitive functions, and also machines, where learning can provide flexible solutions to new problems. The more specific research goal outlined in this proposal is to build learning system that can better guide decisions in complex environments. The system will integrate three major factors of learning, learning about sequences instead of static information more common in current systems, learning hierarchical representations of the environment to provide an efficient way of using such information, and combine such hierarchical temporal memories with learning that is guided by reward. The system will be applied to autonomous robots with the aim to search, identify and map objects in unknown territories. An example application is the identification of artificial objects under water from sonar data gathered by autonomous underwater vehicles (AUVs).There has been many new algorithms to learn from the environment in various circumstances. This research aims to integrate systems that are flexible enough to work in real world environments, such as household robots or robots in the health care system rather than manufacturing applications that are typically more controlled. The hybrid systems proposed here are aimed at learning about the environment to find novel solutions that have not been programed explicitly into the robots. This research program has the potential to advance our understanding of complex learning systems that operate in natural environments.The research is based on a combination of statistical methods, algorithmic intelligence, robotics applications and understanding of biological systems. Learning methods are widely applicable and are and excellent area for educating HQP ""518383,""Traves, Joanne"
"508245"	"Tzanetakis, George"	"Time Information for Multimedia Processing and Analysis"	"The amount of multimedia data available digitally is growing daily at an astounding rate. One of the main challenges in multimedia research is to develop algorithms to extract information from these vast collections of multimedia signals and provide more effective tools for interacting with them. Using a variety of techniques from digital signal processing and machine learning it is now possible to analyze millions of images, videos and songs. Another recent trend is the use of rich expressive ways of interacting with computers moving beyond the traditional screen/keyboard/mouse paradigm. These include gestures on touchscreens as well as full body tracking interfaces based on a variety of sensors such as accelerometers, microphone arrays and depth cameras using structured light. There are few examples of work that combine large scale analysis of multimedia archives controlled by rich multi-modal input. Interactive music performances that combine sensors, actuators, computers and humans are one of the few areas where such synergies are taking place. Building such systems is challenging as not only it requires interdisciplinary expertise but the majority of existing programming environments address only some of the issues that arise. They also lack generic abstractions for expressing time and probabilities which are fundamental in building such interactive systems. Our main objective is to formalize, design and implement better tools, languages and frameworks for building interactive multimedia tools that involve both large-scale machine learning and rich user input. Modeling time is fundamental to interactive multimedia and our goal is to explore how time can become more explicit and part of the abstractions used to program. We plan to elevate concepts from statistics, machine learning, signal processing and data-flow programming from arbitrary conventions in a software library to first class primitives in programming practice. ""512462,""Tzanetakis, George"
"514744"	"Veksler, Olga"	"Energy Minimization Approach to Pixel Labeling Problems in Computer Vision"	"The goal of computer vision is to automatically analyze visual data, such as images and video. Computer vision systems are used nowadays for a wide range of applications: medical image processing, robot navigation, automotive safety, special effects in movies, image search, etc. Despite tremendous progress, there is still a large gap between what a human and a computer vision system can do.A framework that has proven very effective for a variety of vision problems is pixel labeling. It is usually addressed with energy minimization. The major difficulty of this framework is its computational cost, with many energies arising in practice being difficult to minimize. The energies that are currently minimized well are essentially those that encode low-level smoothness properties on a labeling, namely, that most adjacent pixel pairs should to have the same or similar labels. However, in a variety of vision tasks, other properties, such as preference for a particular subset of label configurations between pixel pairs, or constraints on pixel subsets of size larger than two arise. Energies with these more general constraints are still difficult to minimize. My  goal is to develop efficient energy minimization algorithms for these harder-to-optimize pixel labeling problems, and to improve performance of practical applications utilizing minimization algorithms. My research is heavily based on discrete optimization algorithms, in particular on dynamic programming and graph cuts.The proposed applications for evaluating the minimization methods that I will develop are cancerous gland segmentation in prostate images for volume measuring and prognosis and shape priors for image segmentation. More generally, the algorithms that I plan to develop will lead to an improved performance for a wider range of applications, such as object recognition, medical image segmentation, motion correspondence, 3D modeling, etc. ""498624,""Veldhuis, Stephen"
"508237"	"Verbrugge, Clark"	"Optimizations and Designs for Practical Improvements to Program Performance on Multicore Systems"	"This research aims at improving the utilization of modern, multicore computers.  Although these are now commonplace, the techniques required to develop software for such systems are still very complex, and achieving both correctness and efficiency are continuing concerns.  Two approaches are used to address this issue.  The first furthers work on a specific form of automatic parallelization: ""speculative multithreading"".  This technique is applied internally by the runtime system to make use of multiple processors without programmer or user intervention.  The result is a program that executes faster by using any available multiple cores.  This has the advantage that the result applies to existing, legacy single-threaded programs, and does not require sophisticated parallel programming skills, although performance is not always the best possible.  The second approach aims at improving support for newer programming languages and programming concepts that reduce some of the more subtle complexity found in existing, popular parallel programming languages.  It is possible to achieve good efficiency with programming languages that provide access to low-level parallel programming constructs, but this also introduces correctness problems in the potential for ""data-races.""  These kinds of bugs can be prevented through the use of new language designs and idioms, but the best approach is still unclear, and current implementation designs tend to result in trade-offs with program performance.  The research work thus focuses on optimizing specific implementation designs, and developing new programming language paradigms that further reduce the complexity encountered by the programmer.  Both of our main approaches target improvements in feasible and efficient use of parallel systems, and by addressing the core problem from two fundamental directions we expect to make significant improvements to current practice.""504317,""VerduzcoGomez, AdrianaRebeca"
"514707"	"Vetta, Adrian"	"Computation in Games and Networks"	"The aim of this proposal is to provide quantitative analyses of decision making, where decision makers may be single authorities, such as a network operator attempting to minimize traffic congestion, or participants in multi-agent systems such as auctions or social networks.In practice all decision makers are computationally bounded. Thus, for multi-agent systems, we need to know how computational considerations affect game-play; despite its widespread applicability and importance, there is currently little understanding with respect to this question. Furthermore, an agent's choice of action may be heavily influenced by mechanistic restrictions within the system, for example, the actual pricing mechanism used within a market or the actual routing protocol used within a network. Consequently, this proposal focuses on the implications of computational and mechanistic constraints on decision making, with a particular emphasis on applications related to networks.Thus, at a high level, we are interested in the following type of question. Can we build a cheap but functionally effective network? Can we design a market mechanism that leads to high social welfare? What are the dynamics and quality of outcomes produced by specific game-playing strategies (in particular, strategies that agents actually use rather than those prescribed by theoretical behavioural assumptions)? What are the effects of an intervention into a system? Clearly, to model and answer such questions we require tools from a wide range of areas. Towards this end, we utilize techniques from algorithm design and analysis, optimization, game theory, behavioural economics, graph theory, and causal inference.""514569,""Vetterli, Michel"
"509396"	"Viarouge, Philippe"	"Conception de nouvelles chaînes de conversion électromécanique pour aérogénérateurs et hydrogénérateurs de grande puissance"	"Le projet de recherche concerne la conception de nouvelles chaînes de conversion électromécanique pour les aérogénérateurs, les hydrogénérateurs et les hydroliennes de grande puissance. Dans ce domaine, les concepteurs sont à la recherche des chaînes de conversion les mieux adaptées aux contraintes et aux objectifs de performance des divers cahiers des charges d'aérogénérateurs et d'hydrogénérateurs dont la puissance unitaire ne cesse d'augmenter. Le marché mondial de ces systèmes qui concerne plusieurs types de constructeurs (machines électriques, multiplicateurs à engrenage, électronique de puissance) offre des débouchés importants pour les industriels. Les objectifs de la recherche consistent à mettre au point une méthodologie de conception globale des diverses chaînes de conversion électromécanique (MADA et/ou Machine Synchrone à rotor bobiné ou à rotor à aimants permanents avec des multiplicateurs de vitesse mécanique et des convertisseurs statiques adaptés), à l'implanter sous forme d'un outil de CAO efficace et versatile et à l'utiliser pour étudier divers scénarios de conception concernant les cahiers des charges des aérogénérateurs et les hydrogénérateurs de grande puissance. Plusieurs topologies concurrentes actuellement utilisées se distinguent par des compromis structurels différents au niveau du type de machine électrique (Machine Asynchrone à Double Alimentation (MADA) ou Machine Synchrone), du multiplicateur de vitesse mécanique (avec un nombre d'étages et une taille plus ou moins importants)  et des convertisseurs statiques d'électronique de puissance. Ces compromis structurels mènent à des caractéristiques dimensionnelles, à des coûts et à des performances différentes des diverses composantes de la chaîne de conversion. Il serait difficile d'affirmer qu'il existe une topologie qui supplante toutes les autres dans tous les domaines d'application des aérogénérateurs par exemple, aussi bien en terme de puissance unitaire qu'en terme d'utilisation sur terre ou en mer. Le concepteur de la chaîne de conversion doit donc développer une approche méthodologique générique et globale permettant de comparer les diverses solutions topologiques pour les divers cahiers de charges.""507619,""Viau, André"
"509540"	"Vorobyov, Sergiy"	"""Phased-MIMO radar: Transmit beamspace, transmit-receive beamforming, parameter estimation, and applications"""	"The desire for more and more advanced radar technologies is dictated by radar's ubiquitous applicability, ranging from micro-scale radars applied in biomedical engineering to macro-scale radars used in radioastronomy. Modern radar technology processes ever-increasing quantities of information and offers high spatial resolution. However, future radar applications will stretch radar systems beyond their fundamental sensitivity and information limits. For example, current radar technology offers little to counter stealth technology as it is fundamentally inhibited by energy transmitted and received. The prime goal of the proposed research program is to develop and advance a new radar paradigm which combines the advantages of the recent radar technique known under the name multiple-input multiple-output (MIMO) radar (main advantage is waveform diversity) with the advantage of traditional phased-array radar (coherent processing). The use of MIMO radar with colocated antennas enables improving spatial resolution, increasing the maximum number of detectable targets, improving parameter identifiability, virtually extending the array aperture, and enhancing the radar flexibility. However, these advantages come at the price of losing coherent processing gain.Coherent processing gain can be achieved by designing a transmit beamspace transform to form beam(s) towards certain direction(s) in space. In parallel, multiple waveforms can be transmitted per each beam to form a MIMO radar. Such paradigm enables to achieve significant improvements over the existing radar technology. Some particular objectives that will be addressed as a part of HQP training are: developing phased-MIMO radar joint transmit/receive beamforming; developing optimal signaling techniques for phased-MIMO radar; designing competitive techniques based on the tradeoff between resolution and robustness against beam-shape loss; finding efficient implementations for phased-MIMO radar by exploiting signal and antenna array sparsity; designing array processing techniques such as direction of arrival, time delay, channel and other parameter estimation for phased-MIMO radar, and finding their efficient implementations by exploiting sparsity.""507627,""Voroney, Paul"
"509412"	"Wamkeue, René"	"Détection et diagnostic des défauts des systèmes de production d'énergie éolienne"	"Du fait des politiques énergétiques résolument orientées vers la limitation des effets de serre et principalement menées par le Québec et l'Ontario, la production de l'énergie éolienne au Canada connaît une croissance remarquable. Elle est passée de moins de 100MW en 1997 à plus 4000MW en 2011. La tendance est la même sur le plan mondial où la puissance totale installée est plus de 120GW. Cependant, plusieurs centrales éoliennes sont encore mal entretenues. En effet, l'industrie éolienne canadienne pratique plus souvent la maintenance réactive (réparer quand c'est briser). Pourtant les éoliennes sont exposées aux conditions climatiques très rudes et variables, notamment : des météos contraignantes allant du calme aux vents violents, la chaleur tropicale, la foudre, le froid sibérien, la pluie, la grêle et la neige. Ces conditions extrêmes et variables augmentent les contraintes mécaniques sur la structure de l'éolienne entrainant des vibrations, des risques de résonnances torsionnelles, de nombreuses pannes des équipements de production, et quelquefois le bris complet de l'éolienne. Au Canada, il devient impératif pour l'industrie éolienne de se doter des méthodes avancées de maintenance prédictive qui permettront de détecter rapidement les éventuels risques de défaillance et de les corriger rapidement avant la détérioration complète de l'équipement.     Les outils proposés par ce programme de recherche permettront de déceler (détecter) et de localiser les dégradations fonctionnelles de l'éolienne le plus rapidement possible avant qu'elles n'introduisent une panne grave. Les techniques développées contribueront à : réduire les défauts et les pannes dans les éoliennes, à optimiser les techniques de maintenance actuelle, à accroître la disponibilité et la durée de vie du dispositif de production en limitant les temps d'arrêt, à réduire les coûts d'exploitation, à augmenter la production en maximisant les profits. Le programme de recherche met à la disposition des industries éoliennes du pays des méthodes modernes et futuristes de maintenance qui leur permettront de moderniser les interventions en cas de défaillance et d'augmenter la rentabilité de leurs installations.""508188,""Wan, Justin"
"508118"	"Wang, Lingyu"	"A Vulnerability-Centric Approach to Network Security Metrics"	"As today's critical infrastructures and enterprises increasingly rely on networked computer systems, the security of such systems becomes crucial to the economy and society. However, before we can improve the security of a network, it is desirable to be able to measure it, since ""you cannot improve what you cannot measure"". A network security metric is desirable since it will allow for a direct measurement of how secure a network currently is, and how secure it would be after introducing new security mechanisms or configuration changes. Such a capability will make the effort of network hardening a science rather than an art. Emerging efforts on network security metrics, including the Common Vulnerability Scoring System (CVSS-SIG) standard, typically assign numeric scores to vulnerabilities as their relative exploitability or likelihood. The assignment is usually based on known facts about each vulnerability (e.g., whether it requires an authenticated user account). Such approaches share several limitations. First, by considering vulnerabilities on an individual basis, a network security administrator could be misled in a situation where individual vulnerabilities scores are low but these vulnerabilities can be combined to compromise a critical resource. Second, the methodology is no longer applicable when considering zero day vulnerabilities about which we have no prior knowledge or experience, which in fact leads to a major criticism of existing efforts on security metrics, that is, unknown zero day vulnerabilities are not measurable. Third, the numerical scores assigned to vulnerabilities usually lack a well defined semantic, and are not generally related to other measures that can be easily interpreted by human analysts, such as time or dollars. The proposed research will address these pressing issues by proposing a novel vulnerability-centric approach to quantitatively modeling vulnerability information through security metrics.""497412,""Wang, Liqun"
"508110"	"Wassyng, Alan"	"Structuring Assurance Cases to Facilitate Effective Certification of Medical Devices"	"Software is an enabling technology so good that there is now almost no new device/technology on the market that does not depend on software in some way. As software and devices become increasingly complex and safety features, relying more and more on software, get further intertwined with functional features, the chance of creating serious disasters also dramatically increases. This is especially true in the medical domain where there is tremendous pressure to get new devices onto the market in the hope of saving lives and/or improving quality of life, but where those same devices can cause immense harm if they are not safe for use.  Regulatory regimes for software usually rely on checking that an approved development process was used, and then infer from that the software will not contribute to a device failure. Predictably this has not worked well. Safety Cases have been mandatory in several regulatory domains in the UK for many years. Safety cases force manufacturers to provide evidence related to the manufactured product. We are starting to see assurance/safety cases assuming a more prominent role in regulatory regimes in North America. This is especially true in the approach taken by the US FDA in the way they regulate medical devices. Assurance cases are predicated on assumptions that promote satisfactory confidence in the efficacy and safety of the device. These assumptions relate to fundamental issues that have not yet been addressed. I intend to reverse engineer standards to discover the implicit assurance cases embodied therein. I also plan to compare approaches to argumentation from philosophy and formal methods to help create a means by which supporting arguments can be evaluated. My research focuses on determining: i) how to construct and evaluate claims and sub-claims which contribute to the overall dependability and safety of the system; ii) how to evaluate the soundness of arguments in the context of satisfying claims; iii) how to combine evidence of different types to contribute to the confidence that a claim is satisfied; and iv) how to organize the assurance case using a prescribed structure, which will facilitate certifiers being able to build experience in evaluating assurance cases.""514745,""Wasteneys, Geoffrey"
"508100"	"Wildes, Richard"	"Early Representation and Analysis of Visual Spacetime"	"The research that my collaborators and I carry out is in the general area of computer vision, the attempt to endow machines with a sense of sight. From a theoretical point of view this is an important endeavor as it bears on fundamental issues in complex information processing, and may yield results that bear on our understanding of visual information processing in natural systems (e.g., humans), as well. From a practical point of view, machines that can see have the potential to interact with humans in a much more natural and useful fashion than is currently the case. Our particular studies of visual information provide insights into the fundamental properties of temporal sequences of image (e.g., video) and extend our technological capabilities by indicating novel applications. We use mathematical techniques to analyze images, develop processing algorithms based on our analyses and carry out empirical tests to evaluate their performance. This allows us to ask basic questions about temporal sequences of images, a pervasive source of information about our surround world. Questions of interest include: How can complex streams of video be parsed into meaningful components (e.g., persistent structures and moving objects)? How can multiple views of a captured scene be used to reconstruct a three-dimensional model of the scene? This research contributes to Canada in three important ways. First, it advances the discipline of computer vision as we expose basic properties of image information and develop corresponding algorithms for incorporation into vision machines; this allows Canada to compete successfully in a critical area of science and technology. Second, it provides students with hands on experience and skills in vital areas of investigation, computer vision and, more generally, computer science. Students go on to make careers in science, technology, teaching and elsewhere, taking with them the enrichment of having been involved in research. Third, it paves the way for novel ways to practically exploit image data. Applications of interest include video processing for everyday concerns ranging from the internet to personal handheld devices, video enhancement so that content is more readily interpretable and intelligent processing modules for robots.""507178,""Wilds, Christopher"
"514013"	"Wilton, Steven"	"New Architectures and CAD Algorithms for Field-Programmable Logic Fabrics"	"A Field-Programmable Gate Array (FPGA) is an integrated circuit that can be programmed to implement virtually any digital circuit.  FPGA's are widely used in applications such as telecommunications, bioinformatics, visualization systems, and signal processing.  An FPGA's programmability provides users the ability to implement large, complex circuits without requiring access to an expensive state-of-the-art chip manufacturing plant, an expense that is out-of-reach for most small and medium-sized electronics companies.   The costs of manufacturing integrated circuits is growing so dramatically that, in the near future, only the large processor, graphics engine, system-on-chip and FPGA companies will be able to build leading-edge devices.  For everyone else, programmable logic will provide the only option for building an integrated circuit using leading-edge processes.  This will lead to the era of ubiquitous programmable logic, where generic programmable logic devices will be the substrate upon which virtually all applications are built.  This research described in this proposal seeks new architectures, CAD algorithms, and design techniques that will make these large ubiquitous programmable devices viable.  Straightforward scaling of current FPGA techniques to larger chips will not suffice, primarily due to design complexity and designer productivity issues.  Specifically, this research focuses on three subprojects, each of which addresses designer productivity: (1) the design of FPGA architectures that are more amenable to fast CAD tools, (2) the integration of debugging circuitry into an FPGA device, and (3) a methodology for the rapid design and optimization of very large reconfigurable fabrics.   Together, these subprojects will significantly improve the productivity both of designers using programmable devices, and of the developers of the programmable devices themselves.  As a result, the benefits of programmability will be available to a much broader design community than ever before.""516493,""Wiltshire, Benjamin"
"509617"	"Wong, Alexander"	"Multiscale Monte Carlo methods for multiparametric biomedical image processing and analysis"	"Recent advancements in multi-parametric imaging have contributed significantly to the development of improved disease diagnosis protocols.   However, given the quantity and complexities of such multiparametric imaging data, along with image quality tradeoffs associated with maintaining reasonable acquisition times, it is often difficult for research scientists and clinicians to interpret and analyze the acquired data in a meaningful and efficient fashion. The main objectives of the proposed research program is to develop multiscale Monte Carlo methods for multiparametric biomedical image processing and analysis, and to incorporate such methods into a clinically-viable platform for use by radiologists, clinicians, and clinical researchers.  Three important issues will be investigated: 1) Image modeling, 2) Image processing, and 3) Image analysis.  Each of these issues will be investigated within an efficient multiscale Markov-chain Monte Carlo (MCMC) framework.The proposed research can have a significant impact in the area of multiparametric biomedical image processing and analysis. The image processing algorithms being developed have the potential to significantly improve resolution, contrast, and signal-to-noise ratios in imaging data. Such methods would have significant effect on clinical imaging devices, as it allows for overall faster acquisition times while maintaining image quality, as well as reduced radiation dosage in radiographic imaging technologies to improve patient safety. Furthermore, the image analysis methods being researched have the potential to significantly improve both the speed and accuracy of quantitative analysis of multiparametric imaging data, which can greatly aid clinicians in disease diagnosis and research scientists in better understanding the causes and effects of disease.  The developed technologies will be transferred directly into the healthcare industry through active collaborations with health institutions such as Sunnybrook Health Sciences Centre, and companies such as Agfa Healthcare and Tornado Medical Systems.""515944,""Wong, Alicia"
"508014"	"Wong, Bernard"	"Performance Isolation in Shared Cloud-based Storage Systems"	"Cloud computing represents an exciting new opportunity for businesses to free themselves from building and managing their own computing infrastructure. Business needs in computation and online hosting can be met by simply purchasing resources from Cloud providers. The rate of Cloud adoption, however, has been throttled by the inability of Cloud providers to offer performance guarantees that meet customer requirements. The culprits to this problem are also core tenets of Cloud computing: resource sharing and oversubscription. Cloud providers share each server's resources among multiple Cloud computing services from different businesses; this improves resource utilization and reduces the cost of hosting services. Unfortunately, such sharing can lead to inconsistent and unpredictable performance if there is significant resource contention.      The goal of this research program is to facilitate the growth of Cloud computing by improving the state of performance isolation in the Cloud. Strong performance isolation that limits the impact of a service's action on other services competing for the same resources allows Cloud providers to provide consistent and predictable performance. Past work has focused on using virtualization to provide isolation between services contending for resources from a single machine. However, it is often the performance of the underlying Cloud storage systems deployed across thousands of machines that determines the performance characteristics of Cloud-based services. This research program will tackle the performance isolation problem in Cloud storage systems by: identifying the root causes of contention through extensive measurement studies, investigating the impact of different design decisions on performance and contention, exploring innovative solutions to improve performance isolation, and designing and implementing a new Cloud storage system that provides strong performance isolation between services. Overall, by addressing a critical barrier to Cloud adoption through improved performance isolation, this work can have a profound impact on the growing and still volatile Cloud computing industry.""500542,""Wong, Cathy"
"513707"	"Wu, Ke"	"Research and Development of Substrate Integrated Circuits and Systems for Gigahertz and Terahertz Electronics and Photonics"	"The proposed research aims to investigate and explore an emerging and disruptive technology called ""substrate integrated circuits (SICs)"" towards the development of millimetre-wave and terahertz integrated circuits (MTICs). This concept of SICs was pioneered by the applicant over a decade ago. Substrate integrated waveguides (SIWs), which belong to the family of SICs, have been widely studied and successfully explored by researchers around the world, under the leadership of the applicant, for the development of a new generation microwave and millimetre-wave integrated circuits and systems. The fundamental feature of SICs is that almost all non-planar guided-wave structures can be synthesized and transformed into planar form at low cost, thereby making full integrations of such structures possible. Therefore, both planar and non-planar structures can be seamlessly integrated and made of the same processing techniques. This building platform holds the promise of becoming the next generation integrated circuits for millimetre-wave and terahertz systems and unifying the physical mechanisms of electronics and photonics. This research allows the applicant to continue leading international research efforts in discovering new circuit features, new design rules, new innovative devices, and new application areas. This will involve unconventional, multilayered substrates and mixed dielectric-metallic waveguides. Fundamental guided-wave properties will be investigated. Novel theoretical techniques will be studied and developed, and experimental prototypes will be made and demonstrated for antennas, circuits and systems. This project will foster substantial interests of this new technology for future millimetre-wave and terahertz system development towards complete hybrid and monolithic system-on-substrate integration and miniaturization. The applicant, his students and research associates are eager to make a comprehensive characterization and quick development of this revolutionary technology. This research is strategically important for future explorations and applications in the field of information and communication technology. It will allow the applicant to further expand one of the most visible Canadian research programs in the world.""512756,""Wu, Ke"
"509351"	"Yang, Cungang"	"Design and Modeling of Secure Wireless Mesh Networks"	"This proposal focuses on the design and modeling of secure wireless mesh networks(WMNs). The IEEE WMNs standard, 802.11s, is not mature enough to address security concerns of WMNs. To enhance the security of 802.11s WMNs, the first goal of this proposed research is to investigate methodologies of security and efficient techniques so as to make a single-operator WMNs secure and resilient to attacks. In this study, a generic security package will be studied to support fast authentication and exposure-resilient key management. A role-based access control model (RBAC) will be designed to support clients' authorization. A heterogeneity-aware framework for group key management will be studied for the single-operator WMNs. Future large-scale WMNs are expected to comprise numerous mesh networks, each administered by an independent network operator. Different network operators may collaborate to provide continuous connectivity to roaming users for supporting seamless services, such as voice-over-IP, e-conference, e-news, and online games. The second goal of the proposed research is to investigate methodologies of security techniques so as to make roaming in multi-operators WMNs seamless, secure, and efficient. To achieve this research objective, a new trust model is to be developed so that different mesh networks do not require establishing the roaming agreements and clients are no longer bound to any specific network operator. Instead, a client acquires a ticket from a third-party broker whereby to realize seamless roaming across mesh networks administrated by different operators. A secure and fast inter-operator handover scheme will be developed to support client's seamless crossing the boundary of mesh domain boundaries. A general role-based access control will be studied to ensure the authorization of mesh clients. An efficient billing protocol will be developed to foster node cooperation. This study, particularly the consideration and implementation of the fast handoff mechanism, and new trust- and role-based access control models, will be used as the direction and guidance for us to develop a systematic design methodology and to model wireless mesh networks for various applications.""498036,""Yang, Daniel"
"507900"	"Yao, JingTao"	"Machine Learning Methods for Multi-criteria Decision Making"	"Multi-criteria decision making remains an active and challenging research topic.  The proposed research program aims to utilize machine learning methods, such as rough sets and granular computing, in rule induction for complex decision making.  In particular, I will examine three-way decision making, especially when multi-criteria and multi-agent are involved.I will first study issues related to three-way decision making. With rough sets, rules are extracted and classified into three categories called the positive, negative and the boundary rules. However, in many cases, we may not be able to make necessary and timely decisions because there are too many uncertain decision rules. In addition, we should consider both the accuracy of prediction rules and the consequences of applying these rules in real applications. A possible solution is to lower expectations for accuracy by converting uncertain decision rules to certain decision rules. As accuracy is sacrificed, unacceptable consequences may occur in some cases.  Game theory will be used to resolve such dilemmas by locating balanced positions that meet the needs of generalization and accuracy. Mechanisms to find equilibriums or thresholds will be examined.I will then examine real applications where multiple agents or multiple measures are involved in decision making.   In these situations, multiple positive, negative and boundary regions result from using three-way decision methods. A balanced or consensus decision needs to be made.  Traditionally, such decisions were made using majority, committee, or unanimous decisions. A combination of granular computing and game theory will be used to reach an intelligent consensus. Possible applications of this method are feature selection for text categorization and Web-based decision support systems.In summary, I will study the possibility of building hybrid intelligent systems that assist humans to make informative and wise decisions on complex problems involving multiple criteria and multiple agents. It is expected that this will broaden our knowledge of decision support mechanisms.""516032,""Yao, JohnnyJinNan"
"509402"	"Yevick, David"	"Techniques for Error Analysis and Measurement in Optical and Wireless Communication Systems"	"The primary goal of our work is to reduce significantly the time required to model large-scale commercial optical and wireless communication systems by developing or applying advanced physical and numerical procedures.  In particular, we are primarily concerned with different implementations of the multicanonical procedure, which we were the first to extend to communications systems.   This technique both simplifies and refines importance sampling in that it greatly enhances the numerical sampling probability of statistically unlikely states associated with physically interesting events such as system outages.  While a large body of literature has developed since our initial studies, few detailed analyses of commercial systems have been undertaken.  Accordingly, we will investigate coherent wavelength-division-multiplexed coherent communication systems, high-speed electronic compensators, complex modulation formats and error-correcting-codes incorporating the effects of optical nonlinearities and polarization mode loss and dispersion.  We will then apply suitably adapted versions of our techniques to the optimization of networks of coherent, compensated high-speed optical systems as well as to realistic models of wireless communication systems and networks.  Here we will also extend our work on reformulating the time evolution of a system in terms of matrices derived from the multicanonical procedure.  Specifically, we will further develop our previous description involving multidimensional transition matrices in order to predict the outage behavior of optical and wireless systems.  In parallel with our multicanonical studies we will pursue our ongoing work on the application of a comprehensive mathematical model of the frequency dependent behavior of the polarization based on Lie algebraic techniques to high-speed polarization measurement procedures.  Finally, we will continue to develop and examine theoretical procedures for inferring properties of an optical fiber system from the measured polarization behavior at the end of the fiber link. ""518515,""Yew, Samuel"
"514636"	"Yu, Wei"	"Interference Mitigation and Network Topology Optimization for Cooperative Wireless Cellular Systems"	"The rapid and unprecedented growth of smart phones, tablets and laptops is driving an exponential demand for higher capacity, increased reliability and ubiquitous coverage in next-generation wireless cellular networks. The proposed research program will focus on innovative technologies that will power this continuing wireless revolution. The proposed study is based on a central observation that the future exponential growth in wireless networks will need to be enabled by novel cooperative transmission strategies across the network, and by the optimization of network topologies themselves, rather than by improvements of traditional wireless link-level techniques. This re-thinking of network design is necessitated by the fact that base-stations in future networks will become more densely deployed, frequencies more aggressively reused, and the network topologies increasingly heterogeneous. In such an environment, multiuser interference will become a critical bottleneck for service provisioning, and the management and mitigation of interference using cooperative techniques will become a central task.     The goal of this proposal is to develop innovative techniques based on network information theory and stochastic geometry in the design, analysis and optimization of interference management and mitigation methods for future wireless networks. The proposal will focus on two novel network topologies that will have transformative effects on the wireless industry: the network multiple-input multiple-output (network MIMO) architecture where base-stations cooperate in transmitting data to and receiving data from the subscribers, and the heterogeneous topologies where remote radio units are deployed throughout the network to enhance coverage. The proposed research will advance the state of the art in the theoretical capacity analysis of wireless networks, and will have impact in guiding design philosophy, standards development, and forward evolution path of future-generation networks.""512145,""Yu, Wei"
"508117"	"Yu, Xiaohui"	"Supporting Keyword Search over Structured Data"	"Enabling users to access databases using simple keywords can relieve them from the trouble of mastering a structured query language and understanding complex and possibly fast evolving database schemas. Although keyword search technology has matured in the Web arena, supporting keyword search over structured data, such as the data stored in databases and data warehouses, presents unique challenges. When performing keyword search over structured data, the results are no longer existing Web pages, but ""virtual documents"" composed by assembling the keyword-matching tuples from (potentially) different tables. Correspondingly, the space that must be explored during search becomes much larger than that encountered in Web search. The long-term objective of this research is to enable keyword search as an efficient and effective means of database navigation and exploration. To this end, the proposed research will focus on three topics: improving the quality of search results, incorporating domain knowledge into keyword query processing, and supporting keyword-driven data analytics. We expect to propose a series of novel models and algorithms that would improve the functionality, effectiveness, and efficiency of keyword search over the state-of-the-art methods. The results are expected to find applications in a wide spectrum of scenarios, such as business intelligence and e-health.""499912,""Yu, Xiaoli"
"508097"	"Zaïane, Osmar"	"Bolstering pattern mining by taking into account relationships and uncertainty in data"	"By and large, data stored in databases are considered facts and observations that took place. These records are also typically considered mutually independent. For instance, a hospital database contains information about patients that are independent from each other and a record could state a visit at a given date for a particular diagnostic for which a given treatment is given. These are all observed facts. Data mining and analysis tasks assume this independence between records and consider the recorded facts to be certain. However, in many real applications some relationships exist between observations and are known and thus could be recorded. The fact that a person is a friend of another person, or a family member, or the fact that a phone number calls another telephone number creates a relationship between records. These are information networks. Ignoring these relationships in data analysis is a missed opportunity to get better insights about the data. Also, uncertainties in data exist  and can come from the measurement instruments such as sensors or any confidence level attached to the source of information. For instance, the temperature measured by a sensor may not be certain but assumed in a range between a minimum and a maximum; the identity of a car observed at an intersection may only be ascertained with some level of confidence. These are probabilistic data because the value of an attribute could be affixed with a probability level. Uncertainty can also be ascribed to relationships in data to form probabilistic information networks. Unfortunately, most existing data mining approaches assume independence and certainty of data. Very little work has been done on effectively analyzing probabilistic databases or probabilistic information networks to discover  useful new knowledge or patterns in such data collections that takes into account the uncertainties and the relationships.  The purpose of this proposal is to work on devising effective and efficient techniques to mine and learn from probabilistic databases and probabilistic information networks, and show the use and relevance of these techniques in real domains applications where uncertainty is germane to the data collection, such as in automated information extraction from textual data.""504487,""Zaidi, SyedSameenAli"
"508247"	"Zeh, Norbert"	"Algorithms and data structures for memory hierarchies"	"In an increasing number of application areas, massive amounts of data are collected and analyzed to extract the information needed for strategic business decisions and to make new scientific discoveries.  Examples include Walmart's petabyte-sized customer transaction database, a massive index of web pages powering Google's search engine, the collection and analysis of large amounts of genome data made available through modern sequencing technologies, and the construction of massive, highly detailed geographic models using LIDAR technology and satellite imagery.  These massive amounts of data present tremendous opportunities for new discoveries that were previously unimaginable, but extracting useful information from such data sets requires computational tools that are capable of processing them.    The proposed research will lead to algorithms that form the heart of such tools, namely algorithms that make effective use of cache memory to speed up the processing of data sets beyond the size of main memory.  It focuses on fundamental graph problems and problems in computational geometry that arise in a wide range of application domains and aims to develop sequential and parallel algorithms with high cache efficiency for the studied problems.  The aim of the proposal is to gain fundamental theoretical insights into how to design cache-efficient sequential and parallel algorithms, but it also places an equally strong emphasis on engineering efficient implementations of the developed algorithms, in order to verify their practical usefulness and make them available to practitioners.""512097,""Zeh, Norbert"
"509409"	"Zelek, John"	"Visual Perception of 3D Worlds for Haptic Exploration"	"We live in a very visual world; we rely chiefly on our eyes to tell us where we are and what is around us and then make decisions of where to go and how to get there based on that sensed information. Even traffic signs and indicators rely on us using our eyes. GPS (Global Positioning Sensors) have revolutionized personal navigation by providing us with another sensor for telling us where we are.  Unfortunately a GPS requires a prior map as well as clear sight of at least 3 GPS satellites in order to triangulate our position.  What if we had no map of our environment to begin with or we could not rely on a GPS to tell us where we were?  A camera is a sensor that closely resembles our eyes.    Visual SLAM (Simultaneous Localization And Mapping) is the problem of building a map and localizing at the same time without a prior map by only using a camera.  Visual SLAM is a relatively new research area that has shown great promise but is hindered by the problem of robustness and being computational tractable.  Current methods rely on single feature points which accumulate exponentially, especially in real world environments.  We are proposing to use real world objects (or clusters of features) as the landmarks as opposed to feature points.  We have already demonstrated that triads of feature points can lead to better object detection & recognition.  Using these triads of points or the objects themselves can lead to meaningful compact SLAM maps and address current shortcomings.  We also propose other  improvements to visual SLAM that come from  techniques used in Structure From Motion (SFM) techniques such as filtering or error modelling.Visual SLAM can be used as input for  assistive navigation devices for people who are cognitively or perceptually challenged. One way to accomplish this is to convey the perceived visual world represented as a compact map in a haptic modality; as touch is an under-utilized primitive modality. Potential users include people with Alzheimer's and people who are blind.    Other vertical  applications of this technology include military soldiers, first responders, cognitive automobiles, augmented reality and recreational users.""496930,""Zelenitsky, Darla"
"507983"	"Zhang, Huajie"	"Transfer learning for Bayesian networks"	"Transfer learning addresses the problem of how to learn a model for a domain (target domain) by utilizing an existing model from another different but related domain (source domain). The key issue is how to adapt the existing knowledge in source domain to fit target domain. Thus the representation of domain knowledge plays an important role. A Bayesian network (BN) is an explicit graphical representation of domain knowledge. It is natural to perform manipulations on a BN for source domain (source BN) to obtain a new BN for target domain (target BN). Therefore, BNs are suitable in the scenario of transfer learning. On the other hand, although BNs have been intensively studied for decades, learning BNs still remains quite challenging, especially when training data is scarce. The transfer learning approach can provide an effective way to overcome this issue in learning BNs.This proposed research will systematically study the transfer learning methods for BNs. We will first examine the suitability of the existing transfer learning approaches for the task of learning BNs, such as instance transfer, feature representation transfer, parameter transfer, etc. Then novel transfer learning methods that fit the task of learning BNs will be studied and developed. In addition, effective transfer learning algorithms for BNs, including both structure learning and parameter learning, will be studied. The evaluation of learning algorithms in this research is based on experiments on the commonly used benchmark data sets. The existing research in this field is in an early stage. The novelty and significance of this research are reflected in the following aspects: (1). BNs have not been well noticed in the scenario of transfer learning. The proposed research will demonstrate BNs as an effective transfer learning approach. (2). Structure learning is still a challenging issue in learning BNs. This research will provide a new approach to alleviate the structure learning issue, and thus develop more effective and efficient BN learning algorithms. (3). This research is expected to produce effective and efficient novel methods and algorithms for real-world data mining applications. ""509114,""Zhang, Hui"
"509509"	"Zhang, Yunyan"	"Novel Image Texture Analysis for Advanced Structure Characterization"	"Novel Image Texture Analysis for Advanced Structure CharacterizationMy research seeks to develop, extend and validate novel signal processing and analysis algorithms for advanced structure characterization. Currently I am developing new techniques to improve the evaluation of tissue properties from magnetic resonance imaging (MRI) of biological subjects. However, my approaches have broader applications in diverse fields including geoscience, physics, and engineering.The Fourier transform forms the cornerstone of many imaging modalities including MRI and computer tomography (CT). The MR data are initially acquired in the frequency domain and are then reconstructed using the inverse Fourier transform. Therefore, MR images can be processed using advanced, Fourier-based analysis methods to enhance identification of tissue structure without loss of information.Over the past few years I have been engaged in developing and evaluation of advanced methods to detect subtle alterations in the local pattern of image intensity, namely, MR 'texture'. Texture patterns are generally too subtle to be perceived even by trained observers, thus mathematical algorithms are required. My pilot data demonstrate texture analysis to be a sensitive measure of tissue injury and repair.In this proposal, I aim to develop new avenues of texture analysis to enhance health outcomes of Canadians. In particular, I propose to investigate texture spectral imaging and texture anisotropy to improve the sensitivity and specificity of texture analysis. Output from this project may advance our capacity to evaluate tissue structure without modifying existing imaging protocols.""509804,""Zhang, Ze"
"508233"	"ZincirHeywood, Nur"	"Mission Aware Data Analysis"	"The objective of this research program is to develop techniques and tools for analyzing the flow of data in a heterogeneous information technology (IT) environment for mission assurance. The challenge is to detect / identify interesting phenomenon so that the mission / operation of an organization continues even under fault or attack conditions. That is to say, IT in itself only represents a collection of entities, which when deployed, may or may not act to support mission assurance goals. Only through suitable engineering practices can supporting systems and techniques be deployed to ensure that IT collectively functions in support of mission assurance. In such a case the IT infrastructure is said to be mission aware. Given the current landscape of IT infrastructures, IT computing is hinting at a future in which cloud computing and client computing (computing that is not on the cloud) coexist and evolve hand in hand with the increase of data-intensive applications. As the scale and complexity increases, it becomes more and more difficult for administrators and managers to analyze the state of a system because of the continuously evolving nature of these systems. Hence, if we want mission aware systems, the management systems and techniques underpinning today's IT systems need to monitor, analyze and adapt dynamically and predictably to rapid changes in system workload, or environment while remaining true to their objectives. Thus, the purpose of this research program is to automatically monitor and analyze the flow of data on an organization's IT system as it moves from one layer to another. In doing so, we aim to discover interesting phenomenon that might assist us to predict faults, performance changes or simply unknown/new behaviour to manage heterogeneous systems. In order to study these issues, we will employ a data-driven approach where machine-learning techniques will be used, given that they have the potential to generalize solutions to a wider context. In the long term, this new vision provides autonomous and mission aware systems for commercial, governmental and military needs.""510966,""Zingg, David"
"509624"	"Zourob, Mohammed"	"Integrated Biosensors and BioMEMS for Bio-Detection Applications"	"The objective of our research program is to generate novel technologies towards integrated sensor systems which will lead to the development of general-purpose, field-usable sensors with ultra-high sensitivity and rapidity. Our biosensing platforms include sensitive high-throughput optical micro-resonator screening array and optical waveguide sensors. These novel biosensing platforms will employ a wide variety of existing and emerging recognition elements including bacteriophages, phage-display, aptamers and molecularly imprinted polymers. We also focus on integrating the biosensors within lab-on-a-chip systems, which allow automatic sample processing (a tedious step prior to the actual analysis) with high level of biocontainment and minimal handling. Fully integrated biosensing platforms offer ultra-sensitive and reliable multi-analyte detection; their miniature size (a single portable device with disposable sample processing chip) allows easy field deployment and ubiquitous applicability. Our biosensors and lab-on-a-chip systems are suitable for use in environmental monitoring, diagnostic and therapeutic applications at and outside point-of-care. The proposed five-year research program is divided into three themes: (A) Development of integrated ultrasonic lab-on-a-chip for rapid biological sample manipulation and detection, (B) Development of optical highthroughput micro-resonator screening platform (lab-on-a-bead), and (C) Development of phage-based theranostic (therapeutics and diagnostics) platforms. Our research provides multidisciplinary training for highly qualified personnel and will generate significant technological know-how, creating a positive impact on Canadian R&D and economy.""505044,""Zsaki, Attila"
