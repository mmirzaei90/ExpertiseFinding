"id"	"researcher_name"	"application_title"	"application_summary"
"525483"	"Abidi, SyedSibteRaza"	"An Agile Semantic Web Platform for Knowledge-Centric Decision Support"	"Knowledge-based decision support systems provide situation-specific strategic recommendations to assist users to (a) make sound decisions in complex situations to achieve quality outcomes; and (b) plan a course of actions to achieve optimal efficiency. The objective of this research program is to develop knowledge management tools to help the computerization of paper-based knowledge so that one can derive knowledge-based decision support. The knowledge management tools are to be applied to the domain of healthcare which is facing a number of knowledge gaps in the delivery of healthcare. The intent of the project is to computerize clinical guidelines to develop disease-specific decision support systems for healthcare practitioners so that they are able to provide evidence-based care services.  The project will help enhance our ability to dynamically update and interconnect knowledge resources to offer up-to-date and 'complete' knowledge that in operation will have a profound impact on the awareness, response, competitiveness and readiness of individuals, industry and institutions. From the healthcare perspective, the research directly contributes to the provision of evidence-based chronic disease management.""525484,""Bisaillon, Martin"
"525497"	"Abolmaesumi, Purang"	"Intuitive Guidance for Intervention and Diagnosis with Enhanced Ultrasound (i-Guide-U)"	"Spinal needle injections are widely applied for anesthetic purposes: for surgery, for the relief of back pain, and for epidurals in obstetrics. More than two million North Americans undergo these procedures each year. However, about 60,000 patients per year experience complications associated with inaccurate placement of the needle. These complications have three consequences: (1) an increase in pain, anxiety and the length of hospital stay, (2) a small number of patients will suffer temporary or permanent neurologic deficit, and (3) the perceived risk inhibits more widespread use of spinal needle injections. To improve patient outcomes, several ultrasound-guided techniques have been proposed for more accurate placement of the needle. However, anesthesiologists experience poor quality depiction of anatomical features and difficulty in interpreting images, preventing ultrasound from becoming the standard-of-care.My proposal introduces intuitive guidance for intervention and diagnosis with enhanced ultrasound (i-Guide-U) technologies that augment computer models derived from other imaging modalities to improve the quality of ultrasound images and enhance interpretations. For the first time, it combines medical image registration and ultrasound tissue typing to dramatically improve ultrasound image formation quality based on the knowledge of the underlying anatomy. These methods will be integrated and evaluated in the context of spinal needle injection for improved visibility of critical anatomical structures and targeting. This program will train two PhD, five MSc and five undergraduate students in leading edge biomedical engineering research.""525498,""Boudreau, François"
"526339"	"Aboulnaga, Ashraf"	"Systems and Algorithms for Big Data Analytics"	"Analyzing massive data sets to extract knowledge and gain insights has become a cornerstone activity in business and science. This area of research is known as Big Data analytics, and it has become a fertile ground for research and innovation. Businesses now own petabyte-scale data warehouses that are accessed on a regular basis to perform complex analytics, and scientific discovery in many fields has also come to rely on sophisticated analysis of Big Data. The importance of Big Data is illustrated by the April 2012 announcement that the US government will invest US$200 million in research activities in this area. Canada will similarly benefit from research into this area, and it is in Canada's interest to be at the forefront of such research.          The long-term vision of my research program is to develop a powerful and easy-to-use platform for Big Data analytics, plus a suite of algorithms for different application areas that run on this platform. To realize this vision, this grant application proposes research to improve the manageability and tuning of Big Data analytics platforms, focusing initially on Hadoop, one of the most popular systems for Big Data analytics. On the algorithms side, I will develop algorithms for graph and matrix problems, two types of problems that arise frequently in scientific computing.          The proposed research will advance the state of the art in an important and highly competitive area. The research will raise Canada's profile as a technical leader and produce solutions that are useful to Canadian industry. The requested funds will directly support the training of several students. Providing highly qualified personnel in the area of Big Data analytics is very important since there is a significant shortage of skills in this area. For example, the McKinsey Global Institute forecasts that the US will face a shortage of 140,000 to 190,000 employees with Big Data skills by 2018. Canada will likely face a similar shortage. Thus, the proposed research will provide highly qualified personnel with skills of great value in different sectors of the Canadian economy, and will address a significant anticipated shortage in the Canadian labour market.""526340,""Biddle, Robert"
"524085"	"Affes, Sofiène"	"Enabling Radio Access Strategies for Pervasive and Smart Applications of Wireless in the New Digital Economy"	"Wireless communications lie today at the very heart of many key applications that strongly impact most aspects of our daily life. Fundamental paradigm shifts are yet expected due to ongoing developments in wireless. These paradigm shifts shall allow communication infrastructure to self-organize seamlessly for optimized spectrum-, cost- and energy-efficiency. In the new realm of the internet of things (IoT), they shall also enable communication links to take place not only between persons, but also between smart machines (such as home appliances), devices and vehicles (M2M/D2D/V2V), connections due to reach tens of billions by 2020. These paradigm shifts are poised to enable truly smart communications in the future.  Parallel developments in wireless due to successful integration with new sensor and radar systems have recently sprang the emergence of new smart monitoring applications in health, environment, safety, security, etc. And cross-fertilization between smart communications and monitoring allow wireless to stand today as a crucial enabler of future applications in smart management of power grids, vehicles, homes, cities, etc. Due to their increasing pervasiveness, these three smart applications of wireless (communications, monitoring and management) are deeply shaping today the 21st-century digital economy. The  short-term objective of this research program is to tackle the transformative challenges smart communications will shortly face at the radio access level in its current shift from installation (communications for communications) to deployment (communications to the service of new applications). In order to meet the expected data crunch, we seek to devise novel radio access strategies that enable truly smart communications by rethinking some of the key technical drivers foreseen today within the context of HetNet virtualization at both PHY and MAC layers. Virtualization is a fundamental paradigm shift that is pledged to push these strategies to a full-fledged scale of flexibility, manageability and efficiency in cost, spectrum and power. By achieving this immediate goal, the long-term objective of this program is to enable pervasive and smart applications of wireless in the 21st century digital economy.""524086,""Masmoudi, Radhouane"
"533251"	"Agamy, Mohammed"	"Distributed DC-DC Converter Architectures for Renewable Energy Harvesting"	"Distributed DC architectures provide several added benefits to renewable energy harvesting systems. Among these benefits are the higher energy yield, increased system reliability  and power throughput, provide better system monitoring and diagnostic capabilities as well as more flexible system design. High frequency power conversion has been limited to low power applications. However, with the advances in electronic, magnetic and dielectric materials, there is a prime opportunity to establish new limits of operation for these converters. This program will focus on developing novel modular high frequency power converters for renewable energy harvesting applications with specific focus on wind and solar applications. Since high efficiency in addition to high power density are key design requirements, the application of resonant power converters as well as using silicon carbide diodes and/or switches will be investigated along with the design requirements imposed on the medium/high frequency transformers for these applications. Converter fault resilience, as well as energy storage requirements for ride-through support will also be investigated. Multiple interconnected converters in the DC collection grid provide more control flexibility, where converter modulations can be coordinated to optimize overall power conversion system efficiency and dynamic response. The key novelty and main outcome of this program is will be: (i) The development of a family of new modular high voltage high frequency power converters that can be easily installed up-tower in wind turbines. (ii) System level analysis for the integration of a large number of power converters connected in series and/or parallel and the effect of these interconnections on system operation. (iii) Architectures for large-scale solar plants that are  directly tied to medium voltage grid without the need for bulky line frequency transformers.The proposed research will give Canadian industry a competitive edge in modular DC architectures, which can be adopted in different applications, such as renewable energy harvesting, electrification of transportation systems, sub-sea oil and gas and radar power supplies.""533252,""Foster, Stuart"
"520417"	"Ahmadi, Majid"	"Low Power, Area Efficient, High Speed Algorithms and Architectures for Computer Arithmetic, Pattern Recognition and Cryptosystems"	"The objectives of the proposed research are:(a)To further extend the state-of-the-art for the Finite Field Multiplication in F(2**m) for applications to elliptic curve cryptography. We will look at implementation for the proposed architectures as ASIC at the transistor level to reduce power consumption and to improve speed.  We will also look at GPU configurations as a paradigm for implementation of our proposed architectures.(b)To further extend the state-of-the-art for the design, implementation and application of arithmetic circuits using the Continuous Valued Number System (CVNS).The application of this number system for multi-layer neural networks will be investigated. A thorough evaluation of the resulting next-generation circuit designs and their comparison with current state-of-the-art binary designs will be carried out.(c)To further develop robust and low error rate algorithms for pattern recognition applications. Specifically we are interested in applications such as human face recognition using new feature extractors and classifiers.  We are looking into developing algorithms for face recognition under occlusion, varying or poor illumination and low resolution. We will be looking at the optimization of these algorithms for implementation on both GPU, for high speed operation, and low-power architectures. We shall further explore the development of new algorithms for face recognition with 3-D images. The development of algorithms and architectures that can support a face recognition capability on cellular smart phones that can be used to optimize functionality, security and power management based on visual cues will also be investigated.This research is considered significant as further progress in a number of diverse application areas beneficial to Canada is contingent upon developing competitive low-cost, low-power, high-speed area efficient DSP algorithms and architectures that enable synergy.""520418,""Cherney, Edward"
"522959"	"Aïmeur, Esma"	"Learning in the age of exposure"	"We live in an age of exposure, in which it is considered normal for a vast segment of the population to casually disclose their lives on social media, sometimes with disastrous long-term consequences. However, privacy-protection concerns should not hamper the benefits that a society of sharing can bring, a delicate balance indeed. Can we capitalize on this situation in order to foster a better e-learning environment? In this research program, we propose to address the various privacy, publicness and ethical issues inherent to e-learning in a platform that includes a social media environment. Our first purpose is to design a framework in which students as well as instructors are guided through a process that will help them understand and define their privacy and publicness needs and then build a community of trustworthy co-learners who share interest in their subjects of study. This will allow us to implement and validate a platform that corresponds to this design. Throughout the process, students will be protected against potential malicious actions (such as illegitimate access to personal data and tampering with such data), not only from their peers and the outside world, but also from malicious course instructors. Conversely, students are prevented from illegitimately tampering with their own records. We propose to develop two agents called contact manager to help a student through the entire learning process and vigilance manager to help the instructor keep careful watch for possible dangers and misbehaviour. This system sifts through social media such as Facebook, Twitter and LinkedIn in order to build and maintain a growing community of users eager to learn and share in various fields of study. A personalized environment of co-learners is distilled for each individual student, initially under the suggestion of the contact manager but increasingly under the personal control of the student as he progresses not only through his study but also in his awareness of privacy, publicness and ethical issues. Similarly, the vigilance manager help the instructor manage his contacts with the students in his course, the other students in the platform, and the other instructors. This program will culminate with the realization of a proof-of-concept implementation.""522960,""Anctil, François"
"525062"	"Alhajj, Reda"	"Effective and Efficient Data Analysis Techniques for Emerging Data Intensive Applications"	"Recent developments in the technology from handheld devices to sensors to surveillance to microarrays to the wide availability of the Web (e.g., social media, social networking, internet of things, click stream, etc.) allow for electronically capturing, collecting and maintaining huge volumes of structured and unstructured data leading to big data repositories. Example data sources include logs and transactions related to information retrieval via Web access, to e-commerce such as click streams and retail purchases, on-going events such as weather and stock market status, social interactions such as social media and social networking, monitoring systems such as traffic control, fraud detection, and homeland security, and so on. Such data is recognized as a valuable resource for knowledge discovery leading to effective decision making. Yet this can happen only if these large volumes of data can be processed and analyzed effectively, which is increasingly problematic to do by conventional means. Sometimes the data could be skewed where the number of data instances is small compared to the huge number of features like having hundreds of samples and thousands of genes; this could be fixed by data enrichment. In addition, real data generally suffers from various other interrelated problems that require dealing with dimensionality reduction, missing values and noise, scalability under limited computing power (i.e., using commonly available computers), data modeling and integration to uncover specific semantics (e.g., capturing and monitoring behavior and trend in social media data to allow for better recommendations), etc. These problems cannot be treated in isolation and hence there is a need for an integrated framework capable of handling any combination of these problems when present in the data.The main objective of this research program is leading to effective and informative knowledge discovery by developing a framework which integrates efficient and robust network modeling, data mining and machine learning techniques. Students will be involved in various parts of the methodology from developing the necessary theory and algorithms to implementing and testing them.""525063,""Dragicevic, Suzana"
"533797"	"Ashlock, Wendy"	"Ontario"	"CANADA"
"525262"	"Baljko, Melanie"	"Embodied Interaction Design for Articulation Elicitation and Feedback"	"An increasing number of computer science graduates, as part of their employment, will become members of collaborative teams that are tasked with the development of interactive systems.  The increasing accessibility of tools for digital media and software creation means that interactive systems are being developed for a variety of new domains and applications.  The process of designing an interactive system that effectively accomplishes its objectives (such as to provide a certain type of user experience or to support learning) requires a multi-disciplinary repertoire of skills and capabilities.  Increasingly, these systems go beyond mouse-and-keyboard interaction and use gestural or whole-body interaction (using sensor-based input).  The development of such systems requires specialized knowledge about embodied interaction.  In addition, such systems are most often developed iteratively, with a strong reliance on formative, empirical user evaluation at intermediate phases, and thus require specialized knowledge about evaluation methodologies.This program of research will investigate how the usability of articulation feedback systems is impacted by (1) the use of metaphorical, as opposed to realistic, visual representations of the vocal tract and (2) graphical perspective.  Graphical perspective refers to the point of view with which a visualization is shown (such as from the embodied point of view that an in-world agent has or from a disembodied and more encompassing point of view). We will examine several aspects of usability, including the task outcome (effectiveness), the effort required to accomplish task outcomes (efficiency), and qualities of user experience, such as satisfaction and enjoyment.    We will focus on users who are older adults.  In undertaking this work, we will develop multiple new variants of interactive systems that will use electromagnetic articulography in order to track in real-time the user's tongue position during the production of speech sounds.   This work will emphasize the importance of iterative design and on-going empirical testing and evaluation.""525263,""Buro, Michael"
"523734"	"Banihashemi, Amir"	"Processing and Coding for Efficient Sensing and Transmission of Information"	"This proposal is for funding to support a continuing research program on digital and wireless communications, coding and information theory. The focus is on both sensing and sampling of the information as well as its transmission. For both cases, we are interested in devising efficient (low-complexity) algorithms and processes that can operate at very high speeds. We are also interested in implementing such algorithms in VLSI circuits. For sensing and sampling, our focus is a recent technique, called Compressed Sensing (CS). Using the graphical representation of the sensing process, we make connections to the widely popular and vastly explored area of iterative coding schemes, and propose new techniques to analyze CS algorithms, and devise new sensing matrices and recovery algorithms. The goal here is to reduce the complexity of the recovery process and yet be able to recover signals with relatively large fraction of nonzero (non-negligible) elements from a relatively small set of measurements. For information transmission, our proposal covers different layers of a communication system. At the physical layer, we devise iterative coding and processing schemes for low-complexity channel estimation/equalization/error correction. We focus on open problems in these areas such as the analysis of error floor and the design of codes with guaranteed low error floors. At the network layer, we analyze the performance of existing network coding (NC) solutions, and aim to devise new solutions with lower complexity and superior convergence behavior. Finally, we explore the cross-layer design involving two or more of the communication layers. One example would be the joint design of multiple access control (MAC) scheduling and NC for multicast scenarios in wireless multihop networks. The goal is to increase the throughput, reduce the consumed energy or decrease the delay. For the implementation part of this research, we are interested in devising new high-speed architectures and circuits with low power consumption for both sensing and processing of information.  The outcome of this research translates to smaller faster more energy-efficient communication systems that are more reliable and provide a better quality of service.""523735,""Benedicenti, Luigi"
"521848"	"Beaudry, Martin"	"Langages formels, algèbres finies et automates."	"Les algèbres associatives (semigroupes et monoïdes) et non-associatives (groupoïdes, quasigroupes, boucles) permettent de modéliser la reconnaissance de langages de mots, c'est-à-dire de décider si un mot reçu en donnée appartient à un ensemble prédéterminé. De même, on utilise les algèbres généralisées et les algèbres de forêts pour les données bidimensionnelles hiérarchiques que sont les arbres et les forêts. L'étude des relations qui peuvent exister entre différentes classes (variétés) d'algèbres et de langages est un important domaine de recherche en informatique théorique.   Le projet que je propose est axé sur deux directions de recherche. La première consiste à étudier la reconnaissance de langages de mots par des algèbres non-associatives ; cela permet d'établir des descriptions, en termes non-associatifs, de variétés de langages de mots qui avaient été définies en des termes très différents. La seconde direction de recherche consiste à faire un travail analogue avec les algèbres et langages de forêts. Je veux en particulier étudier une manière de construire des algèbres de forêts, le produit contexte, et les limites sur la puissance des algèbres qui peuvent être obtenus de la sorte, en comparant les langages de forêts qu'elles reconnaissent avec des variétés de langages déjà définies en termes de logique formelle ou de variétés de monoïdes.""521849,""Roberts, Gordon"
"520635"	"Beaulieu, Norman"	"Wireless Communication Theory and Applications for Maximal Spectral Efficiency"	"Radio spectrum (bandwidth) is an extremely valuable but limited natural resource. While the userpopulation and the demands for wireless access services each grow rapidly, the spectrum is finite and there will be little, if not no, increases in usable spectrum. Therefore, communications research must focus on the development of new engineering science and technologies that will make much more efficient usage of the dear spectrum.  If not, there will not be sufficient bandwidth available for everyone and the cost of wireless services will skyrocket.  The proposed research targets three areas that promise to improve the efficiency of spectrum usage, cognitive radio, cooperative wireless networks and ultra-wide bandwidth (UWB) wireless for femto-cell wireless access.         The concept of cognitive radio is that so-called secondary users will be permitted to access and use spectrum that is not in use by the so-called primary users, provided that the secondary users do not cause interference to the primary users.  Research is needed in order to determine how secondary users can best determine that spectrum is not in use by primary users, and then on how to best access the unused spectrum without causing interference or service disruption to the primary users.       Cooperative wireless networks enable mobile and portable users to have expanded coverage area and improved quality of service by designing the radios to cooperate with each when they are in geographical promity.  The user radios cooperate by acting as relays for each other's messages.  How best to design the radio network so that users can cooperate raises many questions that need to be researched.       Femto-cells are mini base stations designed for use in a home or small business.  Femto-cells connect a user to his/her broadband access and give the user more bars inside buildings, parking facilites, etc., where coverage might  otherwise be poor.  The femto-cell concept is, so far, very susceptible to interference and research is needed to make the femto-cell concept best serve the home and business user.""520636,""Deen, Jamal"
"528083"	"Belostotski, Leonid"	"High-sensitivity receiver systems for use with RF and THz radio telescopes and commercial systems"	"The proposed research will build on applicant's strengths by expanding the previously started research and embarking on a new research path. The first research objective, which builds on the past success, is to demonstrate highly sensitive, fully integrated, wideband Square Kilometre Array (SKA) receiver operating at low GHz frequency range. The proposed research will continue applicant's prior work by investigating the implementations of other essential components of the direct-sampling high-gain SKA receiver, such as an anti-aliasing filter, a variable gain amplifier and a calibrated power detector. Once furnished with the additional circuitry, an investigation of adding an optical data transfer system, which will also serve as the ADC by employing a time-based ADC architecture, will also be conducted. This complete receiver will be placed inside a ""thick"" Vivaldi antenna and will become a very unique system that converts RF broad-band received signal to a digitized optical output. A demonstration of commercial potentials of the highly sensitive receivers are also planned with a development of a high-end GPS receiver equipped with novel low-noise ESD circuitry that is co-designed with the receiver front-end. A proof-of-concept GPS receiver will be designed to validate its characteristics experimentally.A unique noise parameter measurement system for frequency conversion circuits will be developed, as part of the second research objective, to gain a deeper understanding of noise performance of frequency conversion circuits and provide us with design guidelines for their noise minimization.The third research objective is the development of fully integrated cryogenic 460GHz and 830GHz receivers for Cerro Chajnantor Atacama Telescope (CCAT) in Chile. The work will be broken into a number of projects consisting of a design of a fundamental 437.5GHz VCO, 437.5GHz and 830GHz frequency dividers, a phase-locked loop for the 437.5GHz VCO, 460GHz and 415GHz mixers, 22.5GHz LNA and IF-to-based band converter.""528084,""VanderLoos, Hendrik"
"533257"	"Beltrame, Giovanni"	"Methodologies and Architectures for Probabilistic Real-Time Systems"	"Computing systems that are subject to strict operational deadlines from event (e.g. alarms, commands, etc.) to response are called real-time systems. These are employed in many applications where the response time of a control system can have a critical impact on the safety of people or infrastructure (e.g. airplanes, automobiles, or satellites). With critical applications become more and more complex, the performance of real-time systems has to increase to guarantee appropriate safety standards. The main obstacle to high-performance real-time systems is the unpredictable timing behaviour of modern  computer architectures. Currently, software designed for real-time systems needs to be guaranteed to be executed within the required time frame. Depending on the complexity of the underlying hardware architecture, this level of certainty might  be difficult or impossible to reach.  Within this context, this project will introduce a probabilistic approach: by enabling true randomized behaviour in processors and memory architectures, one can define probabilistic  metrics for the timing behaviour of a system.   This would bring real-time software engineering in line with other engineering domains, that perform risk analysis using component failure probabilities. In this project we will define novel architectures and design methodologies for truly probabilistic real-time systems, focusing on aerospace applications. Successful  implementation of such systems will have tremendous impact on the way critical systems are  designed, and the potential benefits in terms of cost of integration, verification, and certification of real-time software are enormous.""533258,""Lee, Jeffrey"
"525730"	"Benyoucef, Morad"	"Enabling Social Commerce - A New Form of E-commerce that Leverages Social Ties Between Users"	"Social commerce is defined by IBM as ""the concept of word-of-mouth applied to e-commerce"". When a user interacts online, she knowingly or unknowingly spreads the word about a product or brand; hence members of her online community can leverage her opinions in their purchase decisions. As word-of-mouth relies on social ties, it is becoming the marketing strategy of choice deployed on social media and e-commerce websites. For business and academia there is a need to understand social commerce, to study the design of platforms that support it, and to investigate the social ties that are inherent to these platforms, all in a way that satisfies businesses and consumers. The objectives of this proposal are twofold. First, we will study the design of social commerce platforms. There are two categories of such platforms: those that add ""commerce"" features to social media platforms (e.g., applications that enable e-commerce on Facebook); and those that add ""social"" features to e-commerce platforms (e.g., having a button on Amazon for the customer to ""share"" her purchases with her Facebook friends). We will investigate design frameworks and processes for developing social commerce platforms in an efficient and effective way. Second, we will study algorithms that predict the formation of ties (aka links) and those that detect communities on social commerce platforms. This proposal will have an economic impact by enabling social commerce; and a scientific impact by creating and using new methodologies to address complex issues such as innovative social commerce design and more accurate and efficient algorithms for social tie prediction and community detection.""525731,""Rousseau, LouisMartin"
"534276"	"Binder, Polina"	"Québec"	"CANADA"
"530909"	"BlondinMassé, Alexandre"	"Modèles combinatoires pour la représentation et l'encodage de figures discrètes"	"La géométrie digitale est une discipline qui s'est récemment développée, comme beaucoup d'autres, avec l'avènement des ordinateurs. Alors que la géométrie analytique, basée sur le calcul infinitésimal et le paradigme du continu, nécessite des calculs en nombres réels souvent coûteux, la géométrie digitale est mieux adaptée au traitement des images numérisées. Elle possède des ramifications dans des contextes très variés, parmi lesquels on compte l'imagerie médicale, la cristallographie (pour laquelle on a décerné à D. Shechtman le Prix Nobel de chimie en 2011), la physique, la tomographie, la géodésique ainsi que la simulation. Supporté par l'organisme ""International Association for Pattern Recognition (IAPR)"" et son comité technique IAPR-TC18, le domaine regroupe une importante communauté de chercheurs qui font état de leurs découvertes à la conférence internationale ""Discrete Geometry for Computer Imagery"", qui en sera à sa 17ème édition en 2013 à Séville.Une figure discrète est dite première si elle ne peut être obtenue à partir d'une autre figure plus petite en remplaçant chaque pixel qui la compose par une région discrète connexe quelconque. En combinatoire des mots, ceci se traduit par le fait qu'on ne peut obtenir le mot de contour à partir d'un autre en lui appliquant un morphisme homologue. Or, on en sait très peu à ce sujet. Par exemple, peut-on facilement décider si une figure discrète donnée est première ou composée ? Peut-on relaxer les contraintes pour qu'une figure soit composée, en tolérant une certaine erreur basée, par exemple, sur la distance d'édition ? Est-il possible de représenter de façon efficace les images numériques par un ensemble de mots de contour ? Finalement, peut-on utiliser la notion de primalité de figures discrètes à des fins d'encryptage, à l'instar du célèbre algorithme RSA ? Ces questions sont celles auxquelles je propose de répondre dans ce programme de recherche.""530910,""Jackson, Donald"
"524558"	"Blustein, WilliamJames"	"Finding and Using Information in e-Documents"	"My research is dedicated to helping people to make sense of documents.  While the advent of digital media presents many opportunities, the major problems I seek to address are to locate and secure appropriate access to the right documents at the right time from amongst the sheer quantity of documents available to us; to understand and make meaningful use of the content once it is found; to enhance that content by current means which include annotating, clustering, and tagging for both personal and community use; and, lastly, to recombine and restructure documents which speaks to the vision of Bush and Otlet inter alia who saw the potential of hypertext and the Semantic Web.  My coherent research projects are housed within one research group, HAIKU (Hypertext Augmenting Intelligent Knowledge Use).""524559,""Capretz, Miriam"
"527358"	"Boudoux, Caroline"	"Pushing the limits of biomedical optics endoscopy through the development of dedicated instrumentation"	"The past 25 years have witnessed the emergence of a wide array of novel medical imaging modalities exploiting the interaction between laser light and biological tissues. The techniques are now used in biology for observing tissues at architectural (using large field-of-view modalities such as optical coherence tomography), cellular (using confocal microscopy) or molecular (using fluorescence or nonlinear techniques) levels. Translation to the clinical world will allow clinicians to perform minimally invasive diagnoses. While biomedical optics techniques have a resolution and contrast unsurpassed by conventional medical imaging instruments (such as X rays, MRI, PET), their penetration depth is too shallow to perform visualization of internal organs without developing small and flexible instruments known as endoscopes. Additionally, while fluorescent agents may be added to study biological processes in small animals, very few are approved for use in humans. We must therefore rely mostly on endogeneous molecules (eg fluorophores naturally present in human tissues) to obtain specific contrast. The goals of this research are thus to: (1) study the interaction of lasers with biological tissues to develop ways to excite endogeneous molecules, (2) develop novel optical fibre components that will allow exciting these molecules for enhanced endoscopic imaging and (3) design and fabricate miniature lenses and other optical components to obtain high resolution imaging within the narrow confines of endoscopy.The biomedical industry in Canada will benefit from this research through the generation of new intellectual property, the training of highly qualified and specialised personnel as well as the creation of a unique expertise in lens fabrication through a technology called diamond-turning CNC available at École Polytechnique Montréal. The long term impact of this research is to decrease health care cost and increase overall the quality of life of Canadians by providing clinicians with diagnostic tools allowing for more accurate, less invasive screening procedures at stages where full recovery is the most likely outcome.""527359,""Lang, Andrew"
"524918"	"Boulanger, Pierre"	"Skill Transfer and Evaluation of Minimally Invasive Surgery Procedures Using Visuo-haptic Communication"	"Our short term objective is to develop a system capable of comparing an expert surgeon and a trainee using measurements provided by bio-metric signals as they perform their tasks. Our goal is to compare not only dexterity but also task precision and safety.  The long term objective is to determine if complex ambidextrous MIS manipulations can be transferred between an expert surgeon and a trainee by multi-modal visuo-haptic guidance using a What-You-Feel-Is-What-I-Feel (WYFIWIF) paradigm. The WYFIWIF paradigm has been shown recently to be a very effective approach at skill transfer and acquisition for a variety of applications. Both short term and long term objectives complement each other as we will use the measurements provided by the gesture acquisition system to design the visuo-haptic guidance system and to validate how efficient the system works. This new design approaches could represent a scientific breakthrough in human computer interface and advanced augmented reality systems in the field of medical training. In collaboration with CAMIS, the Advanced Man-Machine Interface Laboratory (AMMI), the new UofA chair in surgical simulation, and Quanser Inc. (a Canadian Company specialized in haptic interface development), we will address these important research issues.""524919,""Mattsson, Jim"
"523708"	"Boyd, Jeffrey"	"Vision and Sensing of Human Motion for Athletes and Artists"	"Biometrics typically refer to the application of pattern recognition methods to the measurement of humans for security purposes. We take a broader view, by applying a variety of signal processing and computer vision techniques to the measurement and analysis of human motion, for applications in athletics and clinical treatment of gait disorders.  The prototypical application is an athlete running on a treadmill, but experiencing pain.  We know that if the athlete runs differently, we can eliminate the pain, but gaits are not easily altered.  Using athletics such as this as a case study, we seek to develop algorithms and systems that measure and analyze human motion to produce feedback to that person in real-time.  A working hypothesis is that properly designed feedback will be an effective way to correct a gait.  We have demonstrated the concept in previous work with a system designed to correct the technique of a speed skater who had lost his ability to do cross overs.  We focus on audio feedback because it leaves the eyes free for the athlete's safety, and because we know that people can coordinate their movements with sound.  The proposed research builds on this previous success by exploring the use of various sensors to measure locomotion (computer vision, motion capture, and sensors mounted on the athlete), measuring in a variety of settings (speed skating oval, treadmill, running track, sidewalk), developing signal processing algorithms for synchronization with the various sensors and motions, examining a variety of modes of locomotion (walking, running, skating), and developing and testing algorithms and systems that target a variety of desired outcomes (athletic performance, clinical treatment).  A key part of our methodology is to engage with artists and musicians.  While artists and musicians can use our technological advances to further their aesthetic goals, our scientific work receives a reciprocal benefit as they contribute to the design of audio and visual feedback that is effective in obtaining the desired response from an athlete.""523709,""Sperling, Felix"
"521431"	"Boyd, Sylvia"	"Applying Methods from Combinatorial Optimization in Solving Large-Scale Real World Problems"	"Many practically important optimization problems are combinatorial in nature.  Some examples include the minimum cost design of reliable communication networks, fast printed circuit board production, the building of genetic evolutionary trees as well as the optimization of various scheduling and routing problems.  Unfortunately, may such problems belong to a class called ""NP-hard"", which is considered strong evidence that they  are extremely difficult to solve, and that it is highly unlikely that we will find techniques for solving them that are guarantee to be practical and efficient in all cases.  The lack of efficient methods for such problems is of significant importance as there exist large-scale, real world instances requiring solutions.  Fortunately, in practice, it is often sufficient to obtain solutions for these problems which, if not optimal, are at least guaranteed to lie within a certain satisfactory percentage of the optimal (best) solution, which in many cases is much more viable task.Our research objective is to enlarge the set of such NP-hard problems arising from applied combinatorial optimization models which can be practically and satisfactorily solved.  To achieve this goal we will use the following three approaches:1)  Investigate developing new and improved efficient methods for such problems which provide      solutions guaranteed to lie within a satisfactory percentage of the optimal solution.2)  Investigate ways to improve the success rate of current methods which attempt to find an exact      optimal solution for such problems.3)  Investigate ways of applying combinatorial optimization models and methods to applications       arising in other areas of Computer Science.""521432,""Jenkin, Michael"
"525352"	"Bruce, Ian"	"Computational Models of Hearing"	"The human auditory system is capable of accomplishing tasks such as speech understanding, sound source identification and sound source separation at a level of performance that remains unrivalled by man-made systems. The long-term goals of this research program are to develop and utilize computational models to: i) improve our understanding of how the ear and brain process acoustic and electrical stimulation to perform a range of hearing tasks, ii) better elucidate the neural mechanisms that are utilized within the mammalian auditory system, and iii) develop improved predictors of human speech perception and tools for the design of novel hearing aid and cochlear implant sound processing strategies.This research program will achieve significant improvements in auditory models that are being utilized extensively in the worldwide hearing research and speech & audio processing research communities. In addition, the results will provide new understanding of many of the processing approaches and biophysical mechanisms employed within the mammalian auditory system that enable it to achieve its unmatched level of performance on so many tasks. This in turn will inspire new directions in signal processing algorithms for speech & audio processing and lead to further advances in assistive devices for the hearing impaired.""525353,""Herd, Christopher"
"534147"	"Bruce, Jake"		"NSERC"
"533946"	"Bylinskii, Zoya"		"UNITED KINGDOM"
"527352"	"Cai, Jun"	"Radio Resource Management in Cognitive Radio Networks"	"Cognitive radio is a new concept, which is introduced to solve the spectrum scarcity issue in wireless communications by allowing the share of licensed spectrum with unlicensed users. For cognitive radio based wireless networks, the provisioning of both efficient spectrum utilization and stable services is critical and technically challenging. The proposed research will address this issue through achieving dynamic spectrum allocation between licensed and unlicensed users under the consideration of practical limitations on hardware, spectrum mobility, multihop transmission, and wireless channels. In particular, the proposed research will develop (i) distributed resource allocation algorithms for improved spectrum utilization and quality of service provisioning; (ii) robust resource management mechanisms to combat the potential spectrum sensing errors; (iii) distributed call admission control methods to guarantee the quality of service satisfaction to both new and existing calls; (iv) mathematical models for capacity analysis at both packet and call levels; and (v) effective spectrum handoff methods for smooth and seamless information dissemination among unlicensed users.The research will help to accelerate the commercial applications of the cognitive radio technology in wireless communications. It will develop new networking protocols and control algorithms critical for implementing connections among unlicensed users, even in a dynamic network topology without infrastructure. The cutting-edge research will generate new ideas and knowledge in cognitive radio and enable the development of new products, which will foster a vital competitive edge for Canadian Information and Communication Technology industry in the international market place.The proposal will result in significant benefits to Canadian users, such as new wireless communication services and applications, safe and energy efficient equipments, and global Internet access.""527353,""Chen, Li"
"534801"	"Carrier, PierreLuc"	"Alberta"	"CANADA"
"520733"	"Cartledge, John"	"Terabit-per-Second Coherent Optical Fiber Communications"	"Optical fiber communication systems are used extensively throughout the global telecommunications network to provision virtually all services in the business, educational, healthcare and entertainment sectors. The rising demand for these services, many of which were unimaginable until recently, is driving the expansion of this network at dramatic rates and has pushed the per-channel bit rate of optical signals to 100 Gbit/s for state-of-the-art commercial systems. These systems are enabled by a fundamentally different approach based on coherent detection and digital signal processing. Next generation systems with a per-channel bit rate of 400 Gbit/s are currently being developed, and the need for a bit rate of 1 Tbit/s in the future is seen as certain.    The research on coherent optical fiber communications reported to date has largely concentrated on performing the signal processing functions for a single channel in a conventional wavelength division multiplexed system that uses a fixed channel spacing of 50 GHz. To increase the per-channel bit rate to 1 Tbit/s, a strategy based on simply increasing the modulation rate is not applicable due to system design constraints and technology limitations. A bit rate of 1 Tbit/s can be achieved using a superchannel, which is comprised of multiple closely spaced optical subcarriers which collectively are considered as a single entity. Since superchannels are not compatible with current fixed-grid, wavelength division multiplexed systems, the concept of elastic optical networks has recently emerged as a means of accommodating channels with different modulation formats and symbol rates, and maximizing spectral efficiency. The proposed research embraces these recent innovations by focusing on 1 Tbit/s superchannels in elastic optical networks. The co-existence of such channels on a fiber and the distinct features of superchannels introduce new challenges in system design and performance evaluation. Three Ph.D. students will be trained through this research which addresses techniques for evaluating the system performance, digital signal processing algorithms that exploit the unique properties of the systems, and key subsystems required to support the experimentally intensive investigation.""520734,""Dusseault, Maurice"
"522182"	"Chaibdraa, Brahim"	"31"	"Ontario"
"520203"	"Chaudhuri, Sujeet"	"Ontario"	"CANADA"
"528021"	"Childs, Andrew"	"Algorithms for quantum computers"	"The computers we use today operate according to principles that are rooted in classical physics.  They represent data using bits, each bit taking the value 0 or 1.  But quantum mechanics, the theory describing the behavior of atoms (and, fundamentally, all physical systems), offers a more general notion of information.  In quantum mechanics, data can take on a superposition of the possible classical values.  To exploit this new kind of information, we can envision constructing a quantum mechanical computer, an information-processing machine that operates according to the laws of quantum mechanics.          Key discoveries in the 1980s and early 1990s showed that quantum computers could quickly perform certain tasks that seem to be intractable for classical computers.  For example, there is an efficient quantum algorithm for decomposing an integer into its prime factors.  No such algorithm is known for classical computers, and indeed, many of the cryptosystems in use today (for example, to encrypt financial transactions on the internet) are based on the widely-held assumption that this problem is hard.  Thus quantum computers overturn established notions of what can be computed efficiently, with potentially dramatic implications for technology.  However, only a handful of examples of quantum speedup are known, and the nature of the computational advantage provided by quantum mechanics is still poorly understood.          My research program is focused on better understanding the power of quantum computers.  Specifically, I work to develop new quantum algorithms that achieve dramatic speedup over classical computation.  This research serves both to illuminate the differences between classical and quantum information processing and to find quantum solutions for problems of practical importance, such as search problems and cryptographic tasks.  I am especially interested in developing novel algorithmic techniques for quantum computation, augmenting the toolbox of methods that can be used to design and analyze quantum algorithms.""528022,""Hoffman, Kari"
"521917"	"Cleve, Richard"	"Quantum information and complexity theory"	"The proposal is to investigate computational complexity issues that arise in the context of quantum information. ""Computational complexity"" is concerned with the inherent computational costs of performing computations. ""Quantum information"" is a model of information that captures the quantum mechanical behavior that arises in physical systems. It is natural to consider computational complexity in this model and this has resulted in several remarkable results during the past two decades (such as efficient ""quantum algorithms"", novel cryptographic protocols, reductions in communication complexity, and quantum ""proof systems""). The specific focus of the proposal is on the following two topics.         One topic is the computational complexity of ""non-local games"", which are scenarios where two (or more) cooperating players who cannot communicate with each other respond to questions from a verifier. The scenario is particularly interesting when the cooperating players are allowed to possess quantum systems whose joint state is ""entangled"" (a quantum mechanical analogue of ""correlated""), which permits them to coordinate their answers in ways that are impossible with classical information. An interesting problem it to determine the optimal coordination possible. There are classes of these games where the computational complexity of this problem is well-understood, but the general problem is not currently even known to be decidable.         The other topic concerns the computational complexity of Hamiltonian evolution: simulating the state that arises if an initial state evolves for a specified time-interval under Schrödinger's equation. This is interesting in the context of simulating physical systems, and also because there are quantum algorithms for combinatorial problems that utilize such simulations. There are interesting open questions about the inherent computational costs arising in these processes. Finally, I speculate that some of the ideas arising in this topic might lead to more efficient Trotter-Suzuki type product formulas.""521918,""Hutchinson, Norman"
"525828"	"Daley, Mark"	"Natural computation in the mesoscale dynamics of the brain"	"Nature computes. From the complex branching growth of a tree to the processing of DNA, many natural processes and phenomena can be modelled, and understood, through the lens of computation.The primary drive behind this research program is the framing of the dynamics of natural systems in terms of computation. Once framed this way, we can bring to bear the mathematics of computation to model and understand natural processes. At the same time, the challenge of modelling natural systems extends the boundaries of known mathematics by forcing the development of new concepts and techniques.Within this broad research program, I propose to study the dynamics of large-scale networks in the human brain. We will collect neuroimaging data which, roughly, quantifies ""neural activity"" in spatially constrained regions of the brain. If we assign each region to a node, and draw a line between two nodes if the two regions have similar patterns of activity, we define a brain activity based graph. If, instead of a single graph, we build a series of graphs -- as neural relationships evolve in time -- we now have a dynamic graph. The ""front end"" of the proposed research is to develop optimal methods for inferring such dynamic graphs from neuroimaging data.Once the dynamic graphs have been generated, we can begin to look for patterns in how the graphs change over time. Patterns can be turned into (probabilistic) rules which compose a formal computing system which can then be analyzed using the tools of theoretical computer science. The anticipated outcomes of this program are the development of new models of natural computing and the application of these models to understanding the dynamics of the human brain.""525829,""Guéhéneuc, YannGaël"
"533751"	"Davidescu, Mircea"	"Pennsylvania"	"UNITED STATES"
"534263"	"Denton, Emily"	"Ontario"	"CANADA"
"526322"	"DeSterck, Hans"	"Multilevel Algorithms for Tensor and Network Problems"	"This proposal develops novel algorithms and software for new classes of large-scale scientific computing problems that are emerging in the areas of tensor-based data processing, Markov chain modeling, and social network analysis. These problems have in common that data sets may be very large ('Big Data'), that new scalable algorithmic approaches are required, and that they have tremendous economical potential in our fast-evolving knowledge-based economy. The proposed research will capitalize on groundbreaking work that was produced recently in the applicant's research group. Tensor-based data processing is rapidly gaining importance in data mining, recommendation systems, and cybersecurity; realistic Markov chain models of complex queueing networks that characterize telecommunications systems (e.g., call centres) are challenging due to the high dimensionality of the models; and advanced social network analysis leads to important competitive advantages for social network companies. New numerical algorithms will be developed for those challenging emerging problem classes.The results of the proposed research promise to be of great benefit to Canada: algorithmic exploration of compute-intensive problems with large data sets is a crucial building block in our rapidly evolving information economy. The numerical algorithms developed in this proposal will have direct applications in Canada's economy and society, for social networking analysis, big data in scientific computing, multi-modal tensor data processing, call centre modeling, and cybersecurity. Moreover, the proposed work will train highly qualified personnel with critical algorithmic and large-scale distributed computing skills that are essential for Canada's knowledge economy.""526323,""Pineau, Joelle"
"534841"	"Dick, Travis"	"Québec"	"CANADA"
"533704"	"Dimson, Thomas"		"UNITED KINGDOM"
"526895"	"Dobre, Octavia"	"Cognitive Coexistence of Non-cooperative Heterogeneous Wireless Systems"	"Spectrum sharing has been recently seen as a vital ingredient in enhancing the performance of existing wireless systems, satisfying the growing demand for spectrum access, and enabling new applications. For example, cellular systems can provide improved coverage and capacity via femto cells, in which the spectrum used by the main network is shared over a local area covered by a low power base station. Furthermore, spectrum sharing is receiving increasing attention as a solution to the paradox of spectrum scarcity amid the existence of broad expanses of inefficiently utilized licensed spectrum. In such scenarios, coexistence problems arise between the primary and secondary users of the spectrum, as well as between multiple collocated secondary systems. Usually, the medium access control and physical layer standards do not support inter-network communications, and cooperative sharing strategies are very limited. Hence, heterogeneous wireless systems will need to coexist while operating independently without cooperation. The aim of this research program is to develop an integrated physical, link, medium access control, and network layer approach for the coexistence of non-cooperative heterogeneous systems. This will incorporate real-time multi-dimensional spectrum awareness information (number and type of transmitters, their parameters, location, and traffic characteristics) into the design of the communications protocols. The anticipated research results will strongly impact the evolution of the future radio frequency environment in which increasingly diverse populations of wireless and cognitive radio-based systems will coexist in the same bands. An efficient utilization of the spectrum will thus be enabled, which will open opportunities for new stakeholders, and provide effective alternatives for the wireless industry and consumers to cope with the continuously growing demand for data traffic and services.""526896,""Dumitrescu, Sorina"
"520209"	"Dubois, Eric"	"Multidimensional signal processing for visual and multimedia services"	"Visual data, in the form of diverse types of imagery, remains one of the richest sources of information for both consumers and professionals. With advances in communications services and networks, and image-enabled devices, image-based services have become pervasive in recent years, especially those making use of the Internet and wireless networks. However, many of these services still offer inadequate quality, realism and responsiveness, especially for very high-resolution imagery and three-dimensional environments. The general objective of this research program is to carry out research on new multidimensional signal processing techniques that can be applied to the development of enhanced visual communications and multimedia systems. Three main themes will be pursued: multidimensional signal processing theory; color image processing and rendering; and omnidirectional and high-resolution image and video processing. The specific topics to be researched include: new more efficient multidimensional representations for different types of images and videos, including omnidirectional images; new methods for color image processing, for applications in digital cameras and color displays, among others; design of stereoscopic omnidirectional imaging systems for immersive telepresence applications; and finally, methods for video signal upconversion, such as HDTV to super HDTV conversion. In all cases, the goal is to develop fundamental new theories that lead to higher quality and/or more efficient implementation than current methods. The students trained will acquire high-value skills in image and video processing, including software development. Imaging is becoming ubiquitous in the modern world in all sectors, including business, consumer electronics and personal communications. Thus, all industries in these sectors have a strong need for personnel with skills in imaging and related fields.""520210,""Deshaies, Yves"
"521919"	"Dudek, Gregory"	"Multi-agent robotic interaction"	"Taking advantage of huge recent advances in robotics depends on interaction between a robot, and either the other systems around it, and/or the humans employing it.  We have identified this interaction as a key problem that limits future progress in the near term. As a society, have great robotics technologies on hand or almost mature. What we lack are good formal tools and foundations that let us link diverse types robots to one another to build more capable heterogeneous systems, or let humans and robots work together to allow the human to efficiently and effortlessly make use of automated assistance.  This is akin to challenge faced by computer networks in the 1970s, which was solved via the deployment of the Internet, but not the diverse collection includes both humans and machines, making the challenge much more profound.This problem of developing not merely user interfaces or communication wrappers, but basic representations and interaction paradigms for collaboration between different kinds of agents, where an agent can mean either a person or a robotic vehicle.  Addressing this issue has the potential to dramatically change the way robots and autonomous systems can and will be used in the next 5 to 15 years.Robots are able, or almost able, to accomplish useful tasks, but their adoption will be hampered by our ability to integrate them comfortably, efficiently, safely and easily into our existing infrastructure and modes of behavior. My proposal addresses these challenges of concise and natural inter-agent representation, data sharing, planning and sensing.  As a testing scenario, much of my work will take place on amphibious or underwater vehicles where robot interaction becomes clearly highlighted.  In addition, this addresses clear and open needs in the marine biology community.""521920,""Burke, Maxim"
"534765"	"Dumoulin, Vincent"	"Ontario"	"CANADA"
"534966"	"Durand, Audrey"	"Alberta"	"CANADA"
"522295"	"Dymond, Patrick"	"Searching, algorithms and complexity"	"This proposal for research in Computer Science deals with parallel computing and complexity as applied to problems in the area of multi-agent mobile robotics. The challenge of computing using parallel computers now faces almost every area of computing.  Past economic advances due to progress in computing have depended in large part on continuing improvements in individual processor speeds, but processor speeds are no longer increasing as rapidly as before.  Instead, computing in parallel is becoming the main direction for progress.  The challenge is to make effective use  of parallel hardware in many new contexts.  The research proposed here is principally focused on parallel and distributed computing problems that are related to mobile robotics. This area of research focuses on finding methods for using multiple moving robots. (1) The simultaneous location and mapping (SLAM) problem deals with the question of  ""where am I''  for robots trying to create a map of their local world. This problem is important for the deployment of robots in unknown environments, such as unexplored terrain or severely hazardous locations. We will study sensor requirements and algorithms needed to solve this SLAM problem in different environments. (2) Leader election is another key problem in distributed computing, important as a basis for many other problems.  We consider a set of independent robots with very limited communication capabilities, each one executing an algorithm in order for them together to jointly complete some task.  We will further develop methods for the agents to quickly find a leader, and other basic tasks, by simple one-to-one communications when they meet, even not knowing the actual number of robots involved. (3) High-dimensional robot path planning is important in moving robots to perform tasks. We plan work on a probabilistic approach to this problem.  One place where this could be used is planning motions necessary to efficiently move a complicated tentacle-like robot arm with many joints. (4) Overall this research will produce new methods for collections of mobile robots to solve problems that may be too difficult, too dangerous or too expensive to solve traditionally.""522296,""Maltman, Kim"
"535631"	"Ellert, Bradley"		"NSERC"
"523730"	"ElMabrouk, Nadia"	"Evolution of Genome Organization"	"This is a computational biology program aiming to improve our understanding of genome evolution. The abundance of sequenced and annotated genomes present in public repositories has revealed a tremendous diversity of structures in terms of sequence, gene organization and karyotype, which is the footprint of the various mechanisms taking place during evolution: single nucleotide substitutions are at the origin of sequence diversity, duplications and other content-modifying operations are at the origin of gene family diversity and rearrangements are at the origin of gene order diversity.  Each of these types of footprints has been used separately to address different questions. Sequence similarity is the main criterion used to cluster genes into families and to construct phylogenetic trees. Content-modifying operations are used to infer the orthology/paralogy relation between gene copies, which has important implications towards the annotation and functional specificity of genes.  As for the diversity of gene order, it is used in comparative genomics to infer ancestral features and study the rearrangements (inversions, transpositions, translocations, fusions, fissions) taking place during evolution.  In this program we seek to integrate the three types of footprints in a unified methodology for the Inference of Ancestral Genomes. This is one of the main goals of comparative genomics essential not only from an evolutionary perspective, but also for important implications of the mechanisms taking place during evolution on the genetic and physiological specificities of species, with far-reaching applications to drug discovery and personal medicine. Most existing methods for inferring ancestral genomes stand exclusively on gene order information, ignoring all available phylogenetic information on gene families, which is a serious limitation towards their applicability to genomes with multiple gene copies. The most direct benefit of an integrative method accounting for gene trees will be to avoid a drastic filtering of datasets, allowing an increase of precision of the predicted ancestral features.""523731,""Head, Milena"
"528005"	"Farzad, Babak"	"Algorithmic and Computational Graph Theory and Game Theory"	"Fundamental problems in computer science revolve around the discovery of efficient algorithms. Designing fast algorithms or proving that no fast algorithm exists intimately relies, in many cases, on geometrical and graph theoretic characteristics of the problem. Graph Theory has long been recognized as one of the most fruitful fields for the formal study of such problems.    Recently, with the massive amounts of network data that are becoming computationally available (including some social and biological networks), mathematicians and computer scientists have studied topological characteristics that such networks exhibit. One aspect of my research will focus on the analysis of large-scale networks. This work includes a precise topological analysis and proposing generative mechanisms. Such mechanisms have potential to help us reason, at a general level, about the ways in which real-world networks are organized. These models have novel algorithmic and graph-theoretic questions that I plan to study.     A closely related line of research is the study of statistical aspects of graphs and the probabilistic treatment of random graphs - graphs that are generated by some random process. Random graphs also allow researchers to consider graphs that are both large and unstructured.     Random lifts, a new versatile class of random graphs, is defined by, roughly speaking, randomly selecting the permutation that defines the lift. This allows us to design somewhat structured random graphs to model a variety of important naturally-occurring random graphs. Thus, determining the typical properties of a lift, such as their chromatic number, and how they reflect the properties of the base graph are very important and is another aspect of my research.    I also intend to work on some of the classically more important problems in graph colouring, a traditionally important area of graph theory for computer scientists. In some of these problems, I will make use of the discharging method - the tool by which the famous Four Colour Problem was finally proved in 1979.""528006,""Kulik, Rafal"
"520705"	"Farzaneh, Masoud"	"Étude de la performance électrique des isolateurs dans des conditions de givrage"	"Au Québec et dans plusieurs autres provinces canadiennes, les conditions hivernales sont sévères et persistantes, ce qui favorise l'accumulation de la glace sur les équipements des réseaux électriques, notamment sur les isolateurs, entraînant une diminution de leur performance. La présence de glace, combinée avec la pollution déposée à la surface des isolateurs, est à l'origine de l'initiation et du développement de décharges électriques pouvant mener au contournement électrique de ces équipements et à des pannes de courant,  parfois avec des conséquences graves.    La recherche proposée s'inscrit dans la continuité de la recherche à long terme du candidat. Elle vise à faire avancer les connaissances actuelles sur les phénomènes intervenant dans les décharges électriques et leur développement à la surface de la glace déposée sur les isolateurs pollués. Les modèles développés seront validés en laboratoire en utilisant une infrastructure à la fine pointe de la technologie à la disposition du candidat, dans les domaines de la haute tension, du givrage et de la nanotechnologie, ainsi que des données de givrage provenant de sites naturels d'Hydro-Québec.    Ces connaissances et outils serviront à la conception d'isolateurs plus performants dans des conditions de givrage ainsi qu'au développement de revêtements nanostructurés avec propriétés autonettoyantes et semiconductrices, applicables aux isolateurs, permettant l'amélioration de leur performance électrique dans des conditions de pollution et de givrage.    Outre l'avancement et le développement de nouvelles connaissances et la formation de 18 personnes hautement qualifiées, les retombées de cette recherche seront importantes pour le secteur industriel du transport et de distribution de l'énergie électrique ainsi que pour les manufacturiers d'équipements concernés, par le biais de transferts technologiques, ce qui se traduira par le rehaussement de la fiabilité des réseaux électriques.""520706,""Choi, ManDuen"
"525912"	"Fernandez, JoséManuel"	"Evaluating the risk reduction value of security products against mass market cyber crime"	"Mass-market cyber crime targets common users of computer systems worldwide.  This type of crime exists in many different varieties, such as banking fraud, counterfeit sales, confidence fraud, fake anti-virus extortion, etc., depending on the monetizing scheme used by criminals to turn in a profit.  An almost invariant characteristic of such crimes is that, at some point or other, cyber criminals need to compromise and take control of computers belonging to unsuspecting users.  In certain cases, these compromised machines are only used as accessories to crime, and their owners are not directly impacted by these activities.  However, in other cases such as banking fraud and cyber extortion, significant monetarily loss can be incurred by the owners of compromised machines.  One of the mainstays of protection against this type of risk is the adoption of security solutions installed on these machines, such as commodity anti-virus or anti-malware products.  The deployment and operation of such counter-measures have well-known direct costs, and somewhat well understood performance and usability disadvantages.  However, there is much less reliable quantitative data describing how much value these products are providing in terms of risk reduction with respect to these threats.  The aim of this research project is to develop methods to quantitatively evaluate the efficiency of such solutions at reducing the probability of compromise and the potential direct impacts of mass-market cybercriminal threats.  In order to achieve this objective, we propose a combination of data collection and analysis approaches such as clinical trials with voluntary users, laboratory experiments under controlled conditions, and the use of other sources of macro-economical data on cyber crime.""525913,""Kövecses, Jozsef"
"533774"	"FitzGerald, Nicholas"	"New York"	"UNITED STATES"
"525266"	"Fleming, Michael"	"5"	"NSERC"
"534100"	"FortinSimard, Dany"	"Ontario"	"CANADA"
"534261"	"Fraser, Kathleen"	"Ontario"	"CANADA"
"523835"	"Frey, Brendan"	"A Unified Computational Model of Gene Regulation"	"Understanding how instructions stored in DNA are interpreted differently to generate diverse cellular activities is one of the most fundamental, unsolved problems of our time. This ""software of life"" would explain relationships between different individuals, offspring and species. It would tell us how genetic mutations lead to changes in phenotype or disease. It would offer predictions of the effects of biomolecular disease treatments. It would help us understand, at the cellular level, who we are.Recently, a consortium of thousands of genome researchers from across the globe identified a huge number of DNA elements that appear to be important for controlling activities within cells (ENCODE, 2012). We now now that up to 80% of the human genome contains these important DNA elements, but knowing that something is important is not the same as understanding how it works. This is the topic of our research project. In this project, our goal is to build a ""regulatory model"" that explains how DNA elements combine together to control the genetic messages (mRNAs) that cells use to control most activites. Several research groups, including mine, have been studying this problem for the past 10 years, but progress has been difficult. While we now have many massive datasets that profile genomes and their genetic messages, we lack good computational models that can effectively combine DNA features with descriptions of cellular conditions to predict genetic messages. We will investigate a novel computational model that unifies and generalize previous work, that helps us understand testable biological mechanisms, that can be used to predict levels of genetic messages for novel genes and cell states, and that is based on well-defined, testable principles. While this is a high risk project, the potential impact is tremendous and our preliminary results indicate that there is good reason to believe we will succeed.""523836,""Kozey, John"
"526389"	"Friedlander, Michael"	"Large-scale optimization: algorithm design and analysis"	"Mathematical optimization arises in a wide range of science and engineering disciplines. It can be used by engineers to design systems that are ""best"" in some sense; it can be used by scientists in many fields to find the best model of a given physical process.The goal of optimization is to minimize (or maximize) an objective function while satisfying certain constraints. Applications often give rise to optimization problems with significant structure, which can be exploited to efficiently optimize models with huge numbers of variables.This proposal describes a five-year research program to develop fundamental theoretical tools in optimization and innovative algorithms for solving large-scale problems of practical interest. The proposal addresses issues in algorithm development, analysis, and implementation. It involves the development of general-purpose algorithms as well as of specialized optimization methods for particular applications.In many contexts, optimization is a vital step in a larger procedure; it is therefore critical that optimization algorithms be fast and reliable. Advances in efficiency and robustness are welcomed by optimization users in industry and academia. The proposed research will have an impact on three groups: researchers in optimization, researchers in specific application fields, and industry practitioners.""526390,""Chapman, Colin"
"528019"	"Fung, Benjamin"	"Privacy-Preserving Data Sharing for Health Data Mining"	"Health data mining is the process of extracting useful knowledge from a large volume of health data in an efficient and scalable manner. An effective data sharing system plays an important role in supporting health data mining. The emergence of cloud computing has significantly improved the potential of sharing health data due to its powerful computation capability, flexible service composability, high availability, and low maintenance. In particular, the new paradigm of Data-as-a-Service (DaaS) in cloud computing is to provide flexible, on-demand data service to data recipients regardless of their devices, platforms, times, or locations, while charging only for what they use. These desirable properties make cloud computing a natural choice for the next generation of healthcare information infrastructure. Yet, the major obstacle to adopting this technology in the healthcare sector is a lack of trust in sufficient privacy protection.     Many privacy-enhancing technologies (PETs) have been developed in the last decade, but most have not been effectively utilized in real-life health information systems due to the following problems: First, most PETs do not consider the information needs of health data mining. Second, they are designed for general data-sharing scenarios and are not flexible for accommodating the dynamic data-sharing scenarios in the healthcare sector. Third, they do not follow the standard communication protocols and data formats required in the healthcare sector.    To remove the aforementioned obstacles, the objective of the proposed research is to develop a privacy-preserving DaaS-based health data sharing system for supporting health data mining. The proposed platform will enable health information custodians (HICs), such as hospitals, clinics, and labs, to securely share and integrate their patient-specific data, while the health data miners (HDMs), such as health professionals in other health agencies and researchers in pharmaceutical companies, can still effectively retrieve their required information and perform their anticipated operations and research activities.""528020,""Privat, Jean"
"535497"	"Gehring, Clement"		"NSERC"
"527997"	"GhaderiDehkordi, Majid"	"Exploiting Interference in Wireless Networks: Protocol Design, Analysis and Implementation"	"A major performance limiting factor in wireless networks is interference. Current wireless networks are designed based on the principle of interference avoidance, which advocates for eliminating concurrent nearby transmissions to avoid collisions, as collisions result in waste of scarce wireless bandwidth. The key issue is that a network design that blindly prevents interference is sub-optimal and results in severe inefficiencies in bandwidth-limited wireless networks. With recent advances at the physical layer of wireless networks, concurrent transmissions of nearby links can be successfully decoded if controlled properly. If transmitters and receivers are aware of the interference structure, they can exploit it to better decode their transmissions, and thus greatly improve network performance. This proposal makes a fundamental shift from the traditional protocol design in wireless networks: it designs network protocols that systematically encourage and exploit interference to improve network performance. Specifically, it utilizes three physical layer mechanisms for exploiting interference: multi-packet transmission based on superposition coding, multi-packet reception based on successive interference cancellation, and cooperative communication to design wireless network protocols (specifically, scheduling at link layer and routing at network layer) that turn interference into an advantage.The proposed research will produce techniques that can have substantial impact on the design of future wireless networks. The scope of the proposal includes protocol design, network modeling and analysis, and implementation of selected protocols on an experimental testbed. In addition to potential impacts on the theory and practice of new wireless technologies, the proposed research is an excellent training ground for undergraduate and graduate students in all aspects of wireless networks.""527998,""Allingham, John"
"535475"	"Gingerich, Matthew"	"Ontario"	"CANADA"
"533366"	"Golab, Wojciech"	"Harnessing the power of next-generation shared memory multiprocessors"	"After decades of research, distributed computing continues to provide stimulating challenges and opportunities driven by disruptive innovations in hardware design.  Breaking the mold of traditional distributed computing models, tomorrow's distributed systems will see large numbers of cores connected in complex ways, new instruction set features, advanced power management capabilities, and non-volatile memories.  These design features promise to make future distributed systems more capable than ever before, at the expense of more complex software.The long-term goal of this research program is to harness the power of next-generation distributed computing platforms by revisiting fundamental questions, such as how parallel threads of execution can share resources effectively, and how large systems can survive the failure of software and hardware components.  In the short-term, the program will focus on tools and techniques for implementing highly-parallel and fault-tolerant data structures.  The specific objectives of the program are: (1) to understand the cost of synchronization in emerging multi-core platforms, and use the insights gained to design efficient shared data structures for such platforms; (2) to develop more robust and efficient mechanisms for distributing data structures across clusters of servers; and (3) to devise improved techniques for protecting shared data structures against failures using non-volatile memory.By embracing new opportunities in distributed computing, this program will advance fundamental research necessary for building novel information systems on tomorrow's hardware platforms, and train highly qualified personnel in skills essential to Canada's technological leadership.""533367,""Kadoury, Samuel"
"534811"	"Gorobets, Militina"	"Alberta"	"CANADA"
"528368"	"Gromala, Diane"	"VR Systems for Body Image and Body Schema"	"Recent research in the emerging field of embodied cognition demonstrates the mutual importance of understanding the extent to which technologies affect our physical/physiological factors, and how our assumptions about our bodies affect the ways we design technology. Thus, it is increasingly important to study the fundamental processes of body schemas and body images. A body schema comprises the collection of processes that are responsible for typically non-conscious processes such as posture and movement. A body image refers to attitudes and beliefs we have about our body, which change over time, or when we face disability or disease. Body schema and body image function as continuous, mutually influential processes that provide important information about a user's psychophysiological state.This proposal aims to contribute to increasingly important research on fundamental human processes of body schema and body image, particularly as they change over time. We propose to: (i) investigate computation of body schema and body image by abstracting and modeling relevant correlates of each, (ii) identify relationships of body schema and image that together indicate variable psychophysiological user states, (ii) develop visualizations that correlate to changing user states, initially in VR avatars for range of movement studies in seniors, (iii) develop an interface that enables user annotations of body image data, (iv) develop methods for accurate repeatability of capturing user state data, (v) visualize aggregation of variable timeframes for later analysis of longitudinal data of users' psychophysiological states and (vi) develop methods of empirical assessment. The long-term objective of this research is two-fold: 1. To identify the theoretical underpinnings and formalisms necessary to devise computational models of variable body schema and image data. 2. To develop methods of assessment to provide empirical evidence of effectiveness.""528369,""Farzan, Azadeh"
"528067"	"Grova, Christophe"	"Multimodal integration of functional neuroimaging data"	"More and more non-invasive neuroimaging techniques are now available to study normal brain functions but also brain disorders. However, it does not exist any ideal exploration technique able to measure precisely ""when"" (at a ms scale) and ""where"" (at a mm scale) an activity occurred in the brain. Each neuroimaging technique measures through different mechanisms (neuronal electricity, oxygen consumption) part of brain activity, being sometimes more dedicated to the question ""where?"" (good spatial resolution) or to the question ""when?"" (good temporal resolution). Each of these techniques having its specific advantages and limitations, analyzing each of them individually only provides partial information on underlying brain activity. The objective of my research project is to develop methods to appropriately combine these multimodal data, i.e., data fusion methods, in order to detect additional information that could be missed by considering each modality individually. A typical challenge is to combine modalities measuring directly neuronal activity with high temporal resolution with other modalities measuring indirectly the same function with high spatial resolution, through hemodynamic processes for instance. Most neuroimaging techniques have been developed in order to be sensitive to different brain signals: (1) direct measurements of neuronal bioelectric activity (from scalp recordings) and (2) indirect measurement of brain activity through hemodynamic processes measuring blood flow providing oxygen to the active neurons. The objective of my work is to accurately characterize the link between these two types of measurements when studying normal brain activity but also abnormal epileptic discharges. The first objective will be to localize, within the brain, fluctuations of blood flow measured using optic fibres placed on the scalp. The second objective will consist in using these different modalities to assess the integrity of this link between  neuronal bioelectric activity and hemodynamic processes. The third objective will consist in extracting relevant information on brain activity using these multimodal data to characterize resting state fluctuations.""528068,""Fournier, Valerie"
"534607"	"Hall, Michael"	"Québec"	"CANADA"
"525258"	"Hall, Trevor"	"Photonic network technology for a low carbon digital economy."	"The goal of the proposed research programme is to develop information and communications network infrastructure capable of supporting new applications that push bandwidth and accessibility limits in the context of a low-carbon digital economy. Specifically, it is proposed to design and evaluate the performance of an energy efficient network infrastructure that features a unified control plane to integrate the Internet protocol into optical metro networks to provide flexible and intelligent bandwidth on demand for cloud computing; to develop fast and scalable single-chip transparent optical switches critical to the implementation; and, contribute to the sustainability of the technology through longer term research to transform new concepts in nanophotonic devices and integration from curiosities into ultra-low energy consumption photonic devices that approach thermodynamic limits. The expected results are: system architectures uniquely designed to deal with escalating information and communications technology network energy costs; single chip optical phased array switches and switch fabrics that may be expected to revolutionise photonic network thinking and ignite interest in commercialisation; novel phase-shift elements and coherent electro-optical frequency up-converters that will ensure the competitiveness of 'digital coherent optical transport of radio over fibre' for broadband wireless access and significantly advance the research agenda on 'coherent optical transmission everywhere', which uniquely challenges conventional modulation architectures. The outcomes will contribute to the success of Canada's strategy for a digital economy; tearing down barriers to the realisation of 'sustainable digital cities' vital to the world economy in the third millennium. Most important, the research will develop highly qualified and skilled personnel promoting effective knowledge transfer and helping to fill the employment needs of Canadian industry.""525259,""Fleming, David"
"533859"	"Halpern, Yonatan"	"Québec"	"CANADA"
"535394"	"Hamilton, William"		"NSERC"
"524457"	"Hassanein, Khaled"	"Customized Decision Support for Older Adults in eCommerce Environments"	"Older adults are the fastest growing computer and Internet user group in both personal and workplace contexts in most developed countries.  The natural physical and cognitive changes associated with the aging process result in many difficulties for this user group with online interfaces as well as in being able to make high quality decisions in eCommerce contexts. This situation ultimately deprives them from reaping the full utility of the Internet and causes them to be less satisfied with their overall online experiences.  It also translates into a lost opportunity for online vendors due to the growing size and affluence of this consumer group.  The outcomes of the proposed program of research will result in significantly advancing our understanding of the specific challenges facing this important user group in making eCommerce decisions; and how to enhance their eCommerce decision making abilities through customized decision support that is tailored to their cognitive needs and decision making styles. The results of this research will enrich the literature in this field through the wide dissemination of our research findings in publications and forums targeted at both the academic and business communities. Online vendors could leverage the findings and guidelines developed under this research program to support their older adult consumers affording them substantially enhanced online shopping experiences and, thus by, potentially enhancing the Canadian eCommerce economy.   The proposed program of research will provide graduate and undergraduate students with ample training opportunities to develop skills in experimental design, statistical data analysis techniques, usability assessment methods, qualitative data analysis as well as developing academic, technical and managerial skills that are highly in demand within the Canadian eCommerce industry.""524458,""Guenin, Bertrand"
"524455"	"Hatala, Marek"	"Model and Methods for Determining Effectiveness of Open Learner Models"	"Open Learner Models (OLMs) have been applied in many traditional (desktop) online learning settings as well as in mobile learning. OLMs can fulfill several purposes, of which five are directly related to a student's learning: 1) promote learner reflection, 2) facilitate self-monitoring, 3) support planning, 4) encourage learners' control and responsibility for their learning, and 5) provide a source of information for competitive interaction. Although OLM researchers have been proposing and evaluating different types of OLMs, to date the evaluations have focused mainly on the usability aspects of the user models, and predominantly on teachers and administrators. Evidence that OLMs help to improve students' learning is lacking.This research aims to develop a theoretical model for studying the effectiveness of OLMs for improving students' learning and developing guidelines for selecting the most effective OLM form for a particular student in a particular learning situation. Within the 5 year period we will (a) develop a theoretical model and (b) an experimental method along with instruments (including full prototypes); (c) validate the model through experimental studies with students, and (d) derive a first set of guidelines for the studied combinations of OLM types and learning situations. The main contribution of this research is to provide a solid theoretical framework for establishing scientific evidence of effectiveness of OLMs in improving students learning, and demonstrating how to apply this framework for studying OLMs.""524456,""Molot, Lewis"
"526261"	"Haynes, Ronald"	"Parallel Space-Time Approaches for the Numerical Solution of Partial Differential Equations"	"Mathematically, many models of interest to engineers, economists and scientists are written as partial differential equations (PDEs).  Except for certain idealized situations, the PDEs which result are not possible to solve analytically. Instead we rely on numerical approximations.  I am particularly interested in the study of efficient implementations and analysis of adaptive algorithms for the solution of time-dependent PDEs in two or three spatial dimensions whose solutions exhibit large solution variation, singularity formation or moving fronts. We study methods which attempt to obtain a solution efficiently by concentrating computational effort (in both space and time) in regions where the solution has interesting but difficult to track behavior. The strategy works by using moving spatial meshes to track interesting features of the solution forward in time.  Moreover, there is an opportunity and motivation to study algorithms designed to take advantage of the continually evolving computing hardware - readily available commodity clusters with hundreds or thousands of cores, hybrid CPU-GPU systems and even desktop machines with 4-24 cores. We propose mapping the solution of time dependent PDEs to multi-core environments by dividing the large problem into small pieces computed on individual cores and recombined to give a solution of the original problem using domain decomposition (DD) algorithms.  Such a strategy will be used for both the generation of the adaptive grids, as well as the solution of the physical PDE.  Small scale parallelism in time may be added by computing simultaneous predictions and corrections. Ultimately, we will provide a new, theoretically based, modular platform for the parallel adaptive solution of time dependent PDEs suitable for existing and emerging HPC hardware. This research program provides a route to impact for moving mesh methods, providing a software tool for computational scientists for the solution of complex problems.  Theoretically it will enhance our knowledge of the behaviour of DD algorithms for nonlinear problems.  Finally, it will provide HQP with mathematical expertise and computational competency - transferable skills highly sought by employers.""526262,""Balogh, Michael"
"533297"	"Hossain, Masum"	"An energy efficient platform for mobile computing"	"Our mobile devices are struggling to meet end-user expectation and engineering reality. Compute capability of these devices are driven by exponential growth in mobile apps to support consumer convenience, context such as social networking, security and productivity. As a result, the mobile memory bandwidth is doubling every two years, and predicted 64 Gb/s mobile memory bandwidth requirement by 2017 will be well beyond the projected capacity of industry standard memory interface technology. However, advance in battery technology and thermal budget is falling behind. Based on current trend and international technology road map for semiconductor (ITRS) projection, in 2024 a hand held device will dissipate approximately 7 Watts of power whereas the projected thermal budget sets the limit to be 1 watt only. To summarize, main challenges for future mobile computing are: lower read and write bandwidth, higher latency, and limited processing power for the given battery life. This research proposal is motivated by these challenges. Energy inefficiency in mobile systems is mainly originated from limitation of hardware which is designed to support the highest capacity or peak performance. However, highest capacity of the system is utilized for only a small fraction of its operating time, and most of the time the system is under-utilized. Although the system is under-utilized, the existing hardware does not allow energy consumption to scale accordingly. This research aims at enabling energy efficient high performance mobile platform that will be built upon several key ideas: 1. Scalable on-chip network that enables energy efficient parallel data processing. 2. An intelligent resource management that can sense processor load, and accordingly enable, shutdown or change frequency of the core to compute at a different rates. 3. Agile, fine grained dynamic voltage and frequency scaling schemes that enables the resource manager to maximize energy efficiency. 4. Energy proportional, high bandwidth memory interface. Successful execution of my research will shape mobile processor and memory road map that meets the demand for future mobile computing in an energy conscious manner.""533298,""Mariantoni, Matteo"
"535148"	"JahnsGigglberger, RicardoAlberto"		"NSERC"
"524978"	"Jaseemuddin, Muhammad"	"Connectivity, Resource Management and Application-enabling Infrastructure for Networking of Smart Mobile Devices"	"The growing demand for e-society applications in areas such as health care, social networking, entertainment, and highway traffic management poses enormous challenges to the networking of Smart Mobile Devices (SMDs). These SMDs require power-efficient, high-bit-rate, and heterogeneous wireless networking support to meet the demands of emerging applications. In particular, the ubiquitous personal and social applications designed for SMDs demand more adaptable and power efficient networking protocols and algorithms. Many of these social applications tend to form clusters of users that participate or collaborate on common activities, in which the value of applications outside the cluster is diminished. SMDs are typically battery powered and require efficient power usage for communication. Interference in wireless networks increases power consumption in a number of ways; for example, higher transmit power is needed to maintain the required signal-to-interference-and-noise budget, and additional protocol processing is required to change paths when those in use are blocked. The proposed work will develop techniques to improve the power efficiency of SMDs that includes interference-aware protocol design. The concepts of diversity, cooperation, and heterogeneity will be key components of our design. This research makes significant contributions to the field by: (a) mitigating the impact of interference on medium access control and routing; (b) building on diversity techniques in wireless communication to translate their benefits of power reduction, rate gain, and range extension into improved system performance; (c) exploiting localized clustering to develop interference and mobility models for medium access control and routing; (d) supporting video streaming and collaborative applications through interference-aware mobile multicast routing; (e) creating power-aware cooperative caching by leveraging the collaborative decision-making capacity of current cooperative caching methods in order to optimize its power usage; and (f) developing integrated resource management scheme. The network and application support designed in this project will contribute to the ongoing growth of the mobile application industry.""524979,""Gao, Jun"
"527840"	"Jiang, Hai"	"Distributed Opportunistic Channel Access in Wireless Networks"	"To meet the high-speed wireless communications requirement in the near future, we need to significantly improve the spectrum utilization efficiency. One well-recognized way is opportunistic channel access (OCA), which has been conducted in the literature for channel-aware opportunistic scheduling (or in short from,  opportunistic scheduling) and for cognitive medium access. Opportunistic scheduling means that, in a multiple-user network, at any moment only the user with the best channel quality is scheduled to transmit. Cognitive medium access means that, if licensed users (called primary users) are not active at a time and a location, then unlicensed users (called secondary users) can utilize the spectrum opportunity. The proposed research consists of three components: optimal distributed opportunistic scheduling with quality-of-service (QoS) guarantee, distributed cognitive OCA with time/frequency correlations and sequential sensing, and multiple-user cognitive OCA with opportunistic scheduling. It is expected that the proposed research will provide engineering insights regarding how to jointly deal with multiple-user diversity, time diversity, cooperation, learning, and competition in practical wireless networks. And the research outcomes will help Canadian telecommunications companies develop and deploy low-complexity high-efficiency wireless networks.""527841,""Levi, Ofer"
"527689"	"Jodoin, PierreMarc"	"Analyse de contenu et détection d'anomalies pour des applications en surveillance vidéo et imagerie médicale."	"Mon programme de recherche, pour les 5 à 10 prochaines années porte sur le développement de méthodes de co-occurence statistique pour l'analyse de scènes et la détection d'anomalies.  Mes travaux se divisent en 2 thèmes : (2) la surveillance vidéo et (2) la détection de tumeurs cancéreuses.    Mes travaux en surveillance vidéo visent 2 objectifs, soient l'analyse de scènes et la détection d'anomalies. Pour atteindre ces 2 objectifs, 4 projets de recherche seront mis de l'avant.  (A) Le 1er porte sur l'identification et la segmentation automatique de chemins empruntés par des véhicules et des piétons, des points d'entrée et de sortie, des croisements de routes et des feux de circulation.  Pour ce faire, je compte développer des méthodes d'analyse de mouvement de foules en m'appuyant sur le modèle des matrices de co-occurrence statistique.  (B) Le 2e projet porte sur la recherche d'événements dans des séquences vidéo de très longue durée (des heures, voire des jours de vidéo). Pour ce faire, je prévois développer des techniques devant permettre à un utilisateur d'interroger en temps réel le contenu d'une vidéo de surveillance. (C) Le 3e projet porte sur la détection d'anomalies basée sur les modèles développés en (A) et (B). Les anomalies visées portent sur les changements de comportement d'une foule, les changements dans la dynamique du trafic autoroutier, l'attroupement de personnes, etc.  (D) Le 4e projet porte sur la validation des méthodes de détection de mouvement.  Je prévois mettre sur pied une nouvelle base de données comprenant des vidéos de surveillance acquises dans divers contextes.    Le 2e thème de recherche porte sur l'imagerie médicale et l'utilisation d'images saines pour localiser des anomalies dans des images d'animaux malades.  Dans les 5 prochaines années, je me concentrerai sur la segmentation de tumeurs cancéreuses dans des images par résonance magnétique chez le petit animal. Ce thème repose également sur le concept de co-occurence statistique. Ainsi, étant donné des organes en santé, un tissu sain devrait se retrouver simultanément dans plusieurs organes sains, de la même façon qu'une personne peut être filmée par plusieurs caméras simultanément.""527690,""Walker, Gilbert"
"523542"	"Jurisica, Igor"	"Ontario"	"CANADA"
"533164"	"Kaddoum, Georges"	"Secure digital communications systems based on nonlinear dynamics"	"Chaotic signals have been proposed as broadband information carriers with the potential of providing a high level of robustness and privacy in data transmission. At the present time, practical chaos-based communication transceivers fail to offer supportable data rates of the order of megabit per second. The primary goal of this program lies in providing a novel approach to digital communications from the features offered by chaotic signals that can support fast and efficient data transmission while meeting the current demands for low power, low cost and high security. In the first theme of this research program, we plan to improve spectral efficiency by proposing modulation schemes based on novel mathematical models of these communication systems. Later, we will analyze the benefits of extending some chaotic modulations from mono-carrier to multi-carrier combined with multiple-inputs multiple-outputs transmissions and space-frequency coding to meet the requirements in high data rate. Security has become increasingly important in healthcare, forensics, telecommunications and other fields in order to protect private databases. The applications of Random Number Generators (RNGs) are extremely important for the generation of cryptographic keys, the random initialization of certain variables in cryptographic protocols and secure applications. Because of the extreme sensitivity to initial conditions, many random number generators based on analog and deterministic chaotic phenomena have been proposed. Some of them have only been simulated, while others have not been sufficiently optimized for cryptographic applications. In this research program, we will first study the randomness and the optimized criteria of some proposed RNGs. Then, we propose practical implementations of RNGs that have recently introduced deterministic chaos circuit based on the Markov map. Our future transceivers will integrate these RNGs in order to increase security behaviours. Finally, we plan to implement hardware prototypes of the proposed systems and open new windows for exploring challenging problems in chaos-based communication systems.""533165,""Luong, Lien"
"527330"	"Karumudi, Rambabu"	"Robust ultra wideband radar systems for real-time sensing, characterization, and imaging applications"	"This proposal focuses on developing a robust sensor technology for real-time sensing, characterization, and imaging applications. The proposed sensor technology will be developed based on ultra wideband (UWB) radar principles. The targeted application areas of the research are in geomatics with special focus on tunnel construction, and e-health with specific goal of developing contactless human vital-sign monitoring systems.  The long term objectives of the research are in (a) Geomatics: It is well understood that the lack of real-time surveying instruments and solutions to detect the buried complexities such as unexpected boulders, sudden change from hard clay to unstable sand, and buried gas and water pipes at the tunnel face contribute to the high risk of tunneling projects. The goal of this research is to develop a viable real-time surveying tool that can be integrated with tunnel boring machine to estimate the geological parameters of the tunnel section immediately ahead of the tunnel face. The proposed research also extends the horizons of the state-of-the-art ground penetrating radar technologies. (b) e-health: Significance of contactless sensor technology for monitoring the human physiology, and microwave radiometer for monitoring the temperature of internal organs has been realized. However, fundamental limitation for this technology arises due to the fading radar cross section of the patient. Long term goal of this research is to develop a robust contactless sensor technology for the following applications: (i) contactless measurement of heartbeat, heart rate variability, and breathing rate monitoring, as well as generate alarm signals based on critical condition of the patient (ii) measurement of abdominal fat thickness and determination of quality of fat to help the studies of obesity in a patient.  Even though UWB technology is already in use for numerous applications the state-of-the-art UWB sensor technology limited by its lack of robustness. The proposed research develops the theory and design for robust ultra wideband sensor technology, and also study their implementation and integration to solve the above two problems.""527331,""Crawford, Dorota"
"523494"	"Kazerani, Mehrdad"	"Research on Enabling Technologies for Electrification of Transportation and Integration of Renewables in Remote Communities' Microgrids"	"All indicators point to electrification of transportation and integration of renewables in the energy systems as key technological strategies for the future. Paradigm shifts from conventional, internal combustion engine (ICE)-based to purely-electric in vehicle technology, and from fossil fuel-based to renewable-integrated in electricity generation industry are inevitable due to concerns about adverse environmental impacts of burning fossil fuels, rising oil prices, dependence on politically-insecure existing oil supplies, depletion of oil reservoirs, and low efficiency of energy conversion in conventional vehicular systems. Electrification of transportation and integration of renewables heavily depend on technologies such as electric machines, energy storage, power electronics, wind turbines and photovoltaics. Among these, power electronics is receiving special attention, as the enabling technology for power conditioning and multi-source integration. Based on extensive knowledge and experience accured over many years of research on power electronic converter topologies and control techniques for various renewable energy- and vehicle-related applications, the applicant proposes a research program addressing the challenges faced by the automotive and electric utility industries in development of high-performance plugin vehicles and integration of renewables in isolated diesel generation-dominated microgrids.""523495,""Prescott, Cindy"
"533291"	"Kelly, Jonathan"	"Continuous Self-Calibration for Lifelong Autonomy"	"A fundamental requirement in any multisensor robotic system is precision calibration.  To ensure optimal performance, the sensors need to be properly calibrated, both internally and relative to one another.  Incorrect calibration results in erroneous data fusion and, in the worst case, may lead to potentially dangerous manipulation or navigation failures.  Today, robotic manipulation and navigation techniques usually depend on offline calibration procedures, typically carried out by a human operator.  The calibration process, which is often tedious and time consuming, must be repeated whenever sensors are repositioned or substantial mechanical stresses are applied.  This need for calibration severely limits our ability to build 'power-on-and-go' robots that are able to function independently for long durations.    The proposed research will develop robotic systems that are able to update and maintain their own calibration during normal operation, without the use of external references, equipment, or assistance.  This will involve (i) establishing a unified mathematical (Bayesian) framework for joint calibration of a variety of heterogenous sensors (including inertial, odometric, encoder, visual, LIDAR, and RGB-Depth modalities), (ii) devising methods to determine, within this framework, exactly which system parameters can be calibrated, given motion and other constraints, (iii) designing platform trajectories to calibrate the sensors accurately and rapidly, and (iv) demonstrating successful automatic calibration of multiple sensors on-board a range of autonomous platforms, over periods of days to weeks.  An important aspect of the work is the rigorous assessment and verification of the performance of the calibration algorithms on deployed field robots operating in unstructured and dynamic environments (e.g., planetary rovers traversing rugged terrain).  The research will support the training of highly qualified personnel who are familiar with state-of-the-art sensor systems, their operation and their calibration.  In turn, the work will contribute substantially to developing Canadian expertise in the growing robotics field.""533292,""Wang, Liangliang"
"528343"	"Khoury, Richard"	"Microtext Processing"	"The term ""microtext"" describes a type of text document that is very short (typically one sentence or less), is written in an informal manner and unedited for quality, and includes some metadata. Microtexts have become omnipresent in today's world, and include online user comments, Facebook newsfeeds and Tweets, and search queries. By their very nature, microtexts are a very different challenge from traditional Natural Language Processing (NLP). Their short length hinders NLP methodologies that rely on word frequencies and statistics, and the abundance of spelling mistakes, typos, and unconventional abbreviations make standard linguistic resources unhelpful. Microtext processing should be seen as a new branch of NLP.This proposal brings together four parallel five-year microtext research programs. The first aims to develop microtext understanding algorithms specialized to the task of query understanding and answering using corporate knowledge bases. The second aims to develop microtext understanding algorithms specialized to the task of very precise event detection in Twitter. The third aims to develop microtext understanding algorithms specialized to the task of mining microblog discussions, specifically in the context of retrieving software development information. And the fourth aims to develop a ""military Watson"" that can monitor military chat and inform officers of crucial international events occurring. All four projects are done in collaboration with external research and industrial partners. Moreover, they all feed into a single unifying long-term objective: to pioneer and develop the new field of microtext processing, and create a core of microtext expertise. The development of microtext processing will be the next major milestone in Semantic Web and Web 2.0 development. The amount of microtext generated today is absolutely massive: for example, there are 4.7 billion web queries asked on Google and 340 million Tweets posted every day. Web users actively seek out and embrace new microtext technology. The strategic importance of this research for the Canadian economy cannot be overstated, and our multiple partner companies and organizations will benefit directly from our results.""528344,""LeCorre, Mathieu"
"525284"	"Kienzle, Jörg"	"Concern-Driven Software Development"	"With Model-Driven Engineering (MDE), software development is seen as a process of model production, refinement and integration. Models are built at different points or during different phases of the software development life cycle for different purposes using the most appropriate modelling formalism(s) at the most appropriate level(s) of abstraction for the task at hand, and model transformations are defined that establish relationships among these models. It has been shown that MDE can significantly lower the costs of software development. One of the main reasons why MDE is not widely used in practice is the lack of model reuse. Typically, models for a system under development are created from scratch, rather than reusing already existing models. This makes modelling often more cumbersome than coding, since most programming languages nowadays offer extensive libraries or frameworks facilitating code reuse.This research program proposes concern-driven software development (CCD) as the next generation MDE enabling large scale model reuse. Whereas classic MDE concentrates on models, the main element of focus in CCD is the concern, which encompasses a set of models at different levels of abstraction pertaining to a domain of interest to a software engineer. Concerns can have functional, non-functional, or even intentional characteristics (e.g., security), or pertain to specific solutions (e.g., communication protocols, design patterns). CCD builds on technology from the aspect-orientation, software product line engineering and reuse communities to package a set of models of a concern in such a way that they can easily be reused in an application-specific model. CCD has the potential to transform the software engineering discipline by allowing software engineers to become concern specialists and by enabling the creation of reusable concern model libraries. This will drastically reduce the modelling effort required to apply MDE in practice, and as a result make MDE a viable development choice for any company developing software, even in the context of rapidly evolving software development projects.""525285,""Thywissen, Joseph"
"534791"	"Klassen, Toryn"	"Massachusetts"	"UNITED STATES"
"526237"	"Krishnamurthy, Diwakar"	"Performance evaluation and management of enterprise application systems"	"Organizations have come to rely heavily on enterprise applications to support their customers.  These applications are typically used to provide business-oriented services such as online retailing and customer relationship management.   They are often deployed on complex, multi-tiered platforms, which integrate various components such as Web servers, application servers, and database servers.  An important consideration in building such systems is the Quality of Service (QoS), e.g., fast request response times, that the users experience.  Poor performance can frustrate end users and can cause economic losses to organizations operating these services.  The proposed research program seeks to gain an in-depth understanding of how the workloads experienced by such applications as well as properties of the modern multi-core server and virtualization platforms used to host them influence performance.  The overarching objective of the program is to leverage such insights to devise novel techniques for optimizing and managing the performance of enterprise application systems.In spite of the rapid proliferation of multi-core servers and virtualization, not enough work exists on defining new methodologies that can help organizations optimize the deployment of enterprise applications on such platforms.  A vast majority of existing research has focused on batch and scientific applications.  In contrast, lesser attention has been devoted to enterprise applications, which typically involve more end user interaction and are hence governed by more stringent performance requirements. The few studies that have focused on these applications have not considered in detail the implications of important factors such as burstiness, architectural features of modern server hardware, and virtualization-related software bottlenecks, which can influence their performance.  The lack of guiding methodologies can result in poor deployments, which cause enterprises to incur severe financial penalties and a loss of strategic edge.  This program will bridge this gap through new efforts in the areas of performance tuning, modeling, and runtime performance management.""526238,""Shafikov, Rasul"
"521373"	"Kwan, HonKeungPeter"	"Intelligent digital filter design and applications"	"A digital filter performs digital filtering to process (preserve, attenuate, or modify) different spectral components of a signal. Digital filters have been serving various aspects of our daily life. Personal and home applications where digital filters are involved include mobile phone, hearing aid, headphone, loudspeaker, photography, songs and music, radio, movies, CD, DVD, HDTV, etc. Indeed, the fields of digital filter applications are broad which include communications; cable, radio, and satellite broadcasting systems; antenna, radar, and sonar systems; consumer and industrial electronics; control systems; electric power and machines; biomedical instrumentation; seismic system; remote sensing system; aerospace and transportation systems; enhanced vehicle applications etc. In general, a digital filter can be designed and implemented to meet the high accuracy requirement of a demanding application. However, the nature of nearly all applications of digital filters is to process a signal without making use of the dynamic information of the signal to achieve what can be called intelligent digital filtering. An intelligent digital filter that combines the computing power of a digital filter and the brain-like abilities to process information intelligently is expected to offer unprecedented performances in applications. Imagine an intelligent digital filter in any of your devices such as mobile phone, hearing aid, car radio, and HDTV DVD combo that can continuously monitor different noise environments and self-adjust its filter coefficients to produce a clear sound at a comfortable level. By representing a signal, image, video, genome or any type of information as a numerical waveform, an intelligent digital filter can manifest itself to process a variety of information, and has a scope that has no boundary. This research program on intelligent digital filters will advance their designs and applications and will offer a powerful and unprecedented filter for signal processing. The technology developed will inspire new ideas, methods, and applications; and will enable better living in a forthcoming intelligent digital filter age benefiting individuals, families, organizations, Canada, and the world.""521374,""BrakierGingras, Léa"
"523603"	"Landry, ReneJr"	"Cognitive multi-antenna GNSS/INS Receiver Architectures and Methods for Indoor-Denied Navigation"	"L'objectif principal est de résoudre les limites technologiques et les défis scientifiques liés à la navigation dite « refusée » par GPS dans les environnements intérieurs, de façon autonome, sans infrastructures externes (WiFi, GSM, RFID, etc.). De nouvelles architectures et méthodes de traitement numérique des signaux GNSS seront étudiées pour permettre la navigation à l'intérieur et la détermination de l'attitude spatiale d'un récepteur GNSS autonome évoluant avec des signaux de navigation satellites très faibles (appelés « intérieur »). Le programme exploitera notamment toute la redondance, les nouvelles caractéristiques et possibilités de ces signaux GNSS. La méthodologie retenue consistera d'abord à étudier les avantages de l'utilisation d'un récepteur GNSS multi-antennes ayant la capacité de traiter tous types de signaux GNSS de différentes antennes (ex. : 2 à 8). La diversité des fréquences et des signaux GNSS, le traitement avancé des signaux multi-trajets ainsi que des nouveaux algorithmes de haute sensibilité seront étudiés en utilisant une architecture universelle d'un récepteur GNSS à coeur ouverte et brevetée. Une deuxième approche sera basée sur les principes de la radio cognitive (CR) en télécommunication. Le récepteur intégré proposé sera dénommé récepteur GNSS cognitif multi-antennes (MACGR). Une troisième approche consistera à analyser les avantages de l'utilisation des mesures brutes de capteurs inertiels de très faible coût (gyroscope, accéléromètre et magnétomètre) combinée à un processus d'apprentissage adaptatif comme aide au MACGR. Les retombées des recherches en relation avec la navigation GPS dite « refusée » engendreront et s'intègreront à une industrie d'un potentiel de plusieurs milliards de dollars. Les résultats escomptés ouvriront la porte à de nouveaux domaines, applications et services. Ce programme contribuera aux nouveaux besoins des industriels en matière de guidage à l'intérieur, d'une sécurité accrue pour les premiers répondants, à de nouvelles applications en tourisme, au domaine médical, à la défense et à l'industrie du transport, etc. Les retombées seront également profitables à de nombreuses autres applications du génie, notamment en télécommunications et dans les sciences géomatiques.""523604,""Porter, Stephen"
"526318"	"LaPierre, Ray"	"Nanowires for Energy"	"This proposal is aimed at developing a new solar cell or photovoltaic (PV) technology based on III-V semiconductor nanowires. The highest solar cell efficiencies achieved thus far use III-V semiconductor materials such as gallium arsenide, where research-grade solar cells have exceeded 40% efficiency under concentrated sunlight. These devices employ highly perfect semiconductor thin films in a multi-junction device structure, which consists of sub-cells connected electrically in series.  However, these cells are not optimized due to the need to use materials of similar crystal lattice to avoid the generation of defects in the films that would otherwise reduce efficiency.  Furthermore, the existing thin film technology is prohibitively expensive due to the materials used such as germanium.  To overcome these limitations, this proposal proposes a two-junction cell comprised of nanowires on relatively inexpensive substrates such as silicon. The main advantage of nanowires is the greater freedom to combine materials of different crystal structure to optimize solar cell efficiency. As a result, a nanowire-based solar cell should have optimum efficiency that is comparable to that of commercial thin film cells (>40%), while the former would significantly reduce cost by using cheaper substrates. The efficiency of a nanowire PV device is greatest when the nanowire diameter, length, and spacing are optimized as shown by our recent theoretical work.  Therefore, the main focus of this proposal will be the development of ordered nanowire arrays.  Scientifically, this proposal will provide key aspects in the design of nanowire-based devices. The proof-of-principle devices produced by this proposal will catalyze a significant worldwide increase within the area of nanostructured III-V/Si solar cells, an area where Canada will then be a leading contender.""526319,""Das, Gautam"
"522087"	"LEcuyer, Pierre"	"Modeling and Simulation of Stochastic Systems"	"This research program concerns the simulation and optimization of complex systems that involve uncertainty (stochastic systems). Stochastic modeling and numerical simulation are now primary tools in science, engineering, management, and several other areas. Simulation is often the only practical tool to deal with realistic models of complex systems, which are typically dynamic, stochastic, and nonlinear. Building sufficiently representative stochastic simulation models, designing efficient and reliable simulation methods, and using simulation to optimize decision/operation strategies in these systems, are tasks with enormous practical importance, but which remain very challenging. My goal is to contribute new ideas and methods to address these challenges both from theoretical viewpoints (for example, convergence analysis of algorithms and mathematical analysis of the structure of random number generators) and practical ones (empirical experimentation, software implementation, and adaptation to specific real-life applications). I work on general methodology as well as on selected applications in various fields such as finance, risk analysis, communications, reliability, and operations management in service systems. The main directions of my research, currently and over the next five years, are: (1) Study and improve the methods for generating (pseudo)random numbers by computer, for various usages (simulation, games, lotteries, etc.) and computing platforms (e.g., highly-parallel general-purpose processors with limited local memory); (2) develop and study randomized quasi-Monte Carlo (RQMC) methods for multivariate integration and optimization; (3) develop more realistic ways of modeling complex stochastic systems such as call centers, health-care systems, revenue management systems, and other types of operations management systems; (4) design efficiency-improvement (variance reduction) methods for simulation, including rare-event simulation; (5) develop stochastic simulation-based optimization methods for decision making in operations management.""522088,""AbdElAziz, Alaa"
"533368"	"LeNy, Jerome"	"Design of Robust, Adaptive and Privacy-Preserving Control Networks"	"Control networks connect sensors, actuators and computing devices embedded in our environment to vastly improve our abilities to monitor and regulate the physical world. As these networks become more pervasive and their size and complexity increase, there is a need to further develop the scientific foundations for their principled design, deployment and validation. Indeed, despite recent advances in the field of networked control systems, the gap between the models used to synthesize control systems and the complexity of the embedded platforms on which they are implemented is growing. This gap leads to overly conservative designs and fast increasing costs and delays associated to system development, testing, and maintenance. A second obstacle to the deployment of these ubiquitous embedded networks is the resistance, due to justified privacy concerns, of the individual users who could most benefit from them, as illustrated for example by the installation of smart meters for power control networks.    To answer these challenges, this research program proposes new interfaces between control, computing and communication systems enabling their seamless integration while guaranteeing the overall control performance. Moreover, rigorous privacy-preserving mechanisms for large-scale control networks are introduced for the first time, to protect the users who are increasingly responsible for providing the necessary input data and measurements.    Applications that will drive this research include the model-based design of avionics systems, intelligent transportation systems, energy-efficient buildings and smart grids. The expected outcome will be a set of theoretical results, design methodologies and software tools that simplify the task of building reliable, cost-effective and high-performance control networks, and applicable to systems of realistic complexity. By integrating the proposed privacy-preserving mechanisms, emerging large-scale control networks will also see increased user participation, thereby accelerating their deployment and improving their efficiency.""533369,""Li, Zukui"
"520321"	"Levesque, Hector"	"Representation and reasoning for autonomous agents"	"This research concerns the reasoning that a robot would have to do to interact properly with a world that is changing and incompletely known. Almost all the research in robotics today concerns basic level tasks such as dealing with noisy sensors (like sonar), controlling inexact manipulators (like robot arms), or determining how to move safely without colliding into people or obstacles. Higher level decisions, such as when a robot needs to move at all, in what order it should tackle its jobs, or what it should try to do if something goes wrong, usually end up being scripted in advance. The goal of the research proposed here is to investigate what it would take for an automated agent to be able to make such decisions for itself, based on it knowing what it is trying to do, what it knows about its world, and what it has been able to find out using its sensors. To get these decisions to come out of a general reasoning process, however, it will be necessary to express in a language suitable for automated reasoning, enough of what the agent needs to know about its world, goals, actions, ability, the cognitive states of other agents, collaborative task execution, and so on. The proposed research would examine various aspects of these problems, and attempt to develop high level agent controllers of this sort.""520322,""Madhavji, Nazim"
"534914"	"Lévesque, JulienCharles"	"Ontario"	"CANADA"
"520425"	"Levine, Martin"	"Here's looking at you! Describing Spatio-Temporal Human Events"	"Here's looking at you! Describing Spatio-Temporal EventsWhat does a person concentrate on when looking at another person? The answer is the face and the body. My research addresses both of these activities as observed in videos and based on automating the interpretation of human (i) behavior and (ii) emotions as indicated by their facial expressions. This process is referred to as automated ""visual surveillance"". It is concentrated on creating real-time systems for monitoring spontaneous (not posed or acted) human activities. The long-term goal of this research is to be able to simultaneously interpret both (i) and (ii).Neither real-time nor spontaneity are presently being emphasized in the research literature on visual surveillance. Furthermore, the literature shows that the performance of methods trained on acted or posed databases deteriorates by about 50% when applied to unconstrained videos. The practical objective of this research is based on the fact that currently there are thousands and thousands of indoor and outdoor video cameras in our living spaces, but no way to automatically analyze the obtained unconstrained data. The challenge is to significantly improve the available analytical and algorithmic tools for automated visual surveillance.""520426,""Lowther, David"
"524024"	"Li, Xun"	"Development of active surface plasmonic polariton waveguide"	"Conventional photonic or optoelectronic components are designed to deal with the optical wave directly. These components need a featured size in thousands of wavelength (i.e., in a range from 100micron to 1mm) in order to sufficiently amplify or manipulate the optical wave. Therefore, a chip in a typical size of several centimeters cannot have many such components integrated. The surface plasmonic polariton (SPP) wave, appearing as a resonance between the electromagnetic field and free electrons at a dielectric and conductor interface, can be tightly confined in a tiny cross-sectional area beyond the diffraction limit and can propagate at a much slower speed as compared to the optical wave, which means the SPP wave has a much shorter equivalent wavelength and the SPP device will only need a greatly reduced size for the same function. As such, we can build integrated SPP devices with greatly increased density to fulfill complex functions that will be impossible for conventional integrated photonic or optoelectronic devices in areas like optical communication and optical signal processing. We can also expect a greatly enhanced performance or even new functions on solitary SPP devices due to the largely extended interaction time of the SPP wave with its guiding medium and the greatly intensified nonlinear effect. The propagation loss in current SPP waveguides, however, sets a major limit to their applications. Our proposed active SPP waveguide in this research program is aimed to realize the guidance, amplification, and control of the SPP wave simultaneously. If successful, it will overcome the major obstacle in the exploration of the SPP wave. Consequently, a whole family of devices and their integrations can be developed ranging from the generation, amplification, modulation, transmission, and control of the SPP wave at the optical frequency. Hence it will have a significant impact on the development of advanced components for optical communication networks, sensor systems, and signal processing systems. The structure and device invented and knowledge acquired through this research program may also benefit other researchers in this field and a number of Canadian companies.""524025,""Veawab, Amornvadee"
"527889"	"Li, Yunwei"	"Smart Distribution Grid Power Quality Control using DG-Grid Interfacing Power Converters"	"Increasing concerns about energy costs, energy security, and greenhouse gas emissions, are motivating the power industry to integrate more renewable energy sources (RESs) into the power distribution system through distributed generation (DG). The smart or flexible distribution grid is a new concept that could enable this infrastructural shift in the power industry with improved reliability and power quality. In parallel with this, the distribution grid voltage distortion in North America is increasing rapidly as more nonlinear loads are connected to the system. Poor power quality affects sensitive loads and even interferes with telecommunication systems. The smart distribution grid can provide immediate improvement to power quality. This is especially true considering that RES based DG systems are connected to the distribution grid through grid-interfacing power converters. These grid-interfacing converters convert the RES output voltage to the grid-friendly voltage for system integration. However, the intermittent nature of renewable energy causes these converters to usually operate at lower than rated power. A promising concept is to use the available ratings from these DG-grid interfacing power converters to improve the power quality of the distribution grid. This Discovery program aims to develop the coordinated power quality improvement strategy using DG-grid interfacing converters. Three topics will be tackled: (i) harmonics compensation, (ii) voltage regulation and reactive power support, and (iii) power quality control in a hybrid AC/DC system. The proposed work is urgent and exciting, as research progress in this area will address the increasing power quality concerns through the smart distribution grid. The proposed research can also improve the competitiveness of RES based DG technologies and facilitate large scale utilization of renewable energy. The developed technologies have good commercialization potential, and will help to create high-technology jobs in Canada.""527890,""Bélanger, Simon"
"523760"	"Linder, Tamas"	"Source Coding with Delay and Complexity Constraints: Theory and Algorithms"	"The goal of source coding is to give a compact representation of data (e.g., text, image, audio, video) from which the original data can be reconstructed either perfectly or in a perceptually indistinguishable manner. Source coding is a key component in contemporary information technologies as it facilitates the transmission of information-bearing signals over communication channels such as the Internet or wireless channels. The research will focus on new challenges arising in the analysis and design of lossy source coding algorithms that incorporate practical constraints on memory, complexity, and delay, in contrast with the more established traditional theory of source coding where such constraints are usually disregarded.The objectives are divided into two main interconnected thrust areas. 1) The investigation of fundamental limits, optimal structures, and efficient code constructions for zero-delay source coding and its applications to sensor networks and real-time control with information constraints. 2)  The construction and analysis of low-complexity, low-delay, universal sequential source coding schemes in point-to-point and network settings, and their applications to efficient encoding of highly non-stationary information sources.The training component of the proposed research will provide 2 M.Sc. and 3 Ph.D. students each year with stimulating research challenges and immerse them in important current topics in information  and communication technologies. The research results will provide a deeper understanding of the fundamental principles of lossy source coding as well as its practical aspects, thereby contributing to further innovations in Canadian information and communications technologies.""523761,""Schulte, Oliver"
"522289"	"Lingras, Pawan"	"Recursive and iterative clustering in granular hierarchical, network, and temporal datasets"	"Clustering is one of the frequently used unsupervised data mining techniques for grouping similar objects. The proposed research program will investigate a novel iterative approach to clustering in a granular environment. An information granule represents an object. For example, a customer with certain purchasing patterns could be represented by an information granule. A granule is usually connected to other granules. For example, in a hierarchical environment, a customer granule will be connected to a number of product granules and vice versa. In a granular network, phone users are connected to other phone users. In a granular temporal environment, a daily pattern of events is connected to historical and future daily patterns. Traditionally, clustering of granules is done in isolation without any information on clustering of the connected granules. The primary theme of the proposed research is to simultaneously cluster all the granules iteratively. Each iteration will use results of previous clustering of connected granules, until a stable clustering of all the granules is achieved. In a hierarchical environment such as customers and products, it will mean that clustering of customers uses profiles of product clusters, and vice versa. For networked granules, a phone user is clustered using cluster profiles of the other connected users. In a temporal granular clustering, daily patterns will be clustered based on clustered profiles of historical and future patterns. These repeated applications of clustering are termed iterative in a hierarchy and are termed recursive in networks. The integrated meta-clustering of hierarchical, network, and temporal data is a multi-faceted project. Since clustering is unsupervised and we do not know the expected outcomes, it is important to study the quality of the resultant clustering. In addition to deriving quantitative evaluations, the notion of preference will be used to value a cluster based on how well-connected it is to more desirable objects. The iterative and recursive algorithms will be further modified for fuzzy and rough clustering, which allow an object to belong to multiple clusters. We plan to design, develop, implement, and test variations of the clustering algorithms for retail, mobile phone, engineering, and financial datasets.""522290,""Diamond, Miriam"
"533128"	"Liscidini, Antonio"	"Smart power optimization for wireless transceivers"	"The evolution of wireless communications follows two different directions: one focused on star-mesh architectures, with expensive infrastructure and ""low cost"" mobile terminals (e.g. 4G cellular network) and one based on cheap devices for peer-to-peer communications (e.g. WLAN, Wireless Sensor Networks (WSNs) and RFID). Cellular communication represents one of the most dynamic and fruitful markets of the last decade with more than 1.7 million of mobile phones sold in 2011 and similar volumes expected for 2012. Alongside this consolidated reality, Wireless Sensor Networks represent a new generation of wireless communication tailored to automation and environmental monitoring, with an expected growth from 450M$ up to 2B$ in the next 10 years 2021.For both cellular and WSNs devices, the battery life and the cost of the terminal represent key elements in the development of the next generation of wireless transceivers. The goal of this project is to develop circuits and subsystems able to minimize the power dissipation and limit the cost of such transceivers following two main principles: adaptive optimization and integrated functionality. The former is based on the idea of developing structures whose performance can be reconfigured as a function of the operating condition, discarding traditional worst-case design methodologies. The latter is focused on the design of circuits where currents and devices are ""reused"" to simultaneously perform multiple functions minimizing both area and power consumption.""533129,""Dumoulin, Mathieu"
"528085"	"Litoiu, Marin"	"Adaptive Mechanisms for Large Scale Software Systems"	"This project investigates adaptive mechanisms for large-scale software systems. It focuses on models, algorithms and architectures that allow large scale software systems to respond autonomously to changes in environment and user requirements. As cloud computing services mature and proliferate, an open research question is how to manage  large scale software systems deployed to distributed clouds, where multiple clouds are distributed on the basis of geography, data privacy, latency, or other dimensions.  Adaptive software systems are emerging as a new research and practice area in software systems engineering with the goal of making software more versatile, robust, continuously available and self-managing.First, using various combinations of distributed cloud systems requires an adaptive system that is both integrated and distributed.  We will explore arranging feedback loops, the basic building blocks of adaptive systems, in hierarchical patterns to optimize both local clouds at the low level and globally at a higher level.  We will examine how the loops interact and converge to a global solution, and determine how best to specify the desired global solution.  Second, adaptive systems can function in multiple modes depending on the system conditions, the workload, and the operating environment.  This objective addresses the question of how to model these multiple modes, and in particular what models, loops, and control algorithms are required.  Third, to improve the quality of decisions and the speed of implementation, we will experiment with superimposing two feedback loops, one to respond to long term trends in the application and another to address short term needs.  Accurate trend prediction and division of labor to avoid repeated thrashing and conflict between short and long term goals will be required.  This important work in adaptive systems enables the next generation of cloud computing, in particular the automation of complex systems to decrease costs, increase energy efficiency, and improve the quality of service.  These contributions will advance the state of the art in an area of practical application that is of great interest to Canadian industry, researchers, and citizens.""528086,""Cachazo, Freddy"
"525298"	"Lopes, Luiz"	"Smart sustainable autonomous power systems"	"The electric power system of the future is envisioned to be quite different from today's. Renewable energy sources (RES) will be connected at the distribution level, avoiding transmission losses and feeding local loads in a more sustainable way. Stationary energy storage systems and electric vehicles (EVs) will be used, along with controllable loads, to compensate for the fluctuating characteristics of the renewable sources, optimize the system in terms of efficiency and achieve high power quality and reliability (PQR). This is within the scope of smart grids. For this to happen, the electro-mechanical breakers used nowadays need to be replaced by power electronic converters and associated control systems to provide means for controlling the power flow in this very complex system. Besides, to make the system manageable, it should be broken down into small clusters or micro-grids, and controlled in a de-centralized way following a hierarchical structure. Modern homes and small buildings, if supplemented with RESs and storage units including EVs, have potential to be controlled as net-zero energy nano-grids. The home automation systems in place today do not offer this feature, being limited to load management only. Different internal distribution system configurations (ac, dc, dual dc and hybrid) and required multi-port power converters will be examined for increased reliability. The next (outer) layer, which corresponds to the association of nano-grids into ac micro-grids and connection to the medium voltage distribution system, will also be investigated. In this case, the objective is to develop power electronics interfaces (topology and control system) that will allow the utility operator to exploit ancillary services and control the active power exchanged with the micro-grid. Active power balancing can be carried out by a dedicated energy storage unit, at a third port of this ac-ac power interface, or by the micro-grid operator dispatching the interfaces of the nano-grids in a similar way. In short, this project will focus on the power electronic interfaces and energy management strategies required for the development of sustainable grid-connected and autonomous micro-grids.""525299,""Kherani, Nazir"
"525880"	"Lucet, Yves"	"Computer-Aided Nonsmooth Analysis and Applications"	"The Computer-Aided Nonsmooth Analysis and Applications research program aims at building new computational algorithms, and improving our understanding of deterministic optimization algorithms. The new algorithms will be applied to practical problems such as surface fitting and shape-preserving interpolation. The anticipated results will increase researchers' productivity with the release of a computer-aided nonsmooth analysis software, provide faster optimization algorithms, and improve calculations in fields that use nonsmooth analysis transforms, e.g., thermodynamics, robot navigation, computer vision, and network communication.    The research program will specifically focus on improving deterministic optimization algorithms by providing tighter under-estimates and by studying the second-order information. Finding a global optimum is a very challenging problem but recent progress has made that computation possible in a number of cases. The research herein will further contribute to that progress.    A new toolbox will be created to manipulate functions of two variables, and the resulting new algorithms will be applied to provide tighter bounds in deterministic global optimization. The manipulation and visualization of operators (also called set-valued functions) will be made possible through another toolbox, whose most challenging task will be to visualize objects in four dimensions using a combination of colours and animations. The existing computer-aided convex analysis toolbox will be enhanced with a new algorithm to compute the kernel average, a transform recently introduced in convex analysis that generalizes well-known convex analysis transforms. Finally, a new class of functions will be introduced to remove an intrinsic limitation of piecewise linear-quadratic functions.""525881,""Fraser, James"
"521199"	"MacCaull, Wendy"	"Ontology-driven and adaptive workflow in distributed settings"	"A workflow is a collection of activities performed by individuals, organizations or automated devices, such as computers, to accomplish a clearly defined goal.  Workflow management systems have been developed to manage these activities, but they suffer from serious deficiencies if the activities are part of time-sensitive, data-aware collaborative processes.  These problems are particularly acute for processes that are safety critical, i.e., processes where human lives are at stake.  We seek to develop techniques to model and reason about collaborative workflows, which interface with dynamically changing data at predefined decision points. We use graphical modelling languages, which can be automatically translated to executable code, reducing the possibility of coding error. The long-term goal is executable software systems which provide measures of quality assurance. We will develop methods to check that the workflow model actually achieves precise goals or specifications, through the use of a technique called model checking.  Model checking time sensitive workflows is usually very complex, and we must find ways to reduce the complexity. One way is to develop runtime monitoring methods which inspect the system at specific points to determine if at that point a problem can develop.  To be adaptive, and to facilitate collaboration, we organize facts and rules about the data in a knowledge representation structure called an ontology. An ontology permits the calculation of knowledge from the explicit facts; this reasoning can take a long time: we need to develop fast methods of reasoning, so the implicit knowledge is available in time to guide the workflow. The motivating applications for this research are workflows for healthcare processes, which occur in a variety of settings, must be accomplished in specified intervals of time, and must adapt to patient data and other frequently changing information; lessons learned can be applied to many safety critical processes.""521200,""Mavinic, Donald"
"524580"	"MacLean, Karon"	"Foundations and Applications of Affective Touch"	"Humans and animals communicate with ease and flexibility, to get things done, share emotions and find comfort. Computational systems do not, and the cost in productivity, errors, and stress disorders is immense and growing in step with dependence on connected, always-on technology. Machines endowed with sensitivity to human emotional parameters have the potential to lower this burden. Touch, proximal and socially loaded, is powerful in this respect, triggering strong responses in direct physical human-human or -animal interaction, and subtle effects incidental to using manual controls. However, there has been little design-oriented study of its affective influence.    Touch-based emotional communication is crucial to development and health; human-animal therapy is a valuable, albeit expensive, tool. However, these observations have not systematically examined the touch itself: authentic emotional touching is hard to control and therefore study. A controllable experimental platform is required to identify crucial elements of these transactions, how they work, and whether they can be recreated or supported for purposes such as anxiety self-regulation and chronic pain mitigation.     In the marketplace, industry-driven 'Need for Touch' scales acknowledge the role of touch in consumer decision making, and of individual differences. Touchophilic products command premium prices. Yet, the lack of heuristics or prototyping tools mean design of touchophilic experiences is ad hoc when done at all.    The high-level objectives of this multidisciplinary research are to (a) extend emotional touch communication in a social robot context (based on a robot's ability to sense emotion via touch gesture), develop therapeutic protocols, and establish its therapeutic practice in multiple contexts; and (b) to develop, promulgate and support with tools industry-relevant touch design heuristics for aesthetics and conserving attentional load. This research already involves multiple industry partners and has substantial, already-demonstrated clinical or commercial potential, and provides outstanding opportunities for HQP training.""524581,""Coppolino, Marc"
"525425"	"Madden, John"	"Engineering electrochemical devices: Photosynthetic solar cells and high power capacitors"	"Electrochemically active materials will be investigated that aim to provide low cost solar energy harvesting - including built-in energy storage, as well as high power supercapacitors. Photosynthetic protein complexes, also known as reaction centres, absorb light and separate electrical charge. The process is performed with near perfect quantum efficiency (almost every photon is converted to charge). The objective is to create solar cells that employ these proteins, or their synthetic analogs, to generate electrical energy with reasonable power conversion efficiency. In principle -based on the electronic states available in reaction centres - ultimate power conversion efficiencies of as high as 27.5 % are possible (with genetic modification).  Previous work has demonstrated generation using photoelectrochemical configurations very similar to dye-sensitized solar cells.  We aim to overcome serious challenges that have led to efficiencies well below 1 % including the lack of selectively of electrodes and poor light absorption.  Our group has proposed unique implementation to overcome these challenges. A novel cell design enables good light absorption and built-in energy storage.  A key reason for poor performance are lack of selectivity of electrodes resulting in poor charge transfer kinetics. This will be overcome by employing semiconductor electrodes, in which band positioning favours charge transfer from one mediator and restricts that of the other. With these and other modifications we hope to produce photo-electrochemical cells that demonstrate power conversion efficiencies in excess of 10 %, and store energy over a period of hours. In the longer term it is hoped that the approach will lead to a commercially relevant and environmentally friendly method of producing electrical energy from sunlight.Also proposed is a method of creating fast charging supercapacitors that can replace tantalum capacitors where low internal resistance and millisecond discharge times are required. A novel separator material and porous metal electrodes proposed to achieve this goal.Finally, the program includes the application of novel nanotube yarn based actuator technology, in much stronger than muscle contractions result for low voltage pulses.""525426,""vanKerkwijk, Marten"
"524272"	"Maggiore, Manfredi"	"Virtual Constraints: A New Paradigm for the Control of Motion"	"This proposal concerns the investigation of a new paradigm for motion control in nonlinear dynamical systems. The motion control problem involves designing algorithms to make one or more dynamical systems exhibit a prescribed behaviour. Motion control problems abound in all areas of engineering, and include locomotion in multi-legged robots, flight control of autonomous aircrafts, and formation control of terrestrial, aerial, and space vehicles. The traditional motion control methodology has limited robustness against disturbances and uncertainties in the environment, and it is therefore inadequate to address emerging applications such as locomotion in multi-legged robots and coordination of spacecraft formations. In the last decade, a new paradigm for motion control has emerged which has the potential to overcome the limitations of traditional methods, and may enable a new generation of motion control algorithms. This paradigm relies on the concept of virtual constraint, a constraint on the states of a control system that does not physically exist, but can be enforced via feedback control. The notion of virtual constraint has been used with great success to induce stable walking gaits in biped robots, but much research remains to be done to make the virtual constraint paradigm go beyond biped locomotion, and become general enough to be applicable to a vast array of engineering problems. The proposed research program will lay the foundations for a systematic theory of virtual constraints for a class of mechanical systems and autonomous vehicles, and will investigate its applications to the control of satellite formations and robotic teleoperation. The ultimate goal of this research program is a set of general tools to automatically generate a collection of motion primitives. Each motion primitive will be associated with a virtual constraint, and will correspond to a desired behaviour. An inexpert user will then be able to select a sequence of motion primitives to induce complex behaviours in the dynamical system. The outcomes of this research program may have considerable practical implications in areas as diverse as robotics, aerospace engineering, and rehabilitation engineering.""524273,""Houry, Walid"
"525684"	"Matrawy, Ashraf"	"Security in mobile environments"	"The security of the mobile environment is the focus of this research proposal. This proposed research program will consider the security of the mobile cellular network and location-aware services of the mobile environment.  The research will consider contemporary LTE (Long Term Evolution) and future LTE-Advanced networks as well as earlier 2G and 3G networks as all current operational networks evolved to run these technologies side-by-side and are expected to do for some time.On the mobile cellular network side, the proposal will investigate the relationship between the complexity of the mobile network and security issues. Investigating ways to reduce the mobile network complexity could help reduce its susceptibility to a number of attacks that exploit this complexity. The second investigated aspect is proposing evasion-resistant geolocation techniques to enhance the security of certain location-based services. Several attacks in mobile networks and on the Internet succeed because of the attackers' ability to hide their location. While most mobile devices are equipped with location capabilities, the information obtained from a service about a certain device location cannot be trusted since the device could send false location information to the service. This proposal will look into ways beyond relying on a device's IP address or the information supplied by the device to a service to verify if there is an attempt to evade the geolocation required by certain services.""525685,""YadidPecht, Orly"
"523371"	"Maurer, Frank"	"Software Processes for Small Teams Developing Scalable Applications in the Cloud"	"In the future, more and more software systems are expected to utilize cloud-based infrastructure to address the ""big data"" challenge. ""Big data"" applications pose severe software engineering challenges, including:-What design strategies will better accommodate rapid increases in the data size and the number of users? -How can we effectively test scalable service-based cloud applications?-How can we design software applications that rely on external services so that they are easy to evolve? And how can we design software in a way that allows migration of components among different cloud providers?Small teams, as typically found in small & medium sized enterprises (SME), are particularly challenged in developing ""big data"" applications due to resource constraints imposed from limited budgets.To build on my group's results and research strengths, our research will look at these issues from a software development perspective. Concretely, we will focus on:-Scalable Software-as-a-Service (SaaS) applications: We will work closely with partners to conduct case studies of how cloud-based applications are developed. We plan to investigate software engineering practices that small teams can use to build scalable ""big data"" applications for the Cloud. -Test-driven development: We will investigate test-driven development for building ""big data"" software applications in the cloud. We intend to develop rule-bases for common defects of cloud-based applications.  -Software design & API usability: We will study design patterns and develop guidelines enabling small teams to design, scale, and evolve cloud-based applications. We also intend to study and evaluate the usability and portability of software APIs for building cloud-based applications.""523372,""Lesher, CarlMichael"
"524238"	"McIlraith, Sheila"	"Software, Devices, Data, and People: Working together in the wired world"	"The confluence of a number of trends is resulting in a fundamental change in the way we interact with each other and the world around us: 1) 1/3 of the world's population is now online, with many near-perpetually ""connected"" via a diversity of devices; 2) the so-called Web of Things is upon us.  From thermostats to billboards, everyday things are being instrumented with network-enabled embedded processors so that they can communicate, control themselves or be controlled from a distance; 3) from street corners to software systems, much of the world is being monitored resulting in the ability to query the current and past state of aspects of the world; 4) the cloud has emerged as a means of sharing disparate services and data -- data that is noisy, incomplete, inconsistent, heterogeneous, often un-structured, and big; and 5) people and machines are intra- and inter-operating via a diversity of modalities and in a diversity of roles. Whereas people used to use computers to support their work, there is an increasing trend towards computational processes using people as part of a larger interoperating workflow - so-called Human Computation. Our vision is for a new class of component-based system that composes software, devices, data, and people dynamically, in an ad hoc fashion, in service of achieving some objective, much as a robot seamlessly utilizes a selection of its sensors and actuators in services of a particular task. The outcome of this research program will be the development of mathematical modeling and automated reasoning techniques that support the modeling, analysis, control, and optimization of complex real-world systems. This research program will advance our knowledge in the principles of Artificial Intelligence knowledge representation and automated reasoning as well as making fundamental research contributions to database and software engineering research.  These contributions have the potential to revolutionize the development and deployment of customized, adaptive, component-based systems, enabling seamless interoperation between software, devices, data, and people, in a way that is now impossible without human mediation.""524239,""Cote, Kimberly"
"535495"	"McKenney, Dave"		"NSERC"
"533404"	"McKeown, Martin"	"Assessment of multi-modal, genetically influenced, dynamic brain connectivity in disease states"	"Objects of the Proposed Research Program:This research will continue the applicant's work on statistical signal processing applications applied toclinically-relevant data. The long term goal is to develop novel, fundamental statistical signal processing approaches that will have a lasting impact on the fields of clinical brain imaging. Toward this objective, this proposal focuses on tackling two fundamental topics: data fusion: between imaging modalities as well as between imaging and clinical and genetic data; and dynamical aspects of brain connectivity.Modern imaging technologies has re-ignited a centuries-old debate about how brain activity is represented: after long assuming that brain activity associated with specific tasks is localized, there is now a greater appreciation on how diverse brain areas co-activate. Ongoing, dynamic association between spatially disparate brain regions appears critical for normal brain functioning and disruption of these connectivity patterns is a sensitive marker for disease. Accurate assessment of brain connectivity patterns is thus an overarching objective of this proposal.Scientific ApproachMost analyses examining brain connectivity have been specific to a technology (e.g. fMRI, EEG), each of which occupies a limited area in the spatiotemporal plane. Combining different brain modalities to provide a comprehensive assessment of brain connectivity at multiple temporal and spatial scales is non-trivial, as each modality measures different biological activity (e.g. electrical activity vs. changes in blood flow), and has different statistical characteristics. We will expand our prior work on fMRI, EEG-EEG and EEG-EMG connectivity, as well as characterization of simultaneously-recorded behavioral data with linear dynamical system models, so that this complementary information can be merged together to provide sensitive and specific markers for disease processes.The role of genetic influences in neurodegenerative diseases such as Parkinson's disease is being increasingly recognized. People with genetic mutations putting them at high risk for developing disease provide a rare opportunity to examine connectivity changes before symptoms emerge. In complementary fashion, classifying brain connectivity patterns within an extended family may suggest which members share a common disease-related phenotype, substantially narrowing the search for gene-related alterations associated with as specific disease. However, both genetic and imaging data suffer from common challenges: the data are inherently high-dimensional with relatively few available samples. In order to investigate how genetic and imaging data may be meaningfully combined, we will extend our work on sparse regression and sparse precision matrices.Most current models of brain connectivity assume stationarity of connectivity patterns during performance of a task. However the brain in inherently non-stationary, and modeling dynamic changes in connectivity patterns is still in its infancy. Starting with our prior work on assessing connectivity in fMRI data sets, we will employ a dynamical framework to explore deterministic alterations in connectivity patterns during task performance.Novelty and Expected Significance of Work:Besides providing a highly competitive environment for interdisciplinary training of HQP, this work willprovide a framework for multimodal assessment of brain activity, with widespread potential impact in the assessment of normal brain functioning and in disease states.""533405,""Chaker, Mohamed"
"533295"	"Memisevic, Roland"	"Learning to represent images and their relations"	"Finding a good data representation is a central first step in many intelligent information processing tasks, such as recognizing objects or faces in images. Recent advances in machine learning, known as deep learning and feature learning, have made it possible to automatically learn useful representations from raw data, and they have shown the potential to improve recognition performance in a variety of applications. Since feature learning methods extract representations from isolated, independent observations, they are complementary to traditional computer vision methods, like geometric inference methods, optical flow, or motion processing, which represent structure across multiple images not structure within a single image. This research will develop approaches to learning representations of the relationships between images. These may include images from multiple views of a scene or from neighboring frames in a video. Learning about relations will make it possible to introduce advances from feature learning into a broader set of computer vision tasks where relations are of interest. It will also provide a new, statistical perspective on motion processing, geometric inference and invariant object recognition. Applications of this research include advances in automated video understanding, image and video search on the web, and automatic multi-camera surveillance. New insights on the efficient encoding of image relations will also provide insights and testable hypotheses regarding theories of motion processing and geometric inference in the brain.""533296,""Gilbert, Penney"
"521605"	"Missaoui, Rokia"	"Social Network Analysis and Mining"	"Information networks are a key component of modern information architecture and represent a variety of networks, including social, biological and computer networks. The analysis of social networks like Facebook or Twitter has gained very large interests from researchers in different disciplines and lead to significant results in many fields such as specific group identification, information propagation, influence computation and group destabilization.The goal of the present research program is to analyze and mine heterogeneous information networks, i.e., the ones which have more than one type of nodes and/or links and could represent multi-modal data. The objectives are threefold: (1) handle some key issues in heterogeneous information network analysis such as link prediction and community detection, (2) study the problem of mining triadic and more generally n-ary association rules in order to further apply the findings to heterogeneous information network analysis and hypercube mining, and (3) explore additional operations on patterns that could be helpful to better meet the first two objectives. A pattern is a knowledge artifact like association rules or clusters extracted from data using data mining and analysis procedures.The novelty of this research program comes from (i) the development of a framework for analyzing social networks with a complex structure by adapting advanced data mining and analysis techniques, including formal concept analysis, (ii) the proposal of new solutions to the issues of link prediction and community detection by taking into account the peculiarities of heterogeneous information networks , and (iii) the contribution to the theory of formal concept analysis by proposing new procedures for computing triadic and  n-ary concepts, generators and association rules directly from multidimensional data.""521606,""Harvey, Pierre"
"527712"	"Mitran, Patrick"	"Performance limits and advanced methods in wireless networks"	"With the transition from principally wired communication infrastructure towards versatile, tetherless anywhere communication, wireless communication and mobile data have revolutionized the way society functions. This freedom and almost anywhere connectivity comes at a large cost -- that of expensive and capital intensive infrastructure. Despite these large costs and significant benefits, many key performance characteristics and enabling technologies for wireless networks are not completely understood. The research proposed here will support the PI's ongoing long term research program in wireless networks to address these issues.Specifically, this program will combine advanced, cutting edge wireless communication techniques with innovative network deployment topologies, such as heterogeneous and cognitive networks. The thrusts are to: i) design practical lattice (or structured) coding techniques and architectures for wireless networks, ii) study the effect of intermittent energy reliability (such as from renewable energy sources) and energy harvesting technologies in wireless networks, iii) understand and solve issues in physical layer security in wireless networks, and iv) compute the practical effects of non-asymptotic methods in wireless networks such as the backoff from capacity due to constraints on communication delay.The results of the research will have important scientific and technological impacts. On the scientific side, the research aims to characterize fundamental performance tradeoffs. On the technical side, the insights and algorithms resulting from this research will provide the critical knowhow to conceive and engineer future networks. The research will thus provide benchmarks, insights on performance tradeoffs, and practical algorithms. By advancing personnel trained in this knowhow, the research will enable Canadian industry to remain at the leading edge.""527713,""Maly, Monica"
"535194"	"Morrill, Susanne"	"Alberta"	"CANADA"
"522400"	"Mullins, John"	"Méthodes formelles d'analyse quantitative de la sécurité de l'information partiellement observable dans les systèmes ouverts"	"L'analyse formelle de la sécurité des systèmes ouverts conçus pour protéger l'information est devenu un champ de recherche très actif. Il recouvre aussi bien le développement d'une théorie de la sécurité de l'information que ces systèmes doivent assurer que celui de méthodes aptes à assurer leur correction. L'approche de la théorie du flux de l'information est une approche booléenne au sens où le comportement d'un système est soit bon, soit mauvais relativement à la sécurité requise. Toutefois, cette approche est trop grossière pour décrire le flux d'information probabiliste et prévenir ainsi les attaques basées sur l'analyse statistique capable d'inférer un secret à partir de ses observations ou encore, pour analyser le coût computationnel du flux d'information et prévenir ainsi les attaques contre la disponibilité des ressources. Ces toutes dernières années un champ de recherche sur les modèles probabilistes de flux d'information a émergé. Mais aucun modèle n'a encore été proposé pour quantifier aussi les ressources nécessaires mises en oeuvre par l'environnement pour réussir son attaque. Un tel modèle poserait les jalons d'un modèle formel d'analyse de risque sécuritaire. Ce programme vise donc à: Développer un contexte théorique général pour exprimer les propriétés de flux d'information quantitatives qui étende celui de l'opacité dans les systèmes de transitions au cas où le flux d'information considéré est probabiliste et pondéré; Développer des algorithmes de vérification de ces propriétés; et étudier le problème de compositionalité des systèmes ouverts, relativement aux opérateur de séquentialité et de concurrence,  qui préserve la mesure de sécurité de ces systèmes i.e. celui de décider quand une composante du système peut être substituée à une autre de manière sécuritaire. Le contexte méthodologique est celui des systèmes de transitions probabilistes et pondérés pour la modélisation et celui des séries formelles pour la spécification des propriétés de flux d'information quantifiés. Les méthodes de la vérification sont celles du calcul de mesures dans les Processus de Décisions Markoviens  pondérés (PDMP) et de diverses notions de simulation entre PDMP qui préservent les mesures de sécurité définies.""522401,""Belzile, François"
"521987"	"Nairn, David"	"Advanced Techniques for High-Performance Analog-to-Digital Converters"	"From medical imaging to smart phones, a myriad of devices in our daily lives rely on a specialized circuit known as an analog-to-digital converter or ADC for short. These ADCs translate signals between the physical world and the realm of digital computers. Unfortunately, the capabilities of advanced medical diagnostic tools and personal communications systems are limited by the ADC's ability to perform the signal translations in a timely manner. In this research program, massively parallel ADCs are being explored to overcome the signal-translation bottleneck. It is anticipated that these massively parallel ADCs will lead to significantly faster translation rates, thereby enabling new generations of  medical imaging systems with improved diagnostic capabilities and personal communication systems with a greatly improved user experience.""521988,""Blowes, David"
"535592"	"Newell, Edward"		"NSERC"
"533398"	"NgatchedNkouatchah, TelexMagloire"	"Queuing Analysis of Opportunistic Access in Multichannel Cognitive Radios"	"Due to the propagation characteristics of electromagnetic waves, the frequencies between 10MHz and 6 GHz are the most suitable for wireless communications purposes. With the ever-increasing demand for wireless communications and the popularity of various wireless technologies, the traditional approach to spectrum management based on fixed spectrum allocation, has resulted in scarcity of radio spectrum, due to the fact that most of the available spectrum has been allocated. On the other hand, studies have shown that vast portions of the licensed spectra are rarely used, leading to underutilization of a significant amount of spectrum. As a result, innovative techniques that can offer new ways of exploiting the available spectrum are needed. Cognitive Radio Networks (CRNs), which are based on Cognitive Radio (CR) terminals and wireless networking technologies have been proposed as the solution to current low usage of licensed spectrum problem. CR technology provides unlicensed (also known as secondary) users the capability to share the wireless channel with licensed (also known as primary) users in an opportunistic manner. CR networks are envisioned to provide high bandwidth to mobile users via heterogeneous wireless architectures and dynamic spectrum access techniques, while improving connectivity and self-adaptability of channel environment.The aim of this work is to develop an analytic framework for performing delay/waiting time analysis of opportunistic access in a cognitive radio network. Delay is an important Quality of Service (QoS) metric in wireless networks and can be used to characterize the number of secondary users that can be supported by the network. The outcome of this work will provide answers to questions such as: can an application with a certain required QoS be supported by the cognitive user? Can the cognitive user provide enough ""bandwidth"" for video streaming, voice over IP, or online gaming? These results will benefit the research community, the practitioners in the field, and the Telecommunications industry in Canada.""533399,""Faloutsos, Petros"
"534679"	"Nicolai, Garrett"	"Ontario"	"CANADA"
"523045"	"Okoniewski, Michal"	"Waves, simulation tools and antennas: from medicine to oil and gas"	"The last discovery grant was devoted to the development of electromagnetic simulation tools and methods, new fabrication processes and, in biomedical areas, to remote sensor developments. At the core of the program was the research into applied electromagnetics. The emphasis was on ""applied""; indeed the theory was developed, and simulation tools built to facilitate the design of new devices - students in the program went on to start new high tech businesses. A strong team was established, and a research environment created which fostered collaboration and knowledge transfer between students. This was a successful strategy, and I would like to continue this approach in the next five years. Objectives:The long term goals of the proposed research are to develop sophisticated systems, devices and simulationtools.  The specific aims of the proposed research are:-Develop reconfigurable antennas and antenna reflectarray and transmittarrays, using new micro-pneumatics based reconfiguration techniques-Continue development of Nuclear Magnetic Resonance (micro-NMR) sensor -Develop reconfigurable RF components and deploy them in substrate integratedwaveguides-Continue the development of fast, robust and flexible electromagnetic simulation tools and expand their use to problems in geophysics, -Develop  indoor RF based location systems targeting hospital assets.""523046,""Funnell, Robert"
"525898"	"Orchard, Jeff"	"Neural Computing with Oscillations"	"Understanding how intelligent behaviour emerges from a network of neurons has been the holy grail of psychology and neuroscience for decades.  Scientists are gradually peeling back the layers, exposing the neurological mechanisms behind simple behaviours, one by one.  Oscillations are ubiquitous in the brain.  Understanding their role is fundamental in our understanding of how the brain works in general.       This project proposes to investigate the role of oscillations in neural computing, with a focus on navigation and localization, an animal's perception of where it is in space.  A number of biological observations of the entorhinal cortex (EC) and hippocampus (HC) suggest that they are involved in navigation and localization.  Recent computational models have proposed a variety of mechanisms for how oscillations in these regions contribute to this encoding.  However, these models address the observations in a somewhat piecemeal and disconnected fashion, and lack an overarching, simple framework.      Through our experience in medical image processing, we have seen how the Fourier transform and its properties offer a theoretical framework for understanding and manipulating oscillations.  We plan to apply our knowledge of signal processing and propose a Fourier framework for these navigation processes.  Moreover, we will investigate what self-organizing principles might be involved in forming these oscillating networks.      A principled study of oscillations in this system might translate to other systems and other oscillations. Our work could pave the way to a better understanding of oscillation-related neurological disorders such as epilepsy, Parkinson's, Alzheimer's, schizophrenia, and possibly ADHD and bipolar disorder.  A deeper understanding of how oscillations are leveraged in the animal brain could also lead to breakthroughs in the use of neural models for intelligent robotics and brain-machine interfaces.""525899,""Jemel, Boutheina"
"520319"	"OShaughnessy, Douglas"	"More Accurate and Efficient Analysis for Automatic Speech Recognition"	"Efficient communicating with machines via voice will facilitate interactions that so far have been hindered by awkward interfaces such as keyboards and telephone keypads.  People nowadays have increasing need to interact with computers, yet this is still done mostly by typing.  Even attempts to seek information by telephone often require many cycles of listening to long messages and then pushing a button, because our capability to do reliable automatic speech recognition (ASR) is quite limited.  For some time now, refinements to basic ASR methods established years ago have improved performance, without radical changes to the basic approaches.  The recent vast increase in computational power and memory size has led researchers to attack increasingly difficult tasks such as continuously-spoken, very-large-vocabulary, speaker-independent, noisy speech over the telephone.  For some limited tasks, e.g., recognizing credit card numbers, or words drawn from medium-sized vocabularies and spoken with frequent pauses, recognition accuracy rises above 99%, and hence practical commercial products are available.  However, progress has been slow for the more difficult tasks of recognizing conversational speech or noisy speech.  Furthermore, recognition error rates remain high when generalized speaker-independent models are used to decode speakers not used in the training phase.The theme of our proposed work, increasing robustness of ASR, refers to the tendency for error rates to increase when sounds other than the desired speech corrupt the input signal or when speakers not used in training use the system.  It is our view that a major factor in raising robustness concerns the inadequacies of the current spectral analysis methods.  We propose to replace current analysis methods with a more appropriate technique that resists corruption by noise, and will further allow more efficient ways to adapt the ASR models to each new speaker's voice, including speech with significant accents.  Our research, if successful, would lead to significantly improved ASR performance, both in increased accuracy and decreased computation. It will lead eventually to a much more agreeable way for anyone to interact with computers.""520320,""Plamondon, Réjean"
"524116"	"Ozell, Benoit"	"Environnement d'analyse et de communication des résultats numériques en visualisation scientifique"	"La visualisation scientifique fournit un outil d'exploration et de validation des résultats numériques provenant de simulations de phénomènes physiques.  Mais on oublie souvent que ce même outil doit aussi servir à communiquer des résultats scientifiques à d'autres personnes en exerçant un peu l'art de « raconter en images » une histoire scientifique.  La visualisation scientifique utilise ainsi un langage visuel, basé sur l'infographie 3D, pour la traduction en images des données numériques.  Une visualisation réussie est ainsi une image ou une animation qui présente et communique des éléments scientifiques importants d'une simulation numérique de façon rigoureuse mais facile à comprendre.  On remarque toutefois que l'exploration et l'analyse de la solution en phase d'exploration autant que la production d'un « récit scientifique » pertinent et efficace exigent généralement un bon nombre d'heures de travail.  Il est en effet souvent ardu de déterminer une série de points de vue en 3D produisant une séquence animée qui illustre efficacement un phénomène tout comme il est difficile de bien choisir le mode de représentation des variables et les fonctions de transfert permettant de représenter les valeurs des variables.  Pourtant, même si la plupart des logiciels de visualisation scientifique permettent d'afficher plusieurs types d'images, animées ou non, beaucoup d'environnements servent simplement à la production d'images plutôt qu'à la communication d'informations.  Il n'existe en pratique pas ou peu d'outils et de techniques efficaces pour guider et aider à la création de séquences « visuellement parlantes ».Ce programme de recherche vise à concevoir et mettre en place des techniques et outils pour faciliter l'utilisation d'un langage visuel de communication, en particulier dans le cadre de la visualisation scientifique avec des images et des animations.""524117,""Paquet, Joey"
"524437"	"Panario, Daniel"	"Computations in finite fields and probabilistic analysis of algorithms"	"The overall area of my research is the design and analysis of algorithms. My recent research has centered around two lines: the investigation of algorithms in finite fields and their applications in modern communication areas, and the study of algorithms when randomness is considered. A main area of my research involves computations in finite fields and their applications. Algorithms that work with finite fields play a crucial role in innumerous practical applications in communications, cryptography, coding theory and information theory. My goal in this area is the design of efficient algorithms in finite fields, and the usage of elements from the theory of finite fields in engineering applications. Examples are efficient implementation of arithmetic using polynomials and normal elements, and  constructions of certain types of polynomials over finite fields (irreducible, primitive, among others). The main applications of these studies are in cryptography, with other areas including codes and combinatorial constructions.My second line of research is the probabilistic analysis of algorithms that comprises the study of algorithms when randomness is used. In particular, average-case analysis of algorithms focuses on understanding the behaviour of algorithms on random inputs. An essential goal of the area is to advance in the understanding of randomness. Immediate goals are better understanding of particular algorithms, and the development of new efficient algorithms based on this knowledge. The scientific approach is based on mathematical proofs involving generating functions for counting the properties of interest for the analysis of the algorithms, and asymptotic analysis. My goal in this area is two-fold: to develop algorithms for fundamental problems like searching with better performance than previously known algorithms, and to contribute to the mathematical understanding of random polynomials over finite fields.""524438,""Lisonek, Petr"
"525363"	"Pavel, Lacra"	"Decentralized optimization of dynamic multi-agent networks"	"What if we could guarantee some prescribed global behavior of complex, dynamically evolving networks by devising simple rules that locally guide the behavior of individual network nodes? Systems such as communication networks, power grids, transportation systems  are all examples of  networks comprising many sub-systems that often interact in dynamic ways. Such networks can be regarded as multi-agent networks, i.e., complex, networked systems composed of many relatively simple components, or ""agents"", that sense only their local environment, and have the ability to affect it, and the other agents within their proximity. Centralized control of such networked systems impossible, yet our need to guarantee their robust operation is becoming vital. Our goal is to develop a theory towards their decentralized optimization and control. Such a theory does not exist yet, it is a long-time out, but it is achievable.  Game theory and optimization are both powerful frameworks within which design can be done. Regardless of the specific application domain, the goal is to design local control policies for the individual agents or sub-systems to ensure that the collective behavior is desirable with respect to the system objective. The long-term goal is to have a theory for decentralized optimization of dynamic multi-agent networked systems. Because of its power of abstraction such research has applicability to diverse fields and the potential to be transformative.""525364,""Wells, Glenn"
"521091"	"Pelc, Andrzej"	"Distributed network algorithms"	"The subject of this research is the design and analysis of distributed algorithms working in a network environment. Such algorithms are executed by individual entities in the network, such as processors or mobile agents, without any central monitor controlling the execution. We investigate the impact of incomplete or unreliable information processed by distributed algorithms on their performance, and design algorithms working efficiently in the presence of faults, or in spite of incomplete knowledge of the network. We focus on the following topics: communication in (partially) unknown networks, computational tasks performed by mobile agents in networks, and network algorithms with advice. The objectives of the research program are: 1. Construct efficient communication algorithms (for such tasks as broadcasting and all-to-all communication) for networks whose topology is unknown or known only partially, and for networks some of whose components (nodes and/or links) may be faulty. 2. Construct efficient algorithms for such tasks as exploration and mapping of a (partially) unknown network by mobile agents, gathering all agents in one node of the network, detecting faults in the network by mobile agents, and others, under various restrictions on perceptive and moving capabilities of the agents, on their communication capacities and on their memory size. 3. Establish trade-offs between the amount of information about the network (advice) supplied to nodes or mobile agents and the efficiency of performing a given task in the network, such as communication, graph coloring, network exploration, or gathering. The novelty of our approach is in focusing on trade-offs between the amount of information available to network entities executing a distributed algorithm and the efficiency of accomplishing a computational task. The significance of this research is in showing efficient ways of coping with incomplete information and of handling faults while performing computations in various network computing environments.""521092,""Pigot, Hélène"
"520249"	"Peters, Joseph"	"Modelling and Analysis of Dynamic Networks"	"My proposed research is concerned with the efficient use of resources in dynamic networks, particular multimedia streaming systems, and with distributed decision-making in networks.  The goals of the first topic are to develop analytical methods for establishing performance bounds on burst scheduling in wireless multimedia streaming systems and on energy consumption in wireless mobile devices such as smart phones and tablets.  For burst scheduling problems, I propose to do this by adapting, extending, and specializing analysis techniques that have been developed for radio networks.  For energy consumption problems, I propose to adapt and extend algorithms and sensitivity analysis techniques that have been developed for scheduling resources in evolving and uncertain environments.  The goal of the second topic is to model the types of decision-making and consensus-forming that occur in evolving environments such as social networks.  I propose to do this by combining methods that I have developed to model the spread of epidemics in networks with classical distributed consensus techniques.""520250,""Lai, Edward"
"522501"	"Pierre, Samuel"	"Réseautique du futur"	"L'objectif principal de ce programme de recherche est de contribuer à l'effort mondial de recherche portant sur les architectures de réseaux du futur par la conception de nouveaux mécanismes de gestion de mobilité, modèles de livraison de contenu, cadres de gestion et de contrôle, et algorithmes de routage dédiés aux réseaux à topologie variable dans le temps. À plus long terme, l'objectif visé est la définition et la conception d'un ensemble cohérent de mécanismes, de schémas, de protocoles, d'architectures et de prototypes qui pourront alimenter les cercles de normalisation et inspirer le développement de l'Internet du futur.Pour réaliser ce programme de recherche, nous préconisons une approche holistique qui consiste, dans un premier temps, à analyser globalement les problèmes des réseaux du futur en tenant compte de leurs interactions avant de résoudre chacun des problèmes identifiés pris isolément. Cette approche tiendra compte de la demande persistante de transformation de l'Internet d'un simple paradigme de livraison de paquets hôte-à-hôte en un paradigme plus varié construit autour des données, du contenu et des usagers, au lieu des machines. Ainsi, pour contribuer aux architectures de l'Internet du futur, nous proposerons des méthodes novatrices et de nouveaux protocoles pour aborder les différents aspects susmentionnés, en visant une expérimentation à échelle réelle sur un testbed.L'importance et l'originalité des travaux prévus dans ce programme de recherche résident dans le fait qu'il s'attaque de manière holistique à un éventail de problèmes de fond dont la résolution conditionne les réseaux du futur : politiques de sécurité préservant l'identité et la vie privée des usagers, mécanismes de livraison de contenus capables de faire face aux exigences d'évolutivité des réseaux et de croissance rapide de la quantité de contenus à livrer, entre autres.""522502,""Sobiesiak, Andrzej"
"534318"	"Pinhey, Graham"		"NSERC"
"522122"	"Plett, Calvin"	"Radio Frequency and mm-Wave Integrated Circuits"	"The proposed research consists of the exploration of low cost, fully integrated low power, short-range high-speed wireless communications circuits. A typical application would be to transfer data at high bit rates between devices, for example, video cameras to display devices. Due to an increasing number of personal communication devices being used at low GHz frequencies, there is limited opportunity to for high data rate applications at such frequencies. It becomes necessary to explore higher frequencies where the required bandwidth is available. As an example, there is up to 7 GHz of bandwidth available at 60 GHz allowing data communications at GHz rates. In order to remain low cost, and for the possibility of being fully integrated, it is appropriate to use CMOS or BiCMOS technology which has the advantage that it can be used both for radio frequency front-end circuits, and for digital signal processing in the back end of the transceiver. However, with silicon it is important to do the design carefully to minimize the loss and noise due to the non-ideal substrate. As well, at these high frequencies the wavelength is decreased down to the mm range, and hence it is possible to use distributed components based on transmission lines. The structure of the transmission lines can be optimized to reduce substrate losses, for example by making it coplanar, however, this must be carefully traded off with the increase in layout area. In typical applications, distributed components might be used for matching and coupling circuits. Other existing examples of circuits that make use of distributed components are distributed amplifiers or rotary traveling wave oscillators. The trade-off between the possible higher quality of distributed com-ponents and the smaller physical size of lumped components will be made carefully, choosing structures that are the most beneficial for the particular application. Ultimately, the goal of this research will be to have designed the high-frequency components of the transceiver using an optimal mixture of lumped and distributed components.""522123,""Xie, WeiChau"
"527143"	"Ponomarenko, Sergey"	"Resonant Enhancement of Nonlinear Processes in Statistical Nanophotonics"	"The present proposal aims to research and develop novel ways to control material response to incident electromagnetic waves of very large intensities. In particular, we will explore strong enhancement of nonlinear optical responses of materials due to a resonant nature of the light interaction either with individual impurities, embedded into semiconductor materials, or with collective motions of free electrons at the interface between the air and noble metals. The novel control techniques have at their heart our ability to control statistical properties of light source emitting the electromagnetic waves which resonantly excite the studied materials. The results of the proposed research will find applications to the fields as diverse as physics, biomedical sciences, solar energy harvesting, environmental sciences, and national security. For example, the resonant enhancement of nonlinear responses at air-metal interfaces can be utilised to significantly improve the accuracy of the existing sensor technology. The latter will allow doctors to better detect cancer cells, the anti-terror squads and environmental experts to detect with the unprecedented accuracy even minute amounts of potentially harmful chemicals, down to the single-molecule level!  We also propose to examine the rogue-wave excitation mechanism(s) and their statistical control in resonant optical systems. Rogue waves are the waves of unusually large amplitudes that seem to appear from nowhere and vanish leaving no vestige. The knowledge of the rogue-wave excitation mechanism, statistics and control will pave the way for the realization of a rogue-wave laser, an optical device generating coherent large-amplitude optical pulses in a highly controlled manner. Our explorations into optical rogue waves can help our fundamental understanding of their universal nature, leading, in turn, to novel applications in the areas as diverse as engineering,  where we would like to be able to reliably predict the rogue wave occurence  probability, and the ocean travel where we would like to be able to avoid or suppress the rogue waves which have already lead to a number of fatal seafaring accidents.""527144,""Cameron, Christopher"
"527869"	"Poulin, Éric"	"Influence des modèles de réconciliation de données sur la supervision, la commande automatique et l'optimisation des procédés industriels"	"L'amélioration de la performance des procédés industriels (traitement des minerais, métallurgie, sables bitumineux, bioprocédés, etc.) est cruciale pour l'économie canadienne. La forte concurrence engendrée par l'ouverture des marchés oblige les entreprises à élever constamment leur niveau de compétitivité pour assurer leur survie. Dans ce contexte, la fiabilité et la précision des données de procédé jouent un rôle fondamental. Une information de qualité est essentielle pour assurer une gestion efficace des opérations de production et implanter des algorithmes de commande et d'optimisation performants. Malheureusement, les mesures industrielles sont généralement entachées d'erreurs et il est fréquent que certaines variables stratégiques ne soient pas mesurées ou tout simplement non mesurables.L'objectif du programme de recherche est l'amélioration de la qualité des données par l'intégration des connaissances théoriques et expérimentales du procédé pour l'implantation efficace d'applications de supervision, de commande automatique et d'optimisation. Une méthode permettant de traiter ces aspects est la réconciliation de données. Malgré que cette méthode soit actuellement utilisée en industrie, plusieurs défis demeurent au niveau de la recherche quant à son application pratique. Plus spécifiquement, les problèmes liés au développement du modèle de réconciliation, du calibrage des incertitudes et de l'évaluation des gains relatifs au couplage de la réconciliation de données à d'autres algorithmes seront considérés.La généralité des méthodes développées fera en sorte qu'elles pourront être appliquées à une large gamme de procédés. La majorité des entreprises possèdent actuellement des systèmes de commande et de gestion de l'information adéquats pour la mise en place immédiate des algorithmes qui seront élaborés et générer des retombées à court terme. Puis, comme le programme de recherche s'attaque à un problème de fond, l'utilisation de données de qualité accrue permettra une exploitation plus efficace des procédés à long terme.""527870,""Rudner, AdamDaniel"
"526492"	"Poupart, Pascal"	"Lifelong Machine Learning and Sequential Decision Making for Natural Language Interfaces"	"What could be possible if we let a machine learn continuously over its lifetime? Objectives:  The goal of this work is to develop lifelong learning algorithms with application to dialog management.  At a theoretical level, this research will investigate and advance the principles by which a machine can gradually learn over a long period of time, discover new concepts and generalize concepts to new situations.  This will be put in practice by developing open ended dialog systems for natural language interfaces.  Personalization of such interfaces at the language level and adapting to the habits and preferences of the user will benefit tremendously from continuous learning.Methods: Since continuous learning is a sequential process, the techniques developed will be in the framework of reinforcement learning.  Particular emphasis will be put on the development of non-parametric techniques that do not make a closed world assumption by allowing new concepts (e.g., new words, expressions, habits, goals) to be discovered and represented on the fly.  Hierarchical representations will be employed to organize and reason about the concepts from low level language units to high level user intentions.  The techniques will be deployed in speech and text interfaces for smart phone applications.Novelty and significance: Lifelong machine learning is a new research direction that remains vastly unexplored.  This research will develop the theory and practice of this new paradigm, which will enable a new breed of intelligent systems.  It will contribute to the next generation of natural user interfaces based on speech for smart phones, video-gaming and hands-free car consoles.""526493,""Seers, Patrice"
"524584"	"Rafiei, Davood"	"Enabling queries on relational data on the Web"	"While the World Wide Web provides public access to one of the largest and most heterogeneous data collections ever created, its querying capabilities have remained very much limited to keyword search. This reduces the efficiency of the search and the scope of the information that can be explored or retrieved. More specifically, a keyword search is not sufficient when the search is not bounded to a selection and involves join and set operations over the contents of the documents. For instance, a keyword search is not appropriate for querying relationships, and the Web contains relationships galore. Also, a keyword search is not very applicable when the granularity of the search result is smaller than a document, and the result is a more refined set of items or entities that satisfy some conditions.This proposal is to study and to address some of the issues that hinder querying relational data on the Web, in particular: (1) On extracting and aggregating information, we will study the relationships between different encoding schemes and the conditions under which two encodings may refer to or describe the same fact or express the same relationship. Understanding such relationships is a key in, for example, detecting if a query matches a piece of data, aggregating related pieces and filtering unrelated pieces. (2) On relevance ranking, we will be investigating models that can explain the conditions under which a fact or relationship is more reliable or trustworthy and the relationships between trustworthiness and relevance. (3) On data cleaning and filtering, we will explore strategies and filters that can be used to clean up resources that may introduce noise in the result, and more scalable data structures and algorithms to implement those filters. (4) A better understanding of the aforementioned issues and the solutions that we develop will enable us to look for more advanced searching and querying formulations. Investigating these formulations would further provide us with more opportunities to explore more efficient query evaluation strategies that are applicable in the setting of the Web.""524585,""Shirani, Shahram"
"520697"	"Rahman, Azizur"	"Development of power apparatus and advanced motor drive systems"	"One of the major causes of global warming and climate change is the production of green house gas associated with thermal power generation. Over 60% of generated electric power is consumed by electric motors. For many years, the applicant has been developing high efficiency interior permanent magnet (IPM) motor technology. High efficiency reduces consumption of electric energy which helps in the fight to counteract global warming by reducing gas emission. Over the previous 5-year grant period, the applicant focused on solving the basic design issues of standard line start IPM motors. This has resulted in significant contributions. The published research results have aroused considerable industrial interests and international collaborations. The applicant has received five major IEEE Awards, which indicates that his work is considered very significant by his peers and by industry. However, the demand of fast and precision control of speed is spearheading the design and development of advanced new electric motor drive systems. Thus, the objectives of the proposed research for the coming 5-year period are: a) Development of interior permanent magnet (IPM) traction motor drives for electric and hybrid electric vehicles having highly efficient performances, (b) Design and build energy efficient motor drives using intelligent controllers, c) Design and test innovative new technology for power inverters utilizing new wavelet modulation techniques. These objectives will be achieved in both software and hardware developments. Three Laboratory prototypes will be built and tested. These will be realized with the help of 8 undergraduate, 4 Masters and 9 PhD students and 5 post doctoral fellows. The scientific approaches will involve detailed analysis, design and development of system models and associated power electronics and control interfaces. High performance and efficient motion control technology will dominate the specific applications market worldwide including Canada. The topics of the proposed research on advanced electric machines and power electronics system are significant for economic exploitation in Canada. A five-year grant of $67,000 per year is requested to achieve the objectives of the proposed research.""520698,""Leonard, Jeremy"
"526229"	"Rajapakse, Athula"	"Protection, Control and Optimal Operation of Microgrids with Renewable Generation"	"A microgrid is formed by coordinating the operation of distributed generators (DGs), energy storage devices, and controllable loads in a section of distribution grid so that it can operate in parallel with the grid or as an autonomous power island. Microgrids provide an attractive structure to integrate small-scale generation based on renewable energy resources and energy efficient technologies such as combined heat and power (CHP) generation, and demand responsive loads. However, there are several technical challenges encountered in implementation of microgrids, especially when the generation is from the intermittent sources such as wind and solar photovoltaic (PV). These challenges include coordination of the fault protection in microgrids during both parallel and islanded modes of operation; detection of faults when the generators have limited fault current contributions; optimal use of energy storage and generation resources in islanded microgrids; and control of generation and energy storage to ensure voltage and frequency stability during disturbances. My research aims to develop a range of effective and economical protection solutions suitable for variety of microgrid designs using synchrophasor measurements, transient signals generated by faults, and hybrid approaches. This research will also develop cost effective control and communication structures needed to implement the protection and control functions in a microgrid, and energy management strategies for optimizing the utilization of renewable energy resources. As part of the research, a microgrid test bed that facilitates testing of new technologies through hardware-in-loop simulations will also be developed. The expected outcomes are robust protection, control and operational strategies that enable widespread implementation of microgrids.  My research program is expected to provide training to five Ph.D. and two M.Sc. students.""526230,""Blanchet, Pierre"
"534305"	"Rashwan, Abdullah"		"NSERC"
"528055"	"Rekleitis, Ioannis"	"Autonomous Exploration of Challenging Environments"	"The problem of exploring an unknown environment is one where robotic research pushes the boundaries of knowledge and technology. Coral reef monitoring with underwater vehicles, planetary exploration, aerial surveys for environmental monitoring or for search and rescue all share some similar characteristics despite the diversity of the domains. In all the above domains, a series of common problems need to be addressed. First, the robot should be able to localize itself by fusing information from proprioceptive sensors, and exteroceptive sensors. Algorithms for estimating the position and orientation (pose) of a mobile robot are considered significant enablers for robot autonomy, and thus, a large spectrum of robotics research has focused on them. The second fundamental problem in autonomous exploration is planning the trajectory of the autonomous vehicle in order to ensure accurate estimation and modeling of the environment while at the same time satisfying the criteria of completeness and efficiency.  In recent work I have developed an algorithm for the  complete visual coverage of known environment using an unmanned aerial vehicle in an efficient manner. In addition my work on balancing exploitation and exploration, that is the trade-offs between efficiency and accuracy, which was developed for indoor environments using a graph representation, would be extended for the underwater domain.The above components would enable robots to accurate collect sensory information from the underwater domain in a systematic and efficient manner. Underwater environments are particularly challenging due to limited visibility, locally self-similar structures, and the unpredictability of motions due to surge and currents. The final part of the proposed research would be to develop off-line bundle-adjustment based algorithms for the accurate reconstruction of environmental models combining position, intensity, and color information for used by scientist in the fields of marine biology and planetary exploration [J3]. In particular, autonomous operations over coral reefs would benefit greatly from accurate localization algorithms.""528056,""Tam, Michael"
"533279"	"Rigby, Peter"	"Contemporary Software Peer Review: Modern practices, fault prediction, and extraction of design decisions"	"Peer review has been acknowledged as a software engineering best practice for over 35 years. It is fundamentally a quality assurance technique that relies on experts checking each other's work.  Overly formal processes limit adoption of review practices and do not increase review effectiveness.Contemporary peer review encompasses a series of techniques that allow the review process to fit the development team and to be both lightweight and measurable by managers. Practitioners have driven the development of and tools that support contemporary peer review. While this style of review has been adopted in various forms in software firms, there has been no systematic study of it. Furthermore, the shift from synchronous paper-based review to asynchronous tool-supported review has left a rich archive of sophisticated discussion of software artifacts. The unstructured nature of this archive has meant that it has not been used for fault prediction and summarization of system history. I have developed a tool, ACER, that can accurately extract code elements, such as classes and methods, from freeform text and code snippets.     My work will (1) systematically describe contemporary review to practitioners and researchers;  (2) aid developers in isolating the vulnerable areas of a system and understanding system evolution. My research objectives are to1.1: conduct a systematic investigation of peer review in software firms, including AMD and Microsoft,1.2: adapt and validate contemporary review in new settings (DND),1.3: determine the impact of variations in review regime on software quality,2.1: use ACER to mine the content of review discussion to predict where review effort should be focussed,2.2: extract system evolution and design decisions from review discussions.   My overarching goal is to advance software development as an engineering discipline and to help government and firms produce high-quality software on time and on budget.""533280,""Gilroy, Joe"
"535690"	"Rose, Caroline"	"Nova Scotia"	"CANADA"
"525459"	"Roy, Sébastien"	"Distributed and Co-located Large Scale Antenna Systems in Dense Self-Organized Wireless Networks"	"It is estimated that one third of the world population is now online, a figure that is on the rise. Also, spurred by the ""smart phone and tablet"" era, each individual user consumes an increasing amount of Internet bandwidth, and the proportion of that bandwidth that is consumed wirelessly is also rising. In fact, global IP traffic will nearly quadruple by 2016. Furthermore, at that time, the number of devices connected to IP networks will be 3 times larger than the global population. To sustain such tremendous growth in the wireless sphere while providing better reliability and user experience than today's networks, and to do so at a reasonable cost with minimal carbon footprint present staggering technical challenges. One of the most promising avenues to solve this challenge resides in large-scale antenna arrays (LSAS), whereby cellular base stations for example would be equipped with hundreds or even thousands of small antennas. Theoretical work has shown that this would allow huge capacity gains, as the effect of multiuser interference and noise would vanish. Also, the required transmission power would shrink considerably, possibly by a factor of 1000. In conjunction two other key ideas, distributed antenna systems and self-organization, the present research proposal tackles the practical development of LSAS. Specifically, it addresses the development of algorithms and architectures supporting the evolution of dense and high bit-rate wireless networks (e.g. cellular, WiFi or other) through the use of very large antenna arrays, with a focus on 1- making available the advantages of large-scale antenna systems (LSAS) at a reasonable cost in the near future; 2- in the context of LSAS, developing effective low-complexity array processing techniques for detection, transmission, and channel estimation through (a) antenna or beam selection, and (b) array partitioning techniques; 3- exploring the concept of distributed arrays with self-organization; and 4- whenever appropriate, assess complexity and practical issues with specific hardware prototypes in mind.""525460,""Mirabbasi, Shahriar"
"522805"	"Rusch, LeslieAnn"	"Modulation strategies and phase noise mitigation for spectrally efficient optical coherent communications"	"The optical fiber provides an extremely low attenuation, high bandwidth (terahertz) communications media and is the backbone of modern communications networks. The cost per bit of communications has fallen exponentially over past decades, however we are quickly approaching the capacity limits via exploitation of wavelength division multiplexing with coherent detection and higher order modulation. The increasing level of data manipulations in data centres and in local networks, nonetheless, continues to push demand for greater information density on fibers. This proposal applies experimentation, modeling and numerical techniques to research into new modulation strategies in the context of coherent detection systems. The focus will be on spatial multiplexing in fiber with the concomitant cross modal interference and phase noise enhancement. Spatial multiplexing attempts to create new communications channels in the same optical media by exploiting quasi orthogonal channels - quasi because physical imperfections in materials and devices destroy the theoretical orthogonality. Critical analysis of the fiber and multiplexer induced modal correlations must be examined to determine which, if any, of the spatial multiplexing approaches can increase the information bearing capacity of fiber at reasonable complexities for digital signal processing. We will examine the performance of linearly polarized and orbital angular momentum guided modes for spatial multiplexing within the same analytical framework to quantify and elucidate their relative merits. Our long term goals are to develop expertise and experimental facilities for the examination of spatial multiplexing in optical fibers (multiple core fiber, few mode fibers and orbital angular momentum varieties); develop collaboration in design of specialty fibers supporting spatial multiplexing - reconciling systems analysis with design criteria; and to discover niche applications benefiting from spatial multiplexing.""522806,""Stone, Micheal"
"527278"	"Saeedi, ParvanehAkhtar"	"Computer Vision for Infertility Treatment"	"In today's society, infertility has become more common than ever before and is one of the causes of the ageing population and associated social-economic problems. In the last two decades the use of fertility services has increased dramatically as a result of the trend toward delayed childbearing. Economic and social climates, as well as pursuit of advanced careers, have influenced a significant number of women to defer childbearing to their late 30s and early 40s. Unfortunately, female reproductive capacity is severely reduced by the age of 40, with an increased chance of miscarriage. The Canadian Community Health Survey reports that about 11-16% of Canadian women suffer from impaired fertility (data for 2009-2010).  This translates to about 1 to 1.5 million women with impaired fertility aged 18-44. Assisted reproductive technologies (ART), in particular In Vitro Fertilization (IVF), mitigate the infertility problem with an average success rate of 51%.Some of the critical factors in the success of any IVF procedure are the quality of the sperm, the embryos and their development, and the response of ovaries to the associated medication. Assessment of the above qualities and responses are currently performed manually prior to or during, the IVF procedure. Such assessment requires expert knowledge and is therefore expensive and prone to error. The objective of this program is to offer computer vision solutions to automate and improve some of these tasks, which would then lead to higher success of infertility treatments.""527279,""Alam, Shafiq"
"523758"	"Salomaa, Kai"	"Automata with limited nondeterminism and applications of automata"	"Finite-state machines, or finite automata, are an abstract model of computation and a basis for the study of fundamental questions in computing. There are many applications of finite automata, for example, compilers, text searching, natural language processing, web services and program verification. By studying foundational properties of abstract models, such as finite automata, we can gain deeper insights into the complexity of computation, in particular, descriptional complexity. In descriptional complexity we are concerned with how succinctly certain objects, such as algorithms, can be specified. This is different from computational complexity which quantifies the resources, for example time, used by an algorithm. A commonly used descriptional complexity measure for finite automata is ""state complexity"" which quantifies the minimal number of states of an automaton recognizing a given language. Questions of descriptional complexity have become more important also from an applications point of view since finite automata used in natural language processing typically require millions of states. Nondeterminism is a model of parallel computation. The work of the proposal deals with finite automata employing limited nondeterminism and we want to determine how much we can reduce the number of states by allowing additional parallelism, and vice versa. The crucial task is to establish lower bounds, that is, we have to prove that no machine with fewer states can recognize the same language. We can use techniques of communication complexity to prove lower bounds for the size of automata with limited nondeterminism.The requested funding will support the research activity of 4 graduate students and one undergraduate student each year.""523759,""Dyck, Richard"
"534584"	"SalvailBérard, Adam"	"Québec"	"CANADA"
"534244"	"Sardana, Noel"		"NSERC"
"525539"	"Sarkar, Anoop"	"Multilingual Statistical Machine Translation"	"This proposal is about machine translation from one natural language to another, such as translation from English to French.  As a bilingual and multicultural nation, translation into multiple languages is socially important to Canada. Translation is also strategically important to Canada in order to facilitate our interactions with the world. There have been tremendous advances in the field of machine translation over the last two decades with rule-based approaches being replaced by statistical machine translation. Statistical machine translation solutions are now being offered by major companies like Google, SDL and IBM. However all contemporary translation systems rely on large amounts of data between specific language pairs such as French-English, ignoring many low-resource languages, e.g. Inuktitut, Haitian Kreyol, Malayalam, and Finnish (languages that are spoken by millions of people). This proposal aims to use linguistic insights about language families and use this information to develop multilingual statistical machine translation models and algorithms.  We aim to transfer information from source-target language pairs with a lot of resources, such as French-English to low-resource language pairs, such as English-Inuktitut.  This proposal addresses a basic unresolved issue in statistical machine translation research: learning a translation model between any pair of languages regardless of resource limitations.""525540,""Karsten, Martin"
"525026"	"Schneider, Kevin"	"Evolving Software Collaboratively"	"Software needs to continually evolve to meet changing user needs, business innovation and technological advances (e.g., cloud, mobile, multi-core, multi-touch). This research program investigates approaches for software teams to evolve software to meet these changing requirements. The goal is to support software developers to make accurate software changes quickly while improving the quality and usefulness of the software.Research projects will explore how software changes, how software can be designed to accommodate change, and how team-based development can be coordinated to efficiently and effectively manage change. Models, notations, tools and techniques will be developed and validated with field and case studies. Formal language processing and software analysis techniques will be used to support automated analysis and systematic change of large-scale software systems. The techniques will be applied in a number of application areas with a focus on software development tools that support team collaboration and creativity.The government of Canada estimates that there are over 30,000 Information and Communications Technology firms in Canada with over $140 billion in annual revenues. The results of this research program are intended to improve the competitiveness of Canadian software firms in delivering innovative, quality and usable software that readily evolves to meet emerging information processing advances and end-users requirements.""525027,""Perry, Stephen"
"533476"	"Schofield, Nigel"	"Multi-phase electromagnetic machines and power electronic conversion"	"The next five years will see a substantial increase in the number of all-electric vehicles (EVs), hybrid electric vehicles (HEVs), more electric aircraft, electric ship propulsion, renewables (wind) and other industrial applications using energy/power conversion via electromagnetic machines and power electronic converters.  The emphasis for future designs is continuing improvement in power density and lifetime, factors that ultimately impact system cost and application uptake.To have greater impact on application, it is desirable to integrate machines and their associated power electronic converters into complicated systems that share component count to minimise cost.  Thus while compactness of systems is achieved, reliability of the power conversion components can be compromised by the arduous nature of the system operating environment.The main aims of this proposal are to investigate the impact of using higher phase numbers for electromagnetic machines and their associated power electronic converters specifically targeting improved specific and volumetric power density and increased lifetime.Key objectives are: (A) to significantly improve the prediction of ironloss in electrical grade lamination steels used for the construction of electromagnetic machines, (B) improve electromagnetic machine power density, performance, packaging and hence cost, and (C) reduce or eliminate the system electrolytic (or other) capacitors on the system DC-link; devices that are major inhibitors to system lifetime, volume and cost since they are constrained by operating temperature and altitude.""533477,""Gilmour, Robert"
"527067"	"Schriemer, Henry"	"Ultra-Low Power Integrated Microlasers for Optical Interconnect Technologies"	"Consumer demand for novel information and communications technologies with increasingly sophisticated applications continues to drive the semiconductor industry to achieve ever smaller and faster devices. New ways of sustaining this growth in technology are being sought, and promising approaches are nearing commercial realization. This growth increasingly requires moving immense quantities of information extremely rapidly - between users across vast distances, within devices from the core to peripherals, at the component level from chip to chip, or even on the chip. About 2.5% of our global energy consumption is now dedicated to this task, and this fraction is growing rapidly. The problems of heat and power consumption can no longer be solved at the purely electronic level. Over long distances, we now rely on optical solutions. This proposal brings the optical solution down to the chip level. With my team of graduate students and colleagues, I will design and build a fundamentally new kind of ultra-small laser for eventual seamless integration with the electronic architecture of current and future generations of computer chips. We will use engineered materials called nanowires as the optically active part (the ""gain"") of our microlaser. This is the part with the potential for electronic integration. We will achieve very low power lasing by encasing the nanowires in a material called a photonic crystal (the ""cavity""). This is an engineered material that manipulates and controls the laser system's optical response by either forbidding or allowing light to move in certain ways. The laser is extremely small because photonic crystals work at sizes of only a few optical wavelengths, the fundamental limit to tailoring the flow of light. By independently engineering the materials that realize optical gain and cavity response, we have bypassed a fundamental manufacturing constraint of contemporary approaches to solid state microlasing. We will focus on realizing lasing in photonic crystal architectures, and then work toward achieving electronic integration. Because this integration will be solely through the nanowires, we anticipate that it will be far faster and have much lower power requirements than contemporary approaches.""527068,""Shahbazpanahi, Shahram"
"523762"	"Schuurmans, Dale"	"Learning Predictive Representations from Incomplete Data"	"Almost all data is now digitally stored, with unprecedented computing power available to process it.  These developments have created new opportunities to advance computer interpretation (e.g. natural language processing, computer perception) and intelligent data-analysis (e.g. bio-informatics) by analyzing the massive amounts of complex data stored in text, multimedia, and scientific repositories.  My research addresses the fundamental challenge of synthesizing predictive models from data, focusing on the problem of learning predictors in the presence of latent variables and incomplete observations.  These challenges are heightened by the fact that predictions in complex domains are not just simple class labels or scalar values, but are complex structured outputs---such as parse trees, scene labellings or graph labellings---that involve multiple outputs to be predicted in a coordinated fashion, usually with intervening latent variables.  The key problem is training complex predictors when some of the output or intervening latent variables are unobserved.        To tackle these problems, I will develop convex training principles that combine model optimization with inference of missing components.  Convexity decouples parameter optimization from model quality: a poor result arises from poor modeling choices, not a poor local minimum---thus separating specification from implementation.  A key insight is that training with incomplete data can be tackled by treating missing components as auxiliary variables to be optimized (i.e. inferred) simultaneously with parameter optimization.  Convex formulations of joint training and inference can then be obtained by one of two strategies that I have been developing: working with relaxed equivalence relations over missing components, or deriving implicitly induced regularizers.  These approaches have already led to fundamental advances in unsupervised and semi-supervised training, including state of the art methods for dimensionality reduction, robust estimation, and latent large margin models.  One of my long term goals is to commoditize solution methods for challeng- ing machine learning formulations, such as predictive representation learning and data component discovery.""523763,""OLeary, Stephen"
"522021"	"Shallit, Jeffrey"	"Avoidability and Decidability in Formal Languages and Automata"	"My current research involves two areas from theoretical computer science:  (i) decidability of problems in formal languages and automata theory and (ii) avoidability in words.Decidability is an old theme in theoretical computer science.  Here we have some algorithmic problem and want to know if there exists an algorithm that will infallibly solve it.  One aspect of my work addresses decidability questions involving infinite sequences, particularly those generated by automata.     For example, subword complexity (the number of distinct sub-blocks of length n in the sequence) has been widely studied in the literature.    We might want to characterize, for example, the n for which there exists a sub-block of length n that is unbordered (contains no nontrivial prefix that is also a suffix).  Together with my students, we have developed theoretical tools to generate algorithms to solve these problems, and we have implemented them in software.   As a result, we have been able to solve a number of open problems in the literature.Avoidability has its roots in the work of Axel Thue, a Norwegian mathematician who published two influential papers in 1906 and 1912.  He showed that it is possible to construct an infinite sequence over a three-letter alphabet that contains no two adjacent identical blocks (of arbitrary size), and an infinite sequence over a two-letter alphabet that contains no two adjacent identical blocks followed by the first letter of the first block.  Avoidability is now studied in dozens of papers and has some applications to cryptography.  My work concerns difficult variations of Thue's problem, such as the possibility of avoiding, over an integer alphabet, three consecutive blocks of the same size and same sum.  This will lead to deeper understanding of the inevitable regularities in sequences.""522022,""Ormiston, Scott"
"525270"	"Shen, Weiming"	"British Columbia"	"CANADA"
"533813"	"Sherstan, Craig"	"British Columbia"	"CANADA"
"522035"	"Sousa, Elvino"	"Autonomous deployment, optimization, and self-healing in beyond 4G wireless networks"	"We are targeting the problem of providing wireless access for emerging roadband wireless services with a unified wireless air interface technology. It is desirable to utilize a single wireless technology which is compatible with evolving cellular technologies based on LTE-Advanced. The advantage of using a single technology rather than a hybrid of cellular and WiFi technologies in the same terminal, is simplicity and uniformity in terminal design and network operation.  Another major advantage is that of interference management and network capacity. Cellular technologies have been designed from the early cellular systems to have inherent power control and interference management mechanisms that are far more efficient than what can be done with WiFi networks. This approach to the future evolution of wireless systems recognizes that in terms of engineering design wireless technologies such as LTE have strong advantages over technologies such as WiFi in the sense that they were designed for access to a fixed infrastructure versus WiFi networks that were designed mostly for ad-hoc operation where all devices are mobile terminals. However WiFi networks as deployed had very strong advantages such as lost cost access points and ad-hoc deployment by customers. The aim of the project is to take cellular technologies and enhance the air interfaces to allow them to be deployed in great numbers by users and configured autonomously by the system. The autonomous infrastructuretechnology vision encompasses the current femtocells concept but is much more general in the sense that it treats the regular cellular system and the small-cells as a single unified system, whereas most of the current femtocell solutions are based on a heterogeneous approach to network design.  The proposed solution will have a drastic impact on the evolution of cellular systems in the sense that much greater capacities will be available for new services. Most of the analysis for cellular networks usually assume a system of somewhat regular cellular grid and similar cell size. We need to devise approaches to manage RF carriers, antennas beam patterns, powers levels or cell size, and traffic scheduling for these randomly deployed networks.""522036,""Sayari, Abdelhamid"
"529488"	"Stavness, Ian"	"High-Fidelity Simulation of Human Muscles and Movement"	"Physics-based computer animation has improved our ability to simulate natural phenomena such as colliding objects, splashing waves, and moving characters. Although these simulation techniques can create compelling special effects for movies and games, the practical use of computer simulation in product design, ergonomics, and medicine remains very limited. In order for simulations to have significant impact beyond animation, they must not only be visually appealing, but also physically accurate and evaluated with experimental data. The proposed research program seeks to improve and evaluate the physical accuracy of human body simulations for applications in biomechanics. Our results will overcome the current limitations of human body simulation and improve our ability to diagnose movement disorders, guide rehabilitation, and train athletes.    Two short-term research goals address fundamental limitations of current biomechanics simulations: simplistic visualizations and low-fidelity muscle models. We will create new techniques to visualize internal muscle deformations, realistic face animations, and exaggerated yet plausible character movements. We will also improve the fidelity of complex muscle models, such as those with feather-like structures, internal tendons, and broad attachment areas, to improve predictions of how muscle forces are generated and distributed. To verify our techniques and validate our results, our visualizations and simulations will be compared with measurements from real humans.    This research will have considerable impact. The proposed projects will raise the bar for human simulation in terms of visualization richness, muscle detail, and evaluation methodology. Our computational methods and models will be widely disseminated through two world-leading, open-source software projects: ArtiSynth at UBC and OpenSim at Stanford University. In the long term, we aim to achieve a deeper understanding of human biomechanics that will greatly increase the predictive power of computer simulation for animation, training, diagnosis, and treatment planning.""529489,""Hoey, Jesse"
"521585"	"StDenis, Richard"	"A Theory of Discrete Event Systems as a Foundation for Component-Based Software Engineering/Une théorie des systèmes à événements discrets comme base à l'ingénierie du logiciel"	"There are two ways to consider programming: as an art and as a science. The latter way cannot be done without theories, in particular when the component-based software development approach is used to cope with the complexity of modern software systems, which are built by assembling reusable components. Since control constitutes an important part of a component, it is natural to apply the principle of separation of concerns, such that control explicitly appears, and exploit a control theory to solve the underlying control problems. Therefore, the ever-increasing influence of formal methods combined with software evolution requires reexamining current component models and hierarchical control theories to provide better tools for system developers. In this perspective, the main goal of this research program is to develop formal methods to lay foundations for a new theoretical framework for component-based software engineering, which will be centered around mathematically-based control theories, with applications in service-oriented computing and networks of autonomous agents. The main obstacle is to find formal component models and hierarchical control theories that perfectly fit in order to keep the balance between theory and practice. This research will contribute to make formal methods more scalable to real software applications and increase safety, security and quality of service in software solutions.""521586,""Barron, John"
"525477"	"Steffan, John"	"Programming FPGAs using Software via Overlay Processing Engines"	"Powerful and cheap computing drives innovation and leads to new technologies in many fields that improve lives around the world.  However, if we want to continue the rate of improvement in computing performance seen by the last decades, then we must now innovate ways to reduce the power consumption of computers while improving their performance---and Field-Programmable Gate Arrays (FPGAs) are one of the most promising ways we might overcome this challenge.  FPGAs are integrated circuits that are organized such that they can be programmed and reprogrammed to emulate a hardware design that exactly matches the requirements of an application, and hence can have much better performance-per-watt than alternatives.  This has led to growing commercial interest in using FPGAs for general-purpose and scientific computing---however, those communities mainly employ software programmers that use high-level programming languages such as C, C++, and Fortran, and that normally wait only minutes to compile a program.  In contrast FPGAs currently require expert hardware designers to program and debug them, and a large FPGA design can take up to a day to compile.  A potential solution to this roadblock to the adoption of FPGAs in general purpose computing is overlay processing engines: application-specific processor designs that can be placed on an FPGA but are themselves programmable by high-level programming languages, allowing software programmers to more easily and quickly capitalize on the potential of FPGAs.  The proposed research program will study and develop the worlds most powerful overlay processing engines for FPGAs by creating novel and aggressive FPGA-centric processor architectures, and by leveraging compiler support to increase parallelism and performance while keeping hardware simple.  Processing engines of the proposed calibre have yet to be designed and proven for FPGAs, but if successful will allow FPGAs to be adopted by a much broader range of companies, application developers, and scientists.""525478,""Keselj, Vlado"
"527316"	"StHilaire, Marc"	"Planning and Design of Next-Generation Wireless Networks"	"Wireless and cellular networks have unbelievably spread across the globe during the last three decades. Third generation (3G) networks were very popular and next generation networks (4G and 5G) have the potential to be even more popular. A recent study by ABI Research predicts that global LTE subscriptions will exceed 40 million by the end of 2012. However, these new network generations are costly and complex which makes the planning and design very challenging. In this context, the competitive market of cellular networks mandates operators to capitalize on efficient design tools. Planning tools are used to optimize networks and keep both operators and users satisfied. On one side, users expect to have seamless access to different high quality services with affordable prices. On the other side, operators expect to have an always-operational network with high number of users, capacity and quality with low capital expenditure and operational expenditure. As a result, the main objective of this research program is to develop a new generation of models, algorithms, and software tools in order to plan, design, and operate next-generation wireless networks.""527317,""Vorobyov, Sergiy"
"525491"	"Storjohann, Arne"	"Algorithms for the exact solution to problems in linear algebra"	"Computer algebra systems such as Maple or Mathematica, and more recently SAGE, have become essential tools for mathematicians, computer scientists and educators.  These systems allow computations with exact arithmetic on symbolic inputs over a wide variety of domains, for example integers and polynomials.My research focus is primarily on problems involving matrices with integer and polynomial entries.  Computations with integer and polynomial matrices occur frequently in diverse areas such as cryptography and linear systems theory.  The primary objective of the research is to discover improved algorithms for transforming matrices to certain ""normal forms'' which naturally reveal the structure and information that is encoded in the rows and columns of the matrix.  The goal is to achieve algorithms which are nearly optimal in terms of the number of required bit operations.  The algorithms developed will be space-efficient, using no more intermediate space than required to write down the input and output.A challenge in this work is to design algorithms that correctly handle the exact domains of computation.  For example, the sizes (number of digits) of integers need to be taken into account in the design and analysis of algorithms, especially since integers arising during the computation and appearing in the output can be much larger than those in the input matrix.The long term goal is a portable software library, based on the novel techniques and algorithms arising from the research, for doing exact linear algebra computations with integer and polynomial matrices.  The algorithms arising from the research will allow the solution of larger linear algebra problems, and will make more effective use of computer resources.""525492,""Carette, Jacques"
"521207"	"Suen, Ching"	"Error Reduction in Handwriting Recognition with Applications"	"The objective of this program is to conduct advanced research in computer recognition of handwritten characters, words, and symbols. Handwriting is one of the most challenging subjects in the field of pattern recognition due to the infinite varieties of character shapes and qualities. This research aims to increase the intelligence and capability of computers so that they can read, at high speed and with great accuracy, the information written on various types of documents. These documents include bank cheques, utility and payment slips, envelopes, income tax returns, customs declarations, archival documents, and other business forms where billions of dollars are being spent each week to manually enter the handwritten data from the documents into the computer. Based on decades of experience in (a) playing a lead role in this field and constantly interacting with prominent scientists and industrial researchers around the world, and (b) having guided many graduate students, post-doctoral fellows, and visiting scientists, the applicant proposes the following program:(1) Enhance the reliability of our advanced handwriting recognition systems by (a) introducing new training techniques to  separate good and bad samples, (b) creating very large databases, and (c) discovering distinctive features and vital parts of individual characters derived from eye-tracking studies.(2) Select dynamic sets of complementary features for various stages of multi-stage recognition systems to produce the most  reliable and the best system to recognize handwritten data.(3) Introduce a new breed of hybrid classifiers which can identify confusing character/word shapes, minimize costly substitution errors and maximize performance, so that the newly developed systems can be used in practice to save billions of dollars and a huge amount of manpower.""521208,""Bailey, Richard"
"534840"	"Szepesvári, Dávid"		"NSERC"
"533271"	"Taati, Babak"	"Vision-Based Evaluation of Mobility, Physical Health, and Rehabilitation Progress in Natural Settings"	"The high level objective of this program of research is to develop computer vision tools and algorithms capable of monitoring the movements and actions of humans in natural settings. The application area of this research is in the evaluation of mobility, physical health, and rehabilitation progress in the homes of older adults. Specifically, we will focus on observing the motion, pose, and daily activities of older adults in the home in order to automatically and continually assess their physical health and mobility. Advances in medical care and the aging of the baby boomer generation have resulted in theincreasing size and proportion of the older adult population. Promoting and supporting aging-in-place alleviates some of the healthcare pressures resulting from this demographic change. Withoutproper considerations, however, living alone could pose serious health and safety risks for manyolder adults, particularly for those with chronic mobility or cognitive conditions.Earlier research has shown the acceptability of home video monitoring technologies among older adults if the technology allowed the participants to remain in their own homes. Recent advances in computer vision and machine learning have made real-time mobility and physical health monitoring in natural settings, i.e., in the home, a within reach possibility with high potentials in improving the quality of life of older adults, while at the same time posing interesting and challenging computer science research questions. Individuals or companies interested in learning more about this program of research or interested in contributing or collaborating towards its development are encouraged to contact the principal investigator (B. Taati) via email at taati@cs.utoronto.ca.""533272,""Eames, Brian"
"533442"	"Taylor, Graham"	"Deep Learning and Representation Learning for Sequential Data"	"The pervasiveness of computing has resulted in the production and storage of more data than ever before. Machine learning seeks to transform this deluge of data into intelligent systems that identify patterns and make decisions. It has revolutionized fields as diverse as computer vision, computational neuroscience, biology and the social sciences. But when faced with data that is increasingly complex, how does a machine know which parts are relevant? How does it structure the millions of components into organized units on which it can base decisions?Recent developments in machine learning, known as ""Deep Learning"", have answered these questions by showing how increasingly abstract layers of features can extract informative data representations without human guidance. But to-date, the field has focused on static as opposed to dynamic data: particularly images in the context of visual reasoning. Much of the real-world data we encounter in machine learning applications, however, has temporal dependencies that often extend over large time scales. Examples are human or robot motion, climate data, audio (e.g. music or speech), and finance. Modeling sequences is challenging: even the most sophisticated techniques fail to learn long-term structure. Sequences increase computational requirements as we attempt to untangle more complex explanatory factors underlying the data.I propose a research program that confronts these challenges through the discovery of algorithms and architectures that learn representations from sequences. I also aim to widen the adoption and increase the relevance of Deep Learning outside of academia through interdisciplinary collaborations that will impact fields such as biology, entertainment, and finance. I will train high-quality personnel who can satiate industry's growing demand for data scientists.""533443,""Perry, Christopher"
"522521"	"Thibeault, Claude"	"Impact des techniques récentes et émergentes d'encapsulation des circuits intégrés sur la qualité de leur alimentation et incidences sur le test et diagnostic"	"Ce programme de recherche porte sur les techniques récentes, voire émergentes, d'encapsulation des circuits intégrés, connues sous le nom d'encapsulation 2.5D et 3D. L'encapsulation 2.5D, qui est depuis peu disponible sur certains circuits intégrés commerciaux, consiste à empiler des dés de silicium, les uns à côté des autres, sur une couche intermédiaire passive de silicium, au travers de laquelle on retrouve des interconnections sous forme de colonnes verticales (« Through-Silicon Vias, TSV »). L'encapsulation 3D, quant à elle, consiste à empiler des dés de silicium, les uns par-dessus les autres. Dans ce cas, les TSV sont présents dans les dés empilés. Par rapport à l'encapsulation conventionnelle (2D), ces techniques offrent un très grand potentiel en termes de bande passante et performance, et ce pour une faible puissance de dissipation. Si le potentiel de l'encapsulation 3D est encore plus intéressant, cette technique se heurte à davantage de défis, notamment en cequi concerne le test des circuits.  Ce programme vise à investiguer les effets de l'utilisation de l'encapsulation 2.5D et 3D sur la qualité de l'alimentation des circuits intégrés, en particulier durant le processus de test et diagnostic.  Ce programme s'inscrit dans la suite logique de travaux récemment entrepris. Ces travaux ont entre autres permis d'observer un phénomène de variations de l'alimentation, variations qui sont induites par l'intermodulation de fréquences d'horloge utilisées et qui sont exacerbées aux fréquences de résonance du réseau d'alimentation des circuits intégrés. Les résultats obtenus jusqu'à maintenant, à l'aide de circuits intégrés encapsulés de manière conventionnelle, montrent que ce phénomène peut avoir des incidences sur la performance de ces circuits.  Ce phénomène et ses incidences devraient être amplifiés par les encapsulations 2.5D et 3D. En effet, des boucles de courant supplémentaires peuvent se créer avec ces types d'encapsulation. De plus, des différences plus marquées sont prévues au niveau de l'impédance du réseau d'alimentation pendant le processus d'assemblage et de test, au fur et à mesure que les dés sont empilés les uns à côté des autres (2.5D) ou les uns sur les autres (3D).""522522,""Hill, Ceredwyn"
"533166"	"Tip, Frank"	"Improving the Quality of Web Software"	"Web applications typically consist of a client-side and a server-side that must act in concert despite being written in a complex mixture of programming languages. It is common for significant parts of the client-side code to be generated by the server-side program based on the user's interactions with a web browser, and for the server side to rely on non-relational (NoSQL) databases for increased scalability and fault-tolerance. These factors conspire to make the creation of web applications a highly complex and error-prone task. Programmers currently don't have good tools to help them with the development of web applications, and serious errors and security vulnerabilities are therefore a concern.The first goal of the proposed research is to develop program analysis algorithms that are customized for the domain of web applications. We will use these algorithms as the basis for practical ""refactoring"" tools that assist programmers with transforming source code without accidentally changing program behavior. We will also use the developed algorithms to build tools for test generation, and fault detection and repair. These tools will help web developers significantly improve the quality of their applications, reducing the number of errors and security vulnerabilities. The second goal of the proposed research is to design, implement, and evaluate programming language features for manipulating data that is distributed over multiple locations that are synchronized according to weak (eventual-consistency) policies. Incorporating support for eventual consistency directly into a programming language will make it easier to reason about the correctness of web applications that operate on distributed data. We plan to implement our ideas in a full-scale compiler and implement optimizations that improve scalability by reducing the number of database operations. In all, the proposed research will enable programmers to construct web software of significantly higher quality, thereby reducing the errors and security vulnerabilities experienced by users of that software.""533167,""Wong, Alex"
"523605"	"Trajkovic, Ljiljana"	"Information Routing in Complex Networks"	"My proposed research program focuses on developing methods and tools for performance evaluation of complex networks, including data communication and social networks. This program encompasses the analysis of data collected from deployed networks, characterization and modeling of network traffic, development of tools for analysis of network topologies, and design of routing protocols in complex data networks. My objective is to develop formal analytical and statistical methods for traffic characterization. I will employ the spectral graph theory to analyze various topologies of complex networks and their dynamical behavior. I will utilize probabilistic verification techniques to evaluate the convergence of routing protocols. Machine learning techniques will be used for detection of network anomalies. Performance of network protocols will be evaluated using network simulation tools such as ns-3 and OPNET. Research results emanating from the proposed program will improve our understanding of the underlying mechanisms that govern the behavior of the Internet and social networks. They will help improve the design and performance of routing protocols and enhance network security. A library of developed tools and models will be made publicly available to the research. Quality of service of the Internet heavily depends on routing protocols that govern routing of packets between the Internet service providers across Canada and the world. IP networks increasingly support a diversity of converged network services that currently include real time Voice over Internet Protocol (VoIP), video streaming, fourth generation (4G) wireless and mobile communications, cellular backhaul, and social networking. The proposed research program will contribute to improving the performance of these services.""523606,""Rafferty, Steven"
"526267"	"Tran, Thomas"	"5"	"NSERC"
"526911"	"Truong, Khai"	"Tools to Support Local Community Participation and Development"	"The adoption of technologies such as the radio, automobile, television and Internet, has increased the level to which activities are undertaken individually and alone. For example, more people are now commuting alone much farther than in previous decades. This commuting time has contributed to ""the growing separation between work and home and shops."" As a result, people may have a reduced understanding and appreciation of their neighborhoods. For example, three out of five people know only some or none of their neighbors. This limited investment in neighborhoods has led to a decline in the growth and development of local communities, which has potential impacts on many aspects of people's lives, including their comfort and well-being.I propose to explore how mobile and ubiquitous computing systems can be designed and used to enable people to (1) develop familiarity and ties with their physical neighborhood itself, (2) develop familiarity and ties with other people in their neighborhood, and (3) collaborate with others to restore and transform their local community. Whereas many computing technologies introduced in the past decade have enabled people to grow their social networks and connect with remote acquaintances, this research puts the focus on what few works previously have---the design of mobile and ubiquitous computing tools to support the development of local communities. The proposed research uses the important social problem of how to develop interactive tools in support of community participation and development as a platform for investigating key computer science issues, such as context- and location-aware computing, sensing and inferring user interests, learning and planning user activities, and privacy and security, amongst others.""526912,""Gregory, Ryan"
"533208"	"Tsantalis, Nikolaos"	"A Framework for the Management of Preventive Maintenance"	"Software plays a critical role in our everyday life as more and more of our activities involve the use of software systems from online bank transactions and e-government services to mobile applications in our phones. However, software systems must be constantly updated to address new customer needs, fix errors, improve the performance and response time, and adopt new technologies and infrastructures. The cost of performing the aforementioned maintenance activities highly depends on the underlying design quality of the software systems. In general, software systems that are not well-designed require much more time and effort to make changes, and vice versa.Nowadays, there exist several techniques and tools that analyze software systems and detect opportunities for improving their design quality. However, these opportunities are not equally important and beneficial. Performing a manual inspection to assess their impact is very time consuming and overwhelming for software maintainers, especially when their number is large and they resolve different types of design problems.The proposed research aims to transform the improvement of software design quality into a disciplined, systematic, and tool-supported practice by developing a framework for the automatic assessment and prioritization of design improvement opportunities based on their risks and benefits. The results of the proposed research will contribute to the effective reduction of maintenance cost in existing software systems, which in turn will lead to more reliable software with fewer errors, faster release times for new features, lower software/service prices, and better customer satisfaction in general.""533209,""Bruce, Neil"
"524700"	"Uddin, Mohammad"	"Intelligent Controller Based Cost-Effective, and Efficiency-Optimized High Performance Motor Drives"	"Traditionally, control issues for variable speed drives are handled by fixed gain proportional-integral and proportional-integral-derivative and various adaptive controllers. These controllers have inherent disadvantages as their designs depend on an exact system mathematical model, which is often difficult to develop due to system uncertainties. Moreover, the fixed gain controllers are very sensitive to system disturbances. The main criteria of high performance drive (HPD)  systems used in robotics, electric vehicles, rolling mills, machine tools, etc. are: fast and accurate speed response, quick and smooth recovery of speed from any disturbances.   The ac motor model is often complex and nonlinear as the motor parameter varies widely with operating conditions. In order to deal with the nonlinearities and uncertainties of electric motors and overcome the limitations of the existing controllers in recent years, attention is being paid to various intelligent control algorithms (ICA) such as, fuzzy logic (FL), neural network (ANN), neuro-fuzzy and genetic algorithm (GA) for HPD applications. Simplicity and less intensive mathematical design requirements are the main features of ICA. Despite tremendous research, successful application of ICA for industrial HPDs is very limited due to its high computational burden. Hence, an expensive fast processor is required for implementation of conventional ICA, which is not a cost-effective solution for the industry.     Due to the growing global concern over energy consumption and the environment, high energy efficiency has become another important factor for HPDs. The existing loss minimization scheme for motor drives is developed based on motor model which is not capable of handling system uncertainties.      The objectives of the proposed project are to develop and implement hybrid intelligent controllers utilizing the merits of FL, ANN and GA in order to achieve cost-effective, and efficiency-optimized high performance interior permanent magnet synchronous motor and induction motor drives. The proposed research work has strong prospects for application in pulp & paper mills, electric vehicle, oil and mine industries in Canada.""524701,""deKievit, Teresa"
"524845"	"Valtchev, Petko"	"61"	"Québec"
"525481"	"VanOorschot, Paul"	"Topics in Authentication Technology and Computer Security"	"The program explores open security problems involving Internet authentication, including authentication of users and websites, secure software installation, and security of mobile devices; and aims to design new mechanisms improving Internet security. A first thrust seeks to address known security issues and design flaws in the Secure Sockets Layer mechanism (the Internet's core security infrastructure used by web browsers), and its underlying certificate authority infrastructure and trust model.  We explore root causes of security issues, and design and test tools addressing current threats through evolutionary and revolutionary approaches including redesign of trust model infrastructure. A second thrust pursues design of a foundational methodology and comprehensive framework for systematic evaluation of user authentication mechanisms, with broad emphasis across usability, deployability, and security criteria; its use to accurately identify requirements matching authentication mechanisms with specific application environments; and design and evaluation of new mechanisms with identifiable advantages. A third thrust explores authentication aspects of mobile device security--web site and user authentication, and secure software installation--including design and evaluation of novel mechanisms specifically suiting mobile device limitations (e.g., size, input modalities). To avoid security threats related to non-expert users installing malicious application software, redesign is explored including operating system mechanisms balancing security (software isolation) and functionality (cross-application sharing). The problems explored are among society's most important cyber-security challenges, impacting all Internet users from private citizens, to business and government, to technology providers, and solutions increase trustworthiness of critical Internet infrastructure.  We expect fundamental research results that contribute to stronger real-world Internet security through the design of better authentication and more secure, resilient software installation methods; and to see them deployed, including on mobile devices and in major browsers, through coordinating stakeholders to improve the Internet infrastructure.""525482,""Stuart, David"
"525367"	"Varma, Rajiv"	"Novel Control of PV Solar Farm Inverter as a FACTS Device for Improving Power System Performance"	"PV solar farms operate only during the day and are completely idle in the nights. This research program proposes a novel utilization of the PV solar farm inverters in the nighttime as STATCOM - a Flexible AC Transmission System (FACTS) Device. This novel control allows the solar farm inverter to be used in the night for accomplishing a variety of power system objectives, such as improving system stability, and power transmission capability in transmission systems as well as for performing load compensation in distribution systems. During the day also, the above objectives can be achieved to a substantial extent through the inverter capacity left after production of real power. Similar STATCOM control functionality can also be implemented in inverter based wind turbine generators during no-wind or partial wind scenarios for improving the transient stability of the power system and for other objectives. Since PV solar farms operate during the days and wind farms typically produce peak power in the nights, such novel controls of the solar farm and inverter based wind farm can complement each other and facilitate increased grid connectivity of both the renewable generation systems.""525368,""Chahine, Richard"
"525228"	"Vechtomova, Olga"	"Development and evaluation of information retrieval methods to support users with complex information needs"	"Solving complex real world problems usually requires access to large amounts of information. At the start, a user may have little knowledge of the subject domain and may not know what information he needs in order to solve the problem at hand. Currently available search engines are very good at retrieving topically relevant documents in response to a well-defined query. However, if a user does not have a good understanding of the problem domain, and cannot formulate effective queries, then existing search engines are not very helpful. Currently a user with a complex search task, e.g. ""What do I need to know to start my own company in Canada?"", needs to formulate iteratively several queries, go through large numbers of retrieved documents, and identify important facts, entities (e.g. people, organizations), and their interrelationships in order to obtain the necessary knowledge needed to solve the problem at hand. In the proposed research I will investigate how to provide users with a coherent information seeking experience throughout their complex problem solving processes. This includes the design of methods for finding useful information nuggets, concepts and entities in documents, identifying relationships between them and presenting them in a meaningful and coherent way to the user.The first goal of my proposed research program is to design and implement information retrieval methods and tools for supporting the whole process of complex problem solving. The second goal is to develop methods for evaluating effectiveness of IR methods in supporting the user's complex problem solving. Evaluation of the developed methods will be done by means of both laboratory and user studies.""525229,""Jatskevich, Juri"
"534105"	"Villeneuve, Jérémie"	"Ontario"	"CANADA"
"527276"	"Vincent, Pascal"	"Alberta"	"CANADA"
"529583"	"Vogel, Daniel"	"Subtle Interaction Techniques for Ubiquitous Displays"	"Computers are spreading throughout our environment as embedded interactive displays, and whole-body tracking is poised to become a primary method of input. This could radically change how, when, and where we access digital information. The challenge is that this new computing context requires fundamentally different approaches to the tightly linked aspects of input, interaction, and visualization. Two central concerns about current approaches are that waving your hands for input is tiring and feels socially conspicuous, and attention-seeking displays will add cognitive demands and compromise the visual aesthetics of the environment. However, when interaction is made subtle (""fine or delicate in meaning or intent""), these combined issues of comfort and aesthetics are addressed most appropriately. This work develops and promotes ""subtle interaction"" as a Human-Computer Interaction paradigm.       Leveraging my past research in interaction with large ambient displays, the proposed research culminates with convincing applications realized in novel system configurations that are comfortable to use and harmonious with the architectural environment. The construction of these applications will be enabled by essential contributions of design guidelines and assessment models developed out of formative work in comfort and aesthetics, as well as innovative enabling techniques which address fundamental research problems in input sensing, visualization, and interaction design. By adopting a multi-phased, multi-streamed methodology, the process to reach these goals will also form a diverse, collaborative, and stimulating training opportunity for highly qualified personnel. Ultimately, these tools and techniques will equip designers to apply subtle interaction to other applications and lead researchers in future explorations of subtle interaction issues in new system configurations and computing contexts. This theoretical and practical will enable the full potential of whole-body interaction with embedded interactive displays, leading to public acceptance and commercial viability.""529584,""ByersHeinlein, Krista"
"527334"	"Volat, Christophe"	"Développement d'un modèle numérique générique de prédiction de la tension critique de contournement des isolateurs soumis à de la pollution hivernale"	"Dans plusieurs provinces canadiennes, la majorité des défaillances des isolateurs provoquées par les conditions atmosphériques se produit principalement au cours de la période hivernale. Les interruptions de service qui en découlent sont d'autant plus graves qu'elles surviennent à des moments où l'approvisionnement continu en énergie électrique est primordial. Les contournements des isolateurs en période hivernale demeurent donc l'une des principales cibles des travaux actuels de recherche et développement dans le domaine de la transmission et de la distribution de l'énergie électrique. Il existe deux motivations  principales à cet intérêt, le caractère imprévisible des contournements et l'absence de normes relatives au dimensionnement des isolateurs soumis aux conditions météorologiques et environnementales particulières à la période hivernale (combinaison de dépots polluants, brouillard givrant, accumulations de glace, etc.). Afin de combler les lacunes relatives aux dimensionnement des isolateurs, le présent projet a pour objectif de développer un outil numérique fiable et performant permettant de dimensionner n'importe quel type d'isolateur dédié à être utilisé sous différentes conditions de pollution hivernale. À cet effet, un modèle numérique générique de prédiction de la tension critique de contournement sera développé. La réalisation de ce projet ambitieux nécessite cependant l'atteinte de plusieurs objectifs intermédiaires, les pièces du puzzle, qui permettront d'assembler progressivement le modèle numérique générique final. L'outil numérique générique de prédiction ainsi proposé sera un support incontournable à la conception et le dimensionnement des isolateurs destinés aux régions froides. L'expertise qui sera développée dans ce projet pourra ainsi être transférée à l'industrie canadienne de l'électricité, lui permettant de diversifier ses choix d'équipements d'isolation sur une base solide et scientifique afin de soutenir de manière innovatrice la qualité et la fiabilité de la distribution d'électricité aux utilisateurs.""527335,""Yee, WaiLing"
"525499"	"Walker, Robert"	"The Best of Both Worlds: Reusable Software via Unplanned Reuse"	"Since the 1960s, the idea of reusing pre-existing software artifacts for the development of new software artifacts has been promoted for its potential to reduce costs and improve quality.  Classical software reuse demands that we predict well the future needs of software systems in order for their functionality to be modularized in an easily reused form; this is anticipated reuse. Since our ability to predict the future is not perfect, there occur cases where a developer cannot find a modularized, reusable artifact that meets their needs. A common industrial response is to copy-and-modify fragments of existing artifacts; this is unanticipated reuse. Both anticipated and unanticipated reuse approaches have benefits and weaknesses.  Anticipated reuse allows us to create well-designed and well-documented software artifacts, but at high cost and without guarantee that the investment will be recouped.  Unanticipated reuse permits high customizability, but can lead to poor decisions and without guarantee that important properties from the old system are preserved.This work proposes to take the good aspects of both the anticipated and unanticipated approaches and combine them, augmenting them with support for using feedback from experience to inform the design of reusable artifacts.  My previous work has made unanticipated reuse much more systematic, safe, and less stressful for the developer.  But the situation could be improved even more by automating many of the activities that have to happen during the process, which currently still demand much detailed investigation by a human developer.The success of this research program will see a positive transformation for industry in the way that software artifacts are constructed, comprehended, used, and improved. For academia, this work is a fundamental shift in understanding the problem of software reuse. For HQP, the program is an excellent training ground in software engineering research and practice.""525500,""Czarnecki, Krzysztof"
"523560"	"Wang, Chunyan"	"Low-Power VLSI Systems for Signal Sensing and Processing"	"There are increasing demands for a better performance of the electronic systems, in terms of computation capacity, mobility, and autonomy, to expand their applications in various areas such as communications, biomedical engineering, and remote controls. Power dissipation is a critical concern in the design and implementation of VLSI systems as it limits the density of integrated circuits and the life span of battery-operating mobile/autonomous systems.      Three kinds of electronic circuits are involved in a system performing signal sensing and processing. They are sensors, sensor conditioning circuits, and signal processing circuits. A sensor converts a physical variable into a current or voltage signal, and a sensor conditioning circuit is used to make the signal produced by a sensor adapt to the processing circuits. The low-power design of sensors, sensor conditioning circuits, and processing circuits leads to a reduction of the total power dissipation of the system. The overall objective of the proposed research program is to develop low-power design techniques for the three kinds of circuits. It is three-fold as presented below.1) Ultra-low level energy harvesting in sensor circuits. It is to design integrated sensors capable of collecting low level energy available in a tiny space and of using it to make the sensors self-supporting.2) Developing ultra-low-power sensor conditioning circuits. The conditioning circuits should have a high sensitivity to operate with a very low level signals, a wide dynamic range, and simple structures to achieve low-power dissipation.3) Developing energy-efficient algorithms and architectures for signal processing.     The proposed low-power design techniques would aim for the implementation in standard silicon CMOS fabrication technologies. Integral to achieving the technical objective will be the inclusion and training of a large number of graduate students to become highly qualified personnel needed for the industries.""523561,""Neufeld, Ronald"
"533930"	"Wang, Sida"		"GERMANY"
"527710"	"Wang, Xianbin"	"Cognitive Adaptation of Communications Systems and Networks for Situation-Aware Wireless Security Enhancement"	"With the ubiquitous deployment of broadband communications infrastructure and services, wireless communications is facing many emerging challenges including prominently the issue of wireless security.  The unique security challenge of wireless communications is due mainly to the open and broadcast nature of radio signal propagation. While most of the wireless security risks are from the open air interface, conventional wireless security mechanisms, typically deployed at the higher layers of communications network protocol, are inadequate in addressing the specific vulnerability of radio propagation environment at physical layer. Furthermore, lacking of situational-aware security risk assessment and adaptation in security provisioning solutions in highly standardized wireless networks introduce additional security risks. As a result, these combined factors inevitably lead to vulnerable wireless technologies and infrastructures with widespread security weaknesses due to the ineffective security mechanisms. The primary goal of the proposed NSERC Discovery Grant program is to develop transformative wireless security technologies through exploration of adaptive physical layer security enhancement, cognitive security risk evaluation, and situation-aware optimization and deployment of wireless security strategies.  To achieve this goal, various cognitive adaptation and cooperation capabilities of wireless systems and networks, as well as situation-aware utilization of inherent dynamics and attributes of wireless systems networks, will be explored in the proposed research and training. The broader impacts of proposed NSERC Discovery Grant program include: a) it will bridge the gap between existing communications technologies and increasing wireless security needs; b) it will develop an unified approach that effectively tackles wireless security and communications performance; c) the proposed research will be timely integrated with training of highly qualified personnel (HQP) and industrial collaborations, thus maximize the impact of the NSERC Discovery program via technology innovation and knowledge transfer.""527711,""Zhang, JianKang"
"527987"	"Wang, Xin"	"Spatial clustering of spatial correlated data, multi-valued data, and domain-driven spatial clustering ensemble"	"Spatial clustering partitions similar data objects into the same group (called clusters) based on their distance, density and reachability in space. The primary goal of this study is to design, develop and improve a set of spatial clustering methods and processes that discover meaningful and useful clusters from large amounts of spatial correlated data, multi-valued spatial data and multi-featured spatial data. Specifically, the research will address the following spatial clustering problems: 1) Spatial correlation generally exists in spatial datasets. It indicates the dependency between the spatial and non-spatial attributes, with the chance that some cause and effect lead to it. The research will identify different unbiased measures of non-spatial attribute similarity and spatial correlation. Then the measures will be integrated into different clustering methods.  2) In many applications, the non-spatial attribute of a spatial object can be represented by a set of multiple values in the d-dimensional space. The object is called a multi-valued spatial object. How to discover the clusters of multi-valued spatial objects is a challenging task. One outcome of this research is to define different similarity measures for multi-valued spatial objects. Different spatial clustering methods will be developed based on the measures. 3) Most existing clustering algorithms do not consider the domain knowledge during the clustering process, which prevents users from precisely describing their goals and understanding the clustering results. In addition, for many applications, none of the current clustering methods is suitable due to the multiple features of the spatial data. Each clustering method may cluster the data from a particular feature or may be suitable for part of the spatial area. How to improve the clustering result for spatial data with multiple features with the aid of domain knowledge is an interesting research topic. This research will first study how to improve the spatial clustering ontology. The new domain-driven spatial cluster methods and cluster ensemble methods will then be proposed to combine multiple clustering results to produce useful and meaningful results.""527988,""McGuffin, Michael"
"533195"	"Wang, Yang"	"Ontario"	"CANADA"
"528027"	"Wang, Zhou"	"Quality-of-visual-experience: perceptual assessment, compression and enhancement"	"Recent years have witnessed remarkable advances in digital image/video acquisition, transmission and display technologies. These technologies aim to offer consumers with enhanced experience in consuming visual content, and are creating a vast change in novel multimedia applications. To maintain and increase the competitiveness in the rapidly growing global market, it is critical for Canadian academia and industry to maintain and enhance their leadership in advancing the research fronts and in developing cutting-edge technologies. The proposed project aims to explore the theoretical and application frontiers in quality-of-visual-experience (QoVE) research. First, we will develop novel objective QoVE assessment methodologies suited for network visual communications; Second, we will develop objective QoVE evaluation techniques for three-dimensional (3D) application environment; Third, we will develop novel compression and quality enhancement methodologies optimized for the best 2D and 3D QoVE. To achieve this goal, we will exploit a series of challenging problems - We will develop perceptual video compression methodologies by incorporating QoVE models; We will incorporate QoVE criteria into state-of-the-art sparse representation, compressed sensing, and statistical image modeling methodologies for the denoising, superresolution, and frame rate adaption of video signals; We will also investigate polyview fusion-based methods, and pre- and post-processing methods in network visual communications to further improve the visual experience of end users.     The success of this project will help establish a necessary theoretical foundation and a series of general methodologies for the fast-evolving field of QoVE research, upon which advanced technologies can be built up for many practical applications in digital imaging, multimedia communication, entertainment, gaming, education and medical imaging industries. In the process of achieving our scientific and technological objectives, we will also train highly qualified personnel in related areas for Canada.""528028,""Amon, Cristina"
"520749"	"Webb, Jonathan"	"Québec"	"CANADA"
"524278"	"Wiese, Kay"	"Computational Intelligence and Applications in Bioinformatics"	"Structure prediction of bio-molecules such as RNA and proteins is an important topic in Computational Biology. The structure is often determining the function of the molecule and can also be used to infer genetic origins (phylogeny). This research program focuses primarily on the following two objectives:1) To develop improved algorithms for structure prediction of bio-molecules, particularly RNA. This is an important topic as more and more functional RNAs are discovered. 2) To develop improved visualization techniques, particularly for RNA visualization. For objective 1) the starting point will be the algorithm RnaPredict, developed in my lab and based on evolutionary computation. RnaPredict uses a number of advanced operators and representations. I plan to further evaluate, improve, and extend RnaPredict, particularly in the area of modeling non-canonical base pairs and the prediction of pseudoknots. The starting point for objective 2) will be jViz.Rna, a visualization tool for RNA secondary structure which has a number of benefits over existing tools, including: wide support of input and output formats, ability to produce high quality graphics for reproduction, interactive output for further manipulation by the end user, ability to display pseudoknots in a classical structure plot, and the ability to overlay two different structures for comparison. jViz.Rna also produces quantitative and qualitative comparisons between two structures. One particular challenge of RNA visualization is the the handling of pseudoknots. Most existing tools either do not model them in a traditional structure plot or do a relatively poor job. I plan to extend jViz.Rna to incorporate various topological tools to display more classes of pseudoknots in a planar way. This will also have potential benefits for the visualization community at large.""524279,""Regan, Sharon"
"525463"	"Xu, Changqing"	"Development of PPLN based miniature and compact lasers"	"The objectives of this proposal are to develop miniaturized green lasers for laser display application, miniaturized violet/blue/green lasers for biomedical instrumentation applications, and compact mid-IR lasers for environmental monitoring. The long-term objectives of the proposed program are to enhance collaborations with the leading laser manufacturers, provide Canadian industry with highly qualified personnel and new/improved products, and maintain the leading position of the research on MgO:PPLN in Dr. Xu's group at McMaster University.    Miniature green lasers are strongly demanded in laser display industry. For example, a miniature green laser with an output power of about 1W and thickness of less than 5 mm is considered as an enabling technology for pico laser projectors, while 10W order green lasers with a small footprint (say 20 mm2) are strongly demanded in large screen laser projectors. It is expected that laser display will be revolutionary, bringing a whole new visual dimension to display market.    On the other hand, miniature violet/blue/green lasers are required in bio-instruments. Most of the bio-instrumentation lasers have to rely on the diode pumped solid state (DPSS) lasers based on the second harmonic generation (SHG) technique. However, size of the DPSS lasers has to be reduced.     In addition, compact mid-IR lasers are required in detection of trace gases.  It has been pointed out that the pollutant gases are harmful to human health. Mid-infrared (mid-IR) laser absorption spectroscopy is considered to be the most effective technique to detect the pollutant gases because of its high sensitivity and selectivity. Therefore, an affordable compact, room-temperature operation, and wavelength tunable mid-IR laser is crucial to trace gas detection.""525464,""Wisner, James"
"525162"	"Yang, Boting"	"Pursuit Evasion and Related Problems"	"In a typical pursuit-evasion problem, one or more mobile searchers (or cops) are seeking the capture of one or more clever robbers. Many real-world problems can be modeled by an appropriate pursuit-evasion problem. Some examples include: police officers searching for a fugitive, a search-and-rescue team searching for a missing person, a military troop clearing an area of enemies or mines, computer technicians searching for a mobile virus on a computer network, or even firefighters clearing poisonous gas from a contaminated building. When selecting a model for pursuit evasion, we must first choose a representation for the domain. Researchers have typically modeled the domain by graphs and polygons. Graphs and polygons provide natural models for many domains such as roadways and buildings. They are especially good models since many relevant results from graph theory and computational geometry can be employed when considering the search number. In this proposal we consider the pursuit-evasion problems in which a very fast robber (or virus) is hiding in graphs, polygons, terrains, or networks.  Our proposed research focuses on automating the computation of search strategies in such scenarios. We are concerned with finding algorithms for computing search strategies that are guaranteed to capture the robber. Many interesting questions arise with relation to this problem. For example, given a graph/polygon, what is the minimum number of searchers required to search the graph/polygon so that the robber will definitely be found? and what is the minimum cost to search the graph/polygon? These questions are particularly important for applications where the number of searchers is limited.We are especially interested in investigating computational issues of search strategies that are commonly used by people. For example, in some real-life scenarios, the cost of a searcher may be relatively low in comparison to the cost of allowing a fugitive to be free for a long period of time. If a dangerous fugitive is hiding along streets in an area, the police always want to capture the fugitive as soon as possible. Our research will provide new algorithms and eventually software that will assist in various fast search applications.""525163,""Caruso, ChristinaMarie"
"525020"	"Yao, Jianping"	"Silicon Photonics for Microwave Photonics Applications"	"Microwave photonics is an area that studies the interaction between microwave and optical waves for the generation, processing, control and distribution of microwave signals by means of photonics,which can find numerous applications such as broadband wireless communications, radar, sensors, imaging, and modern instrumentation. The key motivation of using photonics for microwave applications is the large bandwidth and low loss offered by photonics, which makes the implementation of certain important functions possible while in the electrical domain the same functions may not be realizable or the systems become extremely complicated and costly. Different microwave photonics systems have been proposed and demonstrated in the past few years. However, these systems were implemented mainly based on discrete photonic and microwave components, which make the systems bulky, heavy and costly with relatively poorer performance. On the other hand, there is an increasing interest in the last few years in integrating photonic devices on silicon chips, an area called silicon photonics. Compared to conventional photonic devices based on III-V semiconductors, silicon photonics is a technique that allows optical devices to be made economically using standard and well-developed semiconductor fabrication techniques. Significant breakthroughs have been made in the last few years in implementing both passive and active optical devices on a silicon substrate, such as optical waveguides, optical filters, add-drop multiplexers, modulators, and photodetectors. However, very few demonstrations have been reported to implement microwave photonics devices and systems based on silicon photonics. The motivation of the proposed research is to use the well-established silicon integration technologies to fabricate microwave photonic devices and systems on a silicon-on-insulator (SOI) substrate, to demonstrate different microwave photonics functionalities with significantly improved system performance.""525021,""Guo, PeiJun"
"533322"	"Yuan, Ding"	"Toward Automatic Failure Diagnosis in the Cloud"	"As software systems have grown in size, complexity, and cost, it has become increasingly difficult to deliver bullet-proof software, resulting in many software failures in the production environment. This unfortunate fact is further exacerbated by the trend of cloud-computing - where it is even harder to make systems failure-free as they become increasingly more complex and distributed.Once a failure occurs in these production systems (i.e., users cannot receive expected service), vendors need to trouble-shoot it as quickly as possible since every minute of the downtime is costly. For example, it is estimated that Amazon will lose 2 million dollars for every hour of its downtime. Such downtime will be even more catastrophic for those mission-critical software services. For example, the 2003 northeast blackout, caused by a software bug, resulted in over 10 million people in Ontario and 45 million in U.S. out of power for at least 7 hours. Later an investigation report attributes ""failure to provide effective real-time diagnostic support"" as one of the main reasons behind such great damage. Finally, a prolonged trouble-shooting process will frustrate the users and significantly erode the vendor's reputation.The goal of this proposed research is to expedite the trouble-shooting of failures in cloud-based distributed systems. In particular, my research consists three progressive thrusts that together, will automate the diagnosis of such failures, and significantly reduce the downtime of the cloud-based distributed systems.""533323,""Little, Jonathan"
"527893"	"Yuksel, Serdar"	"Optimal design of stochastic networked control systems"	"The goal of the proposed research is to advance the understanding of networked and cooperative control systems under uncertainty and information constraints. Stochastic networked control systems arise in many practical settings (such as in decentralized and adaptive power regulation in the energy/power grid,  automotive applications for collision avoidance, sensor networks and in economics theory). However, there are many fundamental and practical aspects yet to be understood regarding such systems. The proposed research will have three main technical and practical contributions: (i) Development of a quantitative theory on the performance of decentralized dynamic systems given information constraints. This will lead to design principles for practical systems and infrastructure design for a large variety of applications. (ii) Stochastic stability of complex connected systems through event-driven communications and development of stochastic drift criteria for stability of such systems. This is important for decentralized systems which can only communicate when resources are available. (iii) Generation of approximately optimal control policies when controllers are able to learn the dynamics of a system through partial information or training sequences. This is especially important in cognition and learning aspects of controllers for adapting to an unknown environment in which they live and act. The outcomes will allow for the design of adaptive, cognitive and resource-efficient decentralized and networked control systems. In addition to fundamental contributions on theoretical problems regarding decision making under uncertainty, the research outcomes will have important practical impact in the technology sector in Canada and beyond, in view of optimal engineering system design for networked control systems under information constraints. In addition, four PhD, two MSc and two undergraduate students will be trained rigorously under this proposal, and be independent researchers who will benefit Canada's research and technological community.""527894,""Lamoureux, Guillaume"
"523827"	"Zaïane, Osmar"	"Reinforcing pattern mining with uncertainty and connectivity"	"Generally, data stored in databases are considered facts and observations that took place. These records are also typically considered mutually independent. This is true for many application domains. For instance, a hospital database contains information about patients that are independent from each other. A record could state a visit at a given date for a particular diagnostic for which a given treatment is given. The observed facts are recorded as unrelated. Data mining and analysis tasks assume this independence between records and consider the recorded facts to be certain. However, in many real applications some relationships exist between observations and thus could be recorded. For example, the fact that a person is a friend, colleague or family member of another person can be recorded as a connection. In telecommunication, the fact that a phone number calls another telephone number creates a relationship between records. These are information networks. Ignoring these relationships in data analysis is a missed opportunity to get better insights about the data. Moreover, uncertainties in data exist and can come from the measurement instruments such as sensors or any confidence level attached to the source of information. For instance, the temperature measured by a sensor may not be certain but assumed within a range of values; many values recorded in different application domains may only be ascertained with some level of confidence. These are probabilistic data because the value of an attribute could be affixed with a probability level. Uncertainty can also be ascribed to relationships in data to form probabilistic information networks. Unfortunately, most existing data mining approaches assume independence and certainty of data. Very little work has been done on effectively analyzing probabilistic databases or probabilistic information networks to discover useful new knowledge or patterns in such data collections. The purpose of this proposal is to work on devising effective and efficient techniques to mine and learn from probabilistic databases and probabilistic information networks, and show the use and relevance of these techniques in real domains applications where uncertainty is germane to the data collection.""523828,""Pike, David"
"527805"	"Zemp, Roger"	"Frontiers of Photoacoustic Imaging"	"The long-term objective of our proposed research is to develop photoacoustic  imaging technologies that push new frontiers of biomedical imaging. Photoacoustic imaging is an emerging biomedical imaging modality that uses pulsed lasers to excite tissues. When light is absorbed, sound waves are created and used to form images of the optical properties inside the body. We have made considerable progress photoacoustic imaging, however, we perceive some areas where PA imaging needs further development: (1) Aside from blood & melanin-based pigment, until recently little has been done to explore other naturally-occuring contrast mechanisms in the body. We propose a number of techniques to image into tissues to attain information about specific chemical bonds without the need for contrast agents. The proposed methods will also provide information about the time chemical species are in an excited state after being interrogated by a laser pulse.  (2) Quantitation has been difficult in photoacoustic imaging. Ideally we would like to create quantitative images of optical absorption, scattering coefficients (important because cell nuclei are often enlarged in cancers and pre-cancers, affecting scattering), and concentrations of chromophores. A large number of methods have been proposed to algorithmically solve this non-trivial inverse problem, however, each has its own set of problems. More work is needed to develop a useful & robust method for improved quantitation and we propose key algorithms and methods to do this. (3) Improved ultrasound transducer technology is needed. Photoacoustic signals can be weak & sensitivity and frequency response should be maximized. Ultrabroadband & ultrasensitive transducers are proposed to improve the quality, resolution, and penetration depth of photoacoustic imaging. Our proposed research aims to address present limitations to bring PA imaging to new frontiers.""527806,""MacQuarrie, Stephanie"
"521759"	"Zhang, Kaizhong"	"Algorithms and software tools for mass spectrometry data analysis and RNA structure analysis"	"The proposed research program will focus on the development of algorithms and the construction of software tools for peptide, glycan, and protein identification and analysis with mass spectrometry and RNA structure analysis. The long-term objective of the proposed research is to provide biological and health researchers with software tools for characterizing proteins with mass spectrometry and analyzing RNA structures. We will develop novel algorithms to help constructing software tools with improved accuracy and quality.  Within this research proposal four short-term goals will be pursued: (1) Increase de novo sequencing peptide identification rate: De novo sequencing can identify novel peptides that do not exist in any database. Our research will address the low identification rate problem associated with the ''mixture'' spectra, where two or more precursor ions with similar mass and retention time are co-eluted by MS/MS.  (2) Improve accuracy for de novo glycan sequencing: We will develop new algorithms to address the inadequate accuracy in current methods for the determination of glycan structure.(3) Automate complete protein sequencing with MS/MS: We will develop new algorithms for complete protein sequencing without template proteins.(4) Analyze RNA-RNA interaction structure: comparison and inference:  We will study the complexity and design algorithms for RNA-RNA joint structure comparison and inference problems.The proposed research in this application will enable effective cross-disciplinary training, placing HQP in a collaborative environment.""521760,""Johns, David"
"527596"	"Zhang, Lihong"	"Synergistic Synthesis Methodologies and Computer-Aided Design Tools for Analog and RF Integrated Circuits in Advanced Technologies"	"Transistor performance in the 90nm and below technologies is strongly affected by extrinsic layout-dependent effects (LDEs), such as transistor well proximity and stress effects. Recent research has shown that ignoring LDEs at nanometer process nodes can cause 15%-30% variation in the transistor performance, which may readily lead to malfunction of analog/RF integrated circuits (IC). When using leading-edge commercially available analog/RF design methodologies, the associated LDE impact on circuit performance is unknown before the layout is generated. Thus, a large number of design iterations as well as manual intervention are inevitably required in advanced technologies. In this research program, synergistic synthesis methodologies and computer-aided design tools will be studied to make a fusion between performance optimization and physical effects. An LDE- and variation-aware circuit topology synthesis methodology, which can effectively explore the search space proactively by considering associated layout impact, will be developed. A novel two-stage LDE-, variation- and parasitic-aware circuit- sizing methodology, which first deploys a symbolic-equation-based scheme for quick global optimization and then a simulation-based method for refined sizing, will be studied. We will also investigate automatic layout synthesis strategies to address LDE-, variation- and parasitic-related constraints. Analog/RF ICs have been recognized as the design bottleneck for promptly pushing mixed-signal system-on- chip products to market due to high sensitivity to the complicated analog effects. Systematic countermeasures in the LDE-aware synthesis of analog/RF ICs have not yet been addressed worldwide. With enormous potential for commercialization, this proposed research program addresses the increasingly challenging LDE problems and their profound interactive nature that cannot be ignored for analog/RF IC synthesis in nanometer technolo- gies. This program will benefit the analog/RF design community through significant improvements in design productivity and reliability, and will enhance Canada's competitive advantage in this field.""527597,""Rotermund, Harm"
"525465"	"Zhao, Dongmei"	"Energy Efficient Cognitive Radio Networks"	"With the high demands and rapid evolution for wireless communications, energy consumption of wireless networks has escalated significantly, and become a major threat for environmental protection. Meanwhile, energy costs account for a significant portion of a mobile operator's operating expenses. Building energy efficient wireless communication systems has become an urgent need for sustainable development of wireless communication networks and services. Among various wireless communication techniques, cognitive radio networks (CRNs) have attracted a considerable amount of attentions from both the academia and industry. CRNs access the radio spectrum that is allocated but not fully utilized by other networks. Cognitive radio devices have the ability to sense the environment, adaptively adjust their work modes, and effectively provide various services. CRNs are expected to be very widely deployed in the near future to cope with the spectrum scarcity problem caused by the high demands for wireless communications and traditional fixed radio spectrum allocations. Currently, the work on CRNs mainly focuses on how to efficiently detect and utilize the available radio resources and to support various services. The objective of this project is to design and analyze performance of energy efficient CRNs. We will i) identify challenges and complexities raised by the secondary nature of the CRNs, ii) explore the intricate trade-offs between energy efficiency, data transmission performance, and implementations of CRNs for energy saving, and iii) take advantage of various diversities and opportunities brought by the built-in cognitive capabilities of CRN devices to achieve high energy efficiency. This research is of paramount importance for the proliferation of future wireless communication services and for protecting the environment we live in. The outcome of this research will contribute to providing wireless communication services that are both energy efficient and spectrum efficient. It will foster the progress of green wireless communications and further towards green telecommunications in Canada, which will have a long-term impact on the sustainable development of the Canadian society.""525466,""Deschênes, François"
"533253"	"Zheng, Rong"	"A Sequential Learning Framework for Resource Management in Wireless Networks"	"Due to the scarcity of wireless spectrum and growing demands for wireless data services, resource management is a perpetual problem in wireless systems with very diverse characteristics from metropolitan scale networks to personal area networks. When the knowledge of system states and parameters is unknown or incomplete, sequential learning or specifically the multi-armed bandit framework offers an elegant solution to balance the trade-offs in exploitation (practicing the best-known decisions thus far) and exploration (finding out the utility of other decisions).The primary goal of my research program is to advance the fundamental understanding and development of efficient robust solutions for cyber physical systems, and wireless and mobile data networks. The specific objective of the project is to develop a sequential learning framework for a variety of resource management problems in wireless networks. The contribution of the project will be three-fold. First, we will advance the theory of sequential learning and devise new algorithms to address practical challenges arising from wireless networks. Second, we will apply the proposed sequential learning methodologies to develop effective solutions to resource management problems in network monitoring, cognitive radio spectrum access and routing in delay tolerant networks. Third, through testbed development and system implementation, this project will offer HQP involved training opportunities not only in algorithm design and analysis but also hand-on experiences with state-of-the-art wireless network hardware and software.""533254,""Crothers, Evan"
"526398"	"Zhu, Guchuan"	"Deformation Control of Smart Microstructures Governed by Partial Differential Equations"	"The present program aims at developing control strategies for the manipulation of smart microstructures, e.g. microelectromechanical systems (MEMS), described by partial differential equations (PDEs). The main objective of the present program is twofold: 1) to work towards solutions for deformation control of microstructures with in-domain actuation; 2) to apply PDE control techniques to real-life microsystems, in particular deformable micro-mirror control in adaptive optics (AO) systems, in view of enabling the implementation of high performance control systems with low complexity.    Due to technological restrictions in the design, the fabrication, and the operation of micro-devices with large amount of on-chip sensors, open-loop control is the dominant technique used in the operation of large scale micro-mirrors. However, open-loop control schemes are inherently sensitive to environmental fluctuations, disturbances, and model uncertainties. On the other hand, although theoretically the application of closed-loop control may enhance system performance, design methods leading to a control structure that requires at least as many sensors as actuators are not applicable to microsystems with currently available technologies. This raises serious challenges from both theoretical and practical viewpoints.      In order to achieve viable solutions, we consider the application of recently developed PDE control techniques, in particular flatness and backstepping designs. The research emphasis will be put on the control of 2D micro-mirrors with in-domain actuation, corresponding to the typical manipulation scheme of deformable micro-mirrors in AO systems. By exploiting the technique of trajectory planning and feedforward control, the system can be operated with few closed control loops. The number of on-chip sensors can then be considerably reduced. The developed methods will be extended to more realistic devices, including nonlinearity and actuator dynamics. Standard PDEs control techniques (e.g. approximation) will be used as benchmarking tool and extensive experimentations will be carried out to validate and evaluate the developed control systems.""526399,""Bertram, John"
"527386"	"Zhu, Ying"	"Some practical ways of using smart mobile devices"	"The current state of mobile handhelds and their future direction all point to the crucial role they will occupy in managing personal data in both physical and social networks. The proposed research program will investigate several aspects of the mobile handhelds' role as the gateway and the personal computer of choice for managing personal data between the user and both the physical and the inter-networked world.We address four challenges in the mobile handheld device's role as the gateway between the user and theInternet: detecting and preventing leakage of private personal information in social networks, peer-to-peer data provenance tracking, power-aware data synchronization between mobile device and cloud, and onlinepower consumption monitoring of mobile applications. The solutions to these problems will provide the user a fundamentally superior smart mobile experience. The mobile user will be able to enjoy active participation in various social networks without worrying about privacy leakage. She can create, contribute and consume multimedia data whose provenance and authenticity can be traced in a trustworthy way. Her mobile device will also seamlessly and intelligently perform data synchronization with the cloud central storage, so that she will not be limited by the local storage space in using her applications. Moreover, all this is supported by an online power monitoring tool that helps to optimize power usage and maximize battery life.  This research will not only improve smart phone usage for individuals in Canada and around the world, but will also have implications in the ways that businesses and industry apply the technology associated with improved mobile hand-held devices to their business model.""527387,""Gour, Gilad"
"522564"	"Zhuang, Weihua"	"Energy Efficient Wireless Communication Networking"	"With increasing demand for broadband multimedia wireless communication and information services, energy consumption in wireless communication networks has become a major concern. Most existing energy efficient techniques cannot be applied as they are limited for wireless sensor networks, where the sensors have limited power and functionalities with very low data traffic load and no (or low) mobility. As a result, many R&D initiatives have been launched worldwide to develop energy efficient (green) wireless communications and networking technologies. The proposed research program aims to study fundamental tradeoffs between energy efficiency and communication service quality for broadband multimedia mobile communications, and to develop new networking solutions for better compromise between energy efficiency and service quality. In particular, we will focus on three thrusts: 1) adaptive link control according to network dynamics, enabling mobile terminals to alternate between active and sleeping modes, 2) end-to-end information delivery, enabling energy efficient data traffic routing to minimize energy consumption and maximize network lifetime, and 3) network cooperation, which will allow some base stations to be turned off when and where traffic loads are light, and greatly expand the service coverage area of each active base station to ensure continuous service quality. Diverse quality-of-service requirements, data traffic dynamics in multimedia communications, and random user mobility pose significant technical challenges to improve energy efficiency without compromising service quality. The research outcomes are expected to have a profound technological and scientific impact, in providing in-depth understanding and guidelines for developing more flexible and practical energy efficient wireless networking algorithms and protocols, and in helping Canada to establish and reinforce its leadership in green wireless networking. This program will train a large pool of highly qualified personnel, providing them with valuable insights and experiences in distributed energy efficient resource allocation and network control, and enabling them to emerge as expert additions to the Canadian high technology industry labor force.""522565,""Johnston, Mark"
