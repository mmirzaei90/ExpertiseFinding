"id"	"researcher_name"	"application_title"	"application_summary"
"566221"	"Abhari, Abdolreza"	"Data Mining for Information Retrieval and Processing
in Web 2.0 Social Networking"	"In this proposed research program, data mining and fuzzy logic will be used to build a group of novel recommendation systems for the Web2.0 social networking sites Facebook, Twitter, YouTube, and LinkedIn.  For FaceBook and YouTube, fuzzy logic will be employed in the recommendation systems, which will be capable of providing information that is based on the correct interpretation of perception-based data existing in these sites. Therefore, the resulting recommendation systems can provide advice to the user as an additional part of a retrieval system. When in Facebook or YouTube a user says, “I usually drink Tim Horton’s coffee,” or “I like this video”, the meaning of “usually” and “like” varies among users. A fuzzy retrieval system can make relevant recommendation even when the users deploy imprecise words such as “usually,” “most,” and “often” to show their interests. For example, a fuzzy retrieval system can be built specifically for Facebook fans of Tim Horton’s, and can recommend a special cookie for the Tim Horton’s coffee drinker based upon an analysis of users’ interests to determine their favourite tastes.  As domains become more specific in Web2.0 sites (for example in LinkedIn), fuzzy relations between words can be defined and used to direct users who are sending specific search queries. For example, users in a particular network of Linkedin read specific documents that can be used for building a knowledge base that acts as the recommendation system for the users of that network. Therefore, whenever a user who is searching for a job in computer engineers’ network types the word “design,” only the related words such as “chip design” or “IC design” should be shown by the recommendation system, whereas when a job seeker who belongs to the computer scientists’ network types “design,” the suggested related words might be “algorithm design” or “software design”.  Similarly for Twitter, the recommendation systems can analyze the tweets and identify users with the same interests. Twitter has more public information than all other social networking sites; therefore, another research activity proposed for Twitter is using it as a platform for event detection. By using various data mining techniques, this research activity aims to develop a framework that effectively extracts emerging events (useful information about real happenings in the world) from the large data set of tweets.  The last part of this proposal is developing data collection and evaluation tools by using multi-agent systems. These multi-agent systems will be used as data crawlers to actively update the knowledge bases of such recommendation systems. Moreover, the multi-agent systems can also be used for validation of the proposed Information Retrieval (IR) and recommendation systems in comparison with other ontology or semantic-based search engines. Use of the multi-agent systems for evaluation purpose of IR and recommendation systems in social media sites is a novel idea that will provide two more performance metrics (i.e., distributed processing effectiveness and scalability) in addition to the common accuracy metrics (i.e., precision and recall) .  Using fuzzy logic in the above research activities for information processing of the Web2.0 sites is another new idea that has not been developed and implemented prior to this proposed research program.""561040,""Abid, Shaheem"
"566342"	"Abugharbieh, Rafeef"	"Integrative Computational Models for Multi-Modal Analysis of Structural and Functional Neuroimaging Data"	"Medical imaging constitutes one of the fastest growing specialties in healthcare accounting for 35% of the total medical devices market. The global market for diagnostic imaging was estimated at over $20 billion in 2010 and is projected to exceed $26 billion by 2016 [1]. Ubiquitous access to increasingly cost effective non-invasive imaging technologies enabled unprecedented rates of patient data acquisition. With over 5 billion investigations recorded worldwide by 2010 [2], the proliferation of medical imaging continues to be fueled by increasing population age, widening range of diagnostic imaging applications, continuing technical advancements in safe imaging modalities, and increased emphasis on preventive care. For example, in Canada, over 1.7 million magnetic resonance imaging (MRI) scans were acquired in 2012 alone; double the number of such scans in 2003 [3]. The medical imaging revolution however came at a price; a deluge of multi-dimensional high-resolution data posing serious health informatics challenges. The emerging trend is the fusion of different modalities that requires very efficient analysis of massive amounts of imaging data for diagnostic and therapeutic purposes. Fusion technologies and automated image analysis are the major drivers of the global medical image analysis software market, which is expected to grow at a rate of over 7% from 2012 to 2017 to reach $2.4 billion [1]. With over one billion people worldwide (that is one in six humans) estimated to be affected by some brain or nervous system disorder [4], neuroimaging is one of the most important application areas. Brain diseases such as Alzheimer’s disease (AD) and other dementia, mood disorders, schizophrenia, epilepsy, Parkinson’s disease (PD), multiple sclerosis (MS), attention or sensory disorders, among many others, pose serious effects on human health. The prevalence of such diseases continues to increase especially with the world’s ageing demographic and increased life expectancy. In fact, due to their significant disease burden, including the huge associated economic and societal costs, the World Health Organization (WHO) pinpoints neurological disorders as one of the greatest threats to public health [5]. The inherent complexity of captured human brain anatomy and function renders neuroimaging heavily reliant on advanced computational analysis. In the 2013 report summarizing the outcomes of the NSF (National Science Foundation) Workshop on Mapping and Engineering the Brain, neuroimaging was identified as one of the grand challenges requiring new enabling technologies, better knowledge transfer, and multi- and trans-disciplinary research [6]. My proposed research is thus very timely and builds on my very strong record in this area of computational neuroimaging. My plan focuses on solving challenging computational processing and analysis problems associated with the synergistic use of multi-modal brain images. Specifically, I will develop novel paradigms for the analysis of high spatial resolution structural and functional neuroimaging data (mainly MRI) as well as combine such analyses with high temporal resolution brain signals such as Electroencephalography (EEG). My specific objectives include the development of novel integrative computational models of multi-modal neuroimaging data, use of machine learning approaches to capture and quantify the brain’s complex structure and function and elucidate their interplay, integration of anatomical and physiological information within a single unified analysis framework, discovery of novel image based biomarkers of neurological disease, and creation of practical tools and software applications for specific real life neurological disease contexts such as Alzheimer’s, Parkinson’s and stroke.""558537,""Abukhdeir, NasserMohieddin"
"566773"	"Adams, Bram"	"Empirical Analysis of Build System Smells and Refactorings"	"A build system is the infrastructure that transforms the (textual) source code of a software system into an executable program by invoking configurators, preprocessors, compilers, test harnesses, packaging and deployment tools in the correct order. Because this transformation is fundamental, most software development stakeholders interact with the build system on a daily basis. For example, developers use it to obtain an executable to run unit tests on a new feature or bug fix. The build system also enables continuous integration servers to automatically build and test whenever a developer checks in a software change. Finally, the recent trend of continuous delivery, where companies like Intuit, Mozilla and Google release new versions of their software in a couple of days or even hours, requires a powerful, flexible and highly performant build system! However, real-life build systems in industry and open source are quite the opposite: large, monolithic build specification files in '70s era technology (like ""make""), hardly understood by any developer, fully at the mercy of one talented project member. Even though the source code evolves constantly, adapting the build system to these changing requirements is hard and often results in patched-up versions of the existing build architecture. The latter problem is due to the rigid build system architecture, the developers' unfamiliarity with (often arcane) build technology like ""make"", and the low priority of build system changes for unsuspecting project managers. Hence, the build system becomes more complex and less performant, with team members getting frustrated and tampering with code dependencies to speed up builds. Such tampering in turn introduces other side-effects like inconsistent build errors or unpredictable build performance. The main goal of my research program is to derive a set of patterns and best practices for build system maintenance that will enable any software development stakeholder to maintain their build system (e.g., ""make"" or ""ant"" files) in a systematic, disciplined, quantifiable way. In particular, this program aims to compile catalogues of bad smells (indicators of a potential quality issue) and refactorings of the build system (and related entities), and to build developer support to recommend applicable refactorings and perform refactorings selected by the stakeholder. The catalogues will be based on interviews with open source developers and build system engineers, as well as large-scale empirical analyses of thousands of open source systems using three generations of build system technologies. We will also perform user studies with open source and industry participants to validate our IDE support. Given the central role of the build system in software development, our research results will be directly applicable by the Canadian industry, providing competitive advantage to both large software companies, like IBM and RIM, and smaller SMEs. In particular, companies will be able to enhance their dashboards with crucial, but currently missing indicators of the health of their build system. The identified patterns and best practices will provide a succinct vocabulary and methodology for all build system stakeholders to discuss and treat build system problems effectively. Above all, our results will allow practitioners to systematically integrate their build system into a successful continuous delivery process. The research plan will train 4 High Quality Personnel (HQP), i.e., 2 PhD and 2 Master students, who will gain extensive hands-on experience with empirical research techniques and industry-grade build systems. This training qualifies them for the demanding and much sought after roles of build and release engineers for large-scale software systems.""547373,""Adams, Carl"
"555196"	"Ahmed, HazemRadwanA"	"Predicting Cancer-related Protein-Protein Interactions using Particle Swarm-based Network Alignment"	"Protein-Protein Interactions (PPIs), Protein Interaction Prediction, Interaction Network Alignment, Breast Cancer-Related Proteins, Bioinformatics, Particle Swarm Optimization (PSO), Swarm Intelligence (SI), Nature-Inspired Computing, Structural Pattern Recognition, Data Mining""546805,""Ahmed, Maher"
"567583"	"Alajaji, Fady"	"Joint Source-Channel Coding Theory with Applications to Wireless Communication Networks"	"In this application, we seek funding to continue our research program in information theory and wireless communications. The research program concerns the development of advanced joint source-channel coding (JSCC) technologies for improving the reliability, connectivity and speed of wireless communication networks and meeting the rapidly growing demand for seamless and ubiquitous wireless multimedia services. JSCC provides the right approach to construct the best codes for the efficient transmission of data sources over noisy channels, particularly under stringent delay and complexity constraints, and for multi-user networks where Shannon's principle for the separation of data compression and channel coding fails to hold in general. Building on our experience and extensive achievements in JSCC, we will focus on the following three interconnected research thrust areas: (1) The investigation of the fundamental information-theoretic limits, namely capacity and JSCC error exponent, for coding and transmitting information over multi-user networks and recovering it with negligible loss; (2) The development of sophisticated but analytically tractable discrete channel models to judiciously represent the behavior of wireless (correlated fading) networks used with multiple antennas and soft-decision demodulation; (3) The design, construction and analysis of powerful low-complexity JSCC techniques for wireless networks, with applications to wireless multimedia communications and energy-efficient sensor networks. The proposed research will feature the rigorous training of eight highly qualified personnel per year (3 doctoral, 2 Masters and 2 undergraduate students, as well as one postdoctoral fellow), immersing them in timely challenging topics in information theory and wireless communications and resulting in innovative high-quality publications. The results of this research program are anticipated to provide a deeper understanding of both the fundamental principles and the practical aspects of JSCC, thus positively contributing to a sustainable improvement of wireless communications and multimedia technologies.""567417,""Alam, Jahrul"
"555732"	"Altawy, Riham"	"Design and Analysis of Authenticated Encryption Schemes"	"Cryptography, Symmetric primitives, Information Security, Authenticated Encryption, Differential cryptanalysis, Cryptanalysis""546802,""Altenhof, William"
"567895"	"An, Aijun"	"Online Mining of Big Data Streams Using Cloud Computing"	"In a world where data are growing at extraordinary rates, there is huge demand for fast and effective analysis of big data to discover useful information for making business decisions. This research program tackles the problem of discovering useful information from big data streams. Big data streams, characterized by high volume and high velocity, have become ubiquitous as many sources (such as social networks, sensor networks and financial markets) produce data continuously and rapidly. Effectively and efficiently discovering patterns from such massive and fast-evolving data will allow businesses to quickly react to their dynamically changing environment to, for example, perform fraud detection at a point of sale, determine which ad to show, or detect spam in comments on news in which trends change quickly in time. Many challenges exist in discovering useful information from big data streams. To handle very fast data, systems have to process the data as fast as the arriving data. However, most existing data stream mining methods are sequential algorithms that run on a single machine and are limited by the memory and speed of the machine. To mine massive data, parallel and distributed computing over a cloud of computers has become a mainstream solution to achieve low latency and high scalability, and MapReduce has become a popular programming paradigm for easily writing applications that process massive data in parallel in a fault-tolerant manner. However, converting a stream mining algorithm into an online parallel MapReduce-style algorithm poses challenges. Most learning algorithms are highly sequential. Parallelizing such algorithms needs considerable efforts and may require the design of new algorithms. In addition, in stream environments, data flow into the system at a rate over which we have no control. The processing system must keep up with the data rate or degrade gracefully. Resource adaptive online learning with bounded approximation is highly needed, which has not been addressed adequately in the MapReduce-style data processing model. To address the above challenges, we will develop parallel versions of stream-mining algorithms using MapReduce-style distributed stream-processing platforms. We will build on our previous and on-going research in data mining and parallelize the stream-mining algorithms that we have developed recently, which include, but not limited to, classification rule learning, high utility pattern mining, and Monte Carlo based learning algorithms. In addition, we will develop resource-adaptive techniques for learning from big data streams. Adaptive data structures and anytime learning algorithms will be developed that can produce best possible answers under resource constraints and can utilize the extra time and memory, if given, to increase the quality of the answers. Moreover, we will identify the pros and cons in developing parallel stream mining algorithms using the state-of-the-art MapReduce-style stream processing platforms and provide feedbacks to the community as to what is further needed in these platforms for them to better serve online learning of big data streams in the cloud. Mining big and fast data streams using cloud computing is still in its infancy. The proposed research will advance the field by proposing novel solutions to its open challenges and will have a wide range of applications in various fields that produce massive data streams.""556476,""An, Aijun"
"566599"	"Azana, Jose"	"Rethinking the fundamentals of photonic signal processing for ""green"" communications and computing"	"The implementation of photonic circuits for temporal signal generation, processing, and detection in information and communications technology (ICT) systems is generally envisioned as an extremely promising approach to overcome the speed limitations of present electronics-based solutions. The bandwidth advantage of photonics is especially attractive to cope with the ever-increasing demand for “higher capacity at a lower cost” in high-speed telecommunications, information processing and computing platforms. However, photonic solutions still suffer from two main limitations, namely 1) they are typically bulky and 2) they are “power hungry”, preventing the practical use and spread of photonic signal-processing technologies beyond a few niche applications. Significant progress has been made concerning size reduction of photonic structures, through successful device implementations in compact fiber-optics technologies, e.g. fiber Bragg gratings, or in integrated-waveguide configurations, most prominently CMOS-compatible photonic chips. Such footprint improvements in turn have had a positive impact on the energy performance of photonic signal-processing technologies. In spite of this progress, present photonic signal-processing designs intrinsically use much higher amounts of energy than competing technologies (e.g. electronics), and very little effort has been devoted so far to improving their performance in terms of energy efficiency. The research program described in this proposal will focus on the design and realization of photonic signal-processing devices for efficient generation, manipulation, and detection of ultrafast data and control signals, of fundamental importance for broadband telecommunications, and high-speed information processing and computing. The distinctive focus of the program will be on defining innovative, general strategies for optimizing the energy efficiency of the target photonic circuits without trading their processing speed advantage. Particularly, we will pursue the realization of the target circuits in fiber-optics and/or integrated waveguide (silicon photonics) technologies, with an emphasis on fiber/integrated grating structures. The novel design strategies to be investigated will be capable of providing dramatic (orders-of-magnitude) improvements on energy performance over present technologies at processing speeds well beyond the reach of electronics, above the THz range. Sub-systems to be pursued in the mid to long-term will include ‘on chip’ reconfigurable circuits for optical linear filtering, arbitrary optical waveform generation and detection, and ultrafast analog and digital computing. In the short-term, activities at the core of the program will be conducted around four inter-related research projects, involving the development of (a) energy-optimized analog optical signal processors, (b) “green” noiseless waveform amplification methods, (c) “zero-energy” ultrafast optical logical gates, and (d) energy-optimized circuits for ultrafast clock recovery, generation and processing. The outcome of this research could open the path to the use of photonic signal-processing solutions at a much larger scale, directly contributing to overcome present electronic bottlenecks in almost any application or sector relying in ICT infrastructure. The new knowledge generated from this program and the training of highly qualified personnel (HQP) in the targeted strategic areas will contribute to enhance Canada's global competitiveness across a very wide range of high-technology sectors.""554503,""Azana, Jose"
"566837"	"Aziz, Hany"	"Exciton-induced Degradation of Interfaces in Organic Optoelectronic Devices"	"The last two decades have witnessed a strong interest in organic semiconductor-based optoelectronic devices for utilization in various applications including solid state lighting, flat panel displays and solar cells. This interest is motivated by a number of key advantages that organic semiconductors have, such as (i) the possibility to obtain light emission in a wide range of colors (ii) easier and cheaper fabrication including the ability to fabricate them on different substrates and in large areas, and (iii) their unique ability to realize mechanically flexible and transparent devices for next generation electronics. Despite their advantages, devices utilizing these materials, such as Organic Light Emitting Devices (OLEDs), Organic Solar Cells (OSCs) generally have lower durability in comparison to those based on traditional materials. We recently discovered that exposure to light, either from external sources, as in case of OSCs, or to light produced by the device itself, as in case of OLEDs, causes significant changes in these devices, especially at interfaces between different materials. This previously unknown phenomenon appears to be responsible for the fast deterioration in efficiency in these devices.  The proposed research aims to uncover the factors behind this phenomenon and to develop approaches to control them, thus improve device service life. The research will utilize a number of experimental techniques pioneered by the applicant and will build on findings from his previous work in this area. Seven graduate students will participate in this research. The research results will be critical for future development of more stable OLEDs and OSCs, which will be pursued in collaboration with Canadian industrial partners. It will, therefore, provide those partners with a competitive edge in an emerging technology of tremendous potential for next generation electronics and energy conversion devices. The research will also allow training of a new “breed” of Canadian researchers who will acquire expertise in an emerging technology that has the potential to enable new products and market opportunities.""556555,""Aziz, Hany"
"567160"	"AzzedineBoukerche, Azzedine"	"Towards the Design of error-free and scalable spatial temporal localization protocols for heterogeneous  Wireless Sensor Networks"	"In recent years, 3D heterogeneous wireless sensor networks (HWSN) have received a great deal of interests due to their growing potential use in the design of future generations of monitoring, surveillance, safety and security systems. In these kinds of networks, battery-powered autonomous and heterogeneous sensor and actuator devices which are deeply embedded into the physical surroundings are equipped with different sensing, computing and wireless communication capabilities. These autonomous devices collaborate with each other in order to gather, process and disseminate valuable information, as well as to report the collected information to a central server for further analysis, and making them accessible to interested third parties. HWSNs applications include: planetary exploration WSNs, unmanned aerial vehicle (UAV) WSNs, underwater WSNs, monitoring and control of oil-wells, monitoring of habitat and of active volcanoes, EOFS (Environment Observation and Forecasting System) applications, precision agriculture, battlefield monitoring, weather detection, wild animal protection, healthcare and supply chain management applications, and some other monitoring critical area that are inaccessible to humans, as well as emergency preparedness and rescue applications. Several of these critical applications can be more dependant on the 3D localization system than others, where sensors can be deployed in 3D Surface based environment. The relevance of sensory data and the knowledge of the real-time positions of remote sensor nodes can be very critical in these cases where correct events’ interpretation can life-saving. Thus, sensed events must be ordered both in time and space and proper communication and localization techniques in space and time are required to achieve high accuracy and efficiency. This research proposal aims to explore both the theoretical and practical aspects of spatial and temporal localization in HWSNs. We will concentrate primarly on the design and evaluation of a novel suite of scalable, accurate and efficient spatial-temporal localization protocols to foster the integration of space, time, surface, communication and environmental context into the heterogeneous wireless sensor networks in order to provide the sensed data with both time and position information at anytime and location for any 3D HSWN based applications. The PI is responding to the growing demands of complex 3D heterogenous wireless sensor networks over different kind of 3D surface environment due their wide range of potential applications. The vision of this research in a long-term is to design efficient spatial temporal localization management protocols for multipurpose mult-dimensional HWSNs.""561198,""Azzi, Estelle"
"566562"	"Bahreyni, Behraad"	"Putting pn junctions to work: silicon micro-/nano-mechanical devices based on depletion region actuators and sensors"	"A research program for the systemic study of the behaviour and applications of semiconductor pn junctions for transduction at micro- and nano-scales is proposed. When a pn junction is formed, majority carriers from either side of the junction diffuse into the opposite side, leaving behind ionised acceptor and donor atoms that are fixed within the semiconductor lattice. This gives rise to a Coulombic body force between the two sides of a pn junction, which can be modulated by an external voltage. As expected, this force is rather small and has been largely ignored. Employing mechanical resonance allows for effective amplification of this force by as much as six orders of magnitude, making it suitable for many applications of microdevices. The proposal discusses the advantages of pn junction actuation over other transduction mechanisms at micro- and nano-scales and demonstrates that pn junction actuators present an unexploited potential to revolutionise microsystem design and application. The holistic approach of the proposed research program deepens our understanding of the operating principles of pn junction actuators while devising practical device design methods. Existing models for pn junction actuators are applicable only to the simplest of cases. We will develop realistic device models that will be verified using experimental data from fabricated test structures and devices. The models will then be employed to design microdevices for timing, sensing, and signal processing applications. Lowering the device dimensions to nano-scales opens up many scientific and technical opportunities. On one hand, pn junction actuation provides an efficient transduction mechanism at nano-scales, leading the way to the development of nano-mechanical devices for daily applications. On the other hand, the small dimensions of nano-devices provide research opportunities to study the operation of pn junction actuators at extreme cases such as fully depleted structures. Doping is a basic process in microelectronics. It is therefore possible to fabricate micromechanical devices based on pn junction actuators in standard integrated circuit manufacturing processes, and as is shown, without a need for any additional steps. This will allow for co-fabrication of microelectronic and micromechanical devices in any standard foundry process. My research team has expertise on the design of micromechanical resonators, microsensors, nanocomposite materials, and microelectronic circuits. Our laboratory is equipped with all the necessary tools for the electrical characterisation of the micro- and nano-mechanical. Additionally, SFU houses two fully equipped cleanrooms for the fabrication of micro- and nano-devices. We are therefore in an ideal position to leverage our expertise to conduct the proposed research, which will revolutionise the field. Collaborations with Canadian and international researchers exposes the involved HQP to the wider academic and industrial community. The HQP trained over the course of this program will be the leading researchers and entrepreneurs who will transform the scientific and technological achievements of this program into novel solutions in the area of micro- and nano-systems. This research has an immediate impact on the field leading to substantial scientific, technical, and industrially relevant outcomes. Device design methodologies developed through this program can be applied to the design of numerous novel micro- and nano-devices for different applications. Considering that the required resources for device design and fabrication are commercially available, this pioneering research will result in significant economic benefit to the Canadian industry and society in the near term.""566627,""Bahturin, Yuri"
"566383"	"Bateman, Scott"	"Providing Social Feedback in Information Work"	"Information tools (such as programming environments, web applications, or word processors) are used by people everyday to complete a wide variety of tasks. While people often learn how to complete tasks using their information tools, they most often make do with novice skills or basic knowledge. Without higher levels of expertise many people are not able to perform as well as they potentially could, which can lead to work that is slower and of lower quality. While a number of solutions exist to ease the transition from novice to expert (e.g., tutorials), a critical factor in learning and behaviour change is people’s perception of whether new behaviour and skills have been successfully used by others.  The problem addressed by this proposal is that current information tools do not provide appropriate support for people to learn from the strategies and techniques that have been successful for other people. Without this support people will continue to complete tasks in an inefficient manner. To address this problem, this research will apply the idea of social feedback – support derived from the captured behaviour of other information workers. Social feedback builds on theories of how people learn from others, and how people can be persuaded and motivated to adopt new behaviour when using technology. A social feedback tool provides support by following three basic steps: it captures usage behaviour from a group of people as they use an application; it identifies expert skills and desirable behaviour from the captured data; and, finally, by visualizing this data it allows people to see how their skills compare to experts and to learn how they might improve. While social feedback can allow people to complete common tasks more efficiently and with higher quality outcomes, research in this area is still limited. In particular, this research tackles three challenges to the successful deployment of social feedback systems: 1) the requirement for feedback to be provided at the right time and place in order for it to be useful; 2) the difficulty people have with recalling newly learned skills and behaviour; and, 3) the challenge of motivating people to learn new skills and improve their expertise. This proposal contains six separate projects that investigate these challenges within three new application domains (crowdsourcing, programmer education and energy consumption).  This research has strong potential to provide economic and social benefits to many types of work where information tools are used. We will create several technologies that provide data-collection mechanisms, visualization techniques, and design guidelines that allow developers to add social feedback into their existing software. We will also demonstrate the use of social feedback in three specific areas of high interest to the general public and to my industrial partners, who will play an important role in contributing to this work. This research will provide opportunities for HQP to receive training in cutting-edge technologies based in the domains of application, and position them well for further training or advanced positions within industry. This work will advance the understanding of social feedback systems by providing new mechanisms for people to improve their skills with information tools, which will have a positive impact on their productivity and everyday work experiences.""560873,""Bateman, Scott"
"555315"	"Baysal, Olga"	"Maximizing Developer Productivity by Personalizing Software Development"	"Software engineering, Mining software repositories, Software analytics""545520,""BazettJones, David"
"566204"	"Beaulieu, Christian"	"Technical Development of Multinuclear Sodium Magnetic Resonance Imaging"	"Engineering advances over the last 30 years have made magnetic resonance imaging (MRI) a powerful imaging modality for the non-invasive investigation of the human body. Routine clinical MRI measures signal from the hydrogen (1H) nuclei in water. However, with the appropriate hardware and imaging techniques, MRI can also acquire signal from other nuclei such as sodium (23Na) and potassium (39K), ions which may be more specific to aspects of tissue injury and disease than the ubiquitous water molecule. The ability to image sodium can provide a measure of cartilage degradation in osteoarthritis, a debilitating condition affecting millions of Canadians. Potassium is a key ion in brain function, but methods to measure it in human brain are lacking. The purpose of this grant is to develop highly sensitive radiofrequency (RF) coil hardware (e.g. sensitive phased arrays) and imaging methodology to enable quantitative imaging of these nuclei either in cartilage (23Na) or brain (39K). This proposal builds on our decade’s worth of sodium MRI research, which produced world-leading images of human brain and then knee (funded over last 5 years by NSERC). Sodium (and potassium) MRI is very challenging because of low concentration in tissue, small magnetogyric ratio, rapid signal decay, complex spin physics, and the need for nucleus-specific hardware and optimal methods. Our previous sodium MRI research on cartilage of the knee focused on the design of optimal acquisition strategies, but we used only standard volume RF coils. However, more complex and sensitive phased-array RF coils, which consist of many small localized elements, can dramatically increase signal-to-noise ratio and thus yield major gains in image resolution and quantification. While phased-array technology is now standard for regular 1H MRI, it is in its infancy for sodium MRI and needs to be explored not only for knee, but also for other body regions such as wrist, ankle, hip, and spine (where cartilage is also affected by osteoarthritis). Potassium has even greater technical challenges to overcome than sodium, and there are only two recent preliminary reports of potassium MRI in human brain. The novel MRI hardware and methods will be designed, simulated, constructed, programmed, tested, and published by the trainees. The developments will be on a ‘triple strength’ high field 4.7T MRI to enable major gains in signal. We hypothesize that our technical MRI advances will enable the accurate and precise measurement of sodium in cartilage and potassium in brain. Specific Aims: 1) To develop phased-array receive-only sodium RF coils and dual-frequency (23Na/1H) detunable, concentric transmit volume coils for imaging the knee, ankle, and wrist. In this case the volume coil provides uniform excitation and the phased-array provides high receive sensitivity. The capability for simultaneous reception from both the phased-array and volume coils using counter-rotating-current coil elements will be explored to facilitate accelerated image intensity correction related to the spatially varying RF sensitivity profiles of the phased-array elements. 2) To develop transceive dual-frequency phased-array RF coils for sodium MRI of the spine and hip, where uniform volume excitation is not feasible for sodium. An associated aim is to develop and optimize multiple coil element transmission for uniform excitation. 3) To develop quantitative potassium MRI of human brain. After 5 years, we will have developed novel multi-nuclear radiofrequency coil hardware and optimized MRI acquisition to enable imaging of (i) sodium in cartilage of the extremities, hip, and spine which are key structures affected in osteoarthritis, and (ii) potassium in the brain, a novel biomarker of brain metabolism.""560156,""Beaulieu, Christine"
"554241"	"Belzile, Bruno"	"Commande proprioceptive d'une prothèse de main"	"biomécanique, prosthétique, main robotique, sous-actionnement, proprioception, détection tactile""545519,""Belzile, François"
"566936"	"BenAyed, Ismail"	"British Columbia"	"CANADA"
"567471"	"Bengio, Yoshua"	"Deep Learning of Representations"	"Artificial intelligence requires computers to have knowledge of the world around us, knowledge they can use to answer questions and take decisions. Knowledge can be either hard-wired or learned from data and usually both sources of information are used. Machine learning algorithms endow computers with the ability to acquire knowledge from examples. In the age of big data, there is a great potential in the ability to tap this immense source of information. Representation learning algorithms are machine learning algorithms which also involve the learning of representations. The data (such as an image or a document) seen by a computer must be represented, and better representations make it easier to capture the statistical dependencies present and to learn to answer questions. Whereas representations are traditionally crafted by hand and engineered, recent years have shown that they can be learned and this can have drastic impact on the predictive ability of the learners. Deep learning, or learning of deep representations, is the subject of this proposal. It involves learning multiple levels of representation, corresponding to different levels of abstractions. In the past five years there has been a tremendous impact of research on deep learning, both at an academic level and with industrial breakthroughs. This proposal asks what are the current qualitative deficiencies of the current deep learning algorithms and proposes ideas for how to move beyond these limitations and towards AI. The main limitations that are discussed here regard the ways to scale up, because current models still have the size and capabilities of insect brains and do not even reach the perceptual abilities of rodents. Although faster and larger computers will matter, it is hypothesized that such advances in hardware will not suffice. For example, parallelization of the current learning procedures is not trivial and yet this is where computing power continues to grow. There also seem to be numerical optimization challenges that cannot be solved by simply pouring ten times more compute power, such as real or apparent local minima arising in the learning dynamics, for which it is not sufficient to restart thousands of times the optimization procedures: the result remains poor unless a different initialization procedure is used. There are also fundamental challenges regarding unsupervised learning: whereas most empirical progress of recent years has been with supervised learning (where humans have labeled data and told the computer what to answer to many questions), the greatest promise for future breakthroughs may come from unsupervised learning procedures (coupled with supervised learning). The main challenge there arises out of the intractability of the normalization constant involved in probabilistic models with many random variables and many high-probability modes separated by vast regions of low probability, a situation that is the most common in artificial intelligence applications. Although Monte-Carlo Markov chain methods are the most general solutions to this problem, new and maybe radically different ways of addressing the problem may be required and will be explored. Finally, this proposal considers the very basic question of what is a good representation, and proposes to view the different strategies we have to obtain good representations as priors that can help the learner in the very important task of disentangling the underlying factors of variation, i.e., of figuring out and separating from each other the factors that explain the data. As in past research of the applicant, these algorithmic explorations will be evaluated on challenging applications involving real data, from images and video to natural language text, or combining multiple modalities.""554573,""Bengio, Yoshua"
"566216"	"Benlamri, Rachid"	"Semantic Web-Based Medical Documentation Processing for Healthcare Collaboration"	"The present landscape of healthcare is one of escalating costs coupled with an aging population with increasing complexity in concurrent chronic medical illnesses. Care provisioned to these patients are carried out by large interdisciplinary teams that are challenged with being synchronously aware of each member's view points and generating coherent and safe treatment plans. Part of challenges relating to this environment exists in how documentation occurs. The vast majority of documentation consists of unstructured/semi-structured text that occurs in silos. In order to fully understand the team's point of view, a team member would have to inspect every individual entry before being able to provide an integrated contribution to the treatment plan. Therefore, if one agrees that within medical documentation lies the very heart of clinical impressions and planning, what if a computer can (i) correctly parse and annotate medical documentation; (ii) build high semantic structures out of unstructured documents from various electronic sources, and (iii) predict clinical contexts. What actions could then a computer take that benefit the patient, the healthcare team, and all levels of decision makers? This research program addresses the above challenges by investigating a novel approach to medical documentation processing and analysis, whereby the document is no longer inert to information technology, but a rich source for knowledge acquisition and a road map to drive a wide range of software processes which range from collaborative treatment planning to just-in-time decision support. To accomplish this level of intelligence, we adopt a new approach to combining benefits of Model Driven Software Development (MDSD), ontology engineering, and context processing to solve (i), (ii) and (iii).""557189,""Benmokrane, Brahim"
"566796"	"Beschastnikh, Ivan"	"Model inference and testing of distributed systems"	"Recent important trends in software systems, such as big data and complex websites, are made possible by rapid advances in distributed systems. These systems are an integral part of the everyday web experience for billions of people worldwide. For example, the back-end services of all of the major companies are hosted in data-centers that execute complex distributed systems to provide scalable, reliable, and highly available services. However, developers of distributed systems continue to struggle in implementing and maintaining these systems as there are few tools that can help them understand a system's behaviors and test its correctness. The objective of my proposal is to advance the state of the art in modeling and debugging of distributed systems. The outcome of this work will be a set of open source software tools that will help engineers to build robust distributed systems quicker and with fewer mistakes. This work will be relevant to both industry and academics. It will solve problems faced by everyday software engineers as well as extend our understanding of how to reason about complex executions of distributed systems and how to induce executions in these systems to explore and test their behaviors. Further, this research will provide important and invaluable training to both graduate and undergraduate students. It will produce highly-qualified personnel with a distributed systems skill-set that is prized and actively sought after by all the major software companies in Canada.""553593,""Beshara, Simon"
"567557"	"Bidinosti, Christopher"	"Enabling new disruptive magnetic imaging technologies through advanced instrument design"	"My research is focused on the development of two new imaging technologies that show excellent promise for a wide variety of applications. There are a many different imaging methods that have been invented, and while each has its own particular advantages they all share one thing in common: They allow us to “see” underlying structure and processes that were not otherwise directly visible. An excellent example of this is medical imaging, where well-known techniques such as ultrasound, X-ray, or MRI allow us to peer inside the body in a noninvasive way to monitor a baby’s development, check for a bone fracture, or aid in the diagnosis of a possible disease. Medicine is just one small example, of course, and imaging techniques have become an indispensible tool in every field of research, as well as in industry and indeed in everyday life. We use imaging of some form or another to look at – and into – the very small (e.g., molecules and cells), the very large (e.g., geology and weather patterns), and everything in between, such as your luggage at the airport. The search for new imaging technologies, with different capabilities and areas of application, is important not only to science and scientists but to society as a whole. For my research I will develop a new method for performing magnetic resonance imaging (MRI) that uses much less bulky and expensive equipment. This could result in lighter, simpler, and cheaper MRI apparatus, which in turn could bring this important medical imaging technology to small or remote communities. This same technique could also be used in as wide a variety of applications as food inspection to oil exploration. I will also work on the development of a brand new imaging technique known as magnetic particle imaging (MPI). MPI also shows excellent promise for medical imaging, in particular for examining the heart and arteries, as well as the brain and kidneys. It will also find use in biology to track cells and in chemistry to monitor certain types of reactions. The search for still other applications of MRI and MPI is one of the most important long-term objectives of my research program.""566559,""Bidochka, Michael"
"567522"	"Biglarbegian, Mohammad"	"Novel Robust Motion Control Methods for the Formation of Robots with Docking Capabilities"	"The theme of the applicant’s research program is on the development of novel motion control methodologies that can effectively handle disturbance/uncertainties for a frontier class of a high-utility modular mobile robots (MMRs) with docking capabilities. The use of mobile robots is dramatically increasing for applications such as in security, exploration, scientific data collection, search and rescue, etc. In the field of mobile robotics, MMRs represent a leading-edge class that offer significantly improved mobility and sensing compared to non-modular ones. Hence, recent research efforts to exploit their capabilities and enhance their functionalities are timely. Furthermore, developing efficient and novel motion control algorithms for MRRs will enable their utility for a wide spectrum of applications. The motion control methods to be developed in this proposal consist of both path planning and control algorithms for MMRs in formation operating in dynamic environments with the presence of noise and uncertainties, commonly encountered in actual robot operation environments. The formations considered in this proposal are either docked (rigid) or flexible. To the best of the applicant’s knowledge, robust motion control methods for the former (docked) that can handle disturbances/uncertainties in dynamics environments have not been introduced in the literature. For flexible formations, limited useful research has been introduced, yet more research needs to be undertaken to overcome many shortcomings that preclude the implementation of efficient motion controls for MMRs especially with the presence of noise, disturbances. The focus of this research program is on developing novel theoretical framework for motion control of MRRs, but to verify those techniques extensive simulations and experiments will be also conducted. The developed algorithms will be implemented using advanced processors such as reconfigurable computing to further increase their real-time operations. The proposed methodologies can be also applied to other systems which can fully or partially utilize the developed techniques such as in automotive, aerospace, and military/defense. This program aims to train several undergraduate and graduate students on design, modeling, advanced sensing and control of mechatronics systems that are of high demands for several industries such as automation, mining, aerospace, and automotive.""567034,""Bigras, Pascal"
"566046"	"Blaquière, Yves"	"Conception de technologies d'encapsulation multidimensionnelle configurables"	"Les technologies d’encapsulation multidimensionnelle, tel l’empilement de dés de silicium (puces), permettent de réduire significativement les dimensions et le poids, tout en augmentant la fonctionnalité de systèmes électroniques modernes, tels les téléphones cellulaires. Elles intègrent dans un seul boitier des processeurs, des mémoires, des capteurs, des antennes, des dispositifs hétérogènes et électromécaniques, et permettent un très grand nombre de terminaux électriques et optiques entre eux. Ce grand nombre de terminaux est rendu possible entre autres par l’exploitation des techniques récentes de perçage du silicium (Through Silicon Vias, TSVs). Pour faciliter l’intégration, des interposeurs passifs en silicium avec TSVs et plusieurs couches de métallisation sont construits pour s’aligner aux patrons de terminaux (billes, plots) de chaque dé de silicium, et utilisés comme structure passive d’interconnexions entre les puces. Ces structures sont dédiées à chaque puce, et par conséquent, augmentent le coût d’intégration. Par ailleurs, ces technologies d’encapsulation multidimensionnelle présentent plusieurs défis additionnels, entre autres de fiabilité (rendement, gestion thermique, dissipation de puissance, alignement des puces), de testabilité, de diagnostic et de coûts. L’objectif principal à long terme du programme vise à réduire les risques et les incertitudes pour rendre configurables les technologies d'encapsulation multidimensionnelle. L’originalité du programme de recherche consiste en la création d’un interposeur actif intelligent peuplée d’une mer de plots configurables de très petites dimensions, telles qu’un ou plusieurs plots puissent être électriquement connectés à un TSV ou une micro-bille de la puce en contact. L’élaboration d’un réseau d’interconnexions configurables permettrait de relier n’importe quels plots ou TSVs de l’interposeur aux autres. Le principal défi de cette recherche est la grande densité et la taille des micros-billes qui atteignent plus de 25 000 micro-billes par puce avec des diamètres inférieurs à 2 µm. Cette propriété de configuration ouvrira les portes vers un prototypage plus rapide et rendra l’assemblage insensible à l’alignement des plots avec une réduction significative des coûts de réalisation et de test. Plus précisément, mes étudiants et moi aurons comme objectifs à court terme (1) d’étudier et élaborer des structures d’interconnexions et de plots configurables intégrées dans un interposeur intelligent qui pourront s'adapter à tout patron et type de micros-billes de puces en contact avec l’interposeur; (2) d’explorer des méthodes et créer des circuits compact pour que chaque plot de l’interposeur configurable puisse s’adapter à des signaux tant analogiques que numériques ou (3) se configurer pour alimenter en puissance le TSV ou micro-bille de la puce en contact avec le plot avec une valeur configurable pour la tension d’alimentation; (4) d’investiguer et de créer des méthodes et des techniques pour insérer dans l’interposeur des instruments de vérification pour faciliter le test et le diagnostic de la structure d'empilement à 3 dimensions. Seules ces instruments pour observer les caractéristiques électriques, thermiques et mécaniques aux interfaces entre les puces constituerait une contribution majeure aux technologies d’encapsulation. Très peu sinon aucune recherche a évalué la possibilité de rendre configurable les technologies d'empilement multi-puces. Les fruits de cette recherche unique au monde permettront de mieux comprendre les limites et la faisabilité de créer des interposeurs intelligents configurables, et en conséquence d'augmenter substantiellement la compétitivité des industries canadiennes d’encapsulation et d’intégration multidimensionnelles.""559151,""Blaskovits, Terence"
"567517"	"Bois, Guy"	"Hardware/Software Codesign Flow of the 2020s"	"Hardware/Software Codesign Flow of the 2020s According to ITRS, we can foresee SoCs with 1000 or more processors on a single chip in the year 2020. These SoCs will populate the embedded systems of tomorrow (GPS, smartphones, video players, airplane electronics, etc.). Designing and verifying a hardware solution with such a huge number of processing elements, with even more complex software including hundreds of applications to exploit it, will be an enormous challenge. Programmability, adaptivity, scalability, physical constraints, reliability and fault tolerance are among the issues that must be addressed to minimize the cycle time and the development cost of these complex future systems. Hardware/Software (HW/SW) co-design investigates the concurrent design of hardware and software components, by proposing a design and verification flow for simultaneously creating hardware and software that comprises an embedded system. In recent years, and more so in the future, HW/SW co-design will be a must for engineers (hardware, software and system) involved in the development of complex automotive networked systems, aerospace and avionics systems, manufacturing systems, and embedded systems in general. The main objective of this proposal is to explore and experiment with the HW/SW co-design flow of the 2020s by focusing on five main issues: 1) Exploring heterogeneity of these multi-processor architectures and their abstraction; 2) Automated of the architectural exploration; 3) Importing legacy RTL and third-party IP blocks within a HW/SW co-design flow; 4) Improvement of the HW/SW co-verification methodology and 5) A complete HW/SW co-design and co-verification flow for the aerospace and multimedia industries.""564918,""Bois, Guy"
"567778"	"Bose, Prosenjit"	"Geometric Computing"	"My research area is Computational Geometry, a field devoted to the design and analysis of algorithms and data structures for geometric problems. My research focuses both on theoretical and practical issues. The geometric application areas I concentrate on include routing on geometric networks, geometric data structures, mesh generation/manipulation, graph drawing, visualization and pattern recognition. The main goal of my research has been and continues to be to bridge the gap between theoretical and practical efficiency by finding practical algorithmic solutions to applied geometric problems that have theoretical performance guarantees. Below, I provide a few examples of the type of geometric problems I am currently concentrating on. Communication between wireless devices has an inherent geometric component since these devices have a physical location which is constantly changing. Currently, I am applying my expertise in geometry to develop algorithms that take advantage of this geometric component resulting in more efficient and reliable communication between wireless devices. The problems in this area are particularly challenging since the network is dynamic, i.e. the wireless devices are usually moving. Moreover, this dynamic nature means that no single entity has complete knowledge of the whole network, requiring the development of algorithms that are online and use little memory. The standard method for modelling wireless networks is through the use of geometric graphs. A geometric graph is a graph where each vertex is a point in the plane (or possibly higher dimensions depending on the application) and an edge is a line segment or a curve joining two points. My research in this area focuses on the following themes: (1) How to route (i.e. find a path) efficiently in a geometric network, (2) How to construct geometric networks with certain desirable properties? Facility Location is an area where the main focus consists of placing one or more facilities in a region in a manner that optimizes certain criteria with respect to a set of clients. For example, in trying to decide where to build a new fire station in a city, one may want to place it somewhere that minimizes the response time to the target population. Facility Location problems are inherently geometric. Currently, I am studying several variants of this problem and trying to develop efficient solutions. A fundamental question in the manufacturing industry concerning every type of manufacturing process (such as NC-machining or casting) is: Given an object, can it be built using a particular process? I have successfully developed and applied computational geometry techniques to answer some basic problems of this type. Often it is geometric constraints that determine if an object can be built by a particular process. By modelling a process geometrically, we can determine if an object can be constructed at the design stage by determing if a CAD model of an object can be built by a particular manufacturing process. I plan to continue developing algorithms for answering these questions from a geometric perspective.""561324,""Boser, Quinn"
"567348"	"Bouchard, Bruno"	"Intelligence ambiante pour l'assistance automatisée à domicile des personnes atteintes de déficits cognitifs"	"Mes intérêts scientifiques portent sur un nouvel axe de recherche très prometteur appelé ""l’intelligence ambiante"" (IA). Ce créneau scientifique vise l'adaptation des techniques d'intelligence artificielle au nouveau concept d’environnement intelligent, dans lequel on miniaturise un ensemble de dispositifs électroniques (capteurs et effecteurs) pour ensuite les intégrer dans n'importe quel objet du quotidien, de manière transparente pour l'usager, dans le but de lui fournir une assistance ponctuelle et adaptée à son profil. Avec le vieillissement actuel de la population et la pénurie de personnel médical pour les soins à domicile, de nouvelles problématiques émergent de ce nouveau contexte, comme celle sur laquelle je travaille et qui concerne le soutien aux personnes souffrant de déficits cognitifs à l’intérieur de leur résidence. Le programme de recherche proposé dans cette demande vise à s’attaquer aux deux principales problématiques liées à ce type d'assistance, soit la reconnaissance et l’apprentissage des activités de la vie quotidienne réalisées par le résident à partir de capteurs de bas niveau. La théorie de la reconnaissance d’activités, sur laquelle je travaille, a pour but de formaliser la reconnaissance et la prédiction du comportement d’une personne ayant des déficits cognitifs, ainsi que les erreurs de réalisations, à l’intérieur d’un cadre algorithmique robuste et performant, s’appuyant sur plusieurs formalismes, tels que les logiques de description et la théorie des possibilités. Elle est basée sur un principe d’abduction pour la construction d’hypothèses sur les activités possibles, et d’appariement de ces observations avec une librairie de plans. Le défi est de lever les ambiguïtés/imprécisions sur les hypothèses concurrentes en prenant en compte la variabilité des lieux et des moments de réalisation de tâches identiques ou similaires et possiblement ""altérée ou erronée"" de la réalisation (interruption d’activités, erreurs, changement de méthodes, activités concurrentes ou entrecroisées, etc.). Au cours des dernières années, plusieurs avancées importantes sur ces thématiques ont été réalisées au Laboratoire d’Intelligence Ambiante pour la Reconnaissance d’Activités (LIARA), que j’ai cofondé en 2008 et que je dirige. En effet, nous avons développé un ensemble d'outils algébriques et algorithmiques servant à structurer le processus de reconnaissance et d’apprentissage des d'activités, capables de détecter les erreurs cognitives. Dans la littérature, peu de travaux se sont vraiment penchés sur cet aspect important de la problématique qui concerne la reconnaissance d’activités dites erronées. Le programme de recherche proposé ici vise à s’attaquer à plusieurs questions fondamentales toujours ouvertes, telles que : comment prendre en compte le problème de la déformation spatio-temporelle des activités qui se traduit par l’étirement, la compression et la variation dans le temps et dans l’espace d’une séquence représentative d’une certaine activité? Tous les spécialistes reconnaissent la formidable difficulté de réponde adéquatement à ce type de questions et le besoin de nouvelles solutions algorithmiques en IA pour y arriver et ce, afin d'être apte à fournir un soutien technologique adaptée à domicile. Ce projet à caractère multidisciplinaire propose non seulement des avancées importantes liées aux fondements de l'intelligence artificielle, mais propose aussi des retombées concrètes pour notre société vieillissante. Le type de technologie sur lequel nous travaillons a le potentiel de repousser le moment de l'institutionnalisation des individus atteints de déficits cognitifs, en permettant de réduire la charge de travail des proches aidants, tout en redonnant une autonomie partielle à ceux-ci.""554434,""Bouchard, Bruno"
"566722"	"Brewster, Richard"	"Computational complexity of combinatorial problems: graph homomorphisms, packings, and good characterizations"	"The speed and power of modern computers is truly impressive. Given the regular announcements of more and more powerful super-computers, one might believe any reasonable problem can be solved, by brute force, in reasonable time. Surprisingly this is not the case. There are certain problems that seem intractable by nature. Classic examples of such problems include scheduling, vehicle routing, and facility location problems. All of these problems may be viewed as assigning resources (time slots, routes, locations) to some consumer (exam writing students, vehicles, factories). The goal is to assign the resources in the most efficient way possible, subject to constraints restricting the assignments. In general, we call these Constraint Satisfaction Problems or CSPs. The challenge in computing the most efficient assignment is the sheer number of possibilities. For each resource there can be many ways to assign it to a consumer. Moreover, each choice one makes may affect what other choices can be made in the future. In this way we are forced to consider all the possible combinations of assignments of resources to consumers. For a fast computer to exhaustively search through all the possible combinations looking for the optimum assignment could take thousands of years. This is an example of ""combinatorial explosion""; the sheer number of combinations defeats brute force solutions. More efficient algorithms, if they exist, must be employed. The primary aim of my program is to understand the nature of these challenging combinatorial problems. It turns out that some of these problems contain (mathematical) structure. This structure can be exploited to produce efficient algorithms, i.e. we do not need to consider all possible resource assignments, but rather we can zoom-in on the optimal solution. On the other hand, some problems seem to lack sufficient structure to admit an efficient solution. At the highest level the goal of my program is to identify those CSPs which admit efficient algorithms (polynomial time solvable) and those which are intractable (NP-complete). Fundamentally, we would like to know if such a dichotomy of efficient versus intractable holds for all CSPs. The efficient algorithms are typically based on mathematical structure known as ""good characterizations"". A good characterization guides one through the combinatorial explosion to an optimal solution for the CSP or to a proof that the CSP has no solution, i.e. there are too many constraints, at which point the search can stop. In order to use this mathematical structure, we often need to develop new mathematics (in the form of good characterizations for CSPs). The development of this theory can be used to solve particular CSPs, but more importantly, it gives insight into the fundamental dichotomy question above. My research program focuses on combinatorial problems from graph homomorphisms and graph packings and coverings. In the case of graph homomorphisms I am particularly interested in edge-coloured variants. These are a special class of CSPs that are useful for modelling many combinatorial problems. There are many tractable problems in this area that make good student research projects. The engagement of students in research is a particular aim of my program.""564132,""Briand, Danik"
"566744"	"Bridges, Greg"	"Microwave Devices for Biosensors and Single Cell Analysis"	"Studying the physiology of normal and diseased biological cells and monitoring their response to different stimuli or drugs is essential in bioprocess and biomedical research. Many currently used instruments for cellular analysis utilize chemical markers, sophisticated equipment and time-consuming preparation techniques. Electronic based analysis approaches are a promising alternative. The objective of this research program is to develop microfluidics devices integrated with microwave dielectric sensors and high-frequency electric field stimulus to provide new tools for biological media and label-free single cell analysis. Tissues, cells and other biomedia have unique dielectric properties that, if measured, can be used to provide a wealth of information. For example, by monitoring the dielectric state of a biological cell, it can be determined whether it is in a particular growth phase, whether it is healthy or cancerous, or if drugs have entered the cell and initiated changes. Almost all methods for measuring the dielectric properties of a single cell have employed impedance spectroscopy or dielectrophoresis based approaches at relatively low frequencies, below 100MHz, where outer plasma membrane interfacial effects dominate the cell’s response. There is additional information that can be gained by extending this to microwave frequencies, a regime where fields can penetrate the cell interior and organelles and the polarizability of large molecules can be detected, revealing important information on intracellular properties. Microwave dielectric spectroscopic analysis of single biological cells is extremely difficult due to their small size and thus small signals that have to be measured. This research program will explore new dielectrophoresis based and microwave interferometric-based methods, employing distributed transmission lines integrated in a microfluidic chip, to measure the dielectric properties of single cells over a wideband of frequencies. Besides sensing, electronic means can be used to stimulate biological cells. Electroporation is the transient permeabilization of a biological cell membrane, through the formation of conductive pores. Depending on the intensity and duration of the applied field, electroporation can be used to reversibly or irreversibly permeabilize the membrane, enabling transfection, enhancing drug uptake in cancer therapy, inducing programmed cell death, or in the extreme case, lysis. An objective of the research is to develop electronic techniques, implemented in a microfluidic biochip, for applying high-intensity pulsed electric fields to electroporate single cells while simultaneously measuring their dielectric response. This will enable studying the electroporation process and short timescale changes at the cellular level. Electroporation is usually performed using millisecond-to-microsecond pulsed electric fields, where only the outer membrane of the cell is permeabilized. We will develop methods for applying short duration pulses, in the nanosecond regime, where the electric field can penetrate inside the cell. Access to the interior of the cell has the potential for selective electroporation of its organelles, providing a means to enhance the effectiveness of drugs in chemotherapy for example. The outcomes of the research will be all-electronic microfluidic devices integrated with microwave dielectric spectroscopy and high-frequency electric field stimulus capabilities, providing new non-invasive label-free single cell analysis instruments. The ability to perform measurements using nanoliter volumes in an all-electronic manner make this very attractive for integration into a complete lab-on-chip platform.""551057,""Bridgewater, Darren"
"566518"	"Broadbent, Anne"	"Quantum Cryptography"	"Quantum information processing is concerned with what we can and cannot do with quantum information, which is the physical information that is held in the state of a quantum system, as predicted by quantum mechanics. Thanks to amazing phenomena such as entanglement, superposition, interference and no-cloning, quantum information revolutionizes the way we communicate and compute — enabling, for instance, ultra-fast computations and secure communications. Already, the technology for small-scale quantum computing exists; with continued effort, it is just a matter of time until full-scale quantum computing becomes a reality. The unprecedented computing power brought forth by quantum computers is known to completely break the cryptographic codes used to secure e-commerce. Fortunately, thanks to quantum key distribution (one of the earliest discoveries in quantum information processing), we know that quantum information enables a new level of security for communications, thwarting any attack by ultra-powerful devices. As this example illustrates, we are sitting at a crossroads in terms of security: is the digital society ready for the advent of quantum computers? My research aims at answering this question in the positive, ensuring that our information infrastructure remains both functional and secure in the presence of quantum information technologies. On one hand, I deal with the threats that stem from ultra-fast quantum computations, while on the other, I exploit the properties of quantum information in order to provide secure solutions. I shall explore two major questions; the first is the study of efficient methods of delegating private quantum computations, while the second asks if it is possible for a quantum program to hide its own inner workings.""561542,""Broadfoot, Sarah"
"567679"	"Brown, Jeremy"	"Development of a Miniature Real-Time High-Resolution Endoscopic 3D Ultrasound System"	"The proposed research program is focused on developing high-frequency ultrasound technologies for use in high-resolution imaging applications. High-frequency ultrasound is a relatively new technology that uses micro-fabricated imaging probes that are capable of an order of magnitude higher resolution than conventional ultrasound systems currently used in diagnostic imaging. Specifically, the primary objective is to develop miniaturized (<3mm) imaging endoscopes and all of the associated electronic hardware and software that will enable this technology to be used as a tool for visualizing tissues from within the body, as opposed to the current equivalent technology that is limited to external topical applications. Potential applications of such a high resolution endoscope include laparoscopic, auditory, intracardiac, neural imaging, and many more. In conventional pulse-echo ultrasound, the resolution of the imaging system is proportional to the frequency of the ultrasound pulse generated by the transducer, however, higher frequency (higher resolution) comes at the price of lower penetration depth. Conventional low frequency large penetration ultrasound imaging systems are based on piezoelectric transducer arrays. By using a transducer array, which is made of many individually addressed piezoelectric elements, the effective curvature of the aperture can be changed electronically, resulting in a much better image quality. Use of an array also makes it possible to either multiplex a subset of elements across a much larger set of elements (linear array), or to steer the ultrasound beam to different angles from a fixed set of elements (phased array). Because phased arrays have the ability to steer the focused ultrasound beam, they can generate images with a large field-of-view from a much smaller form factor, and this makes them desirable for endoscopic applications as long as the diameter of the probe is not dominated by packaging of the electrical interconnects. This on-going research program has recently solved some of the challenges associated with developing miniature high frequency phased array endoscopes. These challenges include the micro-fabrication of the array elements and interconnecting these to a cabling system in a miniaturized form factor. Although the previously developed endoscope currently represents one of the most advanced ultrasound imaging arrays in the world, there are still a large number of research questions and challenges that need to be addressed before it can be realized as a useful diagnostic technology. The proposed research program will build upon these previous developments by designing, fabricating, and implementing first of a kind electronic hardware and software to allow the imaging endoscope to generate real-time high resolution images. The research program will also develop novel 2D micro-fabricated imaging arrays that will not only produce images of superior quality, but also real-time 3D volumes from an endoscope of identical miniaturized form factor. Currently no other imaging technology exists that combines such high resolution, with the miniature forward looking endoscopic form factor. The potential for this technology to revolutionize diagnostics in several different fields of medicine is very real. Not only could this be of great benefit to the Canadian health care system (and international) in terms of improving diagnostics and surgical guidance, but the technology also represents valuable IP that has many commercialization opportunities on both a local and national scale. This research program is also a very valuable training program for graduate students through a unique combination of high tech micro-fabrication training with conventional biomedical engineering methods.""559750,""Brown, Jessie"
"566904"	"Bulitko, Vadim"	"Interactive Storytelling and Real-time Heuristic Search"	"1. Given an opportunity to interact with their audience, human storytellers are adaptive. Bedtime stories and individualized tutoring are examples of adapting story content to the audience in order to maximize its effectiveness. Such boutique delivery is not scalable for traditional mass markets and, as a result, commercial movies and large-scale lectures follow the one-size-fits-all principle. A computer-assisted medium such as video games can combine the best of both worlds, delivering entertainment customized to each member of the audience on a mass scale. Furthermore, the audience can become an active participant in the story, interacting with the narrative through its own actions. Recent research has attacked the problem of interactive storytelling from two angles. First, narratives have been represented in a formal language so that automated planners can accommodate the audience's actions while respecting the author's narrative goals. Second, automatically acquired models of the audience's preferences are used to tailor the accommodations to them. My research group was one of the first to use automated audience modeling in interactive digital storytelling. Under the proposed research program we will first extend our existing systems with an explicit model of the audience's emotional response. This will allow our automated storyteller to customize its narrative expressly to elicit a series of specific emotional responses from the audience (e.g., hope followed by joy). Second, we will use the ability of our experience managers to automatically generate a multitude of story plots for narrative space exploration. We will start by building an automated assistant to story designers to estimate audience's response to the interactive story space beforehand. We will then add a mechanism to automatically modify the story space to ensure desired audience response. The benefits of this research include more immersive personalized entertainment and training systems, more believable virtual characters and lower development costs. 2. Artificial Intelligence agents frequently need to make decisions with incomplete information in limited time. The applications range from route planning to personal digital assistants to non-playable characters in video games. Real-time heuristic search is an approach to real-time decision-making under uncertainty and is particularly amenable to parallelism due to its strong data locality. To date, my research group has produced a progression of state-of-the-art real-time heuristic search algorithms. We achieved such performance by introducing several key paradigms to the field of real-time heuristic search. As computing hardware has become ubiquitously parallel, my research group will extend our database-driven real-time heuristic search to parallel hardware. We will analyze the resulting algorithms empirically as well as continue our work on the theory of real-time heuristic search. The benefits of this research include more intelligent autonomous agents and, in particular, more believable non-playable characters in interactive entertainment.""544616,""Bull, Shelley"
"555264"	"Bylinskii, Zoya"	"7"	"British Columbia"
"566478"	"Caines, Peter"	"Mean Field, Distributed and Hybrid Control Systems"	"The overall objective of the proposed research program is to advance the fundamental analysis and design of control systems for (i) distributed large scale systems (i.e. systems without centralized control or information which are of high state cardinality) and (ii) systems with both continuous and discrete states and dynamics. Such systems are ubiquitous in the contemporary networked information and computation based environment and so their enhanced control and optimization would bring great benefits.  Mean Field Game (MFG) theory constitutes a fundamental new methodology for the analysis and design of distributed large scale systems which originated in NSERC supported work by the proposer together with his student Minyi Huang and the co-supervisor Roland Malhame' during 2003 – 2007, and, independently, by J.-M. Lasry and P.- L. Lions during 2006 – 2007. Since then the field has undergone vigorous development due to the continuing contributions of these and other researchers world wide; moreover, potential applications of MFG methodology and new, challenging problems are continually being discovered in a vast range of domains.   MFG theory establishes the existence of approximate Nash equilibria generated solely by local feedback for stochastic dynamical system agents in games involving large numbers of agents. The equilibria and the feedback laws applied in the finite population case are obtained via the far simpler solutions to the fundamental infinite population MFG Hamilton-Jacobi-Bellman and (McKean-Vlasov)-Fokker-Planck-Kolmogorov PDEs which are linked by the state distribution of a generic agent, otherwise known as the system's mean field. As the number of agents goes to infinity the approximation error goes to zero.  This proposal maps a five year research program on the following themes: (a) Estimation in MF Systems: a central problem for MF system theory is that partially observed systems with major agents (i.e. non-asymptotically vanishing with population size) have stochastic mean fields which must be recursively estimated along with the major agents' partially observed states. A state estimation theory for non-linear partially observed MFG systems with major and minor agents, and hence stochastic mean fields, will be of great significance and will be exploited in the development of (b) Spatially Dynamic MF Systems, (c) Coalition Formation in MF Systems, (d) Locally Infinite and Large Scale Topology MF Systems, and (e) MF Market and Auction Models. In large areas of systems and control engineering hybrid control systems are synonymous with advanced control methodology. Recent advances have generated both a sophisticated Hybrid Minimum Principle (HMP) theory and computational methods, and a parallel theory of Hybrid Dynamic Programming (HDP). Building on the proposer's work, it is planned to generate (i) a unified HMP - HDP theory, (ii) a theory and associated algorithms for the fundamental class of HDP problems where the systems are subject to state switching constraints, and (iii) an integrated HDP - Model Predictive Control methodology. Success in the overall program will yield 8 PhDs and 4 MEng degrees, plus 4 PDFs, and will contribute to the emergence of new techniques enabling networks of distributed adaptive agents to control large scale systems evolving in random environments; prime examples (already initiated in Canada) include the integration of smart meters, electric vehicle charging systems and localized generation, where the vehicles and generators are themselves optimized using hybrid system control algorithms.""555871,""Cairns, Nicholas"
"566702"	"Chan, Adrian"	"Biomedical signal quality analysis"	"We are currently experiencing an explosive growth in data. This growth includes biomedical data (e.g., electrocardiogram (ECG), electromyogram (EMG), pulse oximetry, blood pressure) which provide valuable information regarding the status and function of the body and are useful in a variety of applications (e.g., health/wellness, biometrics, gaming, and sports/fitness). There exists a large and continually growing body of knowledge regarding the acquisition of biomedical signals, as well as signal processing methods to extract useful information. Biomedical signals, however, can be contaminated due to noise, artifacts, and measurement setup errors; this is particularly true in unsupervised setups (e.g., telehealth), where highly trained operators are not present. Contaminants in the recordings can lead to misinterpretations, inaccuracies, and errors, including misdiagnoses. Despite advances in biomedical instrumentation, contaminants are frequently present in recordings. Currently, biomedical signal quality analysis relies on human experts. This is time-consuming, costly, and prone to human error. In addition, the increases in pervasive, continuous and/or multi-channel monitoring are making manual or semi-automated biomedical signal quality analysis methods impractical due to the amount of data. The objective of this research is to develop novel automated biomedical signal quality analysis methods to detect, identify, quantify, and mitigate contaminants. The proposed research is organized into three main research themes: 1) Multi-scale analysis, 2) Multi-variate analysis, and 3) Pattern recognition. Multi-scale approaches are well-suited to signals that arise from complex interconnected systems, such as biological systems. Recent research indicates strong potential in this approach, compared to conventional approaches that are either time or frequency based. Multi-variate approaches take advantage of redundant and complementary information within multi-channel recordings (i.e., multiple leads for the same signal type) and/or multi-modal recordings (i.e., recordings of different signal types). Pattern recognition methods can be employed to discover and leverage trends within the data; this can be used to detect and identify contaminants in biomedical signals, as well as classify the quality of data (e.g., excellent, good, poor, unacceptable). Methods will be evaluated in terms of performance (e.g., correctly detecting and identifying contaminations) and generalizability (e.g., methods work for various contaminants and combinations of contaminants). The exponential growth in biomedical data is associated with various challenges (e.g., acquisition, transferring, storage, and visualization). This research tackles a key, under-researched, area of quality analysis. There is utility within the large datasets being developed, but the capacity to discern which data has adequate quality, and avoid overly contaminated data, is essential. Outcomes of the proposed research will provide engineering contributions in signal processing and data quality analysis, and in the long-term be applied in the context of biomedical signal instrumentation and measurement. For example, automatic biomedical signal quality analysis methods will enable acquisition setups to be validated, alerting operators of issues and directing them on how to resolve these issues. It will also improve the performance of signal processing methods that extract information from these signals (e.g., increase accuracy of clinical decision support systems, reduction of false alarms). While this research is focused on biomedical data, the concepts and frameworks developed in this research are applicable to other data types.""562430,""Chan, Alvin"
"567035"	"Chen, Xiang"	"Robust and Optimal Control of Physical Systems with Networked Communication Structures"	"There is no doubt that we are in a networked world right now. Many physical, virtual, or hybrid (mixed physical and virtual) systems operate, communicate, or are controlled through different kinds of network structures, wireless or wired. Examples can be given as: internet systems, electric power grids, automotive systems, various sensor networks, etc. The common nature of these networked systems is that they are exhibited as both dynamical systems and networks, i.e., carrying the features of both dynamical systems and networks which brings in both advantages (such as cost reduction for wiring and maintenance, enhanced flexible functions at both system and network levels, etc.) and challenges (such as increased complexities in the interaction among systems or system components, time-sensitive performance compromise due to networked communication, etc. ). For example, on the advantage side, people have seen tremendous reduction of the wiring costs in every automobile nowadays by adopting the Controller Area Network (CAN) bus for critical applications such as engine control and the Local Interconnected Network (LIN) bus for noncritical applications such as power window/seat control; on the challenge aspect, on the other hand, it has already been realized that the networked communication of data among distributed sensors, controllers, and underlying plant systems tends to downgrade the transient dynamical performance of the controlled systems which, if not carefully addressed, could result in the instability of the whole systems. It has been identified that the challenge posed by the communication network is of great interest to many practical time-sensitive industrial systems such as electric power grids, automotive engine control systems, and sensor network, etc. Although this challenge has been attacked in great number of published literature most recently, many important questions remain open, for example, so far one still does not know how to design a controller that could simultaneously address quantizing and noise effects in the network communication channels and regulate the various desired dynamical behaviors of the controlled target systems. Also, some important questions, such as what the networked communication means to the performance of multi-agent systems, how the networked communication would affect the performance of estimation in distributed control systems, and how the networked communication affect the performance of control systems with both time and event-triggered mechanism, etc., still remain untouched, yet these questions are on top of the ‘most-wanted’ list in various industries. In this Discovery Grant proposal, the focus will be on developing systematic design tools to address the time-sensitive performance in physical control systems with networked communication structure. In particular, it is argued that the information theory and control theory should be integrated to generate conceptually new methods to address the said problems and to design robust and optimal control and management mechanisms for the networked dynamical systems. It is also argued that the methods developed would be industrial needs oriented and hence would find their way to be friendly applied in practice.""547739,""Chen, Xihua"
"566655"	"Cheriet, Mohamed"	"Data-Driven Approach to Understanding Ancient Manuscripts"	"Ancient manuscripts constitute a primary carrier of cultural heritage globally, and they are currently being intensively digitized all over the world to ensure their preservation, and, ultimately, wide accessibility to their content. Critical to this research process are the legibility of the documents in image form and access to live texts. Several state-of-the-art methods and approaches have been proposed and developed to address the challenges associated with processing these manuscripts. However, there is a huge amount of data involved, and also the high cost and scarcity of human expert feedback and reference data call for the development of fundamental approaches that encompass all these aspects in an objective and tractable manner. In this research, we propose one such approach, which is a novel framework for the computational pattern analysis of ancient manuscripts that is data-driven, multilevel, self-sustaining and learning-based, and takes advantage of the large quantities of unprocessed data available. Unlike many approaches, which fast-forward to the analysis of feature vectors, our proposed framework represents a new perspective on the task, which starts from ground zero of the problem, which is the definition of objects. In addition, it leverages the data-driven mining of relations among objects to discover hidden but persistent links between them. The problem is addressed at three main levels. At the lowest level, that of images, we tackle the automatic, data-driven enhancement and restoration of document images using spatial, spectral, sparse and graph-based representations of visual objects with a focus on spatial graphs of patch-based representations empowered with spatial proximity and local similarities. In terms of degradation modeling, our approach is to perform a cluster analysis on the data point manifolds. Transfer learning approaches will be also used to discover, model and correct unseen degradation holistically. At the second level, which is transliteration, we use directed graphical models, HMMs, Undirected Random Fields and spatial relations models of patch-based representations in an active learning framework to recognize the live text in manuscript images, in an effort to drastically reduce dependency on human experts and on reference data that are rarely available. In addition, cross-lingual approaches to adaptation in spoken language translations, such as transform mapping at state level, are also considered, in order to allow adaptation across writing styles and even across written languages. Finally, at the highest level, that of network analysis of the relations among objects (from patches and words to manuscripts and writers), we search for ‘social networks’ linking manuscripts. Considering this data-driven approach under the heading of Visual Language Processing (VLP), we hope that it will pave the way for new research in Canada’s upcoming data stewardship plan. This research program will lead to novel paradigms for processing and understanding ancient manuscripts using coherent, data-driven frameworks with tractable solutions. The multi-level structure of the proposed approach enables researchers to collaboratively mine, model and interpret digitized manuscripts, all of which can be achieved thanks to data-driven approaches, which have been largely absent from the field up to now. Empowered by the concept of life-long learning, our research makes the methods and models developed transferable across collections. It will advance the paradigms of image processing, pattern recognition, machine learning and network science, with the potential to impact huge collections of manuscripts and leverage different representation spaces and metrics, and their associated relationships and links.""545458,""Chériti, Ahmed"
"566462"	"Cheriyan, Joseph"	"Approximation algorithms for NP-hard problems"	"Network design, network flows, and graph connectivity occur as core topics in Theoretical Computer Science, Operations Research, and Combinatorial Optimization. Many important algorithmic paradigms were developed in the context of these topics, such as the greedy algorithm for minimum spanning trees and the max-flow min-cut theorem for network flows. Moreover, these topics arise in many practical contexts such as the design of fault-tolerant communication networks and congestion control for urban road traffic. Many of the problems arising in practical contexts are NP-hard. This means that optimal solutions cannot be computed in a reasonable running time, modulo the P .not.= NP conjecture. Hence, research has focused on approximation algorithms, i.e., efficient algorithms that find solutions that are within a guaranteed factor of the optimal solution. My current and planned research focuses on the following three broad interlocking themes. I discuss two of these topics below, and my proposal discusses all the topics in full. 1. Design of approximately minimum-cost networks, including the Traveling Salesman Problem (TSP) and its variants. 2. Design of networks subject to node-connectivity requirements. 3. Lift-and-Project methods for the Asymmetric TSP and related problems. The most famous problem in all of discrete optimization is the TSP. The best known algorithmic result is the 3/2-approximation algorithm due to Christofides from 1976. It has long been conjectured that there exists a 4/3-approximation algorithm for the TSP, and that there exists a 3/2-approximation algorithm for a variant called the s-t path TSP. Two of the outstanding open questions on this topic that I am researching are the following: (*) Improve on the approximation guarantee of 7/5 for an important special case called the GRAPHIC TSP, possibly based on a combination of LP-rounding techniques and ear-decomposition techniques. (*) Improve on the approximation guarantee of 8/5 for the s-t path TSP, possibly based on LP-rounding techniques, coupled with improved structural results on the support graph of LP solutions. The second broad theme of my research addresses the design of networks subject to node-connectivity requirements. One of the basic problems in network design is to find a minimum-cost sub-network H of a given network G such that H satisfies some pre-specified connectivity requirements. The area of minimum-cost network design subject to EDGE-connectivity requirements flourished in the 1990s, and there were a number of landmark results. Progress has been much slower on similar problems with NODE-connectivity requirements, despite more than a decade of active research. Very recently, in a paper co-authored with L.Vegh (Proc. IEEE FOCS 2013), I have obtained a major advance on a fundamental problem in this area: we have a 6-approximation algorithm for the minimum-cost k-node connected spanning subgraph problem, assuming that the number of nodes is at least k^4. Our results and techniques have opened up many new directions in the design of networks subject to node-connectivity requirements. I plan to continue research on these topics, together with graduate students and postdocs. In summary, the high-level goal of my research agenda is to provide significant advances in the areas of Network Design and related areas of Combinatorial Optimization. This has the potential to improve the results and techniques available to all researchers who work in this core area of the computational sciences. Problems such as the TSP are ubiquitous in all modern societies, including Canada; the economy and infrastructure are based on logistics, transport, networks, and on the optimal allocation of scarce resources to critical tasks.""545011,""Cherkaoui, Omar"
"567268"	"Chvatal, Vasek"	"Betweenness, brain models, random number generators"	"I propose to work in two mutually unrelated areas: On the one hand, I propose investigating a conjectured generalization of a classical theorem concerning points and lines in the plane and on the other hand, I propose investigating the extent to which classical models of the brain can exhibit an erratic, disorderly, apparently unpredictable behaviour characteristic of an epileptic brain before onset of a seizure. Any number of points in the plane determine a number of lines, each of which passes through at least two of these points. A theorem in euclidean geometry, dating back to the 1930s, asserts that the number of such lines is at least the number of the points unless all of the points lie on a single line (in which case this is the only line they determine). A conjecture announced in 2008 by my former student Xiaomin Chen and myself aspires to generalize this theorem to the setting of so-called metric spaces, introduced in 1906 by Maurice Frechet. A metric space is a set where a nonnegative real number is associated with every unordered pair of points; this number is referred to as the distance between the two points; it equals zero if and only if the two points are identical; it satisfies the 'triangle inequality', meaning that the distance from A to B plus the distance from B to C is at least the distance from A to C. In 1924, Karl Menger proposed to say that a point B in a metric space lies between points A and C to mean that the distance from A to B plus the distance from B to C equals the distance from A to C. Xiaomin and I defined the line XY in a metric space (where X,Y are two distinct points) as the set of all points Z such that one of X,Y,Z lies between the other two (in particular, the line XY includes both points X and Y) and we conjectured that in every metric space with n points there are at least n lines unless some line consists of all n points (in which case there may be additional lines as well). A proof of this conjecture would reveal an iceberg, of which the original euclidean geometry theorem is just a tip. The motto 'be wise, generalize!' is the stimulus for much progress in mathematics and generalizations may have unexpected applications: one of the well-known examples is the use of non-euclidean geometry in special relativity. Even if the conjecture should turn out to be false in its full generality, it is already known to be true in four special cases. Demarcating the metric spaces where it holds true would be most interesting. Epilepsy is a group of neurologic conditions, the common and fundamental characteristic of which is recurrent, unprovoked epileptic seizures. This affliction is widespread: there are over 50 million epilepsy sufferers in the world today. In attempts to study epilepsy in selected patients, firing patterns of neurons that are located predominantly in their cerebral cortex are recorded as time series called electroencephalograms (EEG). Even though different types of seizures have different EEG manifestations, one frequent occurrence is a transition from an irregular, disorderly EEG before the seizure (the pre-ictal state) to more organized sustained rhythm of spikes or sharp waves during the seizure (the ictal state). In an effort to better understand the development of seizures, I intend to engineer existing models of the brain, so that they display seizure-like behaviour; my first short-term objective is to simulate the pre-ictal flutter of apparently unpredictable firing patterns. I propose to begin with the simplest, and chronologically first, model of the brain, the McCulloch-Pitts networks, before moving on to the biologically more plausible networks of spiking neurons.""546603,""Cianflone, Katherine"
"566349"	"Clark, Jeremy"	"Secure online services for private user data"	"When users are required to, or opt to, disclose private/sensitive data to an organization or government entity for processing, a balance must be struck between security and convenience. Online submission is often the most convenient but it also carries substantial risks. This research program proposes the development and deployment of innovative cryptographic protocols for providing secure online election systems and secure online genetic testing. Any security-critical online service must at least provide users with a process for determining they are connected to the intended party (server authentication) and then provide a confidential communication channel. These properties are intended when a user visits a website beginning with https:// (and the browser displays a lock), however, the trust assumptions and infrastructure this protocol (called SSL/TLS) relies on have been recently questioned. The research program will proceed in three thrusts: one toward improving the SSL/TLS infrastructure, one toward advancing beyond basic SSL/TLS protection for the specific application of online voting, and one that does the same for online genetic tests. When users visit a website over https://, the site provides a certificate of their identity, endorsed by a business or organization called a Certificate Authority (CA). Any CA can issue a certificate for any site. The number of CAs has proliferated, the baseline criteria for validation has declined, and high profile breeches of CAs have been publicly disclosed. This research program will design, implement, and test alternative mechanisms for certification. Online voting has been used municipally across Canada, is to be piloted federally by Elections Canada, and is being deliberated on by some provinces. Unlike online banking, where an incorrect, mistaken, or fraudulent transaction can be seen by the user, ballot secrecy mandates that individual votes cannot be displayed (which would enable vote selling). This research program will design, implement and test novel end-to-end verifiable (E2E) voting systems that provide a provably correct tally while maintaining the secrecy of each voter's ballot, even if the voter is complicit in demonstrating how they voted or uses a malware-infected personal computer to cast their ballot. The decreasing cost of whole genome sequencing has the potential to revolutionize healthcare, allowing genetic tests and personalized medicine. However, your genome is private information. Once it is disclosed or leaked, the privacy can never be recovered. This research program will design, implement and test novel cryptographic protocols for allowing these tests to be performed on an encrypted genome, while offering proof that the test was performed correctly (even if some aspects of the test remain confidential). The results of this research program should be informative and useful to government election agencies, privacy commissioners, standards institutes, internet working groups, other cryptography, security and privacy researchers, and to the elections and healthcare industry. It will further equip a set of security professionals with the skills required to work or consult in any of these areas, and to apply their knowledge and experience to novel domains.""568015,""Clark, Kenneth"
"566380"	"Cockett, James"	"Categorical programming"	"My group at the University of Calgary explores programming paradigms and their formalisms. We are currently exploring formalisms for Quantum programming, Concurrent programming, and Low Complexity programming. In each area the aim is not only to provide a programming language or an account of computation but also a complete categorical semantics with its attendant proof theory and morphology. This entails contributions ranging from theoretical foundations through to programming language design and implementation. Languages which implement formal settings such as these deliver, for their programs, guarantees of desirable properties (e.g. termination, feasibility, deadlock freedom etc.). However, these properties come at a cost: the languages enforce powerful yet uniform disciplines for thinking about programs which are often quite novel and far from current programming practice. Thus, the ability to express even common computational problems efficiently in these settings – their expressivity – is not immediately clear. My group uses prototype implementations to experiment with the programming languages for these settings, in order to understand the programming disciplines they enforce, and to explore their expressivity. As one aspect of our work, in the next granting period, we propose to investigate the utility of differentials both in semantics and in programming languages themselves. Differentials appear both at a ""macroscopic"" level, as the differential of datatypes, and in a less well-understood way at a ""microscopic"" level, through the ability to differentiate programs themselves. The differential of a program gives important information about how it uses its resources. To date this ability has found utility mainly in semantic arguments about programs. We would like not only to further understand the semantic implications, but also to understand whether this ability to differentiate programs might not also have utility at the level of programming itself.""551047,""Cockle, Kristina"
"566387"	"Comtois, Philippe"	"Experimental tools and mathematical models to study electrical-mechanical properties of spatial-temporal patterns in cultured cardiac cells"	"The proposed study is aimed at understanding the link between the biomechanical and electrical properties of cultured cardiac cells and their role on autonomous electrical activity. The high level of complexity of cardiomyocyte (electrically-active cardiac cells) dynamics needs integration of numerous approaches and techniques to uncover the multiscale changes occurring in culture and the functional impact on electrical activity at both the cell and multicellular levels. Here, we proposed a project involving bioinstrumentation development, image acquisition techniques as well as analysis, and modelling works summarized below to better understand the effects of cell deformation on bioelectric characteristics and spatio-temporal electrical self-organization. 1. Develop a combined approach for culture in our bioreactor and data acquisition with electrical and mechanical stimulations for non-terminal experiments and sub-cellular study of electrophysiological properties We have developed a bioreactor for culture of CMs that provided programmed electrical and mechanical stimuli to cells. In parallel, an acquisition system to record fluorescence changes (for example for intracellular calcium transient with fluo-4) to visualize the effects of electrical stimulation and mechanical deformation of the cells in post-culture has been developed. Three important limitations of the proposed combined systems exist at this stage: displacement of the field of view (FOV) under study when stretched, jitter due to linear stepper motors control, and impossibility to acquire fluorescence data while stretching. We propose to correct these limitations by integrating a feedback control motion to stabilize the FOV and to modify the bioreactor stretching electronic circuit to microstepping. 2. Study of the spatial-temporal autonomous electrical activity of isotropic and patterned cardiomyocyte monolayers following culture on elastic substrates: Experiments by our group present varying time-dependent behaviors when recording at a single site. The temporal activity can be transiently or permanently affected following acute electrical or mechanical stimulation. However, it is clear that the changes seen locally are limited to explain what could be variation in the spatial-temporal dynamics. We thus propose to study the stability of spatio-temporal activity of topographically patterned cardiomyocytes. More precisely we will study self-organized activity on electrically-coupled CMs and its stability is perturbed by acute stretch to understand the role of cell deformation. 3. Evaluate, with a mathematical model, the role of heterogeneous dispersion of intrinsic frequencies of autonomous electrical activity on global activity of patterned and unpatterned monolayers Evaluate, with a mathematical model, the role of heterogeneous dispersion of intrinsic frequencies of autonomous electrical activity on electrical self-organization. CMs isolated from neonatal hearts can be either autonomous or non-autonomous cells. In cell culture, initial seeding is random such that how these two populations are distributed within the monolayer is unknown. We propose to look at the effects of having mixture of these two populations on spatio-temporal activity based on a novel approach of mathematical modeling and study how cell topography can influence the autonomous activity. The high level of complexity of CM dynamics needs integration of these innovative approaches and techniques to uncover the multiscale changes occurring in culture and the functional impact on electrical activity at both the cell and multicellular levels.""555005,""ComtoisUrbain, Simon"
"555658"	"Cormier, Michael"		"NSERC"
"566686"	"Crépeau, Claude"	"Quantum-safe Cryptographic protocols"	"While physical implementation of a quantum computer still seems far away, the perspective of such a realization is so profound to the world of cryptography that it is worth considering already what this science will become after the construction of a quantum computer. The study of Quantum-safe (aka Post-Quantum) cryptographic tools and protocols follow from the belief that we must start now to prepare for this future. Quantum-Safe Oblivious Transfer and Fully Homomorphic Encryption ******************************************************* In the classical world there used to be very general techniques to achieve Oblivious Transfer from various cryptographic assumption. In the Quantum-safe cryptographic world we have no such things: several techniques have been proposed to obtain Quantum-safe Oblivious Transfer but no general technique has emerged yet. The search for such a general approach will be one objective of our research. We will also consider the relation between Oblivious Transfer and the related task of Fully Homomorphic Encryption as implemented under the Learning with error assumption or the Approximate Integer GCD assumption. We would like to show that in the quantum world OT is inherently easier to achieve than FHE. Interactive Hashing and Statistical Zero-Knowledge Arguments ************************************************** Another specific target is to study a powerful tool to construct secure protocols known as ""Interactive Hashing"". This tool was used in the classical setting both in the information theoretical and computational framework. Currently, none of the proofs of security of protocols involving interactive hashing is valid in the quantum setting. We will develop new techniques to establish security proofs in the quantum world. We hope to achieve the same result as in the classical world: It is possible to build statistical zero-knowledge arguments for NP under the sole assumption that a quantum one-way function exists. Non-Interactive Quantum-Safe Zero-Knowledge Proofs ******************************************** Another classical cryptographic situation that is no longer available in the Quantum-safe world, is the primitive of Non-Interactive Zero-Knowledge protocols under computational assumptions. Such protocols have been proposed for prefect or statistical Non-Interactive Zero-Knowledge proofs under no computational assumption (although using quantum communication to achieve that). However, it does not seem to apply to situations where only Computational Zero-Knowledge is achievable (for NP-complete languages for example) or where the soundness of the proof system is only valid because a certain computational assumption cannot be broken (Perfect or Statistical Zero-Knowledge Arguments). This research program will contain theoretical work only but will produce significant training of HQPs.""555283,""Crepin, Sebastien"
"566219"	"Dansereau, Richard"	"Signal Separation using Multichannel/Multimodal Side Information"	"The aim of signal separation is to recover source signals from one or more signal mixtures. These mixtures can come from a multitude of domains, including speech mixtures for the so-called cocktail party problem, musical recordings of multiple instruments and/or vocals, electroencephalograph (EEG) signals from the electrical activity of the brain, maternal/fetal electrocardiograph (ECG) mixtures when measuring surface potentials for fetal monitoring, electromyography (EMG) signals from mixtures of motor unit action potentials in muscles, various astronomical datasets with mixtures of weak astronomical sources versus background, and radar or sonar systems. Quite often, source separation is an inductive inference problem where the problem is highly underdetermined such that limited information exists for hoping to find a unique solution. Source separation problems may come with varying levels of information, including single channel mixtures, multichannel mixtures, or mixtures that are accompanied with side metadata that may help with the separation. For example, a music performance could be recorded by a single microphone (single channel), by multiple microphones distributed around the room (multichannel with spatial diversity), or access may be had to the musical score and instrument types (side metadata information). The focus for this proposal is to research develop advanced source separation techniques that multichannel and multimodal metadata as side information to improve the quality in source separation. The signal processing research will concentrate on incorporating multichannel and side information into the source separation problem and in evaluating the impact of such side metadata information. Of particular interest is when there are lack of time alignment information wit the metadata, small variations in time-frequency or sparsity information between the mixtures and the metadata, effects of short-time non-stationarities in the source signals being separated, and use of sparsity constraints in multiple domains. These signal separation characteristics will be research and studied under various applications, including speech, vocals/instrument separation, and EMG signal decomposition, but, the key research focus will be on the core components of source separation in multichannel mixtures with side metadata information. Some of the anticipated outcomes are as follows. First, in the vocal/instrument separation space, we expect to be able to take a musical recording, likely as a stereo multichannel recording, along with side metadata, which could include the musical score, sound samples of the instruments in the performance, an example performance of the piece by a secondary musician, a list of lyrics, a MIDI file, or other types of metadata, and be able to separate the individual instruments in the recording to either isolate or suppress individual instruments. In the EMG signal decomposition space, we expect to be able to take multichannel EMG recordings and determine firing times of individual motor units in the muscle and the corresponding shape of the motor unit action potentials. In the fetal ECG separation space, we expect to be able take a multichannel mixtures containing the stronger maternal ECG and weaker fetal ECG, and be able to suppress the stronger maternal ECG signal in an improved fashion. The proposed core research in signal separation will have wider applicability, where these examples would be used to validate the techniques and determine other limitations that may present themselves in one domain but not another.""545808,""Danziger, Peter"
"566017"	"Daudjee, Khuzaima"	"Scalable and Consistent Management of Large Scale Data"	"Effective large scale data management is becoming important for scientific discovery and for maintaining a competitive edge in business and information technology. Large scale data management, sometimes called “Big Data” management, is often used to refer to the volume of data and the rate at which it is generated. Scientific research organizations like CERN and companies such as Twitter, Walmart and Facebook all generate and manage very large amounts of data. For example, starting 2016, the Large Synoptic Survey Telescope will collect 30 terabytes of data every night for 10 years. Over 100 terabytes of data are uploaded daily to Facebook, which has more than 1 billion active users around the globe. To manage these large scale data, massive distribution over multiple, large, storage and processing server clouds both within and across geographically distributed data centers is required. Furthermore, effective management of these server clouds is needed so that operational costs are reduced. To this end, key research problems need to be addressed for large scale data management. First, dynamic database distribution techniques are required both within and across server clouds to deal with latencies contributing to response times. These latencies can be due to, for example, overall changing patterns in workload requests or the emergence of database hot spots as a result of load spikes. Second, with the growth of data centers, there is a need to achieve savings in terms of server maintenance cost of storing and managing large scale data. Thus, techniques to consolidate database server loads become key to reduce costs and keep large scale data management viable. The goal of this proposal is to design and develop a set of protocols, algorithms and systems that will deliver viable solutions to these research problems using dynamic, on-the-fly, large scale data management techniques. These techniques will advance the state-of-the-art in distributed data management and provide systems designers and administrators with ""knobs"" that can be controlled to vary the degree of dynamism required to reduce the costs of managing the data while quantifying performance trade-offs.""543936,""Daugulis, Andrew"
"567561"	"Davison, Edward"	"Large Scale Control System Theory and Applications"	"LARGE SCALE CONTROL SYSTEMS AND APPLICATIONS BACKGROUND: The field of Control Systems and Automation deals with the problem of modifying the behaviour of a system, so that something ""desirable"" occurs. For example in a chemical factory, which has literally 100's of measurements and 100's of control valves to vary, it is desired that the plant should produce some desired output product , e.g. a polymer. In this case however it is impossible to have a single human or even a group of humans to be able to correctly adjust the valves (inputs) of the plant to achieve the desired product due to the complexity of the plant's behaviour, and due to the fact that disturbances, which are unmeasurable, are always present. To achieve this objective of controlling a plant, a ""controller"" is used, which typically is a set of instructions on a computer. ""Simple plants"" generally have a ""centralized controller"" in which all information of the system is fed into the computer but, for complex systems such as a chemical plant, it is not realistic to have such a centralized controller and a "" decentralized controller"", which consists of many sub-controllers, each of which are all isolated from each other, is used. The problem however, is that the theory of ""decentralized control"" is still incomplete, and decentralized control design is still in its infancy. RESEARCH STUDY PROPOSED: The research project proposed is a continuation of previous studies made by the applicant on so called ""Large scale control systems theory"". In particular the focus is about decentralized control, and in particular the design of decentralized controllers. NEW THEORY PROPOSED: In terms of the new theory being developed, a focus on: (i) Clarifying the properties of decentralized fixed modes (DFM), which are still not well understood, will be made (ii) An attempt on obtaining the fundamental limitations of decentralized control will be made (iii) A new decentralized controller design method for plants will be attempted which has the property that it requires  no mathematical model; it is believed that such a design approach is important, because model building is difficult to carry out in  industrial systems. (iv) A study of various complex decentralized control design methods will be made. NEW APPLICATION PROBLEMS PROPOSED: In conjunction with new theory, it is of great interest to determine how well the theory actually ""works"" when applied to real world problems, and so some application problems will also be studied, which are interesting in their own right. (i) A controller for new generation hard disc drive systems will be designed which is believed will be an order of magnitude faster  than existing disc drives (Western Digital Corp., Advanced Servo-Mechanical Technologies, San Jose, CA, USA are interested in this  new controller design; note:there appears to be no Canadian company to interact with.)  (ii) A large generalized electric power management system and decentralized controller for the autonomous mode operation of a  microgrid system that includes multiple distributed energy resource (DER) units will be carried out. This type of problem is important  in microgrid systems and will be an order of magnitude more demanding than previous studies. (iii) The problem of developing a controller which works ""well"" for the case of patients with spinal cord injury will be carried out. (iv) A study of the decentralized control of third generation large flexible space structures (LFSS) will be made; in particular necessary  and sufficient conditions for a a solution to exist and a controller characterization will be made. The theory will be tested out on  applying it to the MSAT II LFSS.""557364,""Davison, Matt"
"566748"	"Denidni, AhmedTayeb"	"Advanced antennas for future wireless communication systems"	"The performance of wireless communication systems is often limited by interferences, path loss, local scattering and multipath propagation. Therefore, reliable antenna systems should be developed and deployed to enhance the performance and efficiency of future wireless communication systems. In this context, next generation antennas with ultra gain, reconfigurable radiation pattern and broad band operation can be used as an efficient solution for increasing the overall performance of future wireless systems. Traditionally, to achieve this objective, smart antenna arrays have been used. Such a smart antenna system is based on the conventional design strategy of combining large number antenna elements. The back-borne of these systems are based on analog or digital signal processors that automatically optimize the radiation pattern in response to the received signal. The principal drawbacks of these approaches are the high number of antennas fed through complex networks that leads to reduced efficiency due to losses.  As an alternative solution to conventional systems, this research program aims to develop a new class of advanced antennas with high gain, reconfigurable radiation patterns using various artificial electromagnetic materials, such as active frequency selective surfaces (FSS), electromagnetic band gap (EBG) structures and metamaterials. The major goal is to design and develop advanced novel antennas that are capable of providing high gain, broadband operation and pattern reconfigurability. To achieve these attractive features, we propose the development of new tunable active FSS, EBG or metamaterial structures. By controlling the electromagnetic characteristics of these structures, the gain, frequency band and radiation pattern can be controlled and electronically reconfigured. These advanced antennas will be capable of drastically improving the transmission quality in hostile environments. With these features, the proposed antennas present significant potential for future wireless applications. To realize this challenging design platform, the proposed research will be focused on the study and characterization of artificial electromagnetic materials, including agile FSS, EBG and metamaterial structures and their applications in the development of new class of advanced antennas with ultra- gain, broad band width and dynamic radiation patterns.  In this proposal, FSS, EBG and metamaterial structures present interesting research topics because of their unusual, often unexplored electromagnetic properties. A more profound knowledge of these artificial materials holds the promise of optimizing the physical properties of microwave and millimeter-wave antennas, and could also be the key to master other promising technologies. Moreover, the ability to control structural synthesis on the microwave and millimeter-wave scale will lead to the development of new functional materials with unprecedented physical and electrical properties.  Therefore, this program will contribute to (i) analysis, design and implementation mechanisms involved in development of active FSS, EBG and metamaterial structures; (ii) foster highly focused research activities in FSS, EBG and metamaterial structures and their applications in design and development of advanced antennas , consistently with specific needs of the high technology Canadian industry, and (iii) train highly qualified personnel to respond to the scientific and technological challenges of our modem society.""563145,""Denidni, AhmedTayeb"
"565983"	"Dessaint, LouisA"	"Méthodes de sécurité prédicitives et préventives pour la stabilité transitoire des réseaux électriques"	"La complexité des réseaux électriques va en s’accroissant et bien qu’ils soient gérés par des opérateurs expérimentés munis de dispositifs de commande modernes, il se produit régulièrement de larges pannes. Aussi, les grandes compagnies d’électricité tentent-elles de développer des réseaux électriques plus performants et plus verts. Ces nouveaux réseaux électriques appelés « intelligents » font appel aux récentes avancées technologiques dans les domaines de la détection, de la commande, de l’informatique et des communications. C’est dans ce cadre que se situe notre projet qui consiste à concevoir des outils d’aide à la décision destinés à la conduite des réseaux électriques intelligents. Plus précisément, ces outils visent à assurer la stabilité transitoire, laquelle fait appel à des phénomènes très rapides pouvant se produire en moins d’une seconde. Pour se prémunir contre l’instabilité transitoire, notre approche propose des actions prédictives, préventives et correctives. Les actions préventives débutent avec une analyse de contingences qui consiste à déterminer puis à classifier par ordre de sévérité les contingences qui menacent la stabilité transitoire du réseau dans son état actuel. Les contingences les plus dangereuses servent de point de départ à l’étape suivante qui consiste à redistribuer la production (c’est-à-dire les puissances des alternateurs) de façon à minimiser une fonction-objectif telle que le coût de production ou encore les pertes de transport, etc. À ce problème d’écoulement de puissance optimal (OPF, Optimal Power Flow), on ajoute une contrainte de stabilité transitoire de sorte que l’on obtient un TSC OPF (Transient Stability Constraint – OPF). Beaucoup de chercheurs travaillent sur la façon d’inclure les contraintes de stabilité transitoire dans un OPF. Nous désirons utiliser un réseau neuronique pour fournir la contrainte de stabilité transitoire à partir de la distribution de puissance. Le réseau neuronique associé à notre TSC-OPF aurait une distribution de la production en entrée et un indice de stabilité transitoire tel que le CCT en sortie. Cette approche pourrait permettre éventuellement de déduire une fonction analytique dérivable qui servirait directement de contrainte dans la formulation de notre TSC OPF. Cette fonction dérivable permettrait d’inclure les termes de gradient dans la formulation de l’OPF basé sur une méthode d’optimisation déterministe. Nous pensons que la méthode mise de l’avant permettrait de réaliser un TSC-OPF valable pour plusieurs contingences (MC-TSC-OPF) plutôt qu’une seule contingence comme s’est fait actuellement dans la littérature. Par ailleurs, nos actions prédictives basées sur les synchrophaseurs permettront de développer une nouvelle méthode de prédiction en ligne de la stabilité transitoire en combinant un modèle SIME avec des techniques intelligentes d’apprentissage telles que la classification floue et le séparateur à vaste marge (SVM). Enfin, un défi important de notre projet sera d’établir si nos actions correctives basées sur les actions préventives préétablies pourront être appliquées à temps pour éviter une instabilité suite à une contingence grave et ce, en tenant compte des divers délais de communication, de mesure et de prédiction.""548571,""DeSterck, Hans"
"555647"	"Dhindsa, Kiret(Jaskiret)"		"NSERC"
"567398"	"Dietl, Werner"	"Sound Help In Eliminating Latent Defects"	"Everything depends on software. Computers are used in more and more places—from business computers to embedded medical devices, mobile phones, cars, and space craft. However, writing correct software is hard. End users often encounter defects, ranging from misbehavior and crashes, to costly errors like the 1996 Ariane 5 explosion, and to lethal catastrophes like the Therac-25 radiation therapy machines. In 2002, software bugs cost the US economy an estimated USD 60 billion and a 2012 estimate puts the global cost of software bugs at USD 312 billion per year. Software developers spend a considerable amount of their time “debugging” software, either after a test case fails or, much worse, after a customer notices a failure. Finding the cause of potential failures before they happen is even more challenging and time consuming. We must aim to eliminate defects from software before people are affected. Current bug-finding tools help developers find certain bugs, but do not provide any guarantees. Mathematical verification approaches provide guarantees, but require highly-trained specialists and incur large upfront costs. There currently is no lightweight—yet sound!—approach to increase the reliability of software. I propose a 5 year research program to develop Sound Help In Eliminating Latent Defects (SHIELD). This research program will develop cost-effective techniques for efficient, high-quality, safe software development, based on principled, practical, precise, and pluggable program properties. The objectives of the SHIELD research program are as follows. It will use machine-checked program properties to connect human understanding to software. The system will be principled: it must provide sound answers based on reasoning about the software, not guesses based on heuristics. The system will be practical: it must handle real-world software, must be usable by developers, and must be teachable to students. The system will allow the precise expression of interesting properties: it must not force users to use a limited vocabulary or to use a vocabulary at the wrong level of abstraction. Finally, the system must be pluggable: different domains need to enforce varying properties and the system must be easily adaptable to the particular properties of interest. The goal of the SHIELD research program is to provide the theoretical understanding and practical tools needed to define sound verification tools, including their notation and how to check their correctness. I have worked on both the formal definition of type systems and on their practical implementation. My past experience uniquely positions me to make further significant contributions to our understanding of correct software development. The long-term goal of my research is a lofty one: enable software developers to produce correct code, on time, every time. The SHIELD program will cover a wide range of related topics, which will lead to high-quality research publications, the training of HQP, and the improvement of industrial development practices. The SHIELD research program will have huge impact on my field of research and on the software development practices in industry. Developing trust-worthy software is a skill in high demand by both Canadian and international employers. HQP will be trained for both industrial and academic positions.""549022,""DiFrancesco, James"
"566166"	"Dimitrov, Vassil"	"Fast Linear Algebra Solver for Engineering Applications"	"The proposal is aimed at developing a theoretical and applied framework of a new linear algebra set of tools. In many engineering applications, one requires to solve large systems of linear equations. Often the method of choice are iterative methods, due to their fast speed and other attractive computational features. Many of the techniques convert the problems into a summation of consecutive powers of a matrix. In those cases it is always assumed that the Euler's decomposition for such a summation is computationally optimal. The proposal starts with a concrete example showing that this is not the case and, in fact, often one can get significant computational saving by using more efficient decompositions. Exactly how to accomplish the design of such decompositions is one of the theoretical points of this proposal. The algorithmic tools designed as a part of this work will help the creation of more efficient software and hardware solutions. In terms of hardware, the most straightforward application seems to be the optimized implementation of the Goldschimdt's floating point division algorithm. In the case of the widely used today 64-bit processor and the next-generation 128-bit processor, we anticipate to obtain much more efficient algorithms and significantly speed-up this major computational operation. In terms of software applications, the number of practical situations in which we can apply the main results of this research is unlimited. The most direct application will be the use of the novel techniques to various type of imaging - seismic, biomedical, molecular, etc. Of particular interests are the applications in geophysical domains, since in these cases one needs to process matrices of huge sizes and, as the initial findings suggest, the bigger the size of the problem, the more we save. Another possible set of applications - some results on that already accepted for publication - is cryptographic algorithms, that extensively use inversion of finite fields. In one of this cases we already reported a solution, that is 44% faster that the best known in the literature solutions; we anticipate even better results, based on more precise application of the decomposition techniques suggested in this proposal. Another possible - and huge - application, is the speeding up the PageRaking algorithms (for social networks and search engine applications) and GeneRaking algorithms (for computational biology applications). We believe that this is a fascinating and very, very interdisciplinary project that will lead to creating a large number of software and hardware tools for direct industrial application. More to the point, the work on this proposal will lead to training very high-quality personal, both on MSs and PhD level, that will be extremely helpful for the canadian Economy.""559599,""Dimock, Ian"
"567000"	"Dong, Min"	"Cooperative and Cognitive Designs towards Resource-Efficient Wireless Communications"	"Wireless technologies today are at the heart of major applications and emerging systems essential for human productivity and entertainment. Wireless communication capabilities have become the essential requirement for ubiquitous information access, pervasive computing, and social exchange. The exponential growth of demands continuously put pressure on the wireless communication infrastructure to be operationally efficient in radio spectrum, operating cost, and energy consumption. As spectrum and other network resources are limited, this necessitates a fundamental shift in future wireless designs toward a fully cooperative and cognitive paradigm that enables intelligent communication with optimized resource efficiency. Despite the promises, fully cooperative and cognitive design faces major challenges, such as distributed implementation strategies and effective network resource management cognitive the network dynamics and needs. The proposed research program aims to investigate into distributed cooperative algorithms and cognitive network resource managing strategies for cooperative relaying. With the integrated view of network architecture, protocols, and processing algorithms, we target novel unified designs that synergize techniques and strategies across the physical (PHY), medium access (MAC) and network-layers to maximize cooperation gain in a full-fledged scale. The outcome of this research program is expected to contribute substantially to the state-of-the-art research towards establishing our fundamental understanding and insights of cooperative system design, developing foundational policies and innovative practical technologies for future wireless applications and services. The proposed research is vital to the telecommunications sector of the Canadian economy in developing innovative wireless technologies to strengthen the quality and capacity of Canada’s communications infrastructure.""547489,""Dong, Mingzhe"
"567170"	"Dounavis, Anestis"	"Electronic Design Automation Algorithms for Signal Integrity Analysis of High Speed Integrated Circuits"	"With the rapid advances of technology, system complexity and signal speeds, significant demands are placed on Electronic Design Automation (EDA) tools to provide the same efficiency and accuracy. Innovations in fabrication process technology have significantly reduced the feature sizes of integrated circuits and increased packing densities of chips. Aggressive design objectives such as system on chip coupled with increased operating frequencies require multidisciplinary design methodologies such as electrical, thermal and electromagnetic analysis to accurately model the signal and power integrity of high-speed circuits. As frequency increases, interconnects behave like distributed transmission lines and effects such as ringing, signal delay, distortion, attenuation and crosstalk can severely degrade signal integrity. These effects are observed at the chip, packaging, printed-circuit-board and backplane levels. In general, distributed interconnects are modeled using partial differential equations and when discretized produce large system of equations, which are computationally expensive to solve. Furthermore, due to the increased packing densities of integrated circuits, it is becoming essential to model highly coupled interconnect and power distribution networks with many ports. As a result of these technological advancements, the underlying algorithms of traditional EDA tools have become ineffective and in some cases obsolete in addressing the multidisciplinary nature and computational complexity of modern circuit designs. Currently, the design of multi-port distributed networks is not handled appropriately by circuit simulators and stand as one of the major bottlenecks in design in signal and power integrity analysis.  The aim of this research proposal is to develop advanced modeling and simulation algorithms for efficient and accurate analysis and design of high-speed integrated circuits. This will be achieved by extending the scope of macromodeling algorithms to efficiently model large multi-port distributed electromagnetic systems in a unified circuit simulation environment. Furthermore new parallel computing and model order reduction techniques will be developed to reduce the computational complexity of large scale high-speed integrated circuits. The emphasis will be to develop interactive design verification techniques that will enable iterative and repetitive signal and power integrity analysis of large complicated distributed networks. This will lead to better circuit designs, shorter design cycles and lower costs.""562979,""Douplik, Alexandre"
"567948"	"Driessen, Peter"	"Broadband sensor signal processing, detection and classification of rare and unusual events"	"The research program is the area of extracting information from broadband sensors and arrays, both acoustic and electromagnetic. Of particular interest is the detection and classification of rare or unusual events (outliers). We focus on data from three types of sensors: (1) hydrophones in underwater cabled ocean observatory networks, (2) arbitrary microphone arrays made with smartphones, and (3) broadband radio antennas. A key novelty of our approach is to expand upon on our early success with hydrophone data to include other sensor types, and exploring the application of audio signal processing techniques to broadband radio signals. The background noise and unwanted events become increasingly complex as we move from hydrophones to smartphones to radio, with associated increase in risk. The research uses real-world measured data and is guided by interaction with end users. Applications range from sound environment monitoring to marine biology to wireless communications to security. Ocean Networks Canada operates several cable-linked seafloor observatories in the Northeast Pacific Ocean, Salish Sea near Victoria BC and the Arctic. These observatories currently deliver continuous real time information from the seafloor via fibre optic cables connected to oceanographic instruments. Hydrophone data is streamed 24/7 and archived. Most of this data is uninteresting noise. The challenge is to find the rare (unusual) audio events of interest such as shipping and marine mammal activity. The work leverages the $225M investment in Ocean Networks Canada cabled undersea observatories, to fully exploiting the opportunity provided by continuous observation. It is impractical to manually listen, find and classify rare and unusual events in the huge volume of data. The proposed rare event detection and classification algorithms are essential for the success of these sustained ocean monitoring programs. We consider microphone arrays of arbitrary and possibly changing geometry, inspired by the ubiquitous use of smart cellphones with GPS location abilities. Smartphones are a promising platform for experimentation since they include speakers and wireless (radios) as well as microphones and are programmable. In principle, these smart cellphones may be used to gather acoustic data, which may be particularly interesting where people (with smartphones) are gathered in one place (inside or outside). The many smartphones in use offer the potential to generate a huge volume of data. Smartphone array capabilities range from estimating source locations and/or room characteristics (dimensions, material reflection coefficients), de-reverberation, sound source separation, speech enhancement. These capabilities can be used to support and enhance the detection and classification of rare events, thus using the omnipresence of smartphones to create new capabilities in audio sound environment monitoring. Ultra-wideband (UWB) radio systems have been studied extensively and are commercially available. The regulations in the US specify the maximum spectral density (power per unit bandwidth). Most UWB equipment operates in the 3-10 GHz range but the same spectral density is permitted below about 1 GHz. UWB systems in this lower range are much less commonly studied. This radio spectrum also has rare events of interest (e.g. unauthorized transmissions, unusual propagation phenomena) that we wish to detect and classify. These UWB radio systems may be viewed as analogous to audio systems in that their bandwidth spans more than 10 octaves, thus the techniques for rare event detection used with hydrophones and smartphones may be adapted for radio. Applications include wireless communications, infrastructure monitoring and surveillance.""565905,""Driessen, Peter"
"556037"	"Drouin, Alexandre"	"Québec"	"CANADA"
"566423"	"Drouin, Dominique"	"Emerging devices integration above CMOS circuit"	"During the last 5 decades, the microelectronic industry has been able to drastically increase the computing performance while reducing the cost-per-chip. This outstanding improvement is essentially due to the scaling of the CMOS (Complementary Metal Oxide Semiconductor) technology. The International Technology Roadmap for Semiconductors (ITRS) anticipates that this trend or his equivalent in performance will continue for several years resulting ultimately in 5-7 nm gate transistors by 2025. This projection challenges the most optimistic prediction of transistor utilizing the CMOS physics for operation. Thus, innovative solutions must be introduced for post-CMOS technology in the midterm. Furthermore, the power density tendency of assembled modules is reaching level where standard air cooling is not adequate anymore. This energy consumption issue is becoming the predominant factor for introducing new devices. The industry is pursing different approaches for post-CMOS and equivalent performance enhancement. One solution for the former issue is the introduction of the very promising tunnel field effect transistor. For the latter one, tremendous effort have been deployed to increase the number of device by mm2 through 3D integration at the chip level. This proposed research program will address both issues by investigating innovative low-power nanoelectronics device that can be monolithically integrated in 3D with CMOS technology to investigate new functionalities and increase performance of logic and memory devices. The global objective is not to directly replace CMOS technology but rather complement it using back-end-of-line compatible devices. Such approach allows to i) increase devices density due to 3D integration, ii) reduce latency from shorter interconnections, iii) globally reduce energy consumption by reducing parasitic loss. During this 5 years project, we will develop two low-power nanoelectronic devices integrated above CMOS circuit. The silicon nanocrystal tunnel field effect transistor combines our nanodamascene process and amorphous silicon. The potential outcome of such device will be the realization of a low-power transistor operating at voltage below 0.5V with extremely low Ioff current due to tunneling through undoped channel. The second nanoelectronic device will also combine our nanodamascene process with indium oxide nanocrystals to realize embedded non-volatile memory. This two terminals memory has the advantage to be highly scalable and the potential to be embedded with microprocessor. The scientific and social impact of this research is ranging from more energy efficient portable electronic (tablet, smart phone) to universal memory (single memory type regrouping DRAM, hard-disk and flash memory).""545134,""Drouin, Guy"
"567816"	"Dueck, Gerhard"	"Heuristic Minimization Techniques for Reversible Logic Synthesis"	"Heat dissipation is an increasing concern in circuit design. Some of the energy loss is due to the irreversibility of the computations. If computations were reversible, energy loss due to the loss of information would not occur. Thus reversible logic has emerged as an active area of research with applications in quantum computing, low power devices, and nanotechnologies. Reversible functions can be represented as Toffoli networks. Different cost metrics have been suggested for such networks. Clearly, the implementation cost of a function depends on the target technology. CMOS implementations will have different metrics than quantum realizations. The problem can be formalized as follows: given a reversible function and a cost metric, find a realization with low cost. Due to the complexity of the problem, exact solutions are only possible for functions with few variables. Therefore heuristics are required. Reversible logic synthesis can be accomplished in a two-step process. First, find any realization for the given function—this may be far from minimal. Second, apply iterative transformations to reduce the cost. Transformations can be given in the form of rewriting rules (also known as templates.) Recently, some important advances have been made in the understanding and application of templates. One objective of the proposed research is to find efficient ways of applying templates. The number of potential templates is very large. It has been shown that some templates are applied more often, thus contributing significantly to the cost reduction. On the other hand, the application of some templates has never been observed while optimizing benchmark functions. This is due to the fact that the networks to be optimized are obtained in such a way that certain patterns never occur. A classification of templates will help to apply templates more efficiently. Developments in quantum computing may yield new ways of implementation. Reversible functions will then be mapped to such building blocks. Elementary gates will have different associated cost. Existing methods will be adapted to such evolving structures. For example, in the recent past the quantum T-gate has been proposed as a building block in fault tolerant computing. However, the cost of the T-gate is on the order of 100 times more costly than other gates. Thus, the reduction of T-gates becomes the primary objective. Such developments will be closely followed and new synthesis algorithms developed or existing ones will be adapted.""545196,""Duever, Thomas"
"567771"	"Dziong, Zbigniew"	"Architectures and resource management for WAN cloud networking"	"Cloud computing services are expected to grow annually 25% in the period 2012-2016 (IDC forecast). In fact this growth is five times bigger than the overall growth of IT industry, which indicates a major technological shift. Also, public cloud services constitute a significant part of enterprise spending and it is expected that this spending will reach $207 billion by 2016 (Gartners’s IT spending report). Public cloud computing is increasingly relying on WAN networking since the cloud resources can be located quite far from the users. Traditional internet has many limitations and in order to support cloud computing, a cloud support network should be significantly more agile and cost effective. Network as a Service (NaaS) is an important new networking paradigm that can help in achieving these cloud networking objectives. While NaaS has many definitions, in this project we consider NaaS as a service that is offered to users (mostly enterprise users) by a virtual network operator (located in a cloud) in order to manage and optimize WAN performance perceived by the user’s applications. In particular the focus is on latency, reliability and security. In general NaaS can be offered in combination with other cloud services or as a separate service. The importance of NaaS concept is validated by the fact that Cisco recently spent $1.2 billion to acquire Meraki - a leader in NaaS for small and medium companies. Note that while NaaS provides a framework that can facilitate WAN cloud networking it does not solve by itself the limitations of traditional Internet but rather allows creating better architectures and mechanisms build on top of the Internet that can address several issues. Therefore, apart optimization of NaaS for current Internet, it is also important to study which future Internet architectures are fitted best for cloud networking. In this project we propose to develop resource management mechanisms for WAN cloud networking that are based on different NaaS architectures including architectures build on top of future Internet and involving the SDN (Software Defined Networking) concept. These algorithms will be build based on an economic framework that can integrate many objectives of cloud networking. Apart giving solutions for particular NaaS architectures, the objective is to compare the performance to see which NaaS architectures and which future Internet options are most promising for cloud networking. We believe that the project results will be useful in two dimensions. First, the derived models and algorithms for NaaS can help virtual network operators to improve performance of cloud networking. Second, the study of the future Internet options in the context of cloud networking can be useful to decision makers that have a say in the future Internet developments.""560473,""Eady, Alex"
"566864"	"Eagleson, Roy"	"Design and Evaluation of VR and AR Simulator Modules"	"The research proposal spans an overarching project on the design and evaluation of Virtual Reality and Augmented Reality based Simulators. The proposed research will complements work being done through separately-funded projects on the design and evaluation of surgical simulator modules for a specific set of scenarios. In particular, the proposed project will develop generalized visualization and evaluation methodologies; generalizing case studies of Neurosurgical training simulators for training Endoscopic Third Ventriculostomy, Angiography-based catheter navigation, and aneurysm coiling; and training of Colectomies, and Prostate Biopsy procedures). In each of these cases, the goals of the DG-based research projects undertaken by HQP will be to develop and evaluate modules that extend from current simulators, or for developing GPU-based algorithms for 3D interactive computer graphics. We are establishing an approach using a formal Task Analysis phase as part of the Requirements Analysis phase for the complex tasks being trained within these curricula. The task analysis is important, as it allows us to examine the task requirements from an information-processing standpoint; identifying the particular Perceptual, Cognitive, and Motor capacities and constraints on the trainee (the main user of the system), from the standpoint of Cognitive Science and Experimental Psychology. We take this approach seriously, as it allows us to formalize the requirements of the main software classes in the simulator systems, in terms of the input-output requirements for the View classes, the Controller Classes, and those of the Model (MVC paradigm for design and development). This characterization provides us with a Hierarchical Task Decomposition for the simulator modules, which is precise and explicit, to the point where the trainees' Index of Performance can be operationalized as metrics to be logged within the simulator modules. These metrics span the abstraction hierarchy (in the sense posed by Simon and Albus) so that low-level technical skills performance can be addressed by extending the classical Fitts paradigm which respects the speed-accuracy tradeoff, and also to generalize to higher levels of the abstraction involving 3D spatial reasoning, path planning, and sub-task sequencing. Higher-level decision-theoretic skills, and knowledge-level learning, can also be evaluated in terms of experimental paradigms operationalizing task completion times and error rates as analogous metrics. These methodological developments lead naturally to very precisely specified modules that provide natural projects for HQPs at all levels. We have a number of graduate students who will be working mid-way through their thesis this year, and this proposal would fund new students who would come on-stream within this active and interdisciplinary environment. We are also working with several industrial partners on these ongoing projects: Digital Extremes, Inc, a London-Ontario-based digital video game company employing 175 people (well known for their 'Bioshock' and 'Star Trek' titles); the National Research Council (""NeuroTouch"" neurosurgical simulator); Christie Digital Displays and Christie Medical (on their VeinViewer and projector-based systems for Augmented Reality environments). In addition to exposure to these industry-based environments, our HQP are also able to network with other graduate students and academics through our involvement with the National Centres of Excellence project on Graphics and New Media (NCE-Grand), in which this applicant is Project Leader on HLTHSIM. The applicant is also a theme co-leader on our ORF-funded research project at CSTAR entitled, ""Effective Systems for Procedure-Specific Healthcare Simulation"".""552104,""Eamer, Jordan"
"567677"	"Ebrahimi, Mehran"	"Inverse Problems in Medical Imaging"	"This research program explores the broad area of medical image processing and provides solutions to various associated inverse problems such as medical image registration. In general, many real-world inverse problems are ill-posed, mainly because of the lack of existence of a unique solution. The procedure of providing acceptable unique solutions to such problems is known as regularization. Indeed, much of the recent progress in imaging has been due to advances in the formulation and practice of regularization. This, coupled with progress in optimization and numerical analysis, has yielded much improvement in computational methods of solving inverse imaging problems. Image registration, the process of aligning different sets of data into one coordinate system, is a key challenge in many medical imaging applications, e.g., tumour detection and surgery planning, that can be modelled as an inverse problem. Given a template (or moving) image and a reference (or fixed) image, the goal is to find a reasonable transformation that maximizes a predetermined similarity measure between the reference and the transformed template image. Solving this inverse problem is specifically challenging when images of highly deformable tissue, e.g., breast, is considered. Some of the short- and medium-term research objectives of this proposal in this direction are outlined below. 1-Breast Magnetic Resonance Imaging (MRI) is frequently performed prior to breast conserving surgery in order to assess the location and extent of the lesion. Ideally, the surgeon should be able to use the pre-surgery image information during surgery to guide the excision. This requires the prone pre-surgical MR image to be aligned or co-registered to conform to the patient's supine position on the operating table. The future breast Computer Assisted Surgery (CAS) technology will demand novel efficient alignment algorithms that employ the surface information of the breast in the operating room along with the intensity information of the pre-surgical images. 2-Recently, Dynamic Contrast-Enhanced (DCE) imaging has emerged as a powerful screening tool. Accurate registration of DCE images is valuable for proper identification of the lesions. This requires defining regularization expressions that directly incorporate the underlying physical process of this inverse problem. In addition, developing relevant efficient computational schemes is necessary to address the problem. 3-In image registration, researchers generally rely on transformations to describe the alignment process relating two images. However, in the clinical setting, there are many situations where the deformation may have discontinuities. Given the limitations of current approaches to image registration, an open problem is to develop a method that would enable deformable registration in the presence of various types of large-scale discontinuities. The proposed multidisciplinary research program will not only foster scientific advances valuable to the academic community, but also directly benefit the society. The developed computational methods are of significant importance that can shape the future of computer assisted surgery, diagnosis, and treatment planning technologies of highly deformable tissue. Revision surgeries due to misplacement or misdiagnosis impose a huge financial burden on the Canadian healthcare system. This research program is directed towards affordable alternatives using multidisciplinary techniques in collaboration with leading Canadian research institutions. In addition, the proposed program will prepare students in this demanding research field and place them in a more competitive position in academia or industry.""559558,""Ebrahimi, Mehran"
"566508"	"Elliott, Robert"	"Hidden semi Markov models; stochastic control."	"In addition to signal processing, speech processing and other areas, one of the most successful applications of hidden Markov models has been to genome and protein sequencing. My book with L Aggoun and J Moore on ""Hidden Markov Models: Estimation and Control"" was published in 1995 by Springer Verlag and reprinted in 1997. A revised and expanded edition was printed in 2008. I have published several other contributions to hidden Markov models. However, when modelling discrete sequences, for example in discrete time or in biological applications, if the hidden process is a Markov chain its (random) occupation time in any state is a geometrically distributed random variable. For biological applications it appears that more general occupation times should be considered. This leads us to consider semi-Markov models where the probability of jumping to a new state depends not only on the present state but how long the chain has been in that state. We shall consider 'hidden' semi-Markov models, that is situations where the semi-Markov chain is not observed directly but modulates a second, observed, process. We believe these will better model behaviour exhibited in protein and other 'gnome' sequencing. We shall develop for these models the results found in our book and previous works.  A second line of research will use our recent results on backward stochastic differential equations to investigate partially observed stochastic control problems. The adjoint process is described by a backward stochastic differential equation; for partially observed problems this is a backward stochastic partial differential equation. Initial work will consider this problem for the control of a partially observed Markov chain""552642,""Elliott, Robert"
"567541"	"EnrightJerger, Natalie"	"Intelligently orchestrating communication in many-core architectures"	"Improvement in communication for computer architectures will have broad impact on the computing industry and will have significant benefits to Canada. Improved communication will enable novel, larger computer systems that can be leveraged by researchers in a wide range of disciplines from medicine to economics. My research will provide greater computational capabilities to enable the researchers in these fields to solve some of society's most pressing issues. There is a critical need for HQP in the area of computer systems. Students trained on this project will gain expertise in architecture, software, compilers, algorithms and circuits and be highly sought after by numerous Canadian companies. Over the last several decades, the computer industry has doubled the number of transistors per chip with each technology generation (every 18-24 months); this is known as Moore's Law. These high transistor densities have created a power wall, limiting the rate of clock frequency scaling in general purpose processors. As a result, processor vendors such as Intel, AMD and IBM now incorporate multiple processors on a single chip to improve performance, rather than a single core with increased speed and/or complexity. As a result, parallel architectures based on multi-core technology are ubiquitous--they can be found across all types of computer systems from servers to cellphones. To leverage the computational capabilities of these multiple cores, communication between cores is essential. As with any teamwork activity, the best progress is made when all team members are communicating frequently and working together; it is much the same with parallel computer architectures. Communication plays a critical role in overall system performance. My current research agenda focuses on computer architectural and software techniques to streamline and improve the efficiency of the communication between cores. Poorly architected communication fabrics can lead to performance and power bottlenecks for computing systems. It is projected that the energy expended on communication will exceed that consumed by computation in future generations of computing systems. Although multi-core computing has allowed us to temporarily side-step power issues, power is once again a critical issue; dark silicon, or the inability to power on all parts of the chip simultaneously will become a reality in just a few technology generations. Communication will play a vital role in orchestrating dark silicon systems by efficiently moving data to/from hardware accelerators specifically designed to run computations in the most power-efficient manner. The long term goals of this research are to 1. Leverage online learning techniques to optimize the performance and energy-efficiency of the communication fabric. By observing and learning from an application’s runtime behaviour, we can tailor the communication fabric to provide greater energy efficiency. 2. Hardware and software optimizations that trade-off accuracy for performance and energy efficiency. Approximate computing proposes to save power by allowing some error to emerge in computations. For example, image processing can tolerate some error that will not be noticeable to the human eye. We will explore the implications of approximate computing on communication requirements and explore the tolerance of these applications to communication errors. 3. Explore the role of communication in dark silicon architectures and hardware optimizations to facilitate improved communication. In current on-chip network architectures, it is often difficult to power-down a subset of the network. We will explore new topologies and architectures that are specifically designed to be partially powered down to support dark silicon.""563335,""EnrightJerger, Natalie"
"566047"	"Fang, Qiyin"	"Miniaturized optical sensing/imaging technology development for environmental and biomedical applications"	"Optical spectroscopy and imaging allow remote, minimally-invasive monitoring of activities; provide morphological information that describes structure/shape; and may be used as a functional diagnostic tool for measuring of biochemical and physiological characteristics. Recent advances of photonics technology are the driving force behind many novel optical devices development. Various optical spectroscopy and imaging technologies have been investigated for applications in life sciences research (e.g. microscopic imaging), environmental sensing (e.g. water quality monitoring) and medical diagnosis (e.g. endoscopic imaging). In practice, there are strong demands for miniaturized, integrated devices for in-situ applications. For example, in screening of cancers in the gastrointestinal (GI) tract, it is highly desired to have encapsulated, functional optical imaging modalities (fluorescence, Raman, etc.) for minimally-invasive detection of malignant lesions. Another example is implantable microfluidic and optical imaging based cellular analysis devices for continuously monitoring of the biochemical and physiological conditions of the patients as well as the environment. Conventional optical spectroscopy and imaging systems are large, complex and costly optoelectronic instruments comprised of lasers, spectrophotometers, and detectors. Often, their size is the limiting factor for their use in certain applications such as in-situ monitoring of physiological processes and distributed environmental sensing. Recent advances in micro-photonics and electronic devices have led to small but efficient components/modules such as diode lasers, single photon avalanche diodes detectors, microlenslets, and GRIN optics. When integrated with microfluidics technology, fully integrated devices can be made by Micro-electro-mechanical-systems (MEMS) or Micro-optical-electro-mechanical-systems (MOEMS). These technologies are mostly built upon well-established microelectronic technologies for integrated circuitry. When conventional devices are replaced with micro-components, new applications and capabilities can be facilitated. Moreover, it usually leads to significant cost reductions and increases in yield associated with mass production. Although progresses have been made recently in the development of individual components such as micro-cantilevers, micro gratings, and micro-lens, etc, integration of these components into complete micro-spectroscopic and imaging devices remains as a very challenging problem. For a complete optical spectroscopic/imaging system, more complex components such as miniature light sources, photo detectors, and high-speed electronics are also required. Furthermore, medical and environmental application of these technologies raises new requirements such as toxicity, chemical/biological stability, and sterilization concerns. The overall objectives of the proposed research program are to (i) develop integrated micro- optical spectroscopic technologies for spectrally- and temporally-resolved optical signal acquisition; (ii) investigate the integration of high speed photo detection modules into the micro-spectrometer; and (iii) development of a fully integrated encapsulated endoscope. The proposed program will be based on the Micro/Nano Systems Lab and focus on integrated device technology development. Its success will allow translation of such technology to applications in biomedical diagnosis, drug discovery, and environmental monitoring.""564854,""Fang, Qiyin"
"567983"	"Farag, Hany"	"Advanced Design, Planning and Control Algorithms for Smart Distribution Grids"	"Driven by the urgent need to develop cleaner and more efficient, reliable, and resilient electric power grids, the energy sector is currently moving towards an era of smart grids. The main pillar of a smart grid setup is the evolution from a vertically integrated, partially-automated and producer-controlled electric power network to a decentralized one that enables interactions among customers, network operators, and power producers. In response to smart grid initiatives, electrical distribution grids are undergoing a major transition towards a smart distribution grids (SDGs) structure characterized by high penetration of distributed and renewable generation (DG), plug-in electric vehicles (PEV), energy storage devices, and grid-automation devices. The structure of SDGs will be divided into set of microgrids with sufficient generation to meet all or most of their local loads.  Yet, major technical challenges face the realization of SDG vision due to its emergent nature. To that end, the proposed research program aims to develop a unifying framework for design, planning and control strategies of SDGs to facilitate timely and safe transition of distribution grids. In particular, three main projects will be conducted in the proposed research program. First, developing new design and planning strategies for the structure of SDGs (e.g. Optimal allocation and sizing of new components and technologies in SDGs, optimal set of microgrids that can be identified, created, and designed in a typical SDG and optimal configuration of the created microgrids). Second, developing a comprehensive multi-agent distributed control scheme to facilitate a scalable real-time cooperative control in SDGs. The proposed control scheme will integrate several operational scenarios and control objectives for SDGs clustered into microgrids with different configurations and modes of operation; important among these are online self-healing, power quality, and voltage stability. Third, developing real-time smart coordinated control schemes to accommodate high penetration of PEVs into aging distribution grids.  The proposed research will help distribution power utilities to eliminate the current barriers towards the deployment of sustainable and clean energy resources, and smart grid technologies into the utility grid. The proliferation of these new essentials will help improve the overall power system performance by increasing the system reliability, power quality and efficiency while minimizing the environmental impact of the current central power generation plants. The proposed research is expected to create a knowledge-base and tools that are extremely useful to the energy system industry to facilitate the safe transition of current distribution grids into SDGs through better understanding of new requirements, upgrades, and concepts. The proposed research will place the theoretical foundations in the emerging area of SDGs research. Further, it will provide students with skills in vital areas of smart grids and sustainable energy. The energy and environment have been identified as a driving force for the new energy economy; Canada is a world leader in this area thus the accomplishment of the proposed research plan will have significant and measurable economic and environmental impact to Canada.""553622,""Farah, Amjad"
"566529"	"Farzan, Azadeh"	"Advances in Program Analysis"	"The proposed research is in the area of Program Analysis, which is the process of automatically analyzing the behaviour of computer programs. A well-stablished application of program analysis has been in the domain of Software Verification which aims at ensuring the reliability of software, using automated techniques and tools. With the emerging omnipresence of software in all aspects of our lives, ensuring reliability of software has become essential. With multicore processors becoming the default choice for computers and mobile devices and the advent of distributed web services, concurrency has become commonplace in application software. The design of concurrent programs is notoriously error-prone due to the nondeterministic interactions among concurrently executing program threads. The proposed research will advance the state of the art in concurrent software analysis. More specifically, we will focus on achieving this in the presence of some the most widely used programming language features that, combined with concurrency, make the task of program analysis difficult and consequently there is a shortage of effective program analysis techniques. We have made significant progress in the past few years by introducing a novel way of approaching this problem domain, based on the notion of dataflow, and plan to unleash their power for addressing some of the most challenging problems in concurrency research, including: Unbounded Concurrency: program analysis becomes substantially more complicated when there is no a priori bound on the number of interacting processes that constitute a program. This is usually referred to as unbounded concurrency. Many broadly used software systems, for example device drivers, file systems, and concurrent libraries naturally adhere to this model, which makes their verification problem very relevant. Dynamic Memory (heap): All modern programming languages have primitives for creating and manipulating memory dynamically, and it is virtually impossible nowadays to find software that does not use dynamic memory. Reasoning about programs manipulating the heap, which is unbounded, is a theoretically and practically hard problem, even without the presence of concurrency, and particularly more challenging with it. The challenge is usually in producing a precise enough understanding of the heap that will not hinder the task of program analysis. Relaxed Memory Consistency: Concurrent programs have different behaviours under different memory models guaranteeing different levels of memory consistency. In the sequential consistency memory model, there is a single, global view of time in an execution. In relaxed memory models, each processor has its own view of time, and the views may not be consistent. Since no actual computer implements sequential consistency memory model, analyzing programs under a relaxed memory model (a challenging task) is the only possible way of getting practically relevant results. Numerical Uncertainty : Scientific applications, like high-throughput medical imaging or high-performance simulations of climate models, demand great computational resources. These scientific applications are typically run on multicore and multi-processor environments. The problem is that the same concurrent program can produce different results when used with different architectures and compilers, a sensitivity that is especially critical for floating-point computations. These are due to some well-known and some lesser- known dependencies of floating-point behaviour on execution order (which is determined by the concurrent execution model). This puts the portability and reliability of computation results across platforms under question, which makes this under-explored problem area worthy of attention.""557146,""Farzaneh, Masoud"
"567923"	"Fong, Philip"	"Security and Privacy Protection for Social Computing"	"We increasingly store personal information on various digital platforms. Of particular concern are the emergence of social computing platforms (e.g., Facebook, Foursquare) as custodians of information about our private and social life, as well as the shifting culture of the next generation to willingly disclose the minute details of their personal lives via such platforms. These platforms present unique security and privacy challenges that are not addressed by classical protection technologies. The study of Access Control involves the articulation and analysis of protection models for regulating access to information, and the design of mechanisms and technologies for realizing these models. In this proposed research program, I will explore access control innovations that are motivated by the novel security and privacy challenges of social computing. This proposed research program has three objectives. First, it aims at devising innovative protection technologies for addressing the unique security and privacy challenges in social computing platforms. Examples include technologies for the prevention of inference attacks by Facebook applications, protection technologies for controlling access to resources that affect the privacy of multiple parties (e.g., photo tagging), and assessment of privacy risks in geo-social network systems (e.g., Foursquare). Second, this research program aims at formulating mathematical models for improving our understanding of emerging social computing paradigms. Examples include building mathematical theories for modelling social contracts in online communities (e.g., Wikipedia), as well as formulation of mathematical models for the authorization mechanisms of geo-social network systems. A third objective of this research program is to devise supporting technologies that would make it easier for users to interact with advanced access control models for social computing. An example is to design visual formalisms so that users may draw diagrams to specify social contracts for an online community.""554468,""Fontaine, Arnaud"
"567652"	"François, Véronique"	"Space Division Multiplexing"	"The research objective of this proposal is to build a world-class program on space-division multiplexing (SDM) using multicore optical fibers, in order to shape next-generation, terabit-per-second capacity (10e12 bits/s) ultra-dense optical interconnects. We will design, make and test multicore optical fibers and coupling devices compatible with the established technology of two dimensional arrays of high-speed vertical-cavity surface-emission lasers (VCSELs) for the development of active optical cables with dramatically expanded capacity. In recent decades, we have witnessed an incredible revolution in communications and computing based on the advancement of optical fibers, photonic devices and electronic circuits. While single-mode fibers and wavelength division multiplexing (WDM) historically offered plenty of transmission capacity for long-haul and metro networks, the new applications for optical networks, such as machine interconnectivity and local area networks, require both large capacity per fiber core and increased core spatial density. The electronics industry is presently experiencing communications bottlenecks in integrated circuits, and this is particularly evident in inter-chip communications (e.g., between microprocessor and memory, and between multiple microprocessors). Short-reach optical interconnects, which typically span up to a few hundred meters, overcome the bottleneck of electrical interconnects caused by heat dissipation and size congestion: high-performance super computers manufactured in 2013 use more than 10e5 lasers typically; the IBM Blue Water super-computer uses over 1 million optical channels! In computing systems, space is a major concern. However, the benefits of very short reach optical interconnects (less than 1 meter long) still have to be established for inter-chip data communications and core-to-core on-chip communications. Can more space-efficient optical links be engineered, versus using the conventional single-core optical fiber? Recent developments in fiber-optic fabrication techniques have triggered interest in multicore optical fibers to increase spatial density by building several guiding cores in a single fiber of the same diameter. Record transmission capacity of 1.01 petabit-per-second (10e15 bits/s) was demonstrated in 2012 by Japanese researchers at NTT using such a 12-core fiber. The research program will transpose this achievement to optical interconnects by developing customized multicore fiber, active and passive coupling devices, circuits, and architectures to replace the less space-efficient fiber ribbons currently used in the industry. The new optical interconnects will be able to carry independent channels in several tens of guiding cores, thus permitting near terabit/s capacity over a single fiber using 25-Gb/s VCSELs. Our group is already equipped with the necessary design tools, has contributed to the concept of SDM for optical interconnects, has set key Canadian and international collaborations for multicore fiber sourcing, and benefits from world-class laboratory facilities. The proposed research program will contribute to the development of synergies between electronic and photonic integration technologies, which will result in revolutionary changes in photonics manufacturing that will dramatically change design principles and fundamentally improve performance. Canada is a world leader in Information and Communications Technologies (ICT). The proposed research directly supports Canada's strategic priorities in ICT Device and Systems (as stated in NSERC selected Target Areas and Research Topics), by the development of technologies that will dramatically increase the spatial density of optical links.""546950,""François, Vincent"
"566337"	"Frappier, Marc"	"A formal approach to access control and consent management"	"Personal data are widely distributed in enterprise information systems (IS) and shared across thousands of users. Protecting the confidentiality and integrity of personal data is critical in several domains like health, banking and governmental programs. Access control is responsible for controlling who has access to what and when in an IS. The objective of this research proposal is to develop a formal framework for controlling access to personal data stored in information systems. This framework shall ensure that personal data is accessed according to the requirements of the organisation and the requirements of the individuals related to the personal data. This framework shall address the following issues. First, it must include a proper mechanism for modeling IS users, individuals and their personal data, and access control rules, which together form an access control policy. Second, it must include a mechanism for specifying and verifying properties of an access control policy, to ensure that both individual requirements and organisational requirements are taken into account. Third, it must include an access control engine that can apply an access control policy to an access request. Finally, this framework should be general enough to be used in a variety of application domains; in others words, we need to transition from an ad hoc way of developing access control systems to a systematic approach based on a sound and powerful model of what access control consist of.""547669,""Fraser, Ailana"
"567493"	"Frasson, Claude"	"Brain-Based Intelligent Tutoring Systems"	"Our objectives are inspired by some results we have obtained from previous experiments on subconscious learning . In these works we have discovered that brainwaves analysis allowed to distinguish a change, at a moment, between cognitive reasoning and intuitive reasoning, in other words from conscious to subconscious mode. This change is similar to what is observed when learners switch from a cognitive task to gaming. Instead of continuing to find a solution, they play with the system until they encounter a solution but just by trial and error without any cognitive effort. This leads to an important problem in knowledge acquisition which corresponds to a cerebral state: engagement or disengagement. In the first one, the learner is interested (motivated) to explore the possible solutions and could converge to the final solution. In the second one, the learner is no more interested to continue on the task (which sometimes explains the derivation to gaming). Our experiments highlighted also two typical kinds of reasoning: cognitive reasoning and intuitive reasoning. While the first involves generally the conscious part of the brain, the second is more linked to the subconscious part. Sometimes, a problem solving situation will involve successively cognitive and intuitive reasoning, switching back and forth between the two modes. In both cases emotions play an important role to facilitate or not learning. The emotional conditions of the brain allow for the transmission of information not only to the conscious part but also to subconscious region and this last one seems to contain more knowledge and to process the information at a higher speed. As we have seen in our previous studies, the affective part is extremely important as it prepares the brain to accept or not the information. Intuitive behaviour is more present in video games. For instance, in flight training or games the resulting experience and abilities will concern immediate reactions (reflexes) Following our previous works, we want to detect more precisely emotional conditions and cerebral states that lead to engagement or disengagement. This implies to detect the emotional condition (for instance a learner is excited or not) and learning condition (the learning objectives are too high) in which learning occurs, and measure the resulting cerebral state (for understanding more clearly the different steps of learning and the evolution of cerebral states). Moreover, a continuous control of learner’s brain activity could be used by an intelligent system to give advices to the learner and put him into the best emotional conditions for learning. As we can see the brain is at the center of this intelligent system. An intelligent personal adviser (agent) could follow the evolution of cerebral states and provide personalized advices to optimize learning. To reach this goal, we need to explore more possibilities to detect the different steps of engagement and disengagement (when a learner cannot understand the problem and find the solution) through EEG signals. The main problem remains to detect and monitor the learner emotional state with precision and in real time. Emerging device allow to have a direct interaction between the learner and his environment (eye tracking systems, brainwaves interaction with helmets) The general objectives of our research are (1) to build an integrated interactive Brain-based environment able to detect the conditions of engagement and disengagement both in cognitive and intuitive learning,using a video game, and (2) to build a complete Brain-based video game learning environment able to provide the best pedagogical strategies to foster learner’s memorization and knowledge acquisition.""554232,""Fravalo, Philippe"
"567027"	"Freundorfer, Alois"	"3D Ceramic Sensors and Circuits"	"Advances in technology have greatly influenced all aspects of society in the last twenty-five years. As a result, information technology, medical equipment and consumer products such as cellphones, computers, video recorders, automotive parts, etc. rely heavily on microsystems technology (i.e. microelectronics, photonics, wireless circuits and sensors). Future products will desire systems that are more compact, versatile, functional and less expensive. To achieve this, higher operating frequencies and wider bandwidths become necessary. This research will focus on high speed electronics to create a higher level of integration and functionality using advanced low temperature ceramics.  We have developed a unique low temperature ceramic process for depositing thin film, thick film and 3-D ceramics onto integrated circuits (ICs) and printed circuit boards (PCBs). Most ceramics have to be made at around 1000 degrees Celsius or more. This destroys the IC. We can do it at 150 degrees Celsius and maintain the integrity of the IC. No one else has achieved this. This will impact tunable electronics, miniature electronic antennae and filters with the development of appropriate masking methods. We were the first to integrate 3-D ceramics onto ICs and PCBs, reducing product size and cost, and increasing functionality. We will investigate magnetic ceramic materials in our novel low temperature process. This will allow applications to electronics beyond the standard norm because these materials could then be deposited onto ICs and PCBs.  The proposed research will assist the Canadian industry in producing and manufacturing smaller, more versatile and less expensive products. This will provide a substantial competitive advantage globally in medical sensors, automotive radar, millimeter wireless LAN like the future 5G phone and medical diagnostics equipment.""564668,""Frey, Alex"
"566887"	"Funt, Brian"	"Computational Models of Colour Perception with Applications to Camera and Light Design"	"This proposal is about how the colours of objects are perceived by different ‘observers’—different people, different cameras—and under different lights (daylight, LED, fluorescent, tungsten). Colour is a very interdisciplinary field touching on psychology, philosophy, chemistry, physics, and computer science. My approach to understanding colour is to view colour perception as a computational process. As such, the models of colour perception developed in my laboratory are formulated as algorithms that can be tested both in terms of whether or not they simply provide the expected results, and secondly in terms of whether they operate in a way that is congruent with what is known about human colour perception from the psychophysical experiments conducted by psychologists. The fundamental difficulty in understanding and modeling colour perception is that because humans have only 3 types of colour-sensitive cones there is no one-to-one correspondence between the wavelengths of light entering the eye and perceived colour. There are many other difficulties too, such as how the same reflected-light spectrum may look different in different contexts, and these all contribute to colour being a fascinating field of research. These difficulties, however, present problems for the digital camera industry, the digital printing industry, the digital display industry, the textile industry, the lighting industry, the scientific use of colour (e.g., in medical applications), and the digital preservation of artwork. The research projects in this proposal address fundamental issues of colour science that have direct application to all these technology areas. The objectives of the proposed research build on the recent progress in my laboratory that include work on metamerism described in our recent prize-winning paper entitled “Metamer Mismatch Volumes” (Logvinenko, Funt, Godau) and on the development of an illuminant-invariant descriptor for colour hues. The objectives include: (i) determining the full set of colours observable from objects under a given light; (ii) developing a model of material colour constancy; (iii) creating a new measure of the colour fidelity of digital cameras; (iv) investigating metamer mismatching as a tool for evaluating the colour rendering properties of energy-efficient lights; and (v) validating the newly proposed hue descriptor and applying it to tasks such as colour-based object identification. The research budget is primarily for the training of highly qualified personnel; namely, support of student salaries and their conference travel. My previous students now have successful careers in the colour-imaging field.""554850,""Furey, NathanielBruce"
"555869"	"Gallant, Marc"	"Revolutionizing geological surveying through robotic data collection and autonomous data processing"	"mining, mobile robotics, geological surveying, autonomy, mapping, LiDAR, remote sensing""556002,""Gallant, Sara"
"566670"	"Gao, Pu"	"Probabilistic Combinatorics and Random Structures"	"With the popularity of the Internet and many social networks, random graph theory has become an indispensable tool for network analysis. Various random graph processes have been designed to mimic the evolution of real-world networks. Analysis of algorithms on random graphs provides theoretical support for the performance of these algorithms in real-world networks. Many social networks behave very differently from the classical Erdos-Renyi random graph model (also known as the binomial random graph model). New (inhomogenous) random graph models are currently receiving great attention for this reason. In particular, researchers are interested in graphs with power-law degree sequences. In this proposal, I address two problems in this area: (a) enumerating graphs with a specified power-law degree sequence; and (b) rumor spreading on Twitter (modeled by a random directed graph with degree sequences such that the in-degrees follow a power law). The other problems addressed in my proposal have importance in random graph theory and probabilistic combinatorics. They are also closely related to other research disciplines like computer science and physics. Some of these problems are hot topics in theoretical computer science (e.g. solution clustering in random constraint satisfiablity problems (CSPs), and spanning-tree packing in random graphs) and some are fundamental problems in random graph theory (equivalence of different random graph models, the stability of k-cores, and the emergence threshold of k-regular subgraphs). A remarkable phenomenon in random graph theory is that many graph properties (or other random structures) exhibit (sharp) phase transitions. Determining such phase transitions is extremely important in many research areas. For instance, the solution clustering threshold (where the solution space transits from a single cluster to many clusters) of many CSPs (as addressed in my proposal) is believed to correspond to their algorithmic barrier, which is very important in algorithm design in computer science. Research in random graph theory and physics greatly overlaps due to the common interest in characterising phase transitions of random objects. For instance, the two problems in my proposal about CSP clustering and the k-regular subgraph emergence threshold have both been extensively investigated by statistical physicists, through non-rigorous arguments. Solving my proposed problems, with the rigour of random graph theory, will also have great impact in these applied areas. Several problems in my proposal can be viewed as analysing properties of real-world networks (e.g. spanning-tree packing in random graphs and rumour spreading on Twitter). Solving these problem will potentially benefit Canadian Internet companies by giving inspiring insights into properties of massive-scale networks.""545702,""Gao, QiGang"
"555273"	"Gao, Xi"	"Ontario"	"CANADA"
"566781"	"Gao, Yong"	"Computational Problems in Artificial Intelligence and Network Science: Probabilistic Analyses, Graph-Theoretic Characterizations, and Algorithmic Solutions"	"In Artificial Intelligence (AI) and the emerging field of network science, many computationally-hard problems have a natural graph-theoretic or logic formulation. A deep understanding of the nature of these problems and their underlying graph-theoretic structures is indispensable to design well-founded algorithmic solutions and effective modelling tools for (logic) reasoning and problem-solving in AI, and to analyze real-world social, information, and biological networks. My research in the next five years will be centered around two themes, dealing with algorithmic and modelling problems arising in the study of systems and environments that are dynamic, networked, with incomplete information, and sometimes with multiple interacting entities. The first theme focuses on several algorithmic problems related to robust solutions to constraint satisfaction problems and defeasible reasoning with incomplete information. These problems plays an important role in the areas of constraint programming, satisfiability testing, and argumentation in AI. Algorithmic problems with solution concepts of a similar flavor, such as those in graphical games and AI planning in dynamic environments, will also be considered. My research will strive to understand the probabilistic behavior of the various solution concepts, the algorithms for finding such solutions, and the graph-theoretic constructs that characterize tractable subclasses of these problems. The chief goal is to gain insights into the power and limitation of data-reduction and branching rules that are essential for designing and enhancing general-purpose exact algorithms and fixed-parameter tractable algorithms for these problems. The focus of the second theme is on problems from network science, concerning generative random models, graph-theoretic characterizations, and algorithms for community structures widely believed to play a critical role in understanding the organizing principle of a real-world complex network and the dynamic processes taking place in the network. My research under this theme has three main goals: (I) to design generative random models to overcome the difficulties that existing network models have in characterizing the statistics of higher-order structures of a network; (II) to develop, by using sound graph-theoretic constructs, a systematic approach for characterizing community structures that have rich internal structures and are robust against network changes; and (III) to design efficient algorithms for identifying such network communities. The proposed research is expected to be of great practical value and significantly advance our knowledge. The research on the probabilistic behavior of random problem instances and the underlying graph-theoretic structures will offer a unique and novel perspective on several problems that are important in modelling computing tasks in dynamic and networked environments. The work on community structures will help bring the rich body of knowledge from research in graph theory into (social) network analysis. The algorithms and modelling tools developed in the proposed research should be useful for researchers (and practitioners in the software industry) to design better online social networks, to implement more sophisticated software for network analysis, to develop more effective systems to solve real-world optimization problems, and to tackle computational problems in multi-agent systems, bioinformatics, and sociology.""564394,""Gao, Yuxiang"
"556057"	"Gardner, MarcAndré"	"Québec"	"CANADA"
"555677"	"Gawronski, Alexander"	"Computational Approach for Determining Mutations and Mechanisms that Drive Alternative Splicing."	"Bioinformatics, Transcriptomics, Alternate Splicing""549418,""Gawronski, Bertram"
"555823"	"Gehring, Clement"	"Québec"	"CANADA"
"566013"	"GhafarZadeh, Ebrahim"	"INTEGRATED MICROFLUIDIC MICROELECTRONIC SYSTEMS: TOWARD ACCELERATED TWO DIMENSIONAL MAGNETIC RESONANCE SPECTROSCOPY"	"Miniaturization started in the field of microelectronics and the associated fabrication processes revolutionized the way we communicate, process information and compute. Not long ago, a new paradigm based on parallelism was formulated and researchers claimed that miniaturization should operate the same revolution in biology by accelerating bioassays and optimizing chemical reactions when performed in large numbers, on a single chip. Massively parallel biological analysis thus holds enormous potential for a variety of applications such as, drug discovery and detection of chemical/biological hazards. But this potential can only be unleashed to its fullest extent, when the development of Integrated Microfluidic Microelectronic Systems (IFES) is made possible. Nowadays, IFES technology is used in a variety of applications ranging from disease diagnosis to DNA sequencing. Indeed, IFES technology offers many advantages like; high throughput screening capabilities, low sample consumption and rapid analysis. Inasmuch, we undertake the development of the new generation of IFES used in drug discovery applications and necessary for disease management. In this research program, we will exploit, state of the art, microelectronics and microfluidics, to achieve acceleration of a specific molecular sensing technique, namely; Magnetic Resonance Spectroscopy (MRS). MRS is widely used in drug discovery research for screening potent drugs selected from large libraries of naturally derived or synthesized molecules, visualization of the three dimensional structures of target molecules, identification of drug-target structure stability and last but not the least, elucidating fast time scale dynamic changes in molecular conformations. Despite the important role played by MRS in enhancing our understanding of biomolecular structures, this sensing technique lacks of sensitivity. This is a serious drawback of the classical devices used so far to implement the so-called two-dimensional MRS required for drug discovery purposes. This project addresses the resolution of technical problems levied on the use of 2D MRS by integrating a large number of miniaturized MRS probes on a single chip. Microfluidics plays an important role in this endeavor. The handling of drug candidates at a micro/nano-liter scale should be fast enough, for chemical change not to take place before MRS analysis. Moreover, this technology opens up doors for inventing new MRS techniques aiming at, and for the first time, monitoring sub-microsecond dynamic changes in biomolecules. From an economic point of view, and since Canada is considered one of the international capitals of bio-pharmaceutical industries (with $4.2 billion contribution from pharmaceutical sector to Canadian GDP in 2011), the stakeholders for our technology are, potentially, all of the pharmaceutical companies operating on the Canadian territory. This new technology offers great opportunities for new start up companies to take the drug discovery research and development to a whole new level. This translates into an improved quality of life for Canadians.""560827,""Ghaffarizadeh, SeyedArman"
"567347"	"Ghodsi, Ali"	"Larg-Scale Data Analytics: Methodologies and Applications"	"Recent years have witnessed the rise of the big data era in computing and storage systems. With the great advances in information and communication technology, hundreds of petabytes of data are generated, transferred, processed and stored every day. The availability of this overwhelming amount of structured and unstructured data creates an acute need to develop fast and accurate algorithms to discover useful information that is hidden in big data. One of the crucial problems in the big data era is the ability to represent the data and its underlying information in a succinct and interpretable format. Although different algorithms for clustering and dimensionality reduction can be used to summarize big data, these algorithms tend to learn representations whose meanings are difficult to interpret. For instance, the traditional clustering algorithms such as k-means tend to produce centroids which encode information about thousands of data instances, but the meanings of these centroids are hard to interpret. Even clustering methods that use data instances as prototypes, such as k-medoid, learn only one representative for each of the resulting clusters; these alone are not sufficient to capture the insights of the data instances in this cluster. In addition, using medoids as representatives implicitly assumes that the data points are distributed as clusters and that the number of those clusters is known ahead of time. This assumption is not true for all data sets. On the other hand, traditional dimensionality reduction algorithms such as Latent Semantic Analysis (LSA) tend to learn a few latent concepts in the feature space. Each of these concepts is represented by a dense vector which combines thousands of features with positive and negative weights. This makes it difficult for the data analyst to understand the meaning of these concepts. Even if the goal of representative selection is to learn a low-dimension embedding of data instances, learning dimensions whose meanings are easy to interpret allows the understanding of the results of data mining and machine learning algorithms, such as understanding the meanings of data clusters in the low-dimensional space. The acute need to summarize big data to a format that is informative for data analysts motivates the development of new algorithms to directly select a few representative data instances and/or features. This problem can be generally formulated as the selection of a subset of columns from a data matrix, which is formally known as the Column Subset Selection (CSS) problem. Although many algorithms have been proposed for tackling the CSS problem, most of these algorithms focus on randomly selecting a subset of columns with the goal of using these columns to obtain a low-rank approximation of the data matrix. In this case, these algorithms tend to select a relatively large number of columns. When the goal is to select a very few columns to be directly presented to a data analyst or indirectly used to interpret the results of other algorithms, the randomized CSS methods do not produce a meaningful subset of columns. On the other hand, deterministic algorithms for CSS, although more accurate, do not scale to work on big matrices with massively-distributed columns. We propose to address these limitations by developing a new framework that we call Data Downdate. A large variety of important problems in machine learning and data mining such as variable selection, mining representative patterns, and most notably sparse approximation are all special cases of this framework.""561794,""Ghodsi, Ladan"
"566525"	"Goldenberg, Anna"	"Network-based machine learning framework for for data integration in medical applications"	"Recent technological advances have made it possible to assemble very large data collections – Big Data. Aside from the sheer scale, we now have access to multiple data types each describing the same phenomena in their own way. For example, in computer vision, it is now common to combine unlabeled images with text to label images more accurately. In biology and medicine, it has recently become cost-effective to collect genomic, transcriptomic, epigenetic, microbiome and other measurements that describe the state of cells and human bodies in health and disease. It has thus become essential to develop robust and efficient machine learning methods that can integrate multiple data types to gain deeper understanding of the phenomena measured by these data. Majority of existing integrative methods have limitations, e.g. they often require significantly more samples than features (not readily available in biological and medical applications); do not scale to the large number of available features requiring ad hoc feature pre-selection (Shen et al, 2009); do not deal with missing data and noise requiring substantial data pre-processing. In human studies, especially in childhood diseases, where the goal is to combine multiple types of measurements to understand disease mechanisms and reasons for phenotypic heterogeneity, the number of patients is very limited, whereas the number of measurements available for each patient is very large. Majority of existing methods are not applicable in this scenario. It is thus essential to ensure that new methods for biological and clinical data integration are scalable and robust to small sample sizes and many features. Together with my advisees we have developed a robust unsupervised approach to integrate multiple types of biological data, called Patient Network Fusion (PNF) (Wang et al, 2013) that addresses the issues above. Our results on five cancers show that we obtain more clinically relevant subtypes than those previously reported. We believe that networks in general and our approach for patient network fusion in particular set a perfect foundation for the comprehensive integrative framework that we are proposing to build in the course of the next five years. Our research program addresses the problem of data integration from several angles. Building on our recent successes we will develop methods to integrate more data types, specifically, low-signal-to-noise ratio data, such as single nucleotide polymorphisms, into our network-based framework. We will also extend non-negative matrix factorization to perform data integration of biological data using network regularization. These developments will serve as a broad base for the integration framework that will be tested on novel data obtained through our established clinical collaborations. Further, we will investigate several methods for integrative feature selection, i.e. identifying a small set of features stemming from multiple data types that explain majority of the variation in the data. Such methods are essential in shedding light onto the inner workings of biological processes and disease mechanisms. I work in close collaboration with clinicians at the Hospital for Sick Children and internationally to make sure that the methods are applied to real data and can be used and evaluated by clinicians in the process of their development. By working with a selected set of end-user collaborators, we will evaluate and refine our machine learning methods and user-interfaces, and ultimately develop a system that will impact the larger community of researchers interested in biological and medical data analysis.""558303,""Goldenhar, Katelyn"
"567701"	"Gong, Guang"	"Investigation of New Protection Mechanisms and Protocols for Security and Privacy of Smart Grid"	"The smart grid is a next generation power network system in which the generation, transmission, distribution, and management of electricity are upgraded and automated by incorporating advanced computing and communication technologies for improving the efficiency, reliability, economics, and safety of the grid. It consists of a collection of meters/sensors and controllers/actuators that communicate, usually wirelessly, with a substation/data concentrator, and various third party entities. A smart meter measures real time data of energy consumption, and transmits the data to the substation and potentially to a home area network. Communication between the smart devices and the substation is done usually with the ZigBee Smart Energy 1.x Profile, WiFi, or power line carrier (PLC). Communication between smart devices and a home area network typically uses WiFi, ZigBee, Ethernet, HomePlug, or Wireless MBus. The substation communicates with the utility company’s central servers through a wide area network (WAN) using wired connections, WiFi, satellite, or cellular. The smart grid holds enormous promise for improved efficiency, convenience, and sustainability; but it also pose a large risk for the security and privacy of the citizens and companies that depend upon them. Except it inherits all the existing cyber attacks on communication networks, smart grid is uniquely distinguished from all the information technologies (IT) networks or banking network systems in terms of the nature of attacks. The objective of an attacker may not be just gaining unauthorized information, it may exploit a wide range of cyber and physical layer attacks, which result in potentially making erroneous decisions on contingency analysis, dispatch, billing, or leading to a remote termination feature to blackout a region. The challenges for securing smart grid are stems from (a) wireless link runs on smart devices (e.g. ZigBee devices) with limited computational power, compared with smart phones, laptops, and desktop computers, and it requests real time response, (b) the heavy dependence on the heterogeneity of the communication network, (c ) the weak protection of the devices in the field to physical layer attacks, and (d) state estimation of control systems heavily relies on GPS (Global Positioning System) timing signals which are not integrity check protected.  The goal of the proposed research is to investigate a new paradigm in terms of the integration of crypto engine, randomness, transmission and modulation, code signalling, and the space redundancy of pervasiveness of ZigBee smart energy devices to provide lightweight unified protection on confidentiality, integrity and authenticity, and privacy preserving data aggregation with detection and anti-physical layer attacks for smart grid. The secure operation and communication in smart grid are important to prevent financial fraud, environmental accidents, and a host of other potentially disasterous incidents. The proposed research will provide state of the art research on privacy and security of smart grid communication and can help to provide insight into the right mix of secure communications technologies for smart grid deployments in their evolving process, which will benefit academic research, industrial activities and government agencies.""553567,""Gong, Max"
"555881"	"Goodman, Peter"	"A system for finding bugs in the Linux kernel"	"debugging, security, reliability, portability, computer science, operating systems, dynamic analysis, static analysis, binary analysis, compilers""561464,""Goodreau, Carley"
"567114"	"Gorniak, Peter"	"Game Design Intelligence"	"Video Games provide a unique challenge to Artificial Intelligence (AI) research. They take place in artificial worlds that are more easily sensed and acted upon than the real world. However, they are dynamic and contain sources of uncertainty such as player actions that are not easily modelled. They therefore occupy a middle ground between more static simulated environments on the one hand and robots on the other. Furthermore, they place strict limits on the available computing time for AI algorithms, and require support for controlling and managing these algorithms during game design and game play. Addressing these challenges of video games requires innovation grounded in experience with their restrictions. The research program I propose here aims to answer some of the challenges of video game AI design. Its first objective is to integrate academic AI algorithms with those commonly used in video games. Specifically, a project will research merging algorithms that perform automated planning with reactive video game algorithms known as behaviour trees. The results will contribute to academic AI research by expanding the use of automated planners to new domains, as well as video game production by making the use of automated planners accessible to non-researchers. A second objective of the research program is to investigate novel player models that do not require a priori models of the game being played. Formal models of video game worlds are usually not available during game development, but player modelling constitutes an important aspect of game AI. It supports predicting the player’s future actions so AI driven characters can anticipate them, and adjusting the game due to knowledge of the player’s current state. The algorithms to be investigated will utilize “keyhole” techniques that build online player models to perform predictions and offline player models to perform state analysis without relying on prior game models (as if watching the player through a keyhole). Modelling and predicting users without available application models constitutes a novel contribution to AI research and has the potential to transfer to industrial applications easily as it demands less investment from application and game designers. A final objective of the research program is to explore how gameplay based on AI algorithms can change gameplay when communicated to players visually. Often, the expressivity of AI in a game context is limited by the available means to represent AI states and decisions. Games tend to restrict themselves to means like character animations and audio cues in this context. Displaying abstractions of AI algorithms directly in game greatly enhances the possibilities of communicating their workings to players and enabling new types of gameplay. A few suggestive examples of these techniques currently exist in industry, and they indicate a large realm of possibilities that this project will explore. The results will impact academic research in AI and human computer interaction as they will open new avenues to interact with intelligent characters and environments. The game industry will benefit from this research as it will show ways to solve presentation problems for AI in games, and thus open up novel game design spaces.""562508,""Gorr, Adam"
"567239"	"GouinVallerand, Charles"	"Modélisation et conception de l'assistance sensible au contexte auprès de personnes en perte d'autonomie dans le cadre d'espaces urbains intelligents"	"Le vieillissement de la population au Canada comme dans la majorité des pays occidentaux est une réalité qui a des impacts importants sur la société canadienne. Ainsi, 25% de la population du Canada en 2036 sera âgée de 65 ans et plus. Cette forte proportion d'aînées dans la population aura des impacts sur notre société telle que l'augmentation des coûts pour les soins de santé. Fait également important, on note chez une part importante des personnes dépendantes, dont une majorité de personnes âgées, une baisse de la qualité de vie, due à un ensemble de paramètres incluant la perte d'autonomie. Cette baisse de la qualité de vie a des impacts importants. Les conditions sont donc réunies pour amener à des efforts soutenus dans la recherche de solutions permettant d'améliorer l’autonomie des personnes âgées et leur qualité de vie sur la santé mentale de ces personnes et sur le maintien de leur santé physique. L'essor de nouvelles technologies, telles que l’intelligence ambiante, ouvre de nouvelles voies quant à la façon d´interagir avec les systèmes informatiques, d´assister les utilisateurs et de communiquer entre des personnes. Les récents travaux sur les maisons intelligentes démontrent les bienfaits de ces technologies sur l'autonomie des personnes dépendantes (e.g. déficiences cognitives, physiques). Or, il est nécessaire d'étendre ces bienfaits à l'extérieur des environnements contrôlés. Les technologies mobiles et les villes intelligentes offrent le cadre pour créer ces nouvelles voies d'interaction et d'assistance auprès des utilisateurs. Dans le cadre de mon programme de recherche, je travaille à la modélisation, la conception et à l'implémentation de systèmes informatiques permettant d'assister les utilisateurs dans la réalisation de leurs activités de la vie quotidienne (AVQ) en cours de mobilité dans des espaces urbains, en tenant compte du contexte où les utilisateurs se trouvent et de leur profil. Mon premier objectif porte sur la conception, à l’aide de méthode sémantique, d’opérateurs d’agrégation et d’abstraction de l’information contextuelle à travers les couches micro et macroscopique du contexte, un modèle proposé dans nos travaux antérieurs. Mon second objectif de recherche porte sur la conception d’algorithmes de recommandation et de livraison sensible au contexte et adaptée au profil des utilisateurs, afin de recommander des services d’assistance lors de la mobilité d’utilisateurs dans les milieux urbains intelligents. Les approches théoriques guidant la conception de ces algorithmes seront basées sur le semantic matching et le flocking clustering. Enfin et non le moindre, mon troisième objectif consiste donc à la conception et au développement de services d’assistances qui tiendront compte du contexte et du profil des utilisateurs afin d’assister des personnes en pertes d’autonomie dans la réalisation de leurs AVQ en milieux urbains. Plus précisément, j’ai identifié deux types particuliers de scénarios d’assistance : (i) l’évaluation et la conception de services d’assistance à la conduite automobile pour personnes âgées dans le cadre de déplacements dans des milieux urbains; (ii) le développement d’une orthèse logicielle d’assistance à la réalisation d’emplettes chez les personnes atteintes d’un déficit cognitif léger. Ce programme de recherche sera supporté par une équipe de recherche composée de doctorants en informatique cognitive et d'étudiants à la maîtrise en technologie d'information et les travaux de l'équipe se dérouleront dans l'infrastructure de recherche du Laboratoire de recherche en Informatique Mobile et Villes Intelligentes (LIMVI), financée par le FCI et dont je suis le responsable principal.""565755,""GouinVallerand, Charles"
"567236"	"Gray, Bonnie"	"Microinstrumentation featuring flexible polymers: from discrete devices to full electronic and microfluidic systems"	"My research program focuses on the development of innovative microfabrication and enabling technologies for next generation microsensors, microfluidic systems, and microelectromechanical systems (MEMS). Through the Discovery Grant program, my lab (Microinstrumentation Lab, uiL) has pioneered essential enabling technologies through a concentrated and systematic approach. These enabling technologies have included advanced microfluidic flow control; development of micropatterned functional electronic and magnetic composite polymers (CPs); technologies to aid in integrated, chip-to-chip and chip-to-world microfluidic interconnect; and flexible microfluidic and wearable textile-based biosensor device-level technologies. The long-term goals of my research are to establish a comprehensive library of interconnects and microfluidic, microsensor, electronic, and optical components with well-defined functionality and interface requirements to facilitate development of new microfluidic instruments. I plan to integrate CPs into microfluidic systems on rigid and flexible substrates, including commercial microfluidic polymers and textiles, to achieve innovative system performance enabled by these functional materials. The applications of this research will be the creation of “technically fun and socially relevant” microfluidic instrumentation for applications including but not limited to: health care, athletic performance, hazardous environment exposure, immunology, food packaging, biological cell studies, and disease diagnostics. HQP trained during the duration of the program will become Canada’s next generation of technological leaders in these sectors. Over the next five years, the proposed research program will focus on the following objectives: 1. Development of a combined microfluidic and electronic framework (MEF), including power sources and devices with C-CP/M-CP structures, on flexible polymer and textile platforms. 2. Establishment of a library of system-level engineering building blocks of platforms, interconnects, and device packaging to facilitate systematic microinstrument development. 3. Development of methods to combine M-CPs with microfluidic thermoplastic polymers (TPs) to provide new microfluidic actuation mechanisms for these industrially-relevant material platforms. 4. Demonstration of the MEF for combining example devices relevant to wearable monitoring. New and exciting research into wearable electronics, functional nanocomposite polymers, flexible microfluidics, and commercial polymer microfluidics is reported on a daily basis. However, much of this research is specific to each field, despite the enormous potential that the convergence of these research areas has for the future of health care, worker safety, food safety, consumer devices, and commercial microfluidics. The proposed research program offers a new and innovative approach of powerful, general-purpose frameworks to systematically interface CP-based, microfluidic, and electronic devices simultaneously on flexible polymer, textile, or industrial polymer platforms. Such an approach offers practical solutions for industry in areas such as worker safety; UV-index, heart, and lactose monitoring for athletic clothing; and glucose monitoring. The integration of rare-earth M-CP actuators with commercial microfluidics will be pioneering, offering high-deflection, low-power actuators for increased portability. The strength of the proposed research program is its ability to provide technological frameworks to develop not just one instrument, but to provide essential, enabling technologies for on-going development of new wearable and portable microfluidics, placing Canada at the forefront of sustainable development in these areas.""567381,""Gray, Christopher"
"566105"	"Grewal, Gary"	"An Intelligent, Parallel Framework for Field-Programmable Gate-Array Placement and Routing"	"Since their inception circa 1984, Field Programmable Gate Arrays (FPGAs) have seen an enormous growth in usage because they can dramatically reduce design turn-around time and manufacturing costs for prototype circuits and small to high-volume products. This is made possible by the availability of reconfigurable logic blocks, macro blocks, and programmable interconnections on a FPGA. It has been estimated that the world-wide FPGA market will reach $10 billion by 2016, with communication, automotive, medical, computer storage, military and wireless all important end markets. However, as the feature size continues to shrink and device capacity continues to increase in modern FPGAs, the design-automation industry is facing an old problem: the time spent for placement and routing is still dominating the FPGA compilation process. While current placement and routing tools produce quality solutions, compile times can be on the order of hours or even days for the largest designs. Long run times not only adversely impact engineering productivity and costs, they act as a serious impediment to the adoption of FPGAs by software developers who are used to compilation times of seconds or minutes. The primary goal of this research proposal is to develop an intelligent, parallel framework that can help a designer reduce the amount of time spent compiling circuits for FPGAs by performing placement and routing more efficiently and intelligently. The novelty of this proposal lies in combining state-of-the-art machine-learning and data-mining techniques with innovative parallel placement and routing methods running on multicore and many-core parallel architectures. A key feature of the proposed framework is its ability to learn from past placement and routing problems to more effectively and efficiently solve future placement and routing problems. The overall significance of this work will be to provide Canadian industry with scalable, parallel FPGA placement and routing tools that can produce high-quality solutions, while avoiding excessively long compile times. The resulting tools will help reduce the design cycle time and, consequently, design cost. Both design time and design cost are very important considerations for companies manufacturing electronic products based on FPGAs.""561557,""Grewal, Jugvinder"
"555301"	"Grinberg, Yuri"		"NSERC"
"567102"	"Gu, Xijiag"	"High power all-fiber Q-switched and Mode-locked fiber lasers 
for industrial and medical applications"	"High power pulsed fiber lasers have recently become a topic of intensive research due to their high energy and high peak power, which are essential in many applications such as material processing, laser range finding, and second harmonic generation. Among many technologies for pulsed lasers, passively Q-switched and mode-locked fiber lasers are especially interesting because of their simple structure, easy maintenance, and low cost. Picosecond and femtosecond lasers with a pulse energy of several tenths of nJ and peak power at the kW level have been demonstrated. However, despite the progress made so far, many of the reported lasers may not be suitable for industrial and medical applications due to the structure and components used in their designs. For example, some Q-switched lasers use lenses and dichroic mirrors to couple light into and out of the fiber and saturable absorbers; which diminish the advantages of the fiber laser. Some mode-locked lasers use polarization controllers to adjust polarization, which is clearly not suitable for field applications. Many pulsed lasers employ semiconductor saturable absorbers (SESAM) or carbon nanotube absorbers, which are susceptible to high energy pulses and whose long-term stability remains an issue. These issues need to be resolved in order to develop high energy and high peak power pulsed fiber lasers, particularly those of stable, compact, and low-cost modules for industrial and medical applications. Because of this, I propose to develop high power all-fiber Q-switched and mode-locked lasers for industrial and medical applications over the next five years. I will concentrate my research on all-fiber designs so that all components will be spliced together with no optical alignment needed; this will make the laser compact, stable, and low-cost. My research will have two directions: 1. Q-switched laser: the goal is to achieve all-fiber Q-switched oscillators with over 200-µJ pulse energy and over kW-level peak power at variable repetition rates. The oscillator can be used to cut metal sheets by itself, or used as a seed laser to achieve 20- to 30-kW peak power with a single-stage amplifier. Fibers with different core diameters will be used for the gain medium and saturable absorber in the laser cavity. The optimization of the laser cavity structure, fiber core diameters of gain and saturable absorber, their lengths, and other design parameters will be studied. The dynamics of Q-switching will be simulated. 2. Mode-locked laser: the goal is to achieve stable output pulses of 10- to 30-ps pulse width and over 5-nJ pulse energy in linearly-polarized state. When this laser is used as a seeder, a one-stage amplifier will bring its peak power to multiple kW. One approach is to use a SESAM for mode-locking, in which an all-fiber coupling device will be designed to focus the beam onto the SESAM with a proper power density for stable mode-locking. Another approach is to use nonlinear polarization rotation as an equivalent saturable absorber so the power density will not be limited by the lower damage threshold of the saturable absorbers. The Q-switched laser with sub-mJ pulse energy and kW-level peak power can be used for precision cutting and drilling. The Q-switched laser with long pulses can also be used for welding. The mode-locked ps laser, due to its fast energy deposition, will significantly reduce the heat-affected zone in material processing, which will make it an ideal tool for precise micromachining of metals and semiconductors. It will also have many interesting applications in medicine.""560460,""Gu, Yan"
"567857"	"Gueaieb, Wail"	"A Smart Autonomous Pipeline Inspection Robot"	"Currently, very little internal inspection is conducted on gas pipe segments that are known to leak. The maintenance of commercial gas distribution networks is often based on statistical information, such as previous maintenance record, leak surveys, and corrosion data, more than scientific evidence. To date, the state-of-the-art of internal gas pipe inspection systems is based on tethered robot-like vehicles that are inserted in the pipes and operated manually through joysticks. They collect sensory data and store it locally so that it can be processed offline later by trained technicians to identify any signs of defects in the pipe. This process is very costly and time consuming. Typically, it takes a crew of technicians a full day to collect data for a pipe segment of 1 mile, and a week to process it. At this pace, it would be impossible to cover gas and oil distribution networks, which already passed 115,000 in Canada. In 2012, it costed Canadian gas and oil distribution companies $1.1 billion to inspect and maintain their mains. Finding more cost- and time-efficient ways to monitor and inspect distribution and delivery systems has become an urgent necessity, especially with the aging of the mains and the rapid growth of the network driven by high demand on energy sources. The goal of this research is to design and develop a fully autonomous untethered mobile robot capable of long-range navigation inside gas pipes of various sizes and structural configurations to perform in-situ pipe inspection. The robot will automatically diagnose pipes for different types of defects, including cracks, dents, corrosion, accumulated debris, and water infiltration. It will process and analyze its sensory measurements online and wirelessly communicate detailed reports back to a maintenance unit. This would facilitate identifying the exact locations of the defects, generate early warnings for potential anomalous segments, help deciding on the proper repair method (spot, relining, replacement, etc.), and decrease the response time in case of an emergency. The research will help insuring the reliability of Canadian gas distribution networks, which transport about 23% of Canadian exports (including oil exports). It will minimizes the chances of gas leaks, especially in remote areas where technicians cannot reach easily (eg., offshore), and so limit their side effects on the environment. Advancing the state of knowledge in this area will have a positive impact on other economic sectors by paving the road for the development of similar robot inspection systems for oil, water, and sewage distribution networks.""549549,""Guéguen, Céline"
"567364"	"Hafid, Abdelhakim"	"Mobile Vehicular Cloud"	"With the advancement of wireless technologies, vehicular networks support a wide spectrum of novel safety and infotainment services, such as emergency notification, collision avoidance, fleet management, and multimedia content sharing. Although, vehicular network research resulted in several key contributions in multitude of research directions (e.g., novel broadcast and routing protocols), little attention has been paid towards the abundance of computing, communication, sensing and storage resources embedded in vehicles, which if shared among users can give rise to vehicular services hosted in the vehicles themselves. In this project, we propose a novel vehicular cloud architecture that supports a wide range of services provided by mobile as well as stationary vehicles in a distributed manner; these services include Network as a Service, Storage as a Service, Sensing as a Service, Computational Intelligence as a Service and Location Information and Density Estimation-as-a-Service. For efficient support of vehicular cloud services, we aim at developing optimization models to compute optimal/near optimal configurations of vehicular cloud that satisfy certain QoS in presence of several constraints; given the complexity of these problems we will resort to heuristic/meta-heuristic approaches to solve the resulting models. In addition, we will develop novel management techniques for efficient bandwidth utilization, reliable transfer with reduced delay, lesser connections disruption and efficient vehicular density information transfer.""555293,""Haftchenary, Sina"
"555835"	"Hamilton, William"	"British Columbia"	"CANADA"
"555649"	"Harris, John"	"Improving Efficacy, Accessibility, and Socialization through Asymmetry in Digital Game Design"	"Human-Computer Interaction, Interaction Design, Touch Interfaces, Multi-Screen Environments, ComputerSupported Collaborative Work, Health and Wellness, Elderly Care, Digital Games""544996,""Harris, Laurence"
"566087"	"Haslett, James"	"High-Speed Low-Power Data Acquisition and Serial Data Communications"	"As the industry moves into the very deep submicron stages of integrated circuit device scaling, new techniques are being sought to improve performance of all types of analog and digital circuits. Vast amounts of digital logic can be placed in small chip area, allowing digital correction of analog circuit errors resulting from PVT (fabrication process, supply voltage and temperature) variations on circuit behavior. At the same time, the very small devices have small parasitic capacitances and resistances, resulting in much-improved switching speeds and analog bandwidths. Silicon-on-insulator (SOI) technology is particularly attractive, and results in lower manufacturing costs compared to bulk CMOS due to substantially lower mask count requirements during manufacture. These facts have lead to an examination of circuit techniques that move away from conventional voltage-based circuits to circuits that exploit the small switching times and timing differences that are available, to realize circuits such as Analog-to-Digital Converters (ADCs) and Serializer-Deserializer (SerDes) data communications circuits that are ubiquitous in modern electronic systems. Over the past 5 years, we have pioneered the first low-power time-based GigaSample/sec ADCs, enhancing performance using digital background correction of errors due to PVT variations, and also proposed new time-based Gigabit/sec SerDes architectures. Because this approach is new and exploratory, much work remains to be done in order to establish a comparison of performance limits compared to voltage-based circuits, and requires circuit fabrication and testing in the latest scaled SOI technologies. The ADCs that we have designed so far have a unique added advantage that an analog front-end voltage-to-time conversion circuit (a VTC) can be physically separated from a back-end Time-to-Digital Conversion circuit (a TDC), allowing designers to physically remove the noise-producing high-speed switching circuits from the sensitive analog front end, thereby improving the effective number of bits (ENOB) achieved by the converters. This could have special advantages in applications such as those in the world's next-generation radio telescope called the Square Kilometer Array, a 20-country international project in which we are heavily involved, and which will require possibly millions of low power, ultra-low-noise, ultra-high-gain, wideband signal chains for each antenna element in the vast array. The proposed circuits will also find applications in data and voice communications in next-generation software-defined radio, and in all aspects of data acquisition and transmission. In addition to the ADC research, I propose to examine new ways of increasing data throughput rates in SerDes systems without requiring more complicated and power-hungry circuits to transmit and receive the data. We have shown both theoretically and experimentally using field-programmable gate arrays (FPGAs) that high data rates can be achieved in time-based circuits using significantly lower clock rates that are required in conventional voltage-based SerDes systems. We have new ideas for increasing the data rates further without having to increase clock frequency. To compare performance with conventional systems, we need to design, fabricate and test complete chipsets in 28nm and smaller technologies. The ADCs can also be used as direct-digitizing receivers in conventional SerDes systems, and industrial firms are now moving in that direction for next-generation SerDes products. New high-performance TDCs are also being investigated by my research group, and those circuits have direct applications in Phase-locked-Loop (PLL) frequency synthesizers. We have a well-equipped measurement laboratory to support this research.""553752,""Hassall, Cameron"
"567786"	"Hassan, Ahmed"	"Mining Software Repositories to Support the Development and Operation of Ultra Large Scale Software Systems"	"The Mining Software Repositories (MSR) field explores innovative approaches to recover useful information from static record-keeping software repositories like source control and bug tracking systems. The recovered information is used to gain an empirically based understanding of software development practices. This knowledge helps practitioners predict, plan and understand various aspects of their project. Over the past decade, the international MSR community has explored innovative approaches to recover useful and actionable information from such static record-keeping repositories. MSR techniques are now integrated into developer toolkits at some of the world’s largest and most successful organizations: Microsoft, BlackBerry, AT&T Labs, CISCO, Mozilla and Avaya. As one of the founders of the MSR field, I believe that the full potential of MSR can only be achieved by taking MSR results and techniques beyond software development challenges. My ultimate goal is to establish MSR as a strategic decision-making instrument throughout the lifetime of software systems. However, the focus of research today remains on traditional software development where a clear separation exists between the development of the software and its operation, with development cycles lasting months or years. This separation is a key challenge to Ultra Large Scale (ULS) software services where fast and frequent releases are the norm. There is a clear need to leverage knowledge from both the development and the operation of software services. Two examples of benefits arising from this knowledge integration are: i) developers can use operation knowledge (e.g., logs) to better understand how their code is used in the field so they can optimize it, and fix or prioritize the fixing of bugs more effectively; and ii) operators can make use of source code documentation to rapidly understand, triage and resolve field problems. The long-term vision of the proposed program is to systematically uncover the potential and to empirically demonstrate the benefits of MSR in delivering significant improvements in the reliability and cost-effectiveness of today’s long-lived rapidly-evolving software. Short term goals over the next five years will focus on realizing this vision in collaboration with industry partners like BlackBerry, SAP and Avaya. Over the next five years the research program will progress along the following two themes: (1) the recovery, representation and linking of development and operation knowledge in a simple format in order to ease access to such knowledge; and (2) the development of methodologies, tools and approaches that explore and demonstrate the benefits of MSR in understanding and supporting software development and operation activities. In particular, the proposed research program will focus on the use of system logs as a mechanism to bridge the gap between software development and operation. Logs are messages generated from statements inserted by developers in the source code and are often used by operators for monitoring the field operation of a system. However, the rich knowledge in logs has not yet been fully-used because of their non-structured nature, their large scale, and the use of ad-hoc log analysis techniques. The research program proposed will train five highly qualified personnel (HQP): 3 PhD and 2 Master's students. All HQP will collaborate closely with Canadian companies, gaining an appreciation of the challenges facing practitioners on a daily basis and the potential of software data in addressing those challenges.""550861,""Hassan, Assem"
"566539"	"Haykin, Simon"	"Cognitive Dynamic Networks"	"SUMMARY Research into Cognitive Dynamic Networks (CDN), inspired by the human brain, will build on the pioneering work done on Cognitive Dynamic Systems (CDS), research into which has been carried out by Simon Haykin, McMaster University, for about the last 10 years. In descriptive terms, the study of CDN embodies two basically different entities, but integrated together into one project: The Memory, and the Network. The Memory part consists of a layered structure, wherein each layer consists of a set of three cognits (standing for cognitive units), namely: perceptual cognit for perceiving the environment; executive cognit for action on the environment; and working-memory cognit that mediates between the perceptual and execute cognits and stabilizes them. As for the Network, its function is to integrate the sensors (responsible for generating the observables), the memory, and motor controllers under a single umbrella. The net result of this integration is a powerful CDN, which is stochastic self-organized, and complex; it is therefore capable of processing Big Data under the constraint of limited computational resources. Whereas the Memory looks to Cognitive Neuroscience for inspiration, the Network looks to the new fast-accelerating, broadly defined Network Science for mathematical and computational need The CDN project embodies three tasks, summarized as follows: Task 1 involves implementation of the CDN, based on a new software module that can accommodate the processing of Big Data for relevant (i.e., valuable) information. Task 2 studies intrinsic properties of CDN, namely, Stability, Robustness for trustworthiness, and Emergence behaviour, as well as extrinsic properties of CDN, namely, Observability, Controllability, and Universality. Task 3 addresses three areas for which CDN could make a practical difference: Arctic studies, the Power grid, and National security across the Canada-United States border along the Great Lakes.""567452,""Hayley, Shawn"
"567209"	"Hengartner, Urs"	"Protecting Privacy with Smartphones"	"Many people carry a smartphone with them throughout their daily lives. These smartphones are equipped with various sensors, such as GPS or audio, and have access to a lot of data, such as a person's photos, calendar, or contacts. A person may share some of this data with other people. Sometimes this sharing is not expected, resulting in a violation of the person's privacy. Imagine Alice has configured her smartphone to share her location with her family members. However, one day Alice changes her routine and enters a store to buy a birthday present for her spouse. Alice forgets that she is sharing her location with her family, the surprise is spoiled and Alice's privacy is violated. Alternatively, imagine Bob gives his co-workers access to his location with the expectation that they will retrieve this information only if there is a valid need, like having an impromptu meeting. However one day Bob notices that his location is continuously being accessed by one of his co-workers for no valid reason. As a consequence, Bob feels stalked and his privacy violated. It would have been possible for a person's smartphone to avoid the privacy violations in both scenarios by sensing the person's context and detecting unusual events, such as the user being at a new location or an authorized user sending an unusual number of queries. The argument made in this proposal is that today's smartphones have sufficient context information about their owner in order to build a common behaviour model for the owner and to use this model to detect uncommon behaviour that could potentially result in privacy violations. In more detail, I will develop privacy-enhancing technologies that allow a smartphone to protect its owner by detecting two kinds of potential privacy violations: First, violations occurring as a consequence of the owner using an information-sharing app, where the violation is triggered by the owner making a change to her daily routine or by changes to the usual context of the smartphone and its owner. Second, violations occurring as a consequence of the owner using an information-sharing app, where an authorized user of the app accesses information in unintended ways, such as issuing overly frequent requests for the information or asking for the information at unusual times (e.g., at night). Furthermore, I will develop privacy-enhancing technologies that allow a smartphone to protect its owner by not only detecting, but also avoiding potential privacy violations. The objective is to have a smartphone avoid the privacy violation in an automated and intelligent way that limits the involvement of the smartphone owner, prevents sensitive information from being shared, but still makes possible the sharing of information useful to the person asking for this information. For example, a smartphone could automatically replace sensitive information (e.g., Alice's current location) with less sensitive, but still useful information (e.g., Alice's estimated time-of-arrival at Carol's location). Smartphones and their ability to run user-installed apps have caused a lot of concern when it comes to the privacy of mobile users. The long-term goal of the proposed research is to demonstrate that today's smartphones (and future mobile devices, such as Google Glass) have enough context information available to them in order to pro-actively protect the privacy of mobile users. The proposed research will make important contributions to research in privacy-enhancing technologies for mobile platforms. These technologies will encourage more people to use smartphones and smartphone apps. This will benefit the entire smartphone ecosystem (including app developers, smartphone providers, and network operators) and therefore the economy at large.""564241,""Heni, Nazih"
"567536"	"Hirst, Graeme"	"Applied computational models of discourse, argument, and text"	"Our research is in computational linguistics (CL), natural language processing (NLP), and their applications. Themes that run through the work are (i) computationally determining the structure of discourse and argumentation; (ii) computational notions of paraphrase and of the semantic distance between words, larger linguistic expressions, or documents; and (iii) the use of statistical classification methods in text analysis. We propose the following new research: (1) Finding the structure and framing of discourse and arguments: Understanding a speaker's argument entails understanding not only what is presented as evidence and what as conclusion but also how the conclusion follows from the evidence, what the unstated premises (enthymemes) are, and what the implicit framing of the issue is. We will look in particular at opinionated texts in which argumentation structure is fairly explicit on the surface but determining enthymemes and framing is crucial to full understanding. For automatic text analysis, we take quantifiable semantic characteristics of the speaker's presentation of a position as indicators or proxies of the framing, which can then be interpreted qualitatively. In a simple analysis, this could be merely a statistical analysis of the key concepts of the text, as denoted by content words and significant collocations -- something like a topic model. Here, however, we propose a novel, more-sophisticated analysis in which we also look at the actual argumentation structures and discourse relationships of the text and how the concepts adduced by the lower-level linguistic components are used in these structures. This will draw on and extend our recent work on discourse parsing and the identification of argumentation schemes in text. Although these are difficult tasks for which the state-of-the-art is far from perfect, we hypothesize that typical political speech contains a sufficiently well-cued discourse structure that the analyses that we can achieve, although still quite imperfect, will be usefully indicative of issue framing. (2) Finding precedent scientific literature: Researchers often have difficulty searching for past research relevant to, or precedent to, their new or proposed research, and often resort simply to Google keyword searches, which are rarely adequate. We will develop methods for searching scientific literature that use semantic and structural relationships to find publications that are possibly relevant to a new text. We will concentrate in particular on the legacy literature of biodiversity, for which conventional keyword searches are almost invariably insufficient because in this literature, more so than most other fields of science, related concepts are often described or explained in different terms, or in completely different conceptual frameworks, from those of contemporary research. As a result, relevant legacy publications, or even whole literatures, may remain hidden to term-based methods. This goal will not be reached in five years, but it motivates the next stage of our work because it requires bringing together many of the methods of natural language processing developed in our own and other researchers' work of the past decade or more: (a) the recognition of paraphrase and of textual entailment, and measurement of semantic similarity at the sentence level and above; (b) the automatic analysis of the structure and argumentation of scholarly papers and scientific discourse (this is a point of overlap with (1) above, but in scientific texts we expect the micro-structure to be less explicit and the macro-structure more explicit than in opinion texts). (3) We will continue our work on automatic authorship identification; on characterizing aphasic speech; and on the philosophy of CL.""565708,""Hirst, Graeme"
"555355"	"Horkoff, Jennifer"	"Requirements Models as Part of the Software System Lifetime"	"Computer Science, Software Engineering, Requirements Engineering, Conceptual Modeling, Model Reasoning, Goal Modeling, Requirements Modeling, Model Uncertainty, Model Driven Engineering, Business Intelligence Modeling""563474,""HorlockRoberts, Kathleen"
"566444"	"Houghten, Sheridan"	"Computational Techniques for Applications in Bioinformatics and Coding Theory"	"When faced with a difficult scientific problem, the number of possible solutions to consider can be overwhelming. Using a computational search makes it possible to attack problems that would otherwise be too difficult. In a computational search we explore the enormous space of possible solutions to produce results. But not all searches will produce results in a reasonable time: from a Computer Science standpoint, the true problem becomes one of determining how best to manage the search space. To be effective, a combination of strategies must be used. My main objective is to develop and evaluate algorithms and methodologies for computational searches. These will be applied to difficult problems in Bioinformatics and Coding Theory, of importance not only for theoretical reasons but also because of real-world applications. In each case, determining the appropriate algorithms and methodologies will allow us to obtain solutions that would otherwise be unattainable. In Bioinformatics, algorithms relating to DNA sequencing and protein modeling will be developed. DNA sequencing technologies are rapidly advancing and have application in developing personalized medicine. Current sequencing technologies do not allow for an entire genome to be sequenced in one piece; rather, the genome is broken into small pieces that must be reassembled. The success of sequencing technologies relies on the accuracy of the computational techniques employed in assembly. In modeling a protein we are conducting a search, over an enormous search space, to try to find the best arrangement of the protein’s components in space. This may mean trying to find a model that provides a good match to other data, or one that has the lowest energy. Modeling a protein’s structure is crucial to understanding its function. Error-correcting codes have an incredibly wide range of applications. Traditionally used as a means of ensuring reliable transmission of data, it has been realized that they are of use in numerous other areas, including DNA sequencing, flash memories, quantum computing, DNA computing, and many more. The wide range of newer applications, however, requires in turn a variety of codes that in many cases differ significantly from traditional codes. Searching for the best codes for different situations is one of the main tasks that I will undertake. Some codes are mathematically impossible; however, determining whether this is the case is often a difficult task. For some codes, their existence is a long-standing open question. To use codes successfully, one must also address the issue of decoding – correcting errors when they occur. In all of the problems to be considered, the problem structure and search space must be carefully analyzed to determine an appropriate search strategy. We differentiate between searches in which we wish to find all solutions satisfying a set of predefined criteria, and those in which we wish to find one or several optimal solutions. In the first case, we have no choice but to consider the entire search space, although by applying restrictions based on domain knowledge we can eliminate infeasible solutions and drastically reduce its size; here, exhaustive searches using combinatorial techniques are appropriate. In the latter case, it may not be necessary to examine the entire search space, particularly if the problem does not require a solution that is guaranteed to be the best but rather one that is the best known; here, metaheuristics such as evolutionary algorithms are to be considered.""561894,""Houghton, David"
"566788"	"Ikki, Salama"	"High Spectral and Energy Efficient Multi-User Large MIMO (MU-MIMO) Wireless Systems"	"A major concern in beyond 4th generation wireless systems (or 5th generation wireless systems) will be providing seamless global roaming, broadband access, highly interactive multimedia (data/voice/internet) services such as video-conference, multi-party and virtual tele-presence any-time everywhere. Achieving such ubiquitous high-capacity acceptable-quality transmission/reception with energy efficient usage requires several giga bits per second (Gbps) to be transmitted over the wireless channel (air). To acquire this, different advanced techniques have been proposed, among which Large Multiple-Input-Multiple-Output (MIMO) has recently come into view as a significant breakthrough for improving the capacity and energy far beyond systems in use today. In particular, it has been proven that wireless systems with a large number of antennas can attain an indicative increase in data rate/spectral efficiency and communication reliability, and excessive reduction in the amount of total transmitted energy. In recent years, there have also been many reports from the industry, regarding fabrication and testing of the MIMO systems with a large number of antennas, for example, the 12 transmit and 12 receive antenna (12X12), single-user MIMO system by NTT DoCoMo, which can gain a data throughput rate of 5 gigabits-per-sec (Gbps) over a bandwidth of 100 mega-hertz (MHz). Furthermore, the expansion of WiFi standards (the progression from IEEE 802.11n to IEEE 802.11ac to obtain multi-gigabit rate transmissions in 5 GHz band) now considers the 8 transmit and 8 receive antenna (8X8) Multi-user MIMO (MU-MIMO) operation. The main two advantages of Large MU-MIMO are as follows: First, with a very large number of antennas, the channel vectors are nearly-orthogonal and as a result, multi-user interference can be dramatically reduced. Consequently, with high data rate and less transmit energy, many users can be simultaneously served. In particular, with coherent processing, transmit energy can be reduced considerably due to the array gain. Second, this aspect of Large MU-MIMO systems is of great importance, due to the accelerated increase in greenhouse gas emissions resulting from operational energy consumption in base stations. Indeed, energy-aware design is one of the main objectives for the next generation of communication networks. Particularly, the GreenTouch Consortium (a consortium founded by Alcatel-Lucent Bell Labs., and consisting of leading telecommunication companies and research groups), recently introduced wireless systems with a large number of antennas as a key solution for high spectral efficiency and power efficient wireless systems. The first objective of this proposed program is to examine whether and to what extent the promising benefits of Large MU-MIMO systems can still be secured under practical constraints/limitations of the wireless channels. The second objective is to design low-complexity signal processing algorithms and techniques which can secure the potential benefits of practically constrained Large MU-MIMO systems. It has been shown that Large MU-MIMO systems could be used for achieving further reductions of the energy consumption of wireless communications. Although there is increasing interest in this topic, no comprehensive study has been undertaken so far to examine how the antenna array size, the modulation size and transmission power need to be selected in order to attain energy and spectral efficient communications, hence our third target. The fourth objective is to develop new physical-layer security concepts. To do so, efficient, accurate and fast estimation techniques, taking into account the channel time coherence, will be considered to ensure secure communications.""550529,""Ikura, Mitsuhiko"
"555323"	"Inglis, Tiffany"	"Multi-resolution Abstraction and Rasterization"	"Computer science, Computer graphics, Rasterization, Image abstraction, Pixel art, Nonphotorealistic rendering, Visualization, Computational aesthetics, Algorithms, Graphic design""546358,""Ingolfsson, Armann"
"555327"	"JabbarzadehGangeh, Mehrdad"	"Ontario"	"CANADA"
"567591"	"Jacob, Christian"	"Interactive Modeling of Multi-scale Biological Systems"	"I am interested in understanding in emergent properties of seemingly complex systems, where “the sum is greater than its parts”. For more than a decade, I have been collaborating with biologists, medical researchers, and medical educators to design and test computational models of biological systems. Biologists tend to think about their objects of study as discrete entities. Computationally, these entities can be represented by ‘agents’, whose interaction rules are known or can be identified, and ultimately represented by ‘simple programs’ with concise algorithmic descriptions. Once such interacting elements are arranged within compartments, which are organized into interconnected larger units (‘universes’), complex behaviors emerge. As a computer scientist I look at how to capture, illustrate and make seemingly complex systems explorable through 3-dimensional, interactive computer models. The agent-based models, that we develop in my Evolutionary & Swarm Design research lab, help to communicate our fundamental understanding of complex systems. Going beyond carefully crafted animations, the development of new algorithms and methodologies to build such models does not only enhance research in “systems” sciences but creates new computational approaches for generating, visualizing, and exploring complex models in simulated 3D spaces. “To see is to begin to understand”. And to explore our own learning paths through interactive models deepens our understanding. The next steps in my research focus on how to integrate multi-scale models across different levels of resolution in space and time. My current research focus is on how to combine continuous mathematical models with agent-based approaches. The mathematical models capture concentration changes averaged over a large number of particles (such as molecules), whereas agent methods are a better fit for representing entities that appear in small quantities, and where specific interactions among these entities (such as proteins or cells) can be modeled to a high level of detail in 3D. As many complex systems are composed of a hierarchy of subsystems across a range of scales (e.g., from systems and organs to tissues, cells, and proteins), multi-scale computational models need to incorporate and integrate processes that occur at different spatial scales and at different resolutions in time. Making multi-scale models accessible is another important aspect in my research. I investigate user interfaces for visualization, navigation, presentation, and exploration of model hierarchies across temporal and spatial scales. At the same time, I want our models to be run in real time while maintaining interactivity at any point. I strive to make complex models accessible to a wider user base, by exploring solutions for running simulations on mobile computing platforms.""558615,""Jacob, Marcus"
"567059"	"Jaeger, Nicolas"	"Silicon Nanophotonic Devices"	"The research proposed here continues successful research led by the applicant. The work to date has been in several directions. One direction is in the area of ultrahigh-speed, integrated-optic modulators for use in photonic networks, another direction is in the design of novel filters and filter-systems based on ring resonators, gratings, and combinations of ring resonators and gratings, and yet another direction is the design of novel photonic sensors. The devices proposed will, primarily, be fabricated on the Silicon-On-Insulator (SOI) platform. The work has two thrusts, one thrust of the work will be to achieve commercial specifications in the devices fabricated while the other thrust will be purely exploratory and aimed at developing novel device and system architectures. The objective of the work in the area of ultrahigh-speed modulators is to research novel modulator designs that increase operating speed, reduce on-chip real-estate, and lower power consumption, as compared to modulators currently in use, and that facilitate the development of next generation and generation-after-next fiber-optic interconnects and telecommunication systems. The modulators to be investigated will be novel electro-refraction type ""push-pull"" devices using the ""plasma-dispersion"" effect in reverse biassed silicon pn junctions. This work is significant in that modulators operating at the speeds proposed (>40Gb/s) and requiring low drive powers will be needed in next-generation and in generation-after-next fiber-optic interconnects and telecommunications systems. Modulators with both interferometric and polarimetric outputs will be explored. This work will help Canada maintain its position as a leader in telecommunications in telecommunications related technologies. The objective of the work in the area of novel filter design will be to achieve telecom quality (based on ITU-T Recommendations) filters fabricated on the SOI platform. These filters will be fabricated using resonators, gratings, and combinations of resonators and gratings. For example, we have recently demonstrated contra-directional grating-coupled cascaded racetrack resonators exhibiting the Vernier effect capable of achieving interstitial peak suppressions of 29 dB, and eliminating the free-spectral-range of the filter, at the drop port. We have also demonstrated apodized, Bragg-grating-based contra-directional couplers with 30 dB side-lobe-suppression at the drop-port. Nevertheless, further improvement is needed before these filters will be ready for implementation in wavelength-division-multiplexed systems, or dense-wavelength-division-multiplexed, systems. This work will also help Canada maintain its position as a leader in telecommunications in telecommunications related technologies. The objective of the work in the area of optical sensors is to expand the application of integrated-optic sensors and sensor systems within the power industry. The high-speeds, immunity to electromagnetic interference, and non-intrusive nature of optical sensors make them ideal for use in equipment monitoring in power distribution systems. Such sensors will facilitate more efficient power delivery and the production of high-quality power that is demanded in a modern, deregulated industry. This work is significant in that optical sensors offer major improvements over conventional sensors in areas such as sensitivity, accuracy, bandwidth and galvanic isolation. Fiber-optics-based sensors have a large role to play in future power transmission and distribution systems. This work will help Canada remain at the forefront of fiber-optics-based sensor development for power utility applications.""545998,""Jaeger, Wolfgang"
"555980"	"Jaques, Natasha"	"56"	"Québec"
"567918"	"Jatskevich, Juri"	"Modelling and analysis of power and energy conversion systems"	"Electrical energy systems today continue to gain importance. Electrical machines and power electronics penetrate deeper into all aspects of modern life, and are widely used in large-scale power grids, industrial processes, the automotive/transportation industry, home appliances, etc. Thousands of engineers in control centers and research facilities around the world are working full-time developing models and conducting studies of various grid scenarios. Sophisticated computer models are used for operation of electric grids, planning purposes, design and integration of various renewable energy sources and systems, etc. In typical applications, the models are used for simulating time-domain transient studies and/or small-signal analysis with various control objectives. Since the models are used by engineers and researchers many times (often iteratively during the design cycle), both the model accuracy and numerical efficiency (simulation time) are very important. Even a fractional increase in simulation speed will result in very significant savings of engineering man-hours world-wide. However, models of rotating electrical machines and power electronic components are typically the limiting bottleneck in terms of simulation speed and size of systems that can be practically modelled. Canada has been a leader in developing the state-of-the-art solution approaches and computerized tools that enable design and operation of electric power systems of various scales. The electromagnetic transient programs (EMTP), e.g. ATP, Microtran, EMTP-RV, PSCAD, RTDS, RT-Lab, and the Matlab’s SimPowerSystems, and Powertech Labs’ transient stability software DSATools, to name a few, all have originated and/or have been developed by Canadian researchers and are now overwhelmingly used throughout the world as industry-standard tools. The proposed research continues UBC’s long-time tradition of advancing the power systems analysis and simulation tools. The particular focus of this research program is on developing the most computationally efficient and advanced models of rotating electrical machines and power electronic converters for the EMTP-type and state-variable-based programs (including transient stability tools). We are developing advanced synchronous and induction machines models that achieve constant-parameter interfacing circuits (and constant conductance sub-matrix) and avoid the limitations and numerical instability of the traditional qd0-models. We are also developing a revolutionary parametric approach for constructing the dynamic average-value models of switching converters and machine-converter systems. Such models can potentially be automatically generated and realized in many industry-grade software tools. Making the average-value models readily available will have a tremendous impact on how the simulation tools are used by engineers and researchers. The proposed research will enable the next generation of transient simulation tools for real-time and non-real-time use with new capabilities and extended range of applications, capable of much faster simulations of large-scale systems. These tools are essential in enabling the new paradigms of combining the existing centralized electric grid with the elements of decentralized microgrids and alternative energy sources and storage, which together define the evolving future smart grid.""554584,""Jatskevich, Juri"
"555279"	"JavanRoshtkhari, Mehrsan"		"NSERC"
"566320"	"Jeffrey, Ian"	"Optimization and High-Order Fast Algorithms Applied to Microwave Imaging"	"The research program proposed herein aims to develop a set of high-performance computational tools that can be applied to optimize the design of microwave imaging (MWI) systems and to use these tools to discover and design state-of-the-art next-generation MWI systems for biomedical and agricultural applications. The motivation for this research is based on recent work that suggests proper system design and modelling will improve MWI resolution, making MWI more amenable to imaging applications. The proposed computational methods will reduce both the computing power necessary to perform MWI and the complexity of the systems, resulting in lower costs for industrial installations. MWI has been the subject of significant research in the areas of biomedical imaging, security measures and non-destructive quality assurance. We have recently extended its application to the area of monitoring the quality of stored grain crops where there is a need for a tool that is sensitive to the entire contents of a storage container. MWI is attractive because it is both safe (non-ionizing) and inexpensive. The goal of MWI is to non-invasively reconstruct a model of the electrical properties of an irradiated target from a sampling of the electromagnetic fields external to the target. Knowledge of the target properties has practical uses such as tumour detection in biomedical applications, and early detection of rot conditions during grain storage. The adoption of MWI for many applications has been hindered by the relatively low resolution obtained from standard MWI systems, despite the fact that there is no known resolution limit except the signal-to-noise ratio in the measured data, and by the computational cost associated with generating images. Historically, research on MWI has been focused on inversion algorithms. More recently, attempts have been made to quantify the amount of retrievable target information contained in the data as a function of noise. To complement this work, effort will be devoted to improving forward solver accuracy and adjusting controllable system parameters (transmitter/receiver position/type, profile/boundaries of the external medium) in an attempt to improve and/or optimize MWI resolution capabilities and minimize modelling error. To accomplish these goals we will develop a novel parallel, high-order, frequency-domain, software package for solving Maxwell’s equations and then apply this numerical tool to optimization procedures for determining the best transmitter/receiver configurations and external electromagnetic property profiles that balance the cost of system implementation with the accuracy of the images that are produced. I will use the software tools to design, implement and test innovative MWI systems for breast cancer detection and grain storage monitoring. The resulting MWI systems will provide enhanced resolution and faster image generation times at a lower cost. These tools will also enable Canadian research groups to improve the imaging accuracy of their own MWI systems for breast cancer detection, with the goal of a robust, affordable, mass-screening tool to permit early diagnosis of one of the leading causes of premature death amongst Canadian women. Grain-storage MWI systems are innovative, novel, and important to Canada for securing our grain stores for both domestic consumption and export. This work will be undertaken at the University of Manitoba, an institution with an established record for research in both MWI, at the Electromagnetic Imaging Lab, and grain storage monitoring, at the Centre for Grain Storage Research. The outcomes of the proposed research will strengthen Canada’s role in developing emerging technologies that will benefit both Canadians and the global population.""545816,""Jeffrey, Lisa"
"567964"	"Jiang, ZhenMing"	"Leveraging System Behaviour Data to Improve the Process of Load Testing Large Scale Software Systems"	"Many large scale software systems ranging from e-commerce websites (e.g., Amazon and Ebay) to telecommunication infrastructures (e.g., BlackBerry) must support concurrent access to thousands or millions of users. Studies show that many field problems of these systems are due to their inability to scale to meet user demands, rather than feature bugs. The inability to scale causes catastrophic failures and unfavorable media coverage (e.g., the botched launch of Apple’s MobileMe). To ensure the quality of these systems, load testing is a required testing procedure in addition to conventional functional testing procedures (e.g., unit testing and integration testing). Load testing is gaining more importance, as an increasing number of services (e.g., Apple’s iCloud and Google Drive) are being offered in the cloud to millions or even billions of users. Load testing, in general, refers to the practice of assessing the system behavior under load. A typical load test uses one or more load generators that simultaneously send requests to the system under test. During the course of a load test, the system under test is monitored and gigabytes or terabytes of system behaviour data (e.g., performance counters and execution logs) is recorded. The system behaviour data, which is widely available in large scale systems to support problem diagnosis and remote issue resolution, contains rich information about the external environment (e.g., network latency or the rate of the requests) as well as the internal execution state of the system (e.g., request failures or the size of the request queues). However, due to the size and complexity of such data, load testing practitioners currently use it in a limited manner: mainly for ad-hoc high level manual checks (e.g., crash checks and memory leak checks). Little software testing research has been done to improve the load testing process with such valuable information. The long term goal of this research is to leverage the rich information contained in the system behaviour data to improve the process of load testing large scale software systems. This proposed research aims to improve the theory and practices in all three phases of a load test: test design, test execution and test analysis. The following short term research objectives are proposed towards the long term goal: (1) Systematically Validating Load Test Suites (1 PhD student); (2) Adaptive Load Test Execution (2 Master's students); (3) In-depth and Scalable Load Test Analysis (2 PhD students). The expected research outcome will be useful for load testing practitioners and software engineering researchers with interest in monitoring, testing and analyzing large scale software systems.""563831,""Jiang, ZiHua"
"566048"	"Jing, Yindi"	"New MIMO Techniques for Future Generation of Wireless Communications"	"The demands in wireless communications for higher capacity and better quality are constantly increasing. It is predicted by extrapolations of current growth trends that we will experience a 1000-fold increase in data-rate requirement by 2020. In meeting such significant demand increase, MIMO (multi-input-multi-output) techniques are widely believed to be key elements. They have the capability of offering significant boost in data-rate, efficiency, and reliability without additional cost in bandwidth or transmit power. MIMO techniques have been well-developed and employed in current wireless systems, such as the 4th Generation (4G) wireless systems. However, today’s MIMO techniques fall short of the dramatic demand increase for the next generation. I propose to develop new MIMO techniques and concepts for wireless communications of future generation. More specifically, I propose to research on the following three aspects. 1. Multi-User Massive MIMO Designs. Massive MIMO refers to systems whose antenna array size is an order of magnitude more than current multiple-antenna systems, such as a station with hundreds of antennas. It is an emerging technology and preliminary research shows that it has great potential in data-rate, energy-efficiency, reduction of latency, robustness, and multi-user communications. To fully understand and utilize the benefits of massive MIMO, customized designs and new frameworks for performance analysis are needed. In this research program, I plan to work on effective transceiver designs, hardware-economical antenna selection methods, and precise analysis of the fundamental performance of multi-user massive MIMO systems. 2. Energy Efficiency Designs. As the popularity of wireless users and the volume of wireless traffic rapidly multiply, the energy consumption in wireless infrastructure drastically increases. As a result, green communications have attracted significant attention in recent years and will continuously become more important for the next decade. In this research program, I will seek novel energy efficiency measures for multi-user MIMO communications, derive energy efficient MIMO designs that can at the same time maintain the quality-of-service, and analyze the performance of the proposed designs. 3. Channel Information Acquisition for MIMO Systems. Research show that the quality of MIMO communications depends heavily on the availability and accuracy of the channel information. New MIMO techniques bring fresh questions and challenges in the acquirement of channel information. Current MIMO channel estimation schemes are based on linear model and Gaussian estimation. I will develop new non-linear estimation models and rules which potentially achieve better performance in estimating MIMO channels with reduced-rank and with correlation. I will also study the channel estimation for multi-user interference MIMO channel for both traditional MIMO systems and massive MIMO systems. In this research program, I target at proposing novel MIMO concepts and technologies that can fulfill the demands of next generation wireless communications, with high date-rate and high energy-efficiency being the two major objectives. At the same time, I focus on low complexity designs that are scalable and can be applied to systems with a large volume of users and high dimension. The outcomes will help uncover the features of next generation wireless communications. The proposed techniques can provide guidance in the design, analysis, and implementation of next generation wireless systems. I also expect new research directions and ideas to be opened up through the research program.""548540,""Jirasek, Andrew"
"567717"	"Julien, CharlesAntoine"	"Improving Retrieval of Unstructured Information using existing Information Structures"	"Unstructured information generally refers to text-heavy information that is not organized in a pre-defined manner such as what is found on the Web or in collections of texts. Most developments in unstructured information searching have addressed the technical algorithms for retrieving billions of Web pages; meanwhile, interfaces used for Web searching have not evolved at the same pace. To successfully use current Web searching tools, users must generally possess or acquire the vocabulary used by the authors of the relevant documents, and searchers inevitably encounter information needs that they cannot adequately express using one or two vague and broad keywords that often have multiple meanings. This is the vocabulary mismatch problem that plagues information retrieval systems—especially when searching unfamiliar knowledge domains or subject areas. This research program seeks to develop novel online information searching tools that bridge the gap between organized information (e.g., scientific, library, business or personal information collections) and Web searching. It assumes that searchers seek information to meet their needs, regardless of whether the information is structured (e.g., scientific, library or business information) or unstructured (e.g., Web searching or text). For example, structured information can suggest new keywords to better describe users’ needs. This research program capitalizes on existing information organization investments to complement unstructured information retrieval technologies. It will ensure that the tools are useful and appreciated by recording how test participants use the tools over a period of at least 3 months. Published results will include open-source online search tool prototypes, a testing engine that could be used by other interface designers and researchers, and results of the usability testing over time. This research program is innovative by virtue of the novel search systems it will design and test over time. Firstly, it will integrate existing information organization investments with the ubiquitous keyword searching and ranking in order to improve information discovery. For example, users could search a library catalogue and the Web using one integrated tool instead of two different tools they must currently use. Secondly, the tools will be tested over time to ensure that they meet searchers’ expectations and require little or no training. This type of testing over time is very rare and highly appropriate when the objective is to ensure users can truly use and appreciate a tool beyond its initial novelty. This research aims to support students who are interested in improving information exploration and searching technologies: 88% of the budget is given directly to students (i.e., salaries, travel expenses, and computers). The supported PhD and master’s students will be part of an existing research group where they will have the opportunity to collaborate with other research groups from the School of Information Studies, McGill and the University of Montreal. They will acquire skills in research, software design and development, testing, and oral/written communication, which are valuable in academic and industrial settings. Taken as a whole, this research program has the potential to improve the tools Canadian citizens use to search for all kinds of information by suggesting new keywords, grouping similar information together, and tearing down the artificial boundary between organized information collections (e.g., library catalogues or business taxonomies) and Web searching.""554619,""Jumaeva, Gulshermo"
"567597"	"Kabir, MZahangir"	"Photoconductors for radiation imaging detectors: materials issues and device designs"	"There is a huge demand for lowering irradiation dose in various medical radiation imaging modalities especially in general X-ray radiography and real-time radiation imaging. After last two decades of extensive research, amorphous selenium (a-Se) based direct conversion flat-panel digital X-ray detector (the incident X-rays directly generate charge carriers in the photoconductor layer) is recently commercialized for digital mammography. The a-Se detector is not perfect and the main drawback of the conventional a-Se detector under normal operation is its low sensitivity compared to other potential photoconductors such as polycrystalline lead oxide or mercuric iodide. Low sensitivity gives low signal to noise ratio in low-dose imaging and thus severely affects the diagnostic features of the image. Very low-dose medical X-ray imaging could be achieved by; (1) utilizing avalanche multiplication process at a very high electric field in a-Se layer for higher charge signal, and/or (2) replacing a-Se by other potential photoconductors that can provide higher collected charge. Low-cost X-ray imaging can be obtained by utilizing organic photoconductors in X-ray detectors. The research on avalanche a-Se solid state imaging detector is in a very premature level; it needs extensive research works to clearly understand the fundamental underlying physics of carrier generation, multiplication, and transport mechanisms, and to optimize the detector design by examining imaging performances. The PI's research program will endeavor to develop low-dose and/or low-cost medical X-ray/optical imaging detectors by extensively investigating charge carrier generation, transport, multiplication, and noise creation mechanisms in photoconductors, developing mathematical models for analyzing imaging detector performances such as dark current, sensitivity, DQE (detective quantum efficiency), and MTF (modulation transfer function) as a function of field, temperature, detector structure, X-ray induced effects, excess noise, and hence optimizing the detector designs. The proposed work is vital to understand the fundamental physics of X-ray/optical detector operations and photoconductor properties, and to identify the important factors that limit the detector performances, which can ultimately lead to finding the efficient materials and designs for low-cost and efficient radiation imaging detectors for various digital medical imaging applications. This research will advance scientific knowledge in high field transport mechanisms in amorphous and polycrystalline photoconductors. The mathematical models can be used as design tools by researchers in academia and industries. The proposed work has excellent scope for original and fundamental research for graduate students, which will advance fundamental scientific knowledge and transfer it to the next generation. This research will be beneficial to Canada’s R&D efforts in medical digital imaging, and the digital radiation image detector community (e.g., Analogic and Hologic).""560056,""Kablawi, Dana"
"567462"	"Kahrizi, Mojtaba"	"Investigations on fundamental properties/issues of nanowires/structures with applications in solar cell, energy storage, and sensing devices."	"Nanostructures can be used as fundamental elements in manufacturing many devices. Among them, nanostructures made of silicon and zinc oxide, ZnO, with significant physical properties have been the focus of research for a variety applications. Silicon nanowires, besides having many applications in electronic and optoelectronics, are used to fabricate the new generation of sustainable energy devices, such as solar cells. An array of silicon nanowires SiNWs, beside having excellent light antireflection properties, provide opportunities to design light trapping and carrier collection processes independently. Also semiconductor ZnO has been the focus of research for many applications for the last several years because the material is nontoxic, chemically stable, and biocompatible with a direct wide bandgap, strong ionic bonding, and exciton binding energy 60 meV. Despite the significant promises offered by these nanostructures, there are still several challenging issues related to physical properties of these elements; for example, in the case of SiNW solar cells, surface and interface recombination, surface roughness, mechanical and chemical stability, surface morphology, doping control, integration with other modules, and device packaging are some of these issues. In the case of ZnO, in many applications well-distributed and highly oriented NWs are of crucial importance. Furthermore, controllable fabrications of p-type or n-type ZnO NWs are the key for applications in nano optoelectronic devices since, p-type ZnO NWs exhibit a band gap reduction and strong acceptor-related photoluminescence and the n-type NWs have band gap broadening with a strong donor-bound exciton. Therefore, preparing cost-effective and high quality of these nanowires, for any specific application on a large scale and with high throughput, is considered to be very challenging task. The general scope of this proposal is the design, fabrication, and characterization of arrays of silicon and ZnO semiconductor nanowires/nanostructures for various applications. Works will be focused on research and investigations on the fundamental issues related to structures, physical properties, and fabrication of nanostructures with applications in photonics, energy, and sensing devices. The low cost of the developed technique is considered as an important factor. Finally, we design and fabricate devices based on the most promising elements produced in this work; our focus will be on developing a gas ionization sensor (GIS) and photovoltaic devices based on the nanowires/structures grown in this proposal. Therefore, in this proposal the following two main objectives will be studied: 1) Investigating methods to grow low cost but high quality with large throughput of zinc oxide and silicon nanowires. The techniques should enable us to control the morphology, size, aspect ratio, and conductivity of these nanostructures during the growth. Issues like doping, orientation, distribution, and uniformity of the array of nanowires will be addressed. 2) Developing devices based on the generated nano structures described in the first objective. A gas ionization sensor as well as PV devices will be designed and fabricated using the ZnO and silicon nanowires. The possibility of applying these structures for the purpose of hydrogen storage will be investigated. This proposal will have significant impact on reducing our environmental pollutions in two ways: first by developing PV devices to generate clean and renewable energy, and secondly, by developing a miniaturized GIS to detect pollutant gases.""557235,""Kaimanovich, Vadim"
"567182"	"Kelouwani, Sousso"	"Gestion multi-agents de la dynamique de véhicules basse vitesse dans un environnement de navigation stochastique."	"L’électrification du transport est reconnue comme un moyen de réduction des émissions de gaz à effet de serre. Cependant, parmi les verrous technologiques à surmonter afin de permettre une adoption massive du véhicule électrique figure l’autonomie d’opération limitée. Deux problèmes identifiés par la communauté scientifique sont en partie à l’origine de cette insuffisance : la faible densité énergétique embarquée sur le véhicule et le profil de pilotage non éco-énergétique (dynamique avec de fortes accélérations/décélérations, utilisation non optimisée des accessoires de climatisation/chauffage, etc.). L’inefficacité du pilotage est d’autant plus exacerbée dans le cas de véhicules à basse vitesse (VBV), tels que les fauteuils roulants motorisés (FRM), les chariots élévateurs et les petits véhicules utilitaires. En effet, ces VBV évoluent dans des environnements habités, dynamiques et stochastiques, ce qui demande au conducteur de procéder à des manœuvres d’évitement de dangers caractérisées par des accélérations/décélérations brusques. Jusqu’à présent, ces deux problèmes ont été traités séparément, ignorant ainsi leurs effets mutuels sur la dynamique des véhicules. Le programme de recherche investigue une nouvelle approche systémique permettant : (i) l’augmentation de la densité par l’hybridation des sources électriques et (ii) l’assistance active à la navigation pour un pilotage éco-énergétique. Un agent intelligent (module capable de percevoir et d'agir sur son environnement immédiat) sera développé et dédié à chacun de ces aspects. Ainsi, l’agent d’assistance active à la navigation utilisera des capteurs extéroceptifs et une cartographie de l’environnement afin d’agir sur la dynamique du VBV. De son côté, l’agent intelligent de gestion énergétique dont la fonction principale est d’optimiser l’efficacité énergétique, utilisera d’autres capteurs afin d’agir sur la même dynamique. L'intégration de ces deux agents sur le même VBV pose alors le problème de coordination du contrôle. C’est pourquoi trois objectifs principaux sont poursuivis. Le premier vise la densification énergétique par la mise au point d'un agent de contrôle pour les systèmes hybrides (pile à combustible - batteries). Le second objectif concerne la conception d'un agent d'assistance active à la navigation éco-énergétique en environnement stochastique. Enfin, le dernier objectif vise à proposer une nouvelle approche de contrôle collaboratif multi-agents qui ne requiert aucun système de coordination. La démarche scientifique comporte trois étapes essentielles dont la première est consacrée à l’analyse et au développement d’un agent de partage d’énergie hybridée en utilisant l’optimisation non-linéaire d’une fonctionnelle incluant entre autres les cartes d’efficacité et la dégradation des sources ainsi que le coût énergétique. Lors de la seconde étape, un agent intelligent de navigation permettant l’intervention du conducteur dans la chaîne de contrôle du véhicule sera développé en utilisant la cartographie statique basée sur l’approche par éléments finis et une génération de trajectoires utilisant les champs de potentiels artificiels. Enfin la dernière étape s’attèlera à proposer une solution au problème de contrôle collaboratif entre les deux agents en s’appuyant sur la théorie de jeux de Nash. L'originalité de ce programme réside dans l’approche systémique d’augmentation de l’autonomie d’opération d’une plateforme mobile par la mise au point d’un système modulaire de contrôle de sa dynamique qui tient compte à la fois des exigences fonctionnelles et énergétiques. Ces résultats pourront être appliqués à la prochaine génération de véhicules électriques comportant des éléments de navigation active et des sources hybridées à l’hydrogène.""565756,""Kelouwani, Sousso"
"566300"	"Khajehoddin, SayedAli"	"Distributed High Power Density Converters for Renewable Energy Resources with Optional Energy Storage Systems"	"Due to negative environmental impacts and inefficiency of central power plants, renewable distributed generation (DG) systems have proliferated in electrical grid systems. Renewable DGs may include solar, fuel cells, wind, or other renewable energy sources. A major issue confronting designers and users of renewable DGs is the random, fluctuating nature of these energy resources. As such systems are intermittent and prone to stability issues, the use of storage has become crucial and the integration of storage into DG systems has become the mainstream. Tax incentives such as ""Storage 2013 Act"" in the states, subsidies on energy storage systems in Germany, recent investments by U.S. army, and launching of recent projects by consortium of several partners in Europe and Canada are a few examples that attest to the importance of energy storage systems. Therefore, design and implementation of new power electronic systems (PES) that include energy storage option will be vital in the coming decades. PESs are designed to extract power from renewable DGs and provide it to loads and electrical grids. However, most of the existing PESs operate only when the live grid exist, known as grid connected mode of operation. Such systems normally do not support supplying power to loads when there is a power outage, known as islanded mode of operation. PESs, supporting both modes of operations, typically require energy storage devices, and are usually custom designed with much more complexity. For photovoltaic (PV) systems, existing PESs that support both modes are mainly designed in ""central"" configurations where all the PES interface and control systems are designed in one device. Central PESs, however, have exhibited serious shortcomings such as poor energy harvesting in partial shading conditions. Consequently, recent research is focused on the development of ""distributed"" PESs such as microinverters and module integrated converters. In distributed PESs, each module or cell has a designated PES to maximize energy harvesting. Distributed PESs, due to many theoretical and practical challenges, have not yet been studied and designed for islanded mode of operation. Developing a new distributed PES configuration, with optional energy storage that maximize efficiency and minimize production and installation costs is the general vision of this proposal. The new architecture has both advantages of central and traditional distributed PESs, and adds features such as simplicity, inexpensive scalability, support of both modes of operation, and durability. The proposed research has the potential to lead to technologies that offer many benefits to every users, with weak or unreliable grid systems, in remote areas, and grid-connected with or without critical loads. The unique character of the research features gradual expansion of the system proportional to the need and affordability. This is specifically beneficial to poor countries or people with limited budgets. Unlike existing renewable DG systems, the use of this technology in large scale residential applications not only exploits the renewable PESs as an uninterruptable power supply, but also contributes to the development of smart grid concept. This is achieved by helping balance the grid through frequency regulation, peak time demand response, reactive power compensation, and harmonic and sag/swell compensation. Furthermore, this technology can be employed by remote communities, suburban areas with limited utility connection time, equipment stations, any mobile units such as special purpose vehicles or military units, telecommunications, and emergency shelters.""550038,""Khajehpour, Mazdak"
"567605"	"Khedri, Ridha"	"Unifying security and software Product family models to enhance information confidentiality"	"Security and, in particular, information confidentiality are becoming more and more valuable to governments, military, corporations, financial institutions, hospitals, and private businesses as they amass a great deal of confidential information about their employees, customers, products, research and financial status. So far, security-modeling work has been largely independent of system requirements and product family modeling. It is a common practice to model system requirements first and then security is added as an afterthought. Usually the security part of a system is overlaid on the subsystem of the main functionality. It is the long-term objective of this research program to unify feature modeling and other early requirements models with confidentiality models to gain a unified view of the product family and its confidentiality requirements. The aim is to obtain models that can evolve with the changes in the software family environments, which would enable us generate and propagate the needed changes to the security prevention and detection mechanisms. Consequently, we systematically and quickly strengthen the responses of the products of a software family to emerging threats. The proposed research will take the following complementary research directions: 1) Investigate models that capture the domain and security knowledge in preventing indirect unauthorized information leakage through cover channel communication. This direction will involve a formal representation of an ontology that is suitable for an algebraic specification of software systems. 2) Expand our early-established results on product family to equip feature models with a representation of the context and the environment in which each feature of the family is expected to evolve. This context and each feature environment are captured by the domain and security ontology. 3) Explore dynamic instantiation of confidentiality policy and the mechanisms for enhancing information confidentiality. From a set of confidentiality rules given by the security risk management officers, a more thorough and complete set of rules are generated (calculated) taking into account the security knowledge and domain knowledge of the product family. The proposed research would enable us to have software systems that can on the fly update their confidentiality policy and mechanisms with each change in the environment. The security and software domain ontology will capture changes in the system's environment and then a new set of confidentiality rules will formally be calculated and included in the system. I aim at adopting an algebraic approach to formally model and unify security and product families models. The proposed methodology, due to its algebraic flavor, brings a calculational way to generate confidentiality rules and verification of the properties of the unified model. Moreover, it brings rigor in modeling and fosters a disciplined approach to software engineering to handle security aspects. The proposed calculational processes would be easily automated using computer algebra systems and theorem provers. The proposed research represents a rigorous approach to enhance information confidentially in an ever-changing world. The obtained results would bring major contributions to information security and affect research in other areas that use ontologies such as business intelligence and eHealth.""564403,""Khemrajsingh, Amil"
"566631"	"Khomh, Foutse"	"Designing Highly Recoverable Cloud Based Software Applications"	"Cloud computing is an increasingly popular paradigm that allows individuals and enterprises to provision and deploy software applications over the Internet. Customers can lease services provided by these ‘cloud’ applications (a.k.a cloud apps), ramping up or down the capacity as they need and paying only for what they use. Cloud apps typically run on cloud platforms such as Google App Engine, Windows Azure, or OpenStack. Cloud apps are used in about every industry today; from financial, retail, education, and communications, to manufacturing, utilities and transportation. Forrester Research predicts that cloud apps sales will more than quadruple by 2016 (from $21.2 billion this year to $92.8 billion) to account for around 16% of the total software market. However, cloud apps dependability is still a major issue for both providers and users. Failures of cloud apps generally result in big economic losses as core business activities now rely on them. This was the case in December 24, 2012 when a failure of Amazon web services caused an outage of Netflix cloud services for 19 hours. The demand for highly dependable cloud apps has reached unprecedentedly high levels today. Yet, there is still no clear methodology in the industry for developing highly dependable cloud apps. Developers usually delegate dependability issues to the cloud platforms running the apps. A rule of thumb is to replicate services across multiple availability zones (AZ) as summarized by Netflix’s strategy: ""Deploy in multiple AZ with no extra instances – target autoscale 30-60% until you have 50% headroom for load spikes. Lose an AZ leads to 90% utilization"". Yet, stress tests conducted by Sydney-based researchers have revealed that infrastructure and platform services offered by big players like Amazon, Google, and Microsoft suffer from regular performance and availability issues due to service overload, hardware failures, software errors, and operator errors. The response times of these services was found to vary by a factor of twenty depending on the time of day. Therefore, cloud apps should be robust to failures if they are to be highly dependable. The long-term goal of this research program is to develop techniques and tools to improve the recoverability of cloud apps. By reducing the recovery time of cloud apps, we will be able to improve their dependability and reduce the amount of money lost during service-downtime. I will achieve this goal by developing and applying a novel and innovative methodology supported by a framework to incorporate fault recovery mechanisms in the architecture of cloud apps. One big asset of cloud computing is the constantly increasing number of Application Programming Interfaces (API) that allow developers to integrate multiple third party services into cloud apps. Examples of cloud API platforms include Apache CloudStack, Amazon Web Services, Eucalyptus, Simple Cloud, and OpenStack. These service-level APIs provide a lot of redundant services that can be incorporate in cloud apps to implement fault-tolerance. Using patterns like Heartbeat or Watchdog, a cloud app can monitor a specific service on which it depends and, in case of failure of this service, redirect requests to a backup service and trigger a recovery mechanism to maintain high availability. I will propose architectural patterns to integrate and monitor services, as well as a framework to integrate fault recovery mechanisms in the architecture of cloud apps.""552059,""Khondaker, Bidoura"
"567526"	"Kim, IIMin"	"Revolutionizing Physical Layer Security for Wireless Communications"	"In the proposed research, we will develop revolutionary communication security mechanisms by questioning and rethinking fundamental assumptions (and functionalities) of current wireless communications systems. In wireless communications, sending data securely from the transmitter to the legitimate receiver(s) is an extremely important task. This issue of security is becoming even more critical as growing number of people use more wireless devices to send personal data for various reasons and to send sensitive data for credit card or online banking. As smartphones and tablets replace personal computers, wireless data traffic is growing exponentially: the global wireless data traffic is expected to rise from 5 Exabytes (10^18 bytes) in 2012 to 21 Exabytes in 2017 (more than 300% increase). Because adversaries may attempt to gain unauthorized access to data, the importance of secure data transmission cannot be over-emphasized. A current standard approach to secure data transmission is to use secret keys based on cryptography. These days, the wireless networks are evolving from point-to-point communications towards more complex and infrastructure-less networks such as ad hoc networks, cooperative networks, vehicular networks, etc. In these networks, multiple terminals are engaged in communication and the network topology can be very dynamic. In this case, key distribution or management based on classical cryptography becomes very challenging. Furthermore, the actual strength of security achieved by cryptography is sometimes questioned as computing power rapidly improves. To address these issues, a fundamentally new approach has recently received a lot of attention, namely, Physical Layer Security. The functions of a wireless system can be partitioned into multiple logical layers. The lowest layer is called the physical layer, which represents physical transmission of signals from the devices over physical wireless medium. In the new Physical Layer Security approach, the transmitted data is secured at this physical layer (rather than at higher layers where classical cryptography takes place). In the proposed research, we will revolutionize the physical layer security mechanisms. We will first question and study the security measure itself, which will give us valuable insights into fundamental security mechanisms. Then, examining two key issues, sampling and power control, of digital wireless systems, we will develop two new ground-breaking schemes: secure sampling and secure power control. Finally, we will study the security from the perspective of the eavesdropper, (rather than from the perspective of the transmitter). This is indeed a paradigm shift in the area of physical layer security. By understanding the most effective eavesdropper strategies, we will lay the groundwork to eventually build more secure wireless communications systems. The outcomes of the proposed research will provide Canadian industry with new and effective security mechanisms. In Canada, the economic contribution of wireless industry is profound: it contributes $43 billion and about 261,000 jobs. The proposed research will enable Canadian wireless companies to develop practical and cost-effective secure communications systems. Most of all, the proposed research is “specifically and strategically” designed to train HQP in two key areas: i) wideband wireless communications and ii) communications security at the physical layer including cryptography. In the proposed research program, 4 MSc and 5 PhD students will be trained to challenge existing fundamental assumptions in state-of-the-art communications systems to recognize and identify fundamental research issues, and to make innovative contributions in the area.""554731,""Kim, IlYong"
"555973"	"Kim, Joonyoung"	"Speed-independent path control for industrial robots"	"industrial robots, automatic manufacturing, motion planning, dynamics""550956,""Kim, Junchul"
"566397"	"Lakhssassi, Ahmed"	"Dynamique thermique dans les microsystèmes intégrés"	"Ce projet de recherche introduit une nouvelle orientation concernant la caractérisation de la dynamique thermique et le monitorage des pics thermiques dans les microsystèmes intégrés. Le projet de recherche reconnaît l'importance grandissante des aspects thermiques dans les circuits intégrés et les microsystèmes sur puce bio-implantables. Ils nécessitent la convergence de plusieurs techniques, dispositifs, systèmes et technologies de la micro-échelle et bientôt à la nano-échelle comme solution d'intégration pour répondre à des exigences en terme de dissipation de chaleur, de performances et de stabilité thermique. Cependant, les problèmes thermiques résultants de la dissipation de chaleur des nouveaux microsystèmes demeurent un obstacle majeur devant les performances d'opération exigées. D’un autre côté, dans les microsystèmes bio-implantables, la dissipation de chaleur et sa diffusion dans les tissus biologiques deviennent problématique. En particulier, l'ampleur de l'intégration des microsystèmes complets sur puce et l'augmentation de la vitesse d'opération amènent des problèmes thermiques insurmontables. Si ces aspects ne sont pas traités convenablement par la détection et le monitorage cela représentera une menace sérieuse pour le design des microsystèmes complexes du futur incluant les microsystèmes bio-implantables. Cette détection sera applicable à la diffusion thermique en régime dynamique dans les circuits intégrés. De façon générale, ces aspects deviennent critique lors du design des circuits VLSI (Very Large Scale Integration), et les SoC (System on Chip) concernant la dissipation de chaleur, la stabilité thermique, la sécurité et la garantie de la fiabilité de ces nouveaux dispositifs. L'idée principale de notre programme de recherche couvre trois aspects soit: la caractérisation de la dynamique thermique, la diffusion thermique dans les microsystèmes et les techniques de détection spatiotemporelle des pics thermiques. Bien que les microsystèmes sur puce couvrent différents domaines physiques, la caractérisation de la dynamique thermique représente le cœur des préoccupations actuelles et futures. De plus, ces domaines partagent différents liens dont la température est la principale préoccupation commune et qui pose des défis énormes pour les méthodologies de design des microsystèmes. Un des liens communs le plus important et qui dresse des obstacles insurmontables est la complexité grandissante des problèmes sous-jacents à la température. Dans cette recherche, la méthodologie adoptée pour la caractérisation de la dynamique thermique dans les microsystèmes sera basée sur la généralisation des concepts de couplage thermique. Cette méthodologie a donné des résultats très encourageants pour la caractérisation des aspects de la dynamique thermique dans les circuits VLSI. Ces concepts seront alors combinés avec les techniques spatiotemporelles pour être généralisés aux microsystèmes incluant les circuits bio-implantables pour créer une nouvelle méthode de caractérisation de la dynamique thermique. Cette dernière concerne les microsystèmes VLSI dans une perspective de développement d'une unité intégrée de prédiction des pics thermiques, de diffusion thermique et une méthodologie de monitorage des pics thermiques en présence des sources multiples. Notre méthodologie s'inspirera d'un côté des techniques mathématiques appliquées à la diffusion thermique dans les tissus biologique et d'un autre côté des techniques performantes de traitement et de reconnaissance d'images pour garantir en régime dynamique la fiabilité dans les microsystèmes intégrés. Sur cinq ans, trois étudiants 3e cycle, cinq de 2e cycle et une dizaine de stagiaire de 1e cycle seront formés.""547518,""LakinThomas, Patricia"
"566945"	"Lakshmanan, Laks"	"Next Generation Applications of Social Systems"	"Technology has heralded the advent of Online Social Systems such as Social Networks (SN) and Recommender Systems (RS). Data pertaining to these systems--both those that describe them and those that are generated by their users, are essential for research aimed at advancing their state of the art. Recently, increasing availability of real data sets on social systems, and at an unprecedented scale, has facilitated cutting edge research into the design and use of these systems. An important application domain that is common to both SN and RS is marketing. In case of SN research, a major application domain is Viral Marketing (VM). The vision behind VM is to give free samples of a product to a small number of ``seed'' users (consumers) in a SN in the hope that adoption of the product will propagate virally through the network from the seeds to their followers, and their followers, and so on. The seed users chosen must be influential in effecting large cascades of adoption by others. The underlying thesis here is that users are susceptible to the influence of their neighbors: when they observe their neighbors perform an action like buying a product or adopting an innovation, then with some probability, they will be tempted to do the same. Numerous models have been proposed for capturing propagation phenomena which cover many applications including viral marketing, spread of infections, spread of innovation/rumor, etc. In spite of a decade of research with many advances, its penetration into real-world marketing has been modest. RS are the backbone behind the success of companies like Amazon and Netflix: indeed, recommending products to users by leveraging their past profiles has been found to boost product adoption and ultimately sales. RS build profiles on users/items using past user feedback and use the profiles to make future recommendations. The majority of research to date has focused on improving the accuracy of prediction models. While this is important, an equally important business perspective has been largely ignored. In the proposed research, we will focus on this marketing domain and study several fundamental data mining and computational questions that will enable the development of novel next generation applications. Most prior work on VM ignores competition, is based on unrealistic models, ignores the fundamental role of the network owner, or proposes algorithms which do not scale to real SN with hundreds of millions of users. A specific goal of this program is to develop novel models for VM that close the gap between research and ground reality by lifting the above limitations, and design scalable algorithms. We will study novel data mining questions on recommendations motivated by a business perspective: how to make strategic recommendations that optimize expected revenue, while accounting for pricing and boredom effects? how to make efficient and flexible recommendations of not only items but of packages such as travel itineraries, shopping lists, and playlists, with minimal user input? how to recommend events which are evolving or being composed from scratch, to a big collection of users, such as attendees of a large convention? We also intend to study questions at the interplay of SN and RS, such as how to exploit the hidden influence channels in a RS to launch an effective targeted marketing campaign, taking into account complex recommendation models, competition between rival campaigners, and non-monotonic behavior of users who may switch loyalties. Cutting edge research fueled by these questions will drive the technology forward and just as importantly, take both viral or network-driven marketing and recommendation-driven marketing much closer to reality, leading to industry-strength systems and applications.""556714,""Lakshmanan, Laks"
"567088"	"Lalonde, JeanFrancois"	"Bringing Images to Light"	"Practical computer vision applications have, for a long time, found themselves confined to the realm of ""machine vision"": the image-based technology for automatic inspection and analysis used on assembly lines. In recent years, and in combination with the advent of cheap digital cameras, we have witnessed an emergence of computer vision applications in massive-scale consumer applications. For example, ""intelligent"" cameras automatically detect faces and shoot when the person is smiling, search engines successfully retrieve images from a text query, online digital maps are augmented with street-level imagery, and augmented reality applications are commonly available on smartphones. Despite all these advances, this is only the beginning. The computer vision research community is actively working on much more challenging tasks, such as the fully automated understanding of images, which are bound to find their ways onto commercial progress in the not-so-distant future. Despite this progress, computer vision systems suffer from one major limitation: they have trouble adapting to the strongly-varying illumination conditions, especially outdoors. In particular, the sun, with its blinding intensity, creates effects such as highlights and cast shadows that are very difficult to deal with. While the human eye can easily adapt to those variations through millions of years of adaptation, the same cannot be said of digital cameras and of algorithms operating on their images. This research program will focus on the issue of allowing computer vision algorithms to adapt, and even leverage, the lighting conditions in an image. We will develop the new paradigm that jointly reasoning about illumination and scene components can help us better understand images. To do so, we will tackle the following four objectives. We will: 1) introduce new inference algorithms for jointly reasoning about face appearance and illumination conditions, to improve face detection in challenging lighting conditions; 2) develop new lighting models to capture the appearance of objects (people and cars) in a data-driven way to build better object detectors; 3) incorporate physical models of appearance to better segment images into semantic regions; and 4) provide a general framework for understanding how illumination is a useful cue in computer vision, even in the challenging world of consumer photographs. In addition to the applications mentioned above, the activities proposed in the context of this research program will also impact other fields, such as computer graphics, intelligent transportation, and robotics, all relevant to Canada. In computer graphics, additional information on the lighting conditions in images can contribute to improving applications such as pre-visualization, digital asset capture and management, and image editing. Safety features on intelligent transportation systems such as autonomous cars, which often rely on cameras for detecting obstacles on the road, strongly depend on their capacity to adapt to varying illumination conditions. Similarly, outdoor exploratory robots will also benefit from this research through the development of better models of outdoor illumination. The creation of this research program at Laval University will contribute to maintain and even expand Canada's relevance in these fields via its key technical contributions, and by training highly qualified personnel in its development.""559818,""Lalonde, Kathryn"
"567372"	"Lampe, Lutz"	"New Paradigms for Power Line Communications for and through the Grid"	"Reusing existing power lines for data communications is a natural choice given that the power wires and cables are already deployed and the pervasiveness of the electric power grid. It is therefore not surprising that this through-the-grid communications, a.k.a. power line communications (PLC), has long been used by electric power utilities for voice communication and grid monitoring and control. On the other hand, the development of modern PLC technology has long stagnated and received relatively less attention, in particular when compared to wireless communication. This has changed in recent years, when the need for an overhaul of the way electricity is generated, transported, and consumed has become apparent. The vision of a ""Smart Grid"" of the future is tightly coupled with the deployment of a ubiquitous communications infrastructure that enables pervasive sensing and control. Considering the successful track record of PLC as a working communications solution for power utilities, it has been identified as a forerunner technology for smart grid communications, especially for the distribution domain. This renaissance of PLC coincides with significant technological innovations in PLC over the past 10 years, mostly driven by the adoption of signal processing methods originally developed for broadband wireless systems. The trend is further supported by the consolidation of modern PLC technologies in several important PLC standards by the IEEE and ITU-T in 2010 and 2012. Against this background, the gist of this research proposal is to take the innovation of PLC one step further, with its application to support smart grids in mind. We are inspired by recent advances in wireless communications, in particular the introduction of network coding and physical layer security to wireless systems. Because of the similarities of wireless and power line communication channels, due to both media being reused for communication purposes, there are sound arguments that the underlying methods and resulting advantages of network coding and physical layer security carry over from wireless to PLC. Since network coding enables intermediate nodes of a communication network to combine data packets, rather than only forwarding them, we expect that its application helps solving the congestion problem experienced in large-scale PLC deployments. Physical layer security can provide a non-encryption means for secrecy of messages, and thus helps to prevent them against eavesdropping and possible attacks on power grids. We propose to investigate these new communications paradigms for PLC, with a detailed research agenda outlined in the proposal. A third aspect of our research is to exploit the specifics of PLC for signal modulation. Since every element connected to the grid has an effect on the characteristics of signal propagation through the grid, this can be used to condition the channel and/or modulate the signal. Interestingly, a similar paradigm has recently been considered for wireless communications and coined media-based modulation. Thus, again we can draw from similarities between wireless communications and PLC. The research directions proposed in this application are original, timely, and well aligned with developments in industry embracing PLC as important component for smart grid communication with chips sets and systems developed by giants such as Texas Instruments, Freescale, or Maxim Integrated, or the Canadian vendor Corinex. Students will be trained on the latest communication concepts applicable to PLC, wireless, and other communication media. Through the application to smart grid communication, they will be provided with an interdisciplinary context and trained to work with the specific requirements of for-the-grid communications.""551946,""LamyPoirier, Joël"
"567801"	"Lapalme, Guy"	"Natural Language Processing"	"This proposal explores ways to support people when they are speaking or writing in their mother tongue or during the process of acquiring these skills in an another language. This research, that we call Interactive Natural Language Generation, takes into account the specificities of the intended user, an adult already having a command of a first language, not a child learning to speak. It also addresses problems such as interface or navigation. Since an important goal of language is knowledge transfer, we deal with how knowledge is structured, represented, accessed and transformed. Language generation theory and practice have been described by two landmark books: one from Levelt, a psychologist describing mental processes, and another one from two computational linguists (Reiter and Dale) showing how to build tools for fully automated text generation. In this proposal, we focus on the production of language with the help of a machine. It is therefore a hybrid approach drawing on knowledge accumulated in psycholinguistics and linguistic engineering, relying additionally on human knowledge, intelligence and skills. This research program, which involves 2 PhDs, 4 MScs and a postdoctoral fellow, will propose solutions by referring to existing theoretical work and will develop practical web-based applications. Each task is independent but explores a specific facet of our overall framework so as to allow emulation and cooperation between students. The originality of this work lies in its difference from the existing body of work, drawn from different fields: natural language generation and natural language processing. It shows how current technology can be used to assist humans in producing language, or in acquiring this skill in a foreign language. We will focus on building tools and on showing what needs to be taken into account for people to generate language with the help of computers. In order to succeed in this research program, we will rely on our experience with natural language generation in related projects as well as on the preliminary explorations and prototypes we have developed within our current NSERC discovery grant.""567721,""Lapalme, James"
"555882"	"Lapides, Paul"	"Information Visualization of Personal Social Networks and Mobile Data"	"Information visualization, Social networks, Big data, Personal informatics""545081,""Lapierre, Hélène"
"567808"	"Laurendeau, Denis"	"Collaborative Handheld 3D Sensors"	"Summary of proposal – Laurendeau 17782 The research program addresses a promising topic in computer vision and consists in exploring how groups of collaborative handheld 3D sensors can be exploited by human operators for Quality Control. Although the problem of collaborative sensors mounted on mobile robots has been investigated in depth in the past, the topic of collaborative 3D sensors exploited by humans has received little attention. This topic is becoming highly relevant with the advent of fast and accurate handheld 3D sensors that are now widely used in quality control and inspection in general. The expertise in 3D vision developed by the applicant has demonstrated that human operators intuitively know “where” to scan the 3D data but are much less efficient in deciding where new data should be collected or whether or not enough “reliable data” has been collected on the objects. There is thus an urgent need for investigating how 3D sensors could provide guidance to human operators during the data collection process. The objectives of the program are to: 1. study how groups of handheld 3D scanners can be used efficiently by human operators in Quality Control to perform measurements on large man-made objects. The 3D sensors collaboration problem addresses the formulation of an optimization approach for data collection by groups of 3D sensors and also explores sub-problems such as sensor positioning, view alignment, segmentation of 3D data into geometric primitives and features (edges) as well as alignment between object parts and a CAD model; 2. build a plug-in software framework supporting the above collaboration approach; 3. combine Human-Sensor-Interfaces to the optimization approach to help human operators to collect sufficient reliable data using handheld 3D scanners; 4. train highly qualified personnel in 3D vision and collaborative research; 5. disseminate research results in journals and conferences and transfer results to potential industrial partners. According to the ARC Advisory Group, the 3D scanning market is expected to grow at an 8.8 percent annual rate for 2012-2016. Handheld 3D sensors are a growing part of this market. However, these sensors still remain difficult to use by non-vision specialists. The research program proposes to investigate new optimization approaches and human-sensor interfaces to guide human users in achieving better results in their data collection tasks. A major outcome will be the formulation, implementation and validation of a paradigm which will optimize the collaboration of groups of handheld 3D sensors operated by human users, a problem that has received little attention but that is of major importance with the widespread use of these sensors in industry. It is anticipated that the optimization framework and complementary technology to be developed in the program will lead to better ways of using handheld 3D sensors, reduce scanning time and post-processing time, and increase efficiency in quality control. This is highly relevant to the 3D scanning industry, a sector in which Canada is a world leader. The training of highly qualified personnel will also help this industry to maintain its leadership position. There is currently a shortage of specialists in 3D vision and those who will participate in this research will find stimulating jobs.""562910,""Laurendeau, Denis"
"566224"	"Laviolette, FrançoisLaviolette"	"A PAC-Bayesian Analysis of Machine Learning and its Applications to Bioinformatics"	"In Computer Science, learning is often defined as the ability of an agent (which can be an algorithm, a robot, a machine...) to improve its behavior based on experience. Generally speaking, one can say that Machine Learning's main goal is the design of algorithms that allow machines to modify their behavior from the analysis of empirical data. A learning algorithm is designed to recognize complex patterns and make intelligent decisions based on data, or examples. It is important to point out that the set of all possible behaviors, given all possible inputs, is too large to be covered by the set of observed training examples. Hence the learner must “generalize” its observations, in such a way that, with high probability, it will continue to perform well, even when confronted with new examples. Many strategies exist in order to design algorithms that will “generalize” well; some are simple ad hoc heuristic, some are founded on what is called Learning Theory. This research project is based on the second approach. PAC-Bayesian theory is a framework for deriving some of the tightest generalization guarantees available. These guarantees are upper bounds on the probability of error of prediction. Many well established learning algorithms, such as the Support Vector Machine (SVM), can be justified in the PAC-Bayesian framework: they correspond to finding the predictor that minimizes a PAC-Bayesian bound. The kernel Ridge Regression and a regularized version of the Adaboost algorithm can also be viewed as minimizers of particular PAC-Bayesian bounds. Moreover, recently, new classification algorithms also have been discovered that way. PAC-Bayesian bounds were originally applicable to classification, but over the last few years the theory has been extended to regression, structured output prediction, density estimation, and problems based on data that are not independent and identically distributed (non-iid). This opens the way to new families of PAC-Bayesian bound minimization types of algorithms. The first objective of this proposal is to develop new PAC-Bayesian bounds, particularly in more complex setting, like the structured output setting or in the non-iid setting, in order to provide new learning algorithms that will apply in situations actual learning algorithms do not. In this proposal, those new algorithms will be particularly designed for Bioinformatics applications. As example, we are interested in the prediction of binding affinities between proteins and ligands, a problem of central importance in genomics, proteomics and pharmacology. Note that, if a fast and accurate protein-peptide binding energy predictor (that uses only sequence information) was available, we could use that predictor to find a strong binding peptide given any target protein. In this project, we will make use of our strong expertise in Machine Learning, notably in PAC-Bayesian theory, to, among other things, construct such binding affinity predictors from the available data. Moreover, we will use kernel methods (proven to be compatible with the PAC-Bayesian approach). These methods are learning algorithms that make use of similarity functions (called kernel) to learn how to predict. It is a way to include some knowledge about the prediction task into the algorithm. In our protein-ligand affinity prediction example, we will have to look for kernels that encode some (physicochemical) properties of the protein and/or the ligand. There are already some existing kernels that were proposed for Bioinformatics applications; we will use them, but we also intend to design our own. We also intend to develop in the same way learning algorithms that tackle other Big Data problems related to genomics and proteomics.""547478,""LaViolette, Marc"
"566705"	"Lebel, Robert"	"Sparse sampling and constrained reconstruction for accelerated contrast enhanced magnetic resonance imaging"	"Magnetic resonance imaging (MRI) is a powerful modality but remains limited. Time-resolved scans, which aim to capture changing physiology, are severely hindered by long scan times. This is particularly true for dynamic contrast enhanced (DCE) MRI, which aims to visualize the flow of an injected contrast agent as it passes through the vasculature. This technique has significant clinical value yet is rarely performed due solely to acquisition limitations. Hardware advances continue to reduce scan times, but with diminishing returns. New methods for accelerated imaging are urgently needed. My research program established by this Discovery Grant will develop, validate, and translate novel methods for highly accelerated dynamic MRI. My research program will explore sparse sampling and constrained reconstruction—together termed compressed sensing (CS)—for faster MRI. These techniques exploit what is well known from the JPEG file format: images can be stored using a fraction of their original data. CS posits that since images can effectively stored, there is no need to acquire all of the original data. This enables image formation from fewer samples than are typically required—thus faster imaging. Compression becomes more effective as dimensions are added, making 4D imaging (i.e., dynamic 3D) a strong candidate for acceleration. My preliminary work has demonstrated that DCE MRI is amenable to extreme speed-ups: acceleration an order of magnitude beyond conventional methods is feasible. Despite its potential, optimal methods for CS are poorly established. Furthermore, the utility of high-resolution dynamic scans has not been explored: the same source data will allow extraction of multiple pieces of physiological information. Currently, no MRI vendor has productized CS—this is due to technological barriers, not utility. It is clear that CS will play a vital role in medical imaging but effective techniques currently preclude widespread adoption. The projects detailed in this application will form the basis of a trainee focused research program with a long-term goal to develop, implement, and commercialize advanced data sampling and reconstruction technologies for MRI. Specific aims of this proposal are: Aim 1: design novel acquisition and reconstruction algorithms for accelerated 4D imaging. Aim 2: develop a comprehensive DCE exam. Aim 3: implement on-line reconstruction of these techniques. The first aim of this project will focus on optimizing data acquisition methods that are amenable to both conventional image reconstruction (albeit with low temporal resolution) and to constrained reconstruction (for high fidelity images). The second aim of this project will exploit the vastly improved image quality available from Aim 1 to extract multiple pieces of clinical data. This is not possible from current DCE scans and has the potential to revolutionize how contrast enhanced imaging is performed clinically. The third aim will strive to tightly integrate the techniques defined by Aims 1 and 2 with the MRI scanner to enable rapid image reconstruction. This objective is often overlooked yet is crucial to facilitate clinical validation and to expedite commercialization. In summary, this application proposes technological innovations to accelerate MRI. Applications of the proposed inventions reach far beyond our initial clinical application (DCE). Trainee involvement, industrial and clinical collaborations, and commercialization will be central themes in my research program.""564244,""LeBelLandry, Stéphanie"
"566714"	"Lee, Joon"	"Personalized Decision Support Driven by Similarity Metrics"	"Decision making is an important aspect of various fields including medicine, public health, politics, economics, business, retail, and sports. Although optimal decisions rooted in quantitative evidence are desired, challenges arise in the absence of established decision making guidelines supported by scientific research. When there is no clear guideline, decision makers resort to previous training, local culture, and anecdotal experiences, which frequently lead to biased reasoning. Today’s massive production and storage of electronic data thanks to advances in information technology open doors to an attractive alternative, namely data-driven decision making, which extracts relevant knowledge from valuable electronic data and instantaneously delivers it to decision makers. One promising avenue in data-driven decision making that I propose to investigate is personalized decision support based on analysis of similar cases in electronic data. The envisioned core technology is computation of similarity metrics (SMs) that objectively quantify the extent to which two cases are similar to each other. Depending on the application, a ‘case’ can be a patient, organization, consumer, or community. Based on an SM, the personalized decision support paradigm first identifies past cases that are just like a current case under investigation. By analyzing what happened to those similar cases after their equivalent point in time, it is feasible to inform personalized decision making. Specifically, SMs can help forecast the future of the present case or facilitate decision analysis via investigating the effects of past decisions. This proposed research hypothesizes that longitudinal patterns (i.e., data as a function of time), in addition to just static information, play a crucial role in SMs since they capture dynamic characteristics. Now that we have powerful computers, inexpensive data storage, and novel data sources thanks to the Internet and ubiquitous mobile devices, the concept of similarity can be tightened to move away from traditional one-size-fits-all decisions to tailor-made real-time decisions. After all, every case is unique at some level of detail. The primary outcome of this proposed research will be a complete decision support infrastructure encompassing SMs, data warehouses, computational nodes, and software decision support tools with effective user interfaces and data visualization. Public databases will be utilized in order to bypass time-consuming data collection. Mathematical and statistical modeling as well as artificial intelligence will be leveraged to develop both generic and problem-specific SMs, while high-performance computing will be the backbone of the decision support tools. The seamless integration of SMs with personalized decision support and real-time cultivation of knowledge relevant to decision making are completely novel and will push the envelope of the current state-of-the-art decision support processes. This proposed research program is at the core of data science, which is expected to be a significant job creator in the foreseeable future in both academia and industry. However, most universities currently do not operate degree programs in data science because it is a relatively new field in the intersection of several distinct fields including computer science, statistics, and mathematics. By having undergraduate and graduate students lead all aspects of the research, a top priority in the proposed research is to train highly qualified personnel as proficient data scientists equipped with both hands-on computing skills and theoretical foundations.""560962,""Lee, Joshua"
"566312"	"Lehn, Peter"	"System Architectures and Power Electronic Converter Topologies Enabling Flexible AC/DC Power Networks"	"The deployment and advancement of electrical power systems over the past century has enabled an exponential growth in global GDP and in the living standards of the entire developed world. Electrical power transmission and distribution (T&D), be it large scale or small, enables the movement of power from supply to load. In 2012 the global T&D equipment market size was estimated at $131 billion. Today, with the explosive growth in renewable energy production and the pressures to enhance energy efficiencies, the importance of integrating renewables and energy storage into utility networks are forcing a re-evaluation of the design paradigms that have driven T&D technology development over the past several decades. Recently, use of DC networks for collection, transmission and distribution of power have become a viable alternative to AC networks, offering promise for enhanced efficiency, enhanced controllability and, in a growing number of applications, reduced cost. Most existing end consumers however, will remain reliant on AC distribution networks for decades to come, inevitably leading to the emerging dominance of mixed or ‘hybrid’ AC/DC power networks. The proposed research focuses on the development of future hybrid AC/DC power network architectures to achieve system efficiency enhancement, cost reduction and, most importantly, to leverage the control versatility offered by the power electronic building blocks that make up AC-to-DC and DC-to-DC power conversion units. Through exploitation of this control versatility, the large-scale integration of renewable energy sources and energy storage devices may be realized. To facilitate new system architectures that best exploit the benefits of both AC and DC networks, the development of new converter topologies is necessary. These system architectures and conversion topologies will form the power-side infrastructure required to realize the “smart grid” vision as applied to power collection, transmission and distribution. Control of these converter topologies and system architectures are an integral part of the research. The work will exploit recent advances made in the development of modular multi-input/multi-output converter topologies, invented at the University of Toronto, that enable the formation of converter topologies that serve multiple functions. These power electronic topologies not only facilitate the transfer of energy from AC to DC or from one DC voltage level to another, but they also enable a multitude of DC energy sources (be they batteries, solar PV panels or other sources) or loads to be simultaneously integrated directly into the AC and DC networks. Such approaches allow the elimination of unnecessary power conversion stages, driving up efficiency and driving down cost. “New architectures” for the power system will be directly enabled by, and linked to, the power electronic topologies that are developed. The urgency of this work is demonstrated around the globe by the ever growing power transmission constraints, power interruptions and power quality problems that are arising as a result of a non-systematic approach to renewable energy integration into the T&D systems of the past generation. For a further demonstration of this urgency one need only look at the new integrated “Solar PV – energy storage” systems market, which is expect to grow from $200 Million in 2012 to $17 Billion by 2017. It is imperative that Canada participates in this technological revolution, which is reshaping the landscape of electric power engineering. The proposed research will enable the training of 5 MASc graduates, 5 PhD graduates, in addition to the training of numerous MEng students, summer students and BASc thesis students within this critical area.""549570,""Lehner, Bernhard"
"567607"	"Leung, Victor"	"Smart Infrastructures for Radio Access as a Service (SIRAS) - Software-defined Wireless Access Networks for Future Generations"	"To meet the exponential growth of wireless data traffic, anticipated to exceed 300 Exabytes/year worldwide by 2020, mobile service providers are deploying heterogeneous networks that augment conventional macro-cells with micro-, pico-, and femto-cells, and are utilizing a mix of radio access technologies (RAT) like 3G/4G cellular and 802.11n/ac. However, advances in RAT have not been matched by new development of system architectures that facilitate their efficient deployment. New wireless system architectures should be scalable to provide increased capacity, programmable to support new functions and advanced RAT, and self-organized to enable “plug-and-play” deployment. We propose to fill these gaps by addressing important research problems leading to the development of SIRAS, Smart Infrastructures for Radio Access as a Service (RAaaS) to enable cost effective and efficient future generation wireless access by leveraging emerging cloud computing and virtualization techniques, which are promising but not yet widely explored approaches for wireless access networks. SIRAS extends the efficiency of cloud computing and the controllability and programmability of software-defined networks (SDN) to wireless access networks, while exploiting recent advances in distributed antenna systems and software defined radios (SDR). We envisage that SIRAS replaces the current centralized cellular architecture with a distributed architecture employing massively distributed antenna systems and autonomous small cells that collaborate to provide the required Quality of Service (QoS) to individual traffic stream using one of several supported RAT that is appropriate for the specific environment, where the distributed interactions are coordinated by an extended control plane that employs SDN techniques, and resource allocation is optimized on a per-flow basis. This distributed architecture would enable RAaaS by utilizing cloud computing resources in an on-demand basis, in combination with local resources, to virtualize management and signal processing functions in a cost effective and scalable manner. SIRAS is thus expected to be able to offer mobile services by increased infrastructure capacities and guaranteed QoS instantly and efficiently in cost, energy, complexity and latency wherever and whenever the demand arises. To realize this vision, we propose to pursue a new direction in wireless access networking research to develop new architecture, protocols and management algorithms that realize the potential benefits of SIRAS described above. The overall objective of this research project is to make fundamental contributions to this new research direction, which will have significant impacts on future generation wireless access networks. Specifically, our work in the proposed project will be divided into the following inter-related research tasks: (A) SIRAS architectural design to enable virtualized RAaaS for multi-RAT, (B) Algorithm design to enhance and optimize allocation of radio and core network resources for RAaaS, (C) SDN-based protocol design to enable traffic load balancing and management function virtualization in SIRAS, (D) Design of distributed algorithms and protocols to support cognitive and cooperative operations among virtualized SIRAS radio access nodes, and (E) SIRAS testbed development and experimentations. These tasks will be accomplished through novel design and performance evaluations by means of theoretical modeling and computer simulations. Utilizing the proposed fine-tuned experimental testbed, promising techniques proposed for aforementioned tasks in SIRAS will be implemented and evaluated under realistic conditions for proof-of-concept demonstrations and possible commercialization.""554574,""Leung, Victor"
"566110"	"Levi, Ofer"	"Integrated optical sensors and nano-structures for biomedical diagnosis applications"	"This research focuses on developing novel high performance label-free optical index of refraction sensors based on Photonic Crystal Slab (PCS) nanostructures that will improve the sensing detection limit, and apply optical imaging of nano-structured surfaces for nano-spectroscopy of molecules and evaluating molecular interactions within a confined volume. Nanostructures have proven to be extremely useful for biosensing in the past decade using techniques such as Fluorescence, Index of refraction changes, Raman spectroscopy and Plasmonic effects. Nevertheless, their broad use is limited by challenges in repeatability, their detection limit and their high sensitivity to external environment fluctuations (such as temperature, vibrations, liquid handling). In this proposal we seek to 1) increase sensing detection limit with new PCS designs and self-referencing techniques; 2) develop imaging architectures in PCS biosensors to investigate many molecular interactions in parallel. In our designs, we will focus on label-free techniques where binding of a molecule to a surface or molecular interactions result in localized mass concentration and index of refraction changes that can be detected through spectral shifts or intensity changes in an image. We will explore new device physics opportunities in breaking structural and angular symmetry and introducing hybrid metal/dielectric structures for field re-shaping in close proximity (< 100 nm) of the sensing surfaces as well as evaluate many molecular interrogations in parallel, using optical imaging techniques. The proposed program is accompanied by our collaboration efforts with bio-chemists and seeks to accomplish breakthroughs in both fundamental and applied research in nano-photonics and bio-detection and translate our findings to practical bio-sensor schemes. In the past years, our group has acquired skills in developing new biosensors designs and building on our past work and strong collaborations with Toronto local hospitals and Stanford University we have everything in place to bring this proposal to success. We believe that improved detection limit and stability (reduced environmental effect on measurements) will greatly increase the use and performances of biosensors for bio-molecule detection with and without labeling agents. The success of this project will have significant impacts on a broad audience – from cell biologists seeking to study physiological processes in real-time, to diagnosticians searching for rapid, label-free, portable assays. Example end-user applications include low cost, sensitive and environmentally stable blood and urine diagnostic tests, evaluating viral infections quickly and studying molecular interactions within a confined volume that allows parallel evaluation of many drug targets for a disease. Canadian patients will have access to accurate, sensitive and affordable diagnostic tools that can be used in many local clinics and not only in large hospital settings. Furthermore, the research proposed will thrive from cross-disciplinary interaction between the trainees, members of our lab and collaborations with other labs at the University of Toronto and in Canada.""558639,""Levin, David"
"555779"	"Li, Jimmy"	"The spatio-temporal prediction problem for robots"	"robotics, artificial intelligence, machine learning, perception, pattern recognition, motion planning, computer vision, speech recognition, manipulation, human-robot interaction""563496,""Li, Jody"
"567325"	"Li, Zongpeng"	"The Science of Network Coding"	"Network coding, an elegant and powerful concept introduced at the turn of the millennium, is expected to be a core enabling technology in future communication networks, revolutionizing the way information are communicated across geographically dispersed locations. Fundamental benefits of network coding arise in network capacity, error correction, energy efficiency, network security, storage efficiency, transmission delays, and algorithm complexity. While most existing studies in network coding theory are based on information theory and algebraic approaches, The Science of Network Coding is a research program that aspires to change the way people think about network coding, inviting new view points on this simple yet fundamental tool, resulting in new fundamental insights on the subject as well as easier and more effective applications in practice. As evident in its name, network coding is coding performed within a network, and naturally benefits from a computer science and mathematics treatment that explores the underlying network topology, and the associated algorithms, complexity and achievability issues. The Science of Network Coding fills such a gap in existing studies on network coding theory, complementing and competing with the information theory and algebraic approaches. Successes in the science of network coding will enable efficient and effective network coding algorithms for applications in the Internet, in wireless networks, and in distributed storage systems, constituting a key platform technology that further serves high-level applications that benefit from the communication of information, such as cloud computing, big data analytics, smart grids, bioinformatics, and environmental monitoring. Network coding can lead to efficient, robust and secure information gleaning and dissemination among such systems that depend upon and benefit from the exchange of information.""564875,""Li, Zongpeng"
"567412"	"Liaskos, Sotirios"	"Eliciting and Using Goal Variability"	"Goal models are a popular approach for modeling and reasoning about stakeholder intentions in requirements engineering. They have particularly been proposed for use in decision making both during early requirements analysis, when the right set of features for a future system is sought, and at run-time, when stakeholder needs must translate into configurations of a running system. In this proposal we focus on the study of goal models along three directions: (a) their comprehensibility and use as visual artifacts, (b) their elicitation from stakeholders and (c) their utilization in software configuration and customization. With respect to comprehensibility we propose to experimentally study the value of goal models as visual artifacts that facilitate ad-hoc identification of optimal solutions. We are particularly interested in contribution links, elements in the goal models that represent how satisfaction of one stakeholder goal influences satisfaction of the other. We want to study what ways to represent contributions is more suitable for ad-hoc exploration of optimal solutions. We also propose to compare goal models with other decision support visualizations as well as to study whether interaction with automated reasoning tools we have proposed in the past facilitates comprehension. With respect to elicitation, we are interested in how two basic components of goal models come about: the aforementioned contribution elements and variability-inducing decompositions. In terms of contribution elements, we firstly propose to study the role of assumed context and cognitive biases during their elicitation. Our early experiments suggest that such factors can influence the validity of the elicited contribution measures. We also propose to study the role of the choice of elicitation instrument (e.g. various versions of qualitative vs. quantitative scales) in acquiring reliable contribution measures. Variability-inducing decompositions, moreover, are refinements of goals that introduce variation (i.e. decision) points. In the past we have introduced a systematic approach to acquiring such variation points, which we now wish to empirically evaluate in terms of the quality and completeness of the variability it results to. In terms of software configuration and customization, we propose to continue our current work on the design of highly-customizable systems as well as the configuration of existing systems. With respect to designing highly-customizable systems we propose to extend our earlier work on policy-based behavioral customization and define more rigorous ways for developing systems that behave according to policy objects that are directly derived from goal and other requirements models. In terms of existing systems, we propose to study application of conceptual modeling and automated reasoning techniques in order to facilitate configuration of large and complex information systems. We also propose to study common popular systems and the use of data mining techniques for identifying configuration patterns within large numbers of users, aiming at using those patterns for informing and facilitating the configuration process. The proposed research is targeted towards aspects of goal modeling that we find important for enabling its adoption in the industry. We believe that concrete evidence on how we can elicit valid goal models and use them effectively for problem exploration and understanding will make them more appealing to practitioners. Likewise, introducing requirements-based toolsets for defining and controlling variability in software systems both makes the customization process more accessible and efficient and allows software engineers to dramatically increase the adaptability of their systems.""562072,""Liaw, Christopher"
"567282"	"Lin, Xiaodong"	"Security and Privacy in Mobile Social Network and its Applications"	"Nowadays, people across the world are increasingly turning to social media, and industries are also starting to embrace it to improve and broaden their businesses. Social media represents a virtual community in which people with a shared interest, such as a specific hobby or activity, can interact and socialize with each other. As such, online social networks (or online communities of interest) are formed, uniting their members and providing opportunities of mutual interest. The concept of the social network has, in recent years, also been emerging from information and communication technology perspectives in order to provide efficient communication platforms and computing systems to deliver data services to a variety of social networking applications. This model is especially effective when it also takes a user’s current social context into consideration, for example, their physical locations. Recently, with the pervasiveness of mobile devices, such as smartphones and tablets, a mobile version of the social network known as Mobile Social Network (MSN), is booming; MSN is characterized by the use of wireless communication techniques and smartphones. It brings us a ubiquitous and omnipotent communication platform, helping us to stay connected better than ever; however, despite the tremendous benefits of MSN and its applications, MSN still faces many security and privacy challenges. One such challenge is protecting location privacy in booming location-based services in MSNs. These services allow mobile users to utilize their location information to access some location-based services nearby, or share information with other mobile users in close proximity to them. The proposed research is envisioned to address these challenges, particularly, by considering autonomous and decentralized MSN and its applications, which has not been a major consideration in previously reported studies. The proposed research will also help Canada to establish and reinforce its leadership in Information Technology (IT) around the world; it will allow for new technologies and service, and create new innovative directions in mobile computing. As such, new business models and revenue streams can also be fostered and devised. The HQP trained through the program will be equipped with security technologies, as well as wireless communications and mobile computing; these are crucial for success in today's era of mobile computing, and will help to ensure a prosperous future for the Canadian IT industry.""545718,""Lin, Xiaodong"
"567396"	"Liu, Mengchi"	"Design and implementation of big complex semantic data management system"	"In the age of big data, how to effectively store, manage, retrieve, and analyze such large-scale complex data in a timely and efficient manner is a major challenge. The primary goal of the proposed research is to design and implement a cost effective and highly scalable database management system for big complex semantic data storage, query, and analytics by combining and extending ideas from parallel database systems, MapReduce computing platform, and our previous work on complex semantic database model Information Networking Model (INM) and INM-DBMS. A parallel database system is a database management system (DBMS) implemented on a multiprocessor system with high-degree connectivity. It features data modeling using well-defined schemas, declarative query languages with high levels of abstraction, sophisticated query optimizers, and a rich runtime environment that supports efficient execution strategies. MapReduce computing paradigm, started by Google and made popular by the open source Hadoop, is a cost-effective distributed data storage and processing systems on large clusters of low-cost commodity machines connected with high-bandwidth network. It has gained a lot of attention in recent years from industry and research. INM-DBMS is a complex semantic database management system that features hierarchical and composite binary and higher degree relationships and their combinations, built-in semantics for consistency and integrity constraints for various relationships, and rich deductive and active rules. It has a concise and compact but expressive language consisting of three parts, information definition language (IDL), information manipulation language (IML) and information query language (IQL). IDL and IML provide powerful constructs to express the rich semantics and integrity constraints associated with various relationships. The declarative query language IQL effectively incorporates many useful features found in database, XML and logic programming languages such as logical variables, implicit existential and non-existential quantification, explicit universal quantification, negation as failure, tree expressions, etc. It can explore the natural networking structure of objects to extract and construct meaningful results in a concise, natural, and compact way. We will first extend the INM data model to adapt to schema-free and semi-structured, and then implement a big semantic database management system based on it. The system will consist of two layers to achieve high concurrency. The manipulation layer is in charge of definitions, manipulations and queries while Data Layer takes care of data storage. In the manipulation layer, objects are evenly allocate to storage nodes by a hash function and query tasks are analysed and interpreted into parallel tasks via an INM MapReduce library interface to achieve partition balancing and partitioned parallelism. In the data layer, the structured semantic data is partitioned and stored in various nodes to ensure dynamic scalability. In the methodology part, four kinds of nodes are differentiated to execute different jobs. The nodes are independent of each other and separated from material machines, which makes the system highly extensible in physical resource allocation and extremely robust at fault-tolerance. Our developed big complex semantic DBMS can significantly contribute to the effective management, efficient retrieval, and timely analytics of various heterogeneous, semi-structured and unstructured massive data. It can be used in many applications such as semantic search engine, complex social network services with exploded increasing data, large-scale data analysis and data mining, knowledge graph establishing and knowledge discovery, etc.""563231,""Liu, MengTze"
"567330"	"Liu, YanFei"	"A New Power Architecture with Wireless Feedback Control for Next Generation Server Power System"	"With the exponential growth in internet / wireless applications, more and more servers, including data centers, data storage, cloud storage and others, have been built to process the ever-increasing network traffic. Multiple Point of Load power supplies are used in the motherboard to provide regulated voltages to CPU (Central Processing unit), GPU (Graphic Processing Unit), RAM (Random Access Memory), and other digital circuits. It takes long time to design such a power supply system to meet the requirement. In a lot of cases, the design time for the power system becomes the bottle neck in the product development schedule. Therefore, it is very important to reduce the power system design time in order to reduce the product development time. In this research program, a radically new power architecture will be proposed to significantly reduce the power system design time. With the new architecture, feedback signals between the Power Processing Cell (PPC) and Central Control Unit (CCU) will be communicated wirelessly without any Printed Circuit Board (PCB) tracks. Three major benefits can be obtained. The first benefit is much shorter power system design time because all the features of the power system can be programmed with software and no hardware links (i.e. no PCB tracks) between PPC and CCU are needed. The second benefit is lower system cost because standardized PPC and CCU can be designed and made in large quantity, which can reduce the unit cost. The third benefit is extreme flexibility because the functions of the power system can be changed with software and no hardware change is needed. This research program will develop all the technologies to implement the proposed power architecture with wireless feedback control, such as partitioning of PPC and CCU, wireless communication strategy between PPC and CCU, and minimum data transmission technologies to ensure highly reliable operation of the power systems. An experimental power system will also be built to demonstrate the feasibility and advantages of the new power architecture. It is expected that several fundamental patents in the wireless feedback control field will be filed through PARTEQ Innovation (commercialization / patent division of Queen’s University) to protect the intellectual property. In this research program, four PhD students, five master students and ten undergraduate students will be trained in areas of switching power supplies, digital control technology, wireless feedback control, analysis and design of digital feedback systems, firmware and software programming skills, as well as server power system requirement, hardware debugging skills. They will play very important roles in developing and commercializing the new power architecture for next generation server and data center power systems. Power system for server and data center is a multi-billion dollar business. During the transformation from old wired architecture to the new wireless architecture, new business opportunities will be created for Canadian companies and businesses. The HQP trained in this research program will help Canadian power design centres lead the world in developing next-generation power systems with wireless feedback control. Canada will become an international leader in this field with this revolutionary new technology, which will bring job creation and a competitive edge for Canada in this multi-billion dollar industry.""556549,""Liu, YanFei"
"555195"	"Lombaert, Herve"	"Ontario"	"CANADA"
"555817"	"Louie, WingYue"	"A Novel Learning-based Control Architecture for Socially Assistive Robots Facilitating Group Activities"	"Socially Assistive Robots, Human-Robot Interactions, Multi-User Control Architecture, Machine Learning, Artificial Intelligence""551666,""Louis, Wael"
"567098"	"Lutfiyya, Hanan"	"Policy-Based Management of Cloud Systems"	"The resources provided by a cloud infrastructure can be shared by multiple applications from different organizations. Allocating computing resources based on peak demand often results in underutilized computing resources. One approach to dealing with this is to allow for over-subscription. The assumption is that the peak demand of all applications may be more than available computing resources provided by the cloud infrastructure but the peak demand typically does not occur simultaneously. However, it is entirely possible that the peak demand of applications on a single server or a rack or a cluster may exceed the capacity. This requires that resources be allocated dynamically and in a timely fashion in response to overload on any part of the cloud infrastructure. This is referred to as dynamic resource management. There are several challenges being addressed with the proposed work. Some of these include the following: (1) Dynamic resource management requires that issues that arise with conflicting goals is addressed. For example, it is desirable to minimize power consumption. However, a primary focus on power consumption may lead to performance problems. A focus on performance management may lead to increased power consumption. (2) The detection of performance problems is not the same as the identification of the cause. It is important to address this in order to make the correct decisions with regard to re-allocation of resources. (3) Increasingly clients have applications with multiple software components. In re-allocating resources we must consider the impact on migrating a software component if other software components depend on it. (4) There are multiple actions that can be taken to address overload. One challenge is learning which action is best for a particular scenario. (5) The development of software to implement resource management algorithms needs to be made easier. (6) Different cloud providers will make different decisions for similar overload situations. The decisions would be based on the cloud provider’s business goals, which vary from provider to provider. Addressing these problems is important for the next-generation of cloud infrastructures that need to maintain expected performance for client applications and yet be energy efficient.""545039,""Luth, Robert"
"567540"	"MacLachlan, Scott"	"Robust Structured Multigrid Algorithms for Mechanics of Heterogeneous Media"	"With the steady increase in modern computing resources has come an increased demand for high-fidelity computational simulation tools in many areas of science and engineering. This demand is driven by the need for understanding physical processes at scales where prototype engineering or lab-scale experimentation are impractical or intractable, but where the underlying physical processes are well-enough understood to yield accurate mathematical models on which to base a computational approach. Such simulation has, indeed, been a driving force in computation throughout its history, and modern simulation toolkits include highly detailed models that, consequently, have high computational costs associated with them. The principle focus of this proposal is the development, analysis, and implementation of computer algorithms to accelerate these computations. A wide class of physical problems are modeled by a similar family of mathematical equations, focusing on the dynamics of incompressible fluids and incompressible solid bodies. These equations naturally model physical energy-minimization principles coupled with the constraint of conservation of mass. While the equivalent unconstrained energy minimization problems are in a class for which efficient simulation techniques are known, the constrained versions of interest here are not. Thus, the focus of this research is in the extension of these approaches to the constrained minimization models associated with these problems. Two key aspects will be studied, focusing on the development of so-called optimal algorithms for these simulations, and on developing these algorithms to take full advantage of modern computational hardware. The algorithms in this research program are focused on the multigrid methodology, named after the multiple scales of a problem that are used in order to achieve an efficient solution algorithm. For problems with variable material properties, the key challenges in applying the multigrid approach are in developing appropriately averaged models to account for long-distance behaviour, and in developing efficient complementary techniques for resolving short-distance behaviour. In this proposal, a major focus is given to doing the latter in a way that greatly leverages modern high-performance computing architectures to produce algorithms that are efficient both in theory and in practice. The results of the proposed research promise to provide both a strong step forward in the academic study of these problems and software that can be directly applied in other science and engineering disciplines, and in industrial practice. Thus, the expected economic benefit is significant, particularly to the high-technology and bio-technology sectors. The proposed work will directly result in the training of several students, at the undergraduate and graduate levels, in key skills in the emerging discipline of computational science and engineering, enabling them to fill high-demand roles in these industries, essential for the Canadian information economy.""545880,""MacLatchy, Deborah"
"555909"	"Maddison, Christopher"	"British Columbia"	"CANADA"
"566695"	"Magierowski, Sebastian"	"Nano and Microelectronics for Integrated Sensor Arrays"	"This research introduces new fabrication methods and integrated circuits that will result in a miniscule sensor platform consisting of a high-speed computer embedded with a high-fidelity nanosensor array. In particular, it will physically merge two advanced technologies, nanodevice sensor chips and microelectronic computing chips into a single unit. The nanodevices will sense phenomena at very fine scales (e.g. nanoparticles) and turn them into electronic signals; the microelectronics will perform sensitive amplification, digitization, computation, and communication on those signals for high-speed digital analysis and display. The applications of this research to functions such as protein analysis, rapid DNA sequencing, virus detection, nanoparticle filtering, etc. span interests in science, medicine, and industry. Historically, systems built from technologies-of-scale as considered here substantially lower barriers such as cost and accessibility. As a result, this research will lead not only to superior particle identification platforms but also greatly improve people’s access to applications relying on such technology. Since contemporary nanosensor chips often lack an inherent means for computation and contemporary microelectronics often lacks suitable sensory ability an engineered connection between these technologies is needed. Typically, such links are facilitated with macroscale interconnect resulting in bulky and expensive systems. Fusing nanosensors with computer chips will not only greatly reduce the system size, it will significantly improve its performance by endowing the sensors with intelligent intra and inter-connect. In particular this will improve the processing speed of individual nanodevices, reduce the losses accrued by analog signals communicated over long distances, increase the number of sensors operating in parallel, and allow their measurements to be aggregated in only a few broadband digital communication links. The fusion of nanosensors with computer chips has only begun to emerge and consists of many unaddressed challenges that this research will seek to resolve in the context of a particular modality. The nature of the problem also requires a solution that fuses several conceptual elements: devices, circuits, and systems. From the device perspective this research will focus on microfabrication techniques for adhering an array of sensors to a computer chip. At present a working micro-connection between these remains unrealized. This aspect of the research will advance in steps from a connection mediated by a micro-interposer to a direct fusion between the technologies. From the circuit perspective this research will focus on the design of high-speed, low-noise analog circuitry to amplify the low-power signals available from the sensors. It will surpass other work not only in its core performance, but in its ability to scale to many more channels than presently contemplated. It will advance in steps from the optimization of existing electronics to the adoption of exotic integrated circuit techniques for improved speed and low power consumption. From the system perspective this research will focus on the design of an efficient digital readout system capable of sufficiently sampling and communicating the amplified analog signal to the outside world. No such integrated system presently exists for the sensors under consideration even for a single channel, let alone a vast readout array. This work will serve as a seminal proof-of-concept showing efficient means of handling many asynchronous channels in a nanosensor network context. It will also address fundamental issues such as electronic interference and sustainable thermal and physical footprints.""558984,""Magill, Martin"
"567408"	"Maldague, Xavier"	"Innovative Developments of Infrared Thermography for Non Destructive Evaluation"	"Infrared Thermography can be applied to the Non Destructive Evaluation (NDE) of materials and structures: it is called Infrared Thermography for NDE (or TNDE for short). Basically TNDE can be either passive or active. In the active mode, an external thermal stimulation is brought to the sample and response to this stimulus, recorded with an infrared camera, allows to determine the subsurface structure of the inspected component such as the presence of potential subsurface defects (delaminations, etc.). In the passive mode, isotherms are collected as is without perturbation of the thermal state of the object. The proposal concerns the active mode. Despite being increasingly popular, TNDE still presents challenges since many problems arise either at the stimulation stage or at the interpretation stage. The objectives of the proposed research program are oriented towards solution of these problems and they are in continuity with the long term goal of my research activities. The present proposed research program offers good potential for novel and significant research activities as follows: // - STIMULATION/OBSERVATION STAGES //: Objectives are to study in depth, through thermal modeling and experiments, the effects of various inspection stimulation schemes in order to enhance response from subsurface artifacts of interest. Proposal includes development of the following: • Eddy current thermography: exploit the eddy current injected for heating to get more details from a given flaw. The flaw is thus observed with two NDE schemes: thermography and «traditional» eddy current. • Cold Stimulation Thermography using a «cold» instead of a «warm» stimulation for particular applications (such as in restricted areas for which a warm approach is not possible due to over-heating). • Robotic deployment with adapted heating strategies. Robotic deployment is becoming the new TNDE frontier. // - INTERPRETATION STAGE //: Objectives consist to pursue the development of advanced processing methodologies to extract the relevant features of the acquired dataset to compute quantitative information (depth, size, thermal properties) and enable reliable interpretation of data. Signal processing is a key procedure in the analysis of thermal images. Proposal includes development of the following: • Partial Least Squares regression (PLS). PLS enables to decompose and subsequently reconstruct a dataset with the benefit to omit certain unwanted features (related to non-uniform heating/cooling). • Pulsed phase thermography (PPT). PPT explores the dataset in the frequency domain but looses the time information needed to access quantitative information (defect depth). Extension of PPT with Wavelet Transform (WT) is planned to circumvent this limitation. Next, PLS reconstruction will be combined with WT PPT. • Data fusion of signals from Eddy Current (EC) and Eddy Current Thermography to calculate the probability of an event (defect detection) thus improving the NDE diagnosis. EC is sensitive to surface cracks but has difficulties to detect subsurface defects for which thermography is good since thermal diffusion extend beyond the penetration of eddy currents. Work presented here is promising since it will bring fresh ideas to the TNDE community and industrial users while solving critical problems. Moreover, several HQPs will be involved at all stages of the proposed research activities. The significance of the proposed activities can be seen on two sides. The basis of the work has a strong theoretical foundation, while results of the proposed program are well applied, for instance in Canadian industries where contacts have already been established during previous research activities, in order to ensure valuable spins-off.""557304,""Maldague, Xavier"
"567805"	"Malik, Om"	"Electric Power Systems - simulation, operational behaviour, control and protection"	"The primary thrust of the research proposal is the development of real-time digital control and protection schemes for improved performance and stability of electric power systems. It includes (i) the study of specific problems relating to electric machine behavior, (ii) analysis, design and development of power system control and protection schemes, (iii) generation of electric power from renewable energy sources and (iv) power system equipment aging assessment for reliable and economic maintenance Power system characteristics are non-linear. They can also change with time due to changes in transmission network configuration, equipment aging, etc. The fixed parameter controllers, designed off-line for one fixed operating condition, cannot track the changes in system parameters in real-time. Thus over time they cannot provide optimal performance even during normal operation and the system performance degrades. For proper operation, such controllers need to be retuned at regular intervals. This requires the plant to be shut down and is expensive. A solution to this is to employ adaptive control algorithms that continuously monitor the dynamic behavior of the system in real-time and automatically adjust the controller parameters on-line in real-time according to the prevalent system operating conditions. Adaptive controllers, called self-tuning controllers, that adjust controller parameters automatically to suit actual system conditions and are particularly suitable for application to electric generating units, have been successfully developed. Use of artificial intelligence techniques, such as fuzzy logic and neural networks, to design adaptive self-tuning controllers that are robust, effective and universal is being investigated for application to both conventional and renewable, such as wind power, electric power generation. Extension of these control algorithms to FACTS devices to improve the operation and stability of interconnected power systems is also being explored.  A big concern of the power industry is aging power system assets and lack of techniques in assessing the life cycle of power transformers. Algorithms based on intelligent techniques to mine and evaluate the pool of data available with the newly emerging smart grid technologies that the power utilities can use to commence effective condition based maintenance actions and also prevent unexpected unit failures are being investigated. This could provide enormous savings to the utilities. Concerns over the environmental degradation have started a major move towards both large scale and small scale electrical power generation using renewable energy sources. It has resulted in a whole new direction, including wind farms, small scale distributed generation and micro-grids with new operational, control and protection problems specific to these technologies. A significant part of the research is devoted to various aspects of renewable energy generation, such as safe and reliable protection and control of large wind based generation facilities, voltage and frequency control of small scale micro-grids, and islanding protection of distributed generation sources.""559016,""Malivoire, Bailee"
"567022"	"Mandal, Mrinal"	"Development of Novel Computer-aided Diagnosis Systems"	"A phenomenal growth in the use of digital images has been observed in recent times in medical diagnosis by physicians. Picture archiving and communications system (PACS) are being widely used in health care as it offers several advantages such as low cost, improved quality, and flexibility in sharing patient data among the health care professionals. It also opens up the possibility of using computers to speed up diagnosis and reduce subjectiveness. The computer-aided diagnosis (CAD) system are already being used for breast cancer screening in many countries, and is expected to be extended for diagnosis of many other kinds of diseases. The image analysis techniques play a key role in developing a CAD system. However, the characteristics of images vary significantly across different modalities (e.g., X-ray, CT, MRI, histopathology), tissue/organ types, and diseases. It is a huge challenge to develop image analysis techniques for such a wide variety of images. In this proposal, we propose to carry on research in two areas: Computer aided diagnosis of Skin Cancer and Tuberculosis (TB). The skin cancer, especially Melanoma, and drug-resistant TB ared deadly diseases with high mortality rate. Early screening of these diseases through computer-aided diagnosis would be very useful in health care. In skin cancer, the proposed research will explore development of histopathological image analysis techniques to improve the diagnosis of Melanoma and Basal cell (the most frequent type of skin cancer) cancer. Techniques would be developed for analyzing both epidermis and dermis areas in a histopathological image. Techniques will also be developed for detecting clusters of nuclei, which are observed in the junction of epidermis and dermis area. Due to the noisy nature of the images, the performance of existing mitosis detection technique is low, and we would explore developing a robust mitosis detection technique based on the sparse representation and classification technique. Appropriate techniques for 3D whole slide histopathological image reconstruction and visualization will be developed to enable the pathologist to see a slide in 3D. In TB diagnosis, robust techniques would be developed for more accurate diagnosis. In addition to cavity, and acinar shadows, the ""volume loss"" is a hallmark of TB as a result of its destructive and fibrotic nature. The Hilar and mediastinal lymphadenopathy are good indicators of HIV-related TB, and primary TB. Techniques will be developed to detect these indicators, and reduce the false positive rate of typical primary and postprimary TB. Experiments will be conducted with larger databases and with a wider variety of images such as images with acinar shadows, atypical or normal radiographs without acinar shadows, and radiographs with nodules but without acinar shadows. The proposed research will establish a detailed understanding of several important issues related to computer-aided diagnosis of skin cancer, and TB. To our knowledge, we are the first group working on developing a computer-aided diagnosis system for melanoma. We are also one of very few groups who are working on development of efficient computer-aided diagnosis system for TB. A successful development is expected to reduce the diagnosis time and error, and will result in intellectual property.""547893,""Mandal, Saumendranath"
"567790"	"Mann, Steve"	"Intelligent Image Processing and Signal Processing for wearable computing and AR (augmediated reality)"	"With ubiquitous mobile/portable imaging and computing now a reality, and wearable computing (e.g. Digital Eye Glass) just becoming a reality, we are entering an era in which our everyday lives are being or will soon be mediated by vision-based computers. Many of us already carry a camera and computing device (smartphone or cameraphone) most of the time. These devices will soon take the form of Digital Eye Glass to help many of us see better, and improve the quality of our lives. This technology will be especially welcome to the growing population of older individuals with failing eyesight. Moreover, as technologies become more personal, the Digital Eye Glass will help us remember names and faces (e.g. the wearable face recognizer), find our way (digital maps overlayed with Augmediated Reality), and provide us with improved safety (e.g. personal security), health (sensing, etc.) and well-being. From my long-standing personal experience (more than 35 years of inventing, designing, building, and experimenting with wearable computing and Digital Eye Glass since my childhood -- first as an amateur scientist in my youth, and later in my professional life), I have helped this industry evolve. Much of the industry is addressing immediate business interests, but there is a need for research on the foundational aspects of Intelligent Image Processing specifically related to personal imaging devices (small portable or wearable camera systems). In particular, a core issue that remains is getting cameras to ""see"" as well as the human eye does. Already the spatial resolution of cameras has increased from thousands of pixels to megapixels, but image quality (e.g. dynamic range) at each pixel has not kept pace. The first digital camera was invented (by Steven Sasson of Kodak) in 1975, and had 10,000 pixels (based on a Fairchild 100*100 pixel sensor), and 4 bits-per pixel per channel (1 channel, i.e. greyscale). Today, mobile and portable camera phones are up to 41 Megapixels (e.g. Nokia PureView 808), and some cameras are in the gigapixel range, i.e. spatial resolutions that are more than 100,000 times what they were in 1975. Thus many cameras can ""see"" the detail (e.g. ""read"" small print on a newspaper) thousands of times better now than then. But they still cannot match the human eye for dynamic range (i.e. being able to simultaneously sense in low-light and really bright light). Remarkably, the bit depth of modern cameras is typically only twice what the world's first digital camera was 38 years ago (8 bits compared to 4 bits). Recently I have addressed this problem by inventing and steadily improving something called HDR (High Dynamic Range) Imaging. Robertson et al. write: <blockquote> ""<b>The first report of digitally combining multiple pictures of the same scene to improve dynamic range appears to be Mann</b> [Mann 1993]"" -- [Estimation-theoretic approach to dynamic range enhancement using multiple exposures, <i>JEI</i> 12(2)]. </blockquote> HDR is now used by many commercially manufactured cameras, including Apple's iPhone (which implements HDR internally), as well as many surveillance cameras and other specialized imaging devices. My work will involve basic research in creation of an Intelligent Image Procesing chip for widespread use. The work involves development of new branches of mathematics and engineering, combining comparametric equations, superposimetric equations, functional equations, and quantum field theory, with FPGA (Field Programmable Gate Array) architectures. Another goal is to create Actional Systems (a new kind of kinematics) for circuit modeling. This fundamental work will be of great use to other scientists and engineers who work with image processing, Digital Eye Glass, and AR (Augmediated Reality).""561785,""Mann, Udaybir"
"565997"	"Markley, Loic"	"Wave manipulation using metamaterials for imaging, power concentration, and telecommunications"	"As communication data rates increase and mobile electronics proliferate, the need for advanced research into wireless antennas and wireless power transfer is critical. Metamaterials—artificial materials with exotic electric and magnetic properties—provide a means to achieve precisely this kind of research. Metamaterials can manipulate electromagnetic fields (light, radio waves, microwaves, etc.) to produce strange phenomena such as waves that appear to flow backwards and waves that bend “the wrong way” when they hit an interface. With the entire telecommunications industry invested heavily in wireless networks, any improvements in antenna performance will have a wide-reaching impact. Through careful design, the exotic properties of metamaterials can be used to guide the radiation of an antenna so it can maintain a highly directive beam at a fixed angle over a large frequency range. This allows the resulting wireless link to operate over a wider bandwidth and support higher data rates for faster and more reliable mobile connectivity. Metamaterials can also be used to focus the power transmitted by one antenna towards another in order to make wireless power transfer more efficient at larger distances. Improving efficiency is a big step towards putting wireless power sources in every home in Canada and revolutionizing the way we power and recharge our devices. The objective of this Discovery program is to study wave propagation in metamaterials in order to develop novel devices related to imaging, power concentration, and telecommunications. The field of metamaterials is relatively young; the first demonstrative experiment was performed 12 years ago and we still don’t understand all their limitations. It is therefore essential for this program to continue investigating the electromagnetics of metamaterials. These investigations provide us with the insights necessary to apply metamaterials to new devices like negative-refractive-index lenses or improve on existing technology as was done with the miniaturization of microwave circuit elements. Over the next five years this objective will be applied to the two research themes touched upon in the introduction: near-field power concentration using antenna arrays, and transformation-optics metamaterials for antenna design. This program will provide graduate and undergraduate students with an excellent opportunity to be involved in cutting-edge research on topics which industry holds in high demand. Over the last five years I developed antenna arrays to focus electric and magnetic fields to spots smaller than the wavelength of operation. Before the discovery of metamaterials, this was thought to violate a fundamental law of diffraction which states that light cannot be focused smaller than the wavelength. I am now proposing to adapt this focusing technique to concentrate power to subwavelength spots for localized heating of materials and for wireless power transfer applications. Typically, near-field power transfer and heating are performed with a single source element, so the use of multiple elements will extend the range of operation by an estimated factor of two. For my second research theme on high bandwidth directive antennas, I will place metamaterial layers with gradually changing electromagnetic parameters over a leaky-wave antenna in order to redirect the radiated beam at a fixed angle and double the operating bandwidth. Research into the physics of metamaterials provides the theoretical foundation necessary to pursue advanced research on antennas for wireless data links and wireless power transfer. These topics are of great interest to the telecommunication industry and metamaterials provide an exciting opportunity to make a significant contribution to this field.""543878,""Markovits, Henry"
"567577"	"Masani, Kei"	"Neuroplasticity induced by functional electrical stimulation"	"Compromised standing and walking function among people who have paralyzed muscles due to a neurological disease or injury, such as spinal cord injury or stroke, critically limits their ability to perform activities of daily living, and reduces their quality of life. Functional electrical stimulation, which is a method that can artificially activate muscles in the paralyzed limbs by use of electrical stimulation, can lead to improvement of patient’s affected limb function. For example, when a person with a paralyzed arm needs to grasp a coffee cup, muscles that close fingers are stimulated to produce a grasping movement. This is an effective way to regain functional movements, but it is practically difficult to apply to many complex movements which are required during activities of daily living. It was discovered that repetitive use of functional electrical stimulation can cause permanent changes of the neural circuits in the brain and/or the spinal cord, which is the so-called neuroplasticity. This discovery could have tremendous impacts in neurorehabiliation but the neural mechanisms and the effective ways of using functional electrical stimulation to induce the neuroplasticity are not fully understood. One probable mechanism behind the neuroplasticity induced by functional electrical stimulation is the synaptic strengthening at the spinal cord. When a command from brain and the signal caused by electrical stimulation which is traveling backward on the motor nerve meet at spinal cord, the strength of the synaptic connection between the brain and the muscle increases (i.e., how effectively the command from brain travels to muscle). Repeating this process leads to permanent strengthening of the connection between the brain and the muscle, which can increase the muscle force and improve limb movement. The proposed research program aims to explore the effective ways that functional electrical stimulation can facilitate neuroplasticity in the human motor control system, focusing on this synaptic strengthening at the spinal cord, i.e., what is happening in the spinal cord. For this mechanism, two neural signals are required: One from the brain and the other from peripheral nerves. A signal from the brain can be induced simply by the patient’s voluntary effort (i.e., if patient want to move a limb, a command from brain will travel to the muscle). The other way to cause a signal from the brain is to use transcranial magnetic stimulation. By stimulating the cortical area of the brain responsible for the limb moment, transcranial magnetic stimulation activates the responsible neurons and the neurons send a command to the muscle. There are four ways to cause the neural signal from peripheral nerves; 1) to stimulate the sensory nerve trunk at a nerve pass to the target muscle, 2) to stimulate the nerve ending within the target muscle, 3) to stimulate the motor neurons in the spinal cord, 4) to stimulate the motor nerve trunk at a nerve pass to the target muscle. By appropriately combining the brain neural signal and neural signal from peripheral nerves, it is hypothesized that neuroplasticity can be induced at the spinal cord. In this program, I will test each combination systematically to investigate and develop the most effective way to deliver functional electrical stimulation to induce the synaptic strengthening between the brain and the muscle. At the end, this program will develop a basis of a neuroengineering method which can help people with paralyzed muscles regain their lost movements, which will inevitably lead to increased quality of life.""566664,""Mascher, Peter"
"567304"	"Maundy, Brent"	"Design of Advanced Linear Circuits and Systems"	"My research is aimed at designing and developing new circuits capable of performing some of the simplest mathematical functions using analog circuits. Several of these functions include amplification, filtering, multiplication, division, sensing, and the general processing of signals. There is a need to constantly reexamine such functions because technology is rapidly changing and what may work well in one integrated circuit technology may have to be revised in another. One example of such a function is a logarithmic or exponential digital to analog converter (DAC) function that I am seeking new ways to implement in new integrated circuit technologies. This is to meet the challenges associated with shrinking power supplies and transistor sizes. My research into new logarithmic and exponential multiplying D/A amplifiers is focused on using first and possibly second order approximations to logarithmic and exponential functions, and expanding the dynamic range of those functions to meet realistic requirements. The expectation is that hardware savings may be gained without excessive losses through the use of these approximation functions. Logarithmic DACs find application in automatic gain control circuits and gain or volume controls. Another example of general processing of signals is in filter design. All electronic circuits use filters that pass low or high frequencies, a band of frequencies, or eliminate a particular band of frequencies. Much of my proposed and yet untapped research is aimed at allowing more choice of several recently newly introduced parameters associated with the frequency response of these new filters, hence allowing more precise filter design, than conventional methods. To accomplish much of this, research into new filter circuit design is focused on an emerging and exciting use of fractional capacitors in filter structures. This unique and relatively unexplored element has the property that its impedance is no longer pure imaginary, but it has a real component to its impedance. This element whose properties are not found naturally in most materials has been used by our group to date to produce unusual characteristics in filters such as asymmetry and sharp filter responses that are not available through commercial capacitors or conventional means. As part of our ongoing research I have also discovered they can be used in modeling the electrical behavior of human tissue, agriculture and looking for cancers. While fractional capacitors are presently not available commercially, this entire research has the potential to revolutionize many other disciplines of electrical engineering, all the while being cross discipline. Why, because fractional calculus long time use in controls and system identification has not been really applied to addressing electrical engineering problems and seeking new ways of doing things. My research work therefore seeks to add to the general body of knowledge both in the area of analog fractional filtering and modeling element properties. The latter application, to possibly aid in bioimpedance measurements in general, or characterizing tissue or monitoring for physiological changes. Any discoveries made here can be further used as a potential diagnostic tool for cancer detection and monitoring physiological changes due to other health conditions. With this application, the need for low-cost, wearable/implantable monitoring devices using indirect measurement techniques becomes extremely useful in both monitoring and possibly keeping health costs down.""559752,""Maurais, Luc"
"567234"	"McCalla, Gordon"	"Supporting Learners in Open Ended and Ill-Defined Domains"	"My research is in “artificial intelligence in education” (AIED), an area of computer science that explores how to create learning environments that are deeply supportive of learners. AIED draws on (and contributes to) methodologies from artificial intelligence, of course, but also human-computer interaction, data mining, recommender systems, multi-agent systems, the semantic web, and cognitive science. One key AIED issue is personalization: how the learning environment can adapt to individual differences among learners and tailor its interactions to maximize each learner’s achievements. Personalization is the central concern of another area of computer science: “user modeling, adaptation, and personalization” (UMAP). I contribute to both AIED and UMAP research. One of the issues my graduate students and I have been exploring is simulation of learners and learning environments. We have created simulations to explore various hypotheses using the ecological approach, a novel learning environment architecture (developed in my earlier research) that, over time, can adapt its actions based on patterns discovered in learner behaviour. We have shown that simulations can provide insight into a learning environment without the need for experimentation with actual learners. Simulation could thus lead to reductions in the time and cost of building learning environments. Historically, AIED has been focused on domains like algebra, where there is a well-defined body of knowledge. In contrast, my interests have been drawn to ill-defined domains, where new techniques are needed. One such domain is professional ethics, with two projects. A serious game has been built where software engineering students take the role of a project manager and make decisions (with an ethical dimension) and face consequences, thus illuminating subtle trade-offs they will encounter in their careers. In another project we have been developing a system to support learners in analyzing ethics case studies through a novel visualization (an “open learner model”) that shows them the breadth and depth of their analyses, and how these compare to other learners’ analyses. This stimulates reflection, a metacognitive skill necessary in many ill-defined domains. In another ill-defined domain, language, a system has been built to help people learn Russian pronunciation, with the aid of an innovative “historic learner model” that allows them to visualize their progress over time. Another system we have been developing, in reading comprehension, data mines learners’ behaviour as they read documents and answer questions. Patterns have been found that allow accurate predictions of how well they will answer these questions. These patterns don’t emerge, however, without first classifying the questions as to the depth of knowledge needed to answer them (their “Bloom level”), a novel idea in educational data mining. Going forward we are undertaking new iterations of our work on simulation, ethics, and reading comprehension. We also wish to move further into ill-defined domains to look at lifelong learning. Lifelong learning is a special challenge because learners must be tracked over years, and learning must be supported both proactively and reactively as part of people’s busy lives. Lifelong learning is an emerging area of AIED and UMAP, and our efforts here should pay off both in research impact and real world impact given the need for constant upgrading of personal and professional skills in the information age. The techniques we have been exploring, such as the ecological approach and educational data mining, are helpful in tackling the challenges of supporting lifelong learning, and simulation is crucial to explore implications and test new ideas and techniques.""555666,""McCallum, Erin"
"567552"	"Meunier, Jean"	"Elderly 3D balance and gait analysis with a simple depth-camera system"	"Projections for 2031 show that in Canada, people over 65 will reach nearly 25% in 2031. This major demographic transformation will have important repercussions. As the Canadian population is aging, medical care and indirect costs from falls and musculoskeletal problems will increase. For instance, one of the greatest hazards for elderly is accidental falls and 62% of injury-related hospitalizations for seniors are the result of falls. It is therefore essential to innovate in the area of elder healthcare to identify new technologies to support the elderly. In this context, the general long-term objective of this program of research is to develop a low-cost, easy-to-use and portable video system to identify relevant clinical parameters in elder balance and gait for early fall risk assessment and prevention for use in a clinical setting. This will be achieved by considering the following three specific objectives. 1. 3D Reconstruction: Design and development of simple camera settings to capture real-time 3D reconstruction of the body sufficiently accurate to compute reliable balance/gait measurements. The system will consist in a single depth camera (e.g. Kinect from Microsoft) placed in front of the patient and two planar mirrors positioned behind providing “virtual” rear views to get a full 3D view of the subject during a balance or gait test. 2. Balance and Gait Analysis: Development of innovative algorithms in order to identify relevant clinical parameters from the 3D reconstructed body. To this end, machine learning and statistical analysis of shape and motion features will be used. 3. Validation: 3D data accuracy will be evaluated with 3D geometric object (e.g. cylinder) with known dimensions and with human mannequin previously scanned with a gold standard 3D imaging device (e.g. laser scanner or optical digitizer). Balance and gait analysis will be validated with student volunteers performing simulated balance impairment in laboratory. Correlations between our computed measurements and gold standard functional gait and balance tests will be performed to determine the ability of our methodology. This research will set the ground for the development of unique computer vision-based balance and gait analysis system for elderly healthcare professionals. It will contribute to improve clinical care and consequently the quality of life of elderly but also for patients suffering from other diseases or injuries affecting their balance and gait. We expect that this type of imaging systems will be used in clinics, for instance as a screening tool to detect elderly balance/gait problem early, before more sophisticated tests be conducted in specialized facilities. We hope that the relative simplicity of our system will facilitate balance/gait examinations for the elderly patient and encourage clinicians to use it.""544169,""Meunier, JeanLuc"
"567272"	"Miled, Amine"	"Smart Medical Instrumentation On Chips For Early Detection and Treatment of Brain Molecular Disorder"	"Major advances in medicine, drug discovery and new treatments can considerably improve the quality of life of the world population. On the other hand, new diseases are surfacing and affecting the 65 years and plus population. Most of these health problems are associated with neurodegenerative diseases, which are mainly related to a neurotransmitter (NT) concentration disorder in the brain. Alzheimer’s is one of neurodegenerative diseases that affect the largest portion of the world’s elderly population. The Alzheimer’s society of Montreal estimates that the number of patient suffering from this disease will be 34 million worldwide in 2025 and will reach a total of 750 000 patients in Canada in 2031. Up to now, there is no treatment for Alzheimer’s disease, but there are several alternatives to reduce its impact and effects. Our discovery proposal will offer hope for patients suffering from this disease by controlling the neurotransmitters (NTs) concentration in a local area of the brain in order to stop the disorder caused by Alzheimer’s using miniaturized and implantable microsystems. Indeed, NTs are among the most important information vehicles in the brain. Almost all human behavior and body functions are monitored by NTs and any disorder due to these molecules will affect our quality of life and health conditions. Our recently published results show that the electrical properties of cerebrospinal fluid (CSF) are affected by the NTs concentration. We have identified a relation between CSF impedance, conductivity and NTs concentration. Finally, we have proven with glutamate NT and gamma amino butyric acid (GABA) NT that the CSF impedance and conductivity are affected by the change of NTs concentration. As an example, Alzheimer’s disease is related to a considerable decrease of acetylcholine NT which affects CSF properties based on our previous research. Furthermore, as the concentration of NTs can be in the range of sub-nano liters, we need an extremely highly sensitive device with an accuracy of 0.001 S/m and 10 ohm to measure the conductivity and resistivity of the CSF, respectively. An innovative contribution of this work is the establishment of a new perspective for the analysis of brain neuronal activities where one assumes that NTs (acetylcholine, dopamine and GABA) can be differentiated based on their concentration, impedance and/or conductivity. If successful, this could provide an important alternative to the current treatment of patients with neurodegenerative diseases such as Alzheimer’s. We are proposing a new generation of brain implantable devices that are sensing the chemical and molecular brain activity and can also achieve neurotransmitter local delivery in order to stop brain disorder. In order to achieve this objective, we propose a medical instrumentation on-chip (MIOC) to sense and modulate NT concentration in limited area. This discovery covers the following aspects of MIOC design:  1- Development of a new fabrication process to design microelectrode for NTs sensing and manipulation and biosensing techniques for NTs based on carbon nanotubes.  2- Design and Implementation of highly sensitive microelectronics circuit to detect NT concentration change  3- Development of a new in-channel and air bubble liquid sampling technique to sample a few nano-liters of NT. Such a technique, will certainly open other alternatives to replace conventional micro pumps as it does not require any dedicated microfabrication process. The proposed MIOC can also be generalized for blood chemical and molecular disorders such as for cholesterol, sickle-cell anemia, etc. as the sensing and liquid manipulation architecture are the same while the bio-interface differs from one application to another.""562728,""Miled, Amine"
"567554"	"Moez, Kambiz"	"Area-Efficient Low-Power Radio Frequency and Millimeter Wave CMOS Integrated Circuits"	"Area-Efficient Low-Power Radio Frequency and Millimeter Wave CMOS Integrated Circuits In the last two decades, the implementation of radio frequency (RF) building blocks of wireless transceivers along with the rest of systems on a single Complementary Metal Oxide Semiconductor (CMOS) silicon chip has enabled the low-cost integration of several wireless platforms in today’s portable electronic devices such as smartphones, tablets, wireless sensors, and many others. However, the RF part of the system typically occupies a significant portion of silicon chip area because of the necessity to use on-chip passive components particularly on-chip spiral inductors. In addition, RF circuits usually consume a significant portion of total chip power. Growing consumer demand for increased functionality at reduced cost and extended battery life necessitates greater research efforts on design and implementation of area-efficient low-power radio frequency and millimeter-wave (MMW) CMOS integrated circuits (ICs) and systems to lower microchip fabrication cost and reduce power consumption. The long-term objective of the proposed research is to develop new device, circuit, and system structures to enable economical CMOS-based solutions for widespread adoption of radio-frequency and mm-wave devices in mainstream consumer electronic products. To lower the cost of RF and MMW ICs, we plan to develop highly-linear low-noise transistor-based passive components (TBPCs) to replace their area-inefficient passive counterparts in major RF building blocks. These TBPCs such as active inductors emulate the electrical behavior of their passive counterparts only occupying a fraction of the chip area of their passive counterparts. Using the same concept, we plan develop new passive devices such as negative capacitors and inductors for which no passive alternatives exist in order to enhance the performance of RF/MMW circuits and systems. The TBPCs such as negative resistors and capacitors can potentially compensate for the losses and cancel the effect of parasitic capacitors of RF/MMW circuits resulting in gain enhancement, bandwidth extension, and lower power consumption. In order to extend the battery life of portable wireless devices, we plan to work on lowering the power consumption of RF/MMW circuits through development of RF circuits in the moderate inversion region where the ratio of transistor gain to the power consumption is the highest. In addition, fully integrated RF energy harvesting systems will be used to charge the device battery on the go to further extend the battery life of portable electronic devices. Finally, to enable new MMW wireless technologies become commercially available in mainstream electronic products, we aim to design and implement MMW integrated circuits in low-cost CMOS process. In this project, novel structures of RF/MMW devices, circuits, and systems will be developed for variety of applications including high-data-rate wireless communication, wireless sensors, biomedical imaging, security, and many others. As implemented in CMOS technology, these new devices lower the cost and reduce the size of final product making them economically feasible for mainstream consumers. We expect to create new knowledge in the field as the proposed devices will be developed to outperform the previously reported designs in terms performance, area efficiency, and power efficiency and consumption. This new knowledge can be commercialized by start-up companies or through licensing to established corporations and will be disseminated in respected publications to advance the research in the field. In addition, several graduate students will be trained throughout this project as integrated circuits and systems designers.""565580,""Moez, Kambiz"
"566125"	"Moschopoulos, Gerry"	"Power Electronics for Renewable and Conventional Energy Systems"	"Advances in electrical and electronic technology (EET) have revolutionized society in ways that were unimaginable only a few years ago. Advances in consumer electronics, computational power, data storage and global networking have facilitated significant quality-of-life improvements that are now taken for granted - from the mundane, such as online banking and office communications, to the miraculous, such as medical breakthroughs and economic globalization. EET is essential to the functioning of industry, academia, government and society at large. The amount of energy needed to power and maintain the infrastructure needed for this technology, however, is considerable and is expected to increase exponentially as society becomes even more dependent on EET and energy consumption continues to skyrocket. There are two general approaches that can be taken to address the problem of escalating energy needs that threaten to overtake available energy capacity. The first is to increase energy capacity, but there is little appetite in today's society to invest in the construction of new conventional energy sources such as coal plants due to environmental concerns and renewable energy solutions such as solar and wind are considered to be more attractive in a modern and environmentally conscious society. The second approach is to improve the energy efficiency of today's technology, in order to ""squeeze"" as much energy as possible from existing energy sources while minimizing inefficiencies. The proposed project seeks to consider solutions that take into account both approaches by proposing advances to renewable energy systems and energy utilization with respect to their power electronics. New and improved power converters for renewable energy systems will be developed and researched as a means to help increase their energy capacity and new and improved converters for conventional energy systems will be developed and researched as a means to reduce energy waste. The novelty will be in the originality and improvement in cost and/or efficiency of the proposed alternatives to existing converters. Cost and complexity will be reduced by using topologies that require fewer active switches, efficiency will be increased using techniques that reduce the power lost in the converter switches when they are being turned on and off with high frequency, performance will be improved by the development of novel architectures that take advantage of the new building block converters that will be developed. The research will be focused on solar and wind renewable energy systems and on ac-dc converter interfaces for conventional energy systems as well as for renewable energy systems, as part of microgrid and smart grid technology. The proposed research will help strengthen Canadian capabilities in renewable energy systems and train highly qualified personnel – 3 PhD students, 4 Master's and 2 undergraduate students - to develop multidisciplinary skills for the 21st century.""554274,""Moscovitch, Morris"
"567725"	"Munzner, Tamara"	"Accelerating the Improvement of Visualization Design Methodology"	"My research creates interactive visualization tools to help people find patterns in data. For this grant I propose work that will lead to a faster and better process for making these tools. The modern era is characterized by the promise of better decision making through access to more data than ever before. When people have well-defined questions to ask about data, they can use purely computational techniques from fields such as statistics and machine learning. However, many analysis problems are ill specified: people don't know which of many questions to ask. In such cases, the best path forward is an analysis process with a human in the loop, exploiting the powerful pattern detection properties of the human visual system. People use visualization (""vis"") tools for many reasons. They can be stepping stones to gaining a crisper understanding of analysis requirements before developing formal models. They might be aimed at the designers of another system to help them refine that system's algorithms, or at end users of another system who are trying to decide its behavior is reasonable, for example to see if the results of a machine learning system are trustworthy before deploying it. In addition to these transitional uses, vis tools are also designed for permanent use, where a human intends to stay in the loop indefinitely. A common case is exploratory analysis for scientific discovery, where the goal is to speed up and improve a human's ability to generate and check hypotheses. An example from my own work is a tool to help biologists studying the genetic basis of disease through analyzing DNA sequence variation. Vis tools can also be used for presentation; for example, the New York Times has deployed sophisticated interactive visualizations. Many vis idioms have been proposed: loosely speaking, idioms are distinct approaches to creating and manipulating visual representations, ranging from simple scatterplots to complex depictions of network structure at different scales. However, end users are offered little general guidance about how to choose an appropriate idiom for their domain. The research I propose here will bridge this gap, offering a general methodology for vis design. This will allow vis practitioners to develop better tools more quickly, thus providing benefits to the many people who rely upon vis for business-critical decisions. I am well positioned to conduct the proposed work because of my experience in three other threads of research, all of which I am continuing to advance under other funding. One is also methodological, but addresses the high-level process of vis research by proposing research models. The other two, the “technique-driven” and “problem-driven” legs of my research program, involve building tools. I will use three main evaluation methods in my new work on methodologies for vis design. I will conduct (1) controlled quantitative lab studies and (2) qualitative field studies, both of which are techniques with a long history in human-computer interaction and cognitive psychology. I have also proposed a new method, (3) data studies, which invert the emphasis of lab studies by collecting judgements about a large number of datasets from a small number of human experts. A major focus of my research program is providing HQP with vis expertise to Canadian industry and academia; the demand already outstrips the supply. The need for vis tools is becoming increasingly pervasive across industry, science, health, and the exploration of personal and public data. Supporting foundational visualization research in Canada also opens the door to the creation of spinoff companies; for example, a recent US vis startup (Tableau) was one of the biggest IPOs of 2013, and now has a $4B market capitalization.""551074,""Murai, Keith"
"567152"	"Mussbacher, Gunter"	"Concern-Driven Requirements for Adaptive Socio-Technical Systems"	"The research program’s vision is to build the next generation, model-based software requirements engineering environment based on large-scale software reuse to address the challenges of *high-impact, socio-technical systems*. Such systems are characterized by the complex interrelatedness of people with conflicting goals, technology, organizational processes, and the diverse social context in which these systems are deployed. For example, providing affordable health care to millions of uninsured US citizens falls into this category, as does a United Nations program to coordinate vaccination regimes around the world. Furthermore, systems like that need to constantly adapt to the ever changing social, economic, and technical environment to remain relevant and useful. Large-scale systems have to take many concerns into account (e.g., availability, security). Separation of concerns is the ability to concentrate on one concern, while temporarily forgetting about all other concerns, allowing humans to handle the complexities of such systems. However, some concerns, called *crosscutting concerns*, are intricately interconnected with many other concerns, making it difficult to analyze concerns individually. Existing techniques for advanced separation of concerns address this issue by formalizing the relationship of a crosscutting concern with other concerns, which enables reasoning about the impact of a crosscutting concern on a system. Consequently, systems can be analyzed and built by composing a set of reusable concern units, regardless of whether concerns crosscut or not. Nevertheless, *concern-driven development*, i.e., the systematic application of large-scale concern units across software development phases, has not yet received significant attention. Much can be learned from success stories for software reuse, ranging from generic, small-scale programming libraries to planned, large-scale Software Product Lines (SPLs). SPLs are product-oriented, i.e., they describe a family of similar products, e.g., a set of hearing aids. This research pursues concern-orientation instead – leading to *Software Concern Lines* – to maximize the potential reuse benefit, because concerns are applicable to not only one product family but across a wide range of product families. In addition, the need for run-time adaptation requires support when selecting a reusable unit out of a set of similar, applicable ones for the current context. The lightning speed with which changes in context propagate through today’s information world means correct selections need to be made quickly for relevant run-time system adaptation. Such support is not yet readily available. This research proposes concern units that include a set of coordinated, heterogeneous, reusable models at different levels of abstraction and use the most abstract, most high-level requirements model possible when composing concerns to adapt to changed context. The research program’s objective is to advance concern-oriented modeling languages, frameworks, and tools for requirements engineering of adaptive socio-technical systems, taking social, economic, and technical context into account for run-time system adaptation with the help of reusable concern units. The research program’s impact is improved large-scale reuse and hence improved productivity, leading to innovation in and increased competitiveness of the many organizations served by the Canadian software industry. Software Concern Lines are expected to become a viable development choice for the Canadian software industry even in the context of rapidly evolving software development projects, and will allow the software industry to further specialize, bringing software engineering more in line with other engineering disciplines.""561611,""Musselman, Hailey"
"567434"	"Myrvold, Wendy"	"Difficult Combinatorial Search Problems and Graph Theory Models and Algorithms for Carbon Frameworks"	"Theme 1. Difficult Combinatorial Search Problems The graph coloring problem takes as input a graph G and the aim is to color the vertices of G so that adjacent vertices are different colors. This problem is so hard that there are some reasonably small graphs such that finding an optimal coloring is either overly time consuming or not feasible using existing algorithms. One goal of my research is to find faster graph coloring algorithms. The graph coloring problem has many applications including determination of conflict-free schedules, map coloring, and register allocation. Venn diagrams give pictorial representations of all possible logical relations between a finite collection of sets. We plan to exhaustively generate new classes of Venn diagrams. The computational results can be used to investigate conjectures about Venn diagrams. One example of an open question is Winkler's conjecture that any simple Venn diagram can be extended to a larger simple Venn diagram with the addition of a single curve. An (r, g)-cage is a graph having a minimum number of vertices such that each vertex has degree r and the minimum cycle size is g. Cages have attracted a lot of interest from the graph theory community and have properties that make them appealing for a network topology. There are many values for r and g where there is a large gap between the lower bounds given for an (r,g)-cage and the upper bound coming from the number of vertices in a smallest known existing r-regular graph of girth g. Our intent is to try to close those gaps. Theme 2. Graph Theory Models and Algorithms for Carbon Frameworks Graphs are often used by chemists as models for molecules. The molecules considered in this research have carbon frameworks. Fullerenes correspond to 3-regular planar graphs with face sizes 5 or 6. Although fullerenes were only recently discovered (1985) they have been the subject of intense research because of their unique chemistry and potential application in materials science, electronics, nanotechnology and medicine. Benzenoids are unsaturated molecules composed of fused hexagonal rings. More generally, polycyclic aromatic hydrocarbons (PAH's) have mixtures of ring sizes 4, 5, 6 and 7. They occur naturally and as atmospheric pollutants from burning fuels, and can be carcinogenic. The goal of this research is to better understand properties of these molecules such as currents, stability, geometry, and resonance energy. Understanding induced currents is critical for interpreting chemical characterisation by Nuclear Magnetic Resonance. Graph theory models for current represent current by giving a direction and magnitude to each edge of the molecular graph. Graph theory models for currents can be easier to implement and give a simpler representation of the answer than some other numerical approaches. They facilitate prediction of currents for infinite families of molecules. One of our goals is to compare current models to each other looking for inconsistencies and anomalies. The next step is to refine or redevelop the graph theory based methods so that they more accurately reflect the current. The graph theoretic approaches published so far are better suited to benzenoids which are often fairly flat. We plan to develop extensions for predicting current in 3D structures such as fullerenes, or cases where the graph has no perfect matchings or where the cycle sizes vary as for PAH's. Further research will involve determining correlations between computed graph invariants and chosen molecular properties. Definition of new invariants is all too easy, but we will identify those that are efficiently computable and offer information content related to geometry and energy of molecular and extended carbon frameworks""561666,""Na, MengXing"
"555241"	"Nagavalli, Sasanka"	"Neglect Benevolence in Multi-Robot Systems"	"Multi-Robot Systems, Human-Robot Interaction, Swarm Robotics, Multi-Robot Coordination, Dynamical Systems, Algorithms, Distributed Optimization, Control Theory, Artificial Intelligence, Dynamic Planning""563071,""Nagib, Hasan"
"567509"	"Nasiopoulos, Panos"	"Immersive, Participatory Television: A New Paradigm of Social Media"	"Traditional broadcast television has been slow to adapt to competing technological advances in content creation, transmission, and consumption made possible by the Internet and a plethora of digital media devices. While the production values and editorial capabilities of broadcast TV remain superior to those of digital media alternatives, its ability to engage digital-savvy younger viewers is waning. We propose to address this significant problem by creating a participatory platform for broadcast TV that enlists viewers as content contributors, and adds further dimension to programming through Augmented Reality and Web-based content enhancements. Crowdsourcing contextually relevant content in an era of ubiquitous video cameras will usher in an era of participatory TV, where the viewer is no longer passive but an active contributor to programming. Remote communities rarely visited by reporters would have their voices heard. Contributors, appropriately screened by a “trustworthiness” tool to be developed in this research, would be rewarded by peer recognition, possible financial incentives from the broadcaster, and career development opportunities. Tweets and other social media notifications would draw viewers’ attention to the program and deepen the pool of virtual reporters/producers. This democratization of TV programming would not only increase viewer engagement; it could have the corollary effect of an increased sense of citizenship and social inclusion/responsibility. Viewer engagement can be further increased by adding the ability to pause programming and seamlessly access supplemental contextual information relevant to a segment, on the main or a second screen. A viewer watching an episode of “Republic of Doyle” on CBC, for example, may wish to pause the program and access a Google Street View image of downtown St. John’s, or watch a recommended National Film Board documentary about Newfoundland history following the show. They may wish to rotate and flip an Augmented Reality 3D image of a human heart while watching an educational program about coronary disease. Over time, personalization features developed in this research will present choices to the viewer based on past viewing/information-accessing patterns, and will be captured in a user profile. Within the context of video-processing and interactivity, specific research tasks will address creation/retrieval/organization of structured interactive content, crowdsourcing and social interaction, verification, trust and quality assurance, recommendation, personalization, 3D augmented reality, and user evaluation. Enabling technologies will be targeted for incorporation into set-top boxes or future televisions. Local/national broadcasters will be approached for feasibility and prototype testing. The novel immersive TV experience to be realized by this research will help contextualize an overwhelming amount of digital media source material through broadcaster expertise, while at the same time adding dimension and a heightened sense of social engagement to the traditional TV viewing experience. Viewers from along the demographic spectrum will be attracted to broadcast television, ensuring its continued relevance. New social communities of contributors and viewers will develop around specific interests, further enriching the content of broadcast TV and increasing its audience base.""552684,""Nasiopoulos, Panos"
"567437"	"Nedil, Mourad"	"Wireless Body Area Networks for advanced underground mines applications"	"Wireless communication technologies in mining environment will have a significant impact on mine operations in the coming decades. For instance, continuous monitoring of workers and equipment is a crucial safety item. However, when a disaster occurs, regrettably, it is still noticed that some of the fundamental problems have not been resolved yet. In fact, underground mine are known to be an environment into many fatal events might occur (e.g. Fires, toxic gases such as presence of Carbon Monoxide CO and Methane, oxygen concentration, etc.). The rescue process could be also highly risky without detecting environmental information with high accuracy. In order to fully exploit the benefit of wireless technologies in underground mines, a new type of wireless network emerges: Wireless Body Area Network (WBAN). In this research program, we seek to exploit the promising features of WBAN with wearable antennas-sensors in the miners’ protective clothing to provide information about the wearer’s state of health and environment parameters in this harsh environment. These wearable antennas should meet bio-compatible and size limit requirements and need to be flexible for wear comfort and sufficiently robust to ensure a good communication link. However, several phenomenon like geothermal heat, mining machines, high temperature and humidity are present in this harsh environment where conventional substrate water/humidity absorption can lead to unacceptable antenna losses. Hence, new materials engineering and technologies must be identified properly to tackle simultaneously these challenges of performances. In the first part of this research program, we propose to develop a novel class of wearable fluidic antennas based on Polydimethylsiloxane (PDMS) elastomeric substrate that tolerate severe mechanical shock by flexing instead of breaking. The flexibility of PDMS substrate enables conforming to any shape and to fabricate multilayered structures by simple bonding techniques. These characteristics make them suitable for wearable antennas. On other hand, the conventional solid patch antennas, which uses a copper as a conductor, are poorly suited for flexible devices because it fatigues when bent repeatedly and undergoes irreversible plastic deformation. We aim to fully utilize the deformability of the PDMS substrate by replacing the solid metal components with a fluid metal, thus allowing significant stretchability and flexibility over conventional, solid antennas. Actually, multipath and shadowing effects in this environment (underground mine) make the signal levels fluctuate severely while the miners move around. The quality of the link can be drastically improved using Multiple Input Multiple Output (MIMO) techniques to combine signals transmitted and/or received by multiple antennas. This MIMO system will be designed and built to overcome the lack in the link budget and the line of sight blockage limitations due to the miners and machinery present in this environment. In addition, the human body has a high dielectric constant with a high loss tangent and low conductivity at the microwave frequency band. Therefore, the gain and radiation efficiency of the antenna can be deteriorated when it is operated on the human body. The purpose of the second part of this research program is to evaluate the performance of underground WBAN-MIMO communication channel in terms of path loss, channel correlation, capacity, gain and average spectral efficiency in order to establish an appropriate radio communication system in underground mine.""555393,""Nedil, Mourad"
"566595"	"Ng, WaiTung"	"Gallium Nitride (GaN) Power Transistors, Gate Driving Techniques and Next Generation Integrated Power Converters"	"Gallium nitride (GaN) power transistors promise to be the game changer for the next generation integrated power converters. Traditional silicon based power MOSFETs are already reaching their material limits for power conversion. Aluminum gallium nitride (AlGaN)/GaN high-electron-mobility transistors (HEMTs) with high breakdown field, high-mobility 2-D electron gas (2DEG), high saturation velocity, and low intrinsic carrier density are emerging as the ideal candidate for the implementation of high frequency (10’s to 100’s MHz) and high efficiency (>90%) converters. The wide bandgap materials also allow GaN power devices to operate with a high junction temperature (> 200°C), relaxing the need for expensive heat removal mechanisms. The availability of GaN on silicon substrates can further reduce the cost of fabrication by making good use of existing silicon-based manufacturing facilities. This proposal is focused on the development of two key enabling technologies to fully exploit the potential of integrated GaN power converters: enhancement mode metal insulator semiconductor field effect transistors (MISFETs) and intelligent gate driving techniques. Currently, majority of the GaN based power transistors are depletion mode HEMTs. The normally “on” characteristics made them less favorable when robustness and fail-safe characteristics are critical. Monolithic integration of silicon-based devices and GaN devices is still not practical at this time (although GaN on Si technology will eventually allow the co-existence of CMOS circuits and GaN power HEMTs). As a result, there is a critical need to develop novel methods/structures to implement true enhancement mode GaN power devices with silicon processing compatibility, low leakage current, long term reliability and simple gate drive requirements. The development of enhancement mode GaN power transistors will involve the investigation of suitable gate insulators and device structures, passivation techniques and silicon compatible ohmic contacts. In particular, the oxidation of AlGaN layer, recessed gate etching techniques, silicon nitride passivation and gold-free ohmic contacts such as recessed Ti/Al/W contacts will be studied. In order to take advantage of the inherent switching speed of the GaN power devices to implement integrated power converters with fast transient response, high efficiency, and compact form factor, dedicated gate driving techniques are essential. Due to the presence of parasitic inductance and capacitance, rapid switching between ground and supply voltage levels will produce ringing oscillation, leading to unwanted power losses (reduced efficiency) and electromagnetic interference (EMI). The finite turn-on and turn-off switching speeds between the high side and low side power transistors in a typical output stage could also lead to a momentary short between the supply voltage to ground (shoot through current) during every switching period. This would result in unwanted power loss. The second theme of this project encompasses the design intelligent gate driver ICs to provide precision control of the gate voltage during switching to suppress both EMI and shoot through current in GaN power converters. The development of gate driving ICs with continuous dead-time correction (in sub-nanosecond increment) and dynamically programmable output resistance will allow high speed switching (>100’s MHz) and low switching loss to be achieved simultaneously. Finally, the proposed work will also explore methods to combine the CMOS gate drive circuits and the enhancement mode GaN power transistors to realize the next generation high speed, high efficiency integrated GaN power converters on the same silicon substrate.""555132,""Ng, YanWah"
"566006"	"NgatchedNkouatchah, TelexMagloire"	"Resource Allocation in Coded Cooperative Relay for Multi-channel Cognitive Radio Networks"	"By providing unlicensed (also known as secondary or cognitive) users the capability to share the wireless channel with licensed (also known as primary) users in an opportunistic manner, as long as the interference introduced to primary users is kept below a predefined threshold, cognitive radio technology has emerged as a promising solution to improve the utilization of wireless spectrum resources. Cognitive radio networks are envisioned to provide high bandwidth to mobile users via heterogeneous wireless architectures and dynamic spectrum access techniques, while improving connectivity and self-adaptability of channel environment. This technology is, however, limited in coverage and network connectivity due to emissions restrictions imposed on secondary cognitive radio terminals. On the other hand, cooperative communication technology has been studied extensively for traditional wireless networks and shown to be a powerful approach to combat the detrimental effects of wireless channels and improve system performance by exploiting the spatial diversity over dispersed nodes. It allows multiple wireless nodes to share their resources (bandwidth and power) and assist each other in information delivery, with the objective of achieving better overall performance, i.e. greater reliability and efficiency. Moreover, coded cooperation, that integrates channel coding with cooperative communication, provides significant performance gains for a variety of channel conditions while maintaining the same information rate, transmit power and bandwidth as a comparable non-cooperative system. The aim of this research program is to provide a novel practical framework for the fundamental problem of resource allocation (relay selection, power, rate, and channel) in energy-efficient relay-based cognitive radio networks. This will integrate coded cooperative communication technology into the design of the communications protocols of cognitive radio systems to improve the performance of both spectrum sensing and secondary transmissions, and to reduce the required transmission power of each cognitive user with quality-of-service guarantee. The anticipated outcomes will provide some scientific insights on issues related to the efficient development and operation of reliable cognitive radio networks with high data rates, and thus strongly impact the future and evolution of wireless communications, by contributing to the large-scale deployment of the cognitive radio technology. These outcomes will be of significant importance to the research community, the wireless industry, government labs, the practitioners in the field, and spectrum regulatory agencies.""551590,""Ngô, AnhDung"
"567842"	"Nielsen, Christopher"	"Coordinated set stabilization : application to robotic manipulators"	"Consider the problem of designing algorithms that make a collection of robotic manipulators work together to paint the fuselage of an aircraft. The paintbrush attached to the end-effector of each robot is charged with staying on the surface that describes the shape of the fuselage. Each robot's end-effector is restricted, through feedback control, to move along the surface describing the fuselage. On the surface, the team of robots must communicate and co-operate to coordinate their motions so that they do not unnecessarily paint over the same region too often and efficiently complete the task. Partially motivated by the above example, the proposed research project studies the coordination of nonlinear control systems on pre-specified sets in their output spaces. Given a heterogeneous collection of nonlinear control systems, the objective is to design a feedback control laws, one for each system, that drive the closed-loop output of each system to its pre-defined target set. The target sets must be invariant, in other words, if each system is initialized on its target set, it remains on the target set. Once on the target set, the systems should cooperatively complete application specific tasks in an optimal manner. This proposal focuses on (a) set stabilization of nonlinear systems while ensuring desired dynamics on the target set (b) coordinated set stabilization for nonlinear control systems with emphasis on Euler-Lagrange systems (c) experimental verification of coordinated set stabilization algorithms. The expected outcomes of this proposal are distributed nonlinear control algorithms whose effectiveness are mathematically proven and demonstrated experimentally on laboratory equipment. This research is partly motivated by the emergence of self-organizing factories in industrialized nations as a way to increase productivity and compete with low-cost manufacturing in developing nations. North America and Europe are moving to significantly increase industrial value creation. The next stage of industrial manufacturing, a move towards `smart factories', has been described as a fourth stage of the industrial revolution. This notion of `industry 4.0', or `the internet of things', is modernizing manufacturing processes and increasing productivity in industrial manufacturing in North America. The 'internet of things' has been identified as a disruptive technology that will transform life, business, and the global economy. For example, Boeing is using a team of robots, working together, to paint the wings of the Boeing 777. These robots have boosted output and dramatically reduced the time required to paint the wings from 4.5 hours to 24 minutes. This research aims to generate systematic methods to design control algorithms for coordinated set stabilization of systems with nonlinear dynamics. We seek to mathematically prove the correctness of the algorithms while also experimentally demonstrating their effectiveness on mobile robotic manipulators. Mobile robotic manipulators that can work co-operatively are well suited to re-configurable, flexible manufacturing systems. Solving the fundamental problems considered in this proposal related to the design of nonlinear control algorithms promises to help enable the 'internet of things' in manufacturing.""546601,""Nielsen, John"
"565966"	"Noureldin, Aboelmagd"	"Next Generation Smart Navigation Technology – From Systems to Smart Agents"	"The recent advances in information and communication technologies necessitate the development of reliable positioning and navigation (POS/NAV) systems. Wireless positioning technologies including Global Navigation Satellite Systems (GNSS) like GPS in open sky and wireless Local Area Networks (WLAN) indoors are presently widely used for POS/NAV applications. However, they still suffer from several shortcomings such as signal blockage, interference and multipath in some challenging environments. Integration of GNSS/WLANs with other sensors (like accelerometers, gyroscopes and magnetometers) can bridge the accuracy gaps in such challenging environments. However, current integrated POS/NAV systems still depend on traditional methods (like Kalman or particle filtering). These methods suffer from significant limitations in integrating other sophisticated sources of information such as digital maps, knowledge bases, geographical information systems, image bases, and other sensors like vision and laser scanners. This gap calls for a paradigm shift in the traditional concept of navigation. The new concept has to transform the POS/NAV technology from the traditional view of individual and isolated application-specific systems toward the concept of smart navigation agents that can read, process, interpret, infer, learn from, adapt to, communicate and integrate a wide range of information and sensors measurements. The strategic long-term goal of this research is to establish and realize a reliable and accurate POS/NAV platform customizable to several applications to enhance the safety, security and quality of life. To achieve this goal, this project targets integrating GNSS/WLAN technologies with number of emerging sensors/systems that have not been sufficiently explored for POS/NAV purposes. The integration of such growing number of sensors/technologies is a challenge that cannot be addressed using only traditional methods like Kalman or particle filtering. Therefore, this research aims at designing an efficient framework for integrated POS/NAV based on modern soft computing techniques. This framework will lead to developing navigation agents that can smartly manage and coordinate this heterogeneous set of sensors/technologies into a consistent, accurate, and robust smart navigation system. Moreover, this project targets the realization and the deployment of the developed smart navigation agents in different mobile platforms for several applications utilizing the recent advances in computation capabilities and the availability of the above-mentioned sensors and sources of information. This project offers a fundamental conceptual change in the filed of multi-sensor integrated navigation. A POS/NAV system that can work seamlessly on both outdoor and indoor environments and on different mobile platforms has the potential to become a ubiquitous and enabling technology for many applications. The significance of this research is in the opportunities it opens to enhance quality of life in many areas. For instance, people with disabilities and limited mobility will enjoy an enhanced autonomy and safety. Car industry will benefit from a POS/NAV system that increases driving efficiency and enhances both driver’s experience and safety. Unmanned vehicles widely used in national defence and mining industry seeking meter level accuracy will also benefit from the proposed POS/NAV technology. Furthermore, students and researchers acquiring knowledge in the above technologies will be a great asset to the academia and industry in Canada.""546823,""Nourelfath, Mustapha"
"556033"	"Ocariza, Frolin"	"Facilitating DOM-JavaScript interactions in web applications"	"JavaScript, web application, reliability, Document Object Model, Integrated Development Environment, code completion""551164,""OCarroll, Denis"
"566019"	"Ono, Yuu"	"Wearable ultrasonic sensor technology for biomedical monitoring"	"One of the particular and significant issues for elderly people is that once they experience a physical disorder, they are often placed on bed rest. This requires special and intense care from caregivers to maintain their daily activities. Also, musculoskeletal diseases impair a patient’s physical ability. Early diagnosis of such ailments is crucial for appropriate treatment in order to prevent further progression of the disease. In addition, mechanical properties of biological tissues, such as stiffness and viscosity, change due to the disease, aging, injury and fatigue. Thus, such properties contain significant and useful information for medical diagnosis. X-ray and MRI are capable of providing anatomical images with high spatial resolution for medical diagnosis. However, they are rather expensive and available only in clinics or hospitals. Ultrasound has been a powerful tool for medical diagnosis and is widely used since it is noninvasive, risk-free (no ionizing radiation exposure), and capable of capturing images in real-time at a high frame rate and at low-cost. Ultrasonic diagnostic techniques can provide not only static images of anatomical structures but also real-time dynamic information of internal tissue motion associated with physical and physiological activities. If one can obtain internal tissue motion and mechanical properties quantitatively, in addition to conventional qualitative images, such augmented information would greatly aid doctors in making fast, accurate, and appropriate diagnoses. In addition, it is beneficial if real-time and continuous ultrasonic monitoring and diagnosis could be performed outside of clinical facilities and laboratories, without restricting daily life activities. Current non-invasive, wearable and/or non-contact techniques employed for monitoring physical activities, such as muscle contractions, include mechanomyography, surface electromyography, and 3D motion capture. However, these techniques lack sensitivity/spatial resolution in the tissue depth direction, or are capable of only monitoring the external motion at the body surface but not the detailed motion of internal tissues. Therefore, it might be difficult or impossible to assess the individual muscle located deep in the body. The objective of this proposed research program is to develop a wearable ultrasonic sensor and associated signal processing algorithms that can continuously monitor internal tissue motion in real-time during physical activities. Features of this sensor are: lightweight, flexible, mass-producible, low-cost, and thus, disposable. Disposability of the sensor is desirable for a medical device that contacts a human body since it does not require disinfection or sterilization after its usage. The wearable ultrasonic sensor technology to be developed in this proposed research program is able to measure the mechanical behavior and properties of tissues of interest quantitatively. The developed technology will enable monitoring of physical conditions of a subject outside of hospitals, clinics and laboratories. Furthermore, people will be able to wear cost-effective ultrasonic sensors that will continuously monitor their health conditions. The applications of the develop technology are not only in the areas of medical diagnosis and health monitoring but also in rehabilitation, kinesiology and sports for effective and efficient physical training. Science, engineering and clinical researchers could have benefits from this new ultrasonic sensing technology to study and understand a human physiology by monitoring tissue motion of their interest during various physical activities.""544446,""Ooi, BoonTeck"
"566498"	"Pal, Christopher"	"Big Data Processing and Analytics - Mining Noisy Visual Data and Learning Transferrable Predictive Models"	"The term ‘Big Data’ has recently emerged to characterize a wide variety of techniques and problems that involve the capture, management, processing, analysis and use of large quantities of data. The increase in our ability to capture, store and process data has reached a point where the impact of Big Data is now seen on the front page of national newspapers. The applicant has extensive experience across a variety of prototypical Big Data problems, including a previous discovery grant on ‘large scale data mining’. Research will focus on developing broadly applicable techniques for visual data processing and analysis through looking at a variety of problems involving visual data with the potential for high impact. Key areas will consist of medical image analysis, object and activity recognition focusing on applications to video indexing, next generation computer animation, intelligent transportation and robotics. Research will focus upon the following common needs, problems, challenges and research questions identified through the applicant’s first hand experience with previous Big Data research projects, namely: 1) The need for acceleration techniques capable of processing large data sets including pre-processing, feature extraction, data modeling, optimization and analysis techniques. 2) The need for principled theory and implementations of techniques that optimize over relevant measures of performance while also accounting for different pre-processing, model complexity, resource constraints and the amount of labelled vs. unlabeled data. 3) The challenges associated with how to most effectively exploit potentially enormous quantities of unlabeled data, as well as weakly or partially and/or noisily labelled data. 4) The problems associated with data collection and reducing human labelling effort. 5) The open questions of how to more effectively transfer learned models or representations obtained using data in one domain to another domain. To achieve these goals we will perform experiments using standard evaluation data sets as well as further develop, curate and create a number of our own data sets covering the themes of faces, emotions, human activities, scene types, objects and medical imagery. In particular, to obtain large quantities of weakly or noisily labelled data and perform experiments on methods for learning with noisy labels we will further develop a large dataset of video that has been annotated by Descriptive Video Services for the blind. We will also collect our own 3D and 4D object and activity recognition data sets centred on the themes of intelligent transportation, robotics and computer animation. Recent research combining large data sets with highly accelerated optimization of deep neural network learning techniques has yielded impressive results on a wide variety of competitive problems. Here we will explore and compare the ways in which other novel deep architectures and other techniques can benefit from the combination of big data and algorithm acceleration. Accelerated algorithms will then be used to develop techniques for the complete optimization of pipelines including both pre-processing steps and hyper-parameters. We also wish to enhance the transferability of models and representation when applied to test data that has been collected in related but different settings. We hypothesize that complete pipeline optimization may lead to more transferrable methods if validation sets representative of the underlying types of domain transfer are used. For visual recognition, we also hypothesize that techniques explicitly accounting for the 4D nature of our world may yield improved transferability. The project will result in the training of highly qualified personnel in the high demand area of Big Data.""545292,""Pal, Rajinder"
"567344"	"Perkins, Theodore"	"Inference and Scaling in Stochastic Dynamical Systems"	"Dynamical systems are a ubiquitous feature of our world: from the small (the motions of single atoms) to the large (galactic dynamics), from the concrete (foraging patterns of ants) to the abstract (flow of information on the internet), from the quick (neural impulses) to the slow (evolutionary trajectories of different species' genomes). Being able to understand such systems is crucial to fields as diverse as biology, chemistry, physics, economics, engineering, and, not least, computer science. My research focuses on computational methods for the analysis and estimation of dynamical systems, particularly stochastic dynamical systems. The current proposal focuses on two recent problems on which my lab has made important breakthroughs: characterizing path distributions for discrete-state discrete-time stochastic systems (Markov chains), and inference of maximum-probability paths for discrete-state continuous-time stochastic systems (continuous-time Markov chains). On the path distribution problem, we recently clarified and improved upon a 50-year old conjecture by Mandelbrot by proving necessary and sufficient conditions for the distribution of paths generated by a Markov chain to be powerlaw. We also showed that finite and stretched-exponential distributions are possible, with the distribution type depending only on the structure of possible state transitions, and not on the exact transition probabilities. We developed graph-theoretic computations to discriminate between the cases, and efficient eigenvalue / dynamic programming computations to determine distribution parameters. In this proposal, we will investigate the uses of our theory for model selection. In the case that empirical scaling of the path distribution is observed and a Markov model needs to be estimated from the data, how can we incorporate scaling information into the model estimation process? We will also work to extend our results to more sophisticated models of sequential data--in particular, stochastic context-free grammars, which are relevant to the analysis of natural language, music, genome structure, and many other complex real-world systems. For the path inference problem, we have recently developed an approach called State Sequence Analysis for identifying the most probable sequences of states visited by a continuous-time Markov chain over some period of time. Using this approach, we obtained novel insights into a number of domains, including stochastic protein folding, the evolution of drug-resistance mutations in HIV, and ion channel dynamics. In the present proposal, we will seek to extend our results to noisy / partial observation models, as well as more general forms of waiting-time distributions in the states of the system. This will allow State Sequence Analysis to be applied to a much wider range of target domains.""564508,""Perlman, Mark"
"566563"	"Peters, Terence"	"Effective Visualization for Image-guided Interventions"	"Augmented reality (AR), where computed models are merged with real-world data to provide enhanced visualization of one’s surroundings, is finding use in many area from architecture, gaming, education, sports and navigation. One domain where AR has a great deal of (as yet unrealized) promise, is in medicine to guide minimally-invasive therapy or drug delivery to organs inside the body. In this application, instead of exposed targets being accessed manually during an open procedure, they are approached by means of tools placed through ports in the patient’s skin. The rationale for minimally-invasive interventions (MII) is that in many cases more trauma is caused to the patient by the access route than by the therapy itself. Recent improvements in medical imaging technology have motivated the development of minimally-invasive intervention techniques, with virtual representations of organs or targets, registered to the patient, augmenting images from an endoscopic camera, an ultrasound transducer, CT or MRI acquired prior to the procedure, or various combinations of the above. The use of virtual information to augment the intra-operative image view has been the enabling factor in these procedures. This proposal seeks to develop and demonstrate appropriate AR strategies that are intuitive to use and that will enable new minimally-invasive approaches to be realized in a manner that is intuitive and acceptable to the user. AR, Virtual Reality (VR), and Augmented Virtuality (AV) where a virtual model is fused with additional data that may not directly re-create a realistic visual environment, have all played a part assisting interventional procedures but have not become part of mainstream surgery. The aim of this research is to demonstrate that the use of VR/AR/AV techniques, appropriately employed in an intuitive user interface, significantly lowers the cognitive load of the user, and significantly increases operator performance for minimally-invasive procedures. These ideas are described here for three representative projects, including Laparoscopic surgery, Cardiac surgery and Neurosurgery, among the spectrum of ongoing lab activities.""544091,""Petersen, Nils"
"566435"	"Peyton, Liam"	"A Framework for Instrumentation and Integration of Process Performance Management"	"Our increasingly on-line and wireless-connected world makes it ever more feasible to collect data to monitor business processes as they are taking place. Managers and workers should be able to access and react to that data while the business process is taking place to ensure quality, safety, security, regulatory compliance and improve performance. Unfortunately, the performance management frameworks that enterprises currently use to collect and communicate such data typically make the data available after the fact. In practice, there is a significant organizational and architectural gap between information technology that supports online business processes, and information technology that supports performance management frameworks. The two are usually addressed independently of each other when building information systems. As a result, essential relationships between business processes and performance management are poorly communicated. Enterprises know at a high level how well business processes performed after the fact, but the insight as to which combination of steps in a process contributed to success or failure is often uncertain or communicated too late. Healthcare process performance management with a focus on quality of care (wait times, readmission rates, infection rates, patient satisfaction) is an application area for this research as is aeronautics process performance management with a focus on safety. Security and regulatory compliance are important to both as well. Our long term objective is a comprehensive information technology framework for process performance management that can provide dynamic statistical control and surveillance of business processes to ensure quality, safety, security, compliance, and performance that is accessible to managers and workers alike, while those business processes are taking place. The framework must provide a sufficient level of sophistication so that process performance management can be fully integrated and instrumented into the day to day operations and management of business processes in complex situations such as health care and aeronautics process performance management. A major hypothesis at the core of our research is that process performance management applications should be defined and implemented independently of the business processes they monitor and the performance management frameworks that communicate overall enterprise performance. By having a flexible, well-defined architectural integration point between the two, a process performance management application can be concisely and precisely defined that manages the relationship between operational business processes and strategic enterprise goals. This will enable the application to be flexibly and efficiently developed, deployed and updated on a continuous basis as the business processes and strategic goals of an enterprise evolve. Our research will focus on three short term objectives that support our long term objective. PPM Meta-model: development of an application meta-model and a model-driven approach for generating process performance management (PPM) applications. PPM Architecture: development of a PPM engine to execute PPM applications and a catalog of patterns for integrating the engine with business process and performance management architecture. PPM Methodology: development of a test-driven methodology for building PPM applications that ensure quality, safety, security, compliance, and performance as processes and strategic goals evolve. All three short term objectives will be validated using case studies and design science research methodology in complex domains such as healthcare or aeronautics process performance management.""548347,""Pezacki, JohnPaul"
"555417"	"Pierre, Samuel"	"Infrastructure Cloud
"	"La planification des machines virtuelles dans un environnement « inter-Cloud » est un processus qui consiste à trouver le schéma d'allocation le moins coûteux pour un ensemble de machines virtuelles dans différentscentres de données. Ce problème peut aisément s'assimiler à celui du « Bin Packing » qui est connu comme un problème difficile. Différentes méthodes de résolution ont été proposées pour le résoudre. En effet, des approches automatiques ont été privilégiées au détriment de la résolution manuelle. Car, en raison des mises à jour et surtout de la taille des problèmes, il est très difficile, voire même impossible, d'évaluer manuellement l'éventail de toutes les possibilités. Pour ce qui est des méthodes exactes, elles ont l'avantage de fournir la solution optimale. Cependant, le temps de calcul peut être prohibitif. Une manière efficace de contourner ce problème consiste à recourir à des heuristiques conduisant généralement à un bon compromis entre la qualité des solutions et le temps d'exécution. L'objectif principal de ce programme de recherche est de concevoir un cadre (Framework) multi-objectifs, assorti d'un ensemble d'outils logiciels, pour la planification globale des machines virtuelles dans un environnement « inter-Cloud » dans la perspective d'une allocation optimale des machines virtuelles aux centres de données, en tenant compte des interactions entre les différentes particularités de ces machines.L'infrastructure demandée comprend trois composants essentiels : un Cloud privé (A), un Cloud public (B) et un Cloud de stockage (C). Le Cloud privé (A) constitue un espace de travail virtualisé, desservi par 12 postes de travail servant de points d'accès à l'infrastructure. Le Cloud public (B) est un réseau virtualisé qui donne accès à un environnement de machines virtuelles. Le Cloud de stockage (C) regroupe des unités de stockage physique qui serviront de ressources à partager entre les différents utilisateurs de l'infrastructure.""559604,""PierreDoray, François"
"566842"	"Plamondon, Réjean"	"Human movement modeling and understanding: biomedical, biometrical and neurocognitive applications."	"1-Context : Human movement modeling is of great interest for the design of pattern recognition systems relying on the understanding of the fine motor control as well as for the development of intelligent systems involving in a way or another the processing of human movements. Among the models providing analytical representations of the trajectories, the Kinematic Theory of rapid human movements and its lognormal families of models have been used to explain most of the basic phenomena reported in classical studies on human motor control and to study basic factors involved in the fine motricity. Several software packages have been developed by our team over the years to extract the lognormal parameters from various pen tip velocity curves under different experimental data acquisition conditions and set-up. These different tools allow a researcher to study handwriting through the parametric lognormal description of the neuromuscular networks involved in a given task. 2-Goal: The present proposal aims at elaborating a fundamental and theoretical background for any handwriting processing applications as well as providing some basic knowledge that can be integrated in the development of many automatic systems in three fields of applications: Biomedical: We will focus on three main diseases that affect neuromotor control: Cerebrovascular accidents, Parkinson disease and Alzheimer disease. We will also study how children learning handwriting slowly move and master their motor control behavior. Biometrical: We will focus on signature verification and writer identification to point out which combinations of neuromuscular features are the most representative of a person writing and signing habits. Neurocognitive: We will focus on extending the theory to eye movements, incorporating visual feedback to the models, generalizing the theory to the modeling of 3D movements and putting the whole emergent approach in the context of a universal and global physical model. 3-Methodology and strategy. Our methodology is based on the exploitation of our parameter extraction algorithms to study the behavior of the neuromuscular system under various experimental conditions for different practical applications. Since there are numerous potential pathways that can be explored (which cannot be done by a single isolated team), we have signed recently several research cooperation agreements with various national and international partners. These agreements allows our collaborators to use our software in the context of non-profit research projects in which our team plays the key role of an active associate providing basic knowledge to successfully use our technology and direction in research to many students, resulting in the formation of highly qualified personnel. 4-Impact. A part from training numerous students and researchers, the whole program will lead to important theoretical breakthroughs in neuromotor modeling and set the knowledge background for the design of numerous automatic or interactive devices in three lucrative fields of pattern recognition. This can be illustrated by a typical actualization of the vision that supports our research. In years to come, the gesture and the handwriting habits of people using intelligent handheld devices will be monitored and analyzed by their own apparatuses, in the context of the our neuromuscular models. These systems will be able to identify their users, protect their equipment and sensitive data, to follow the evolution of their fine motor control to check for any departure form lognormality that might necessitate medical intervention. These new tools will facilitate man machine interactions in numerous applications where handwriting and drawing remains the most natural and ergonomic way to communicate.""557806,""Plan, Yaniv"
"567079"	"Plourde, Eric"	"Development of innovative speech enhancement algorithms based on the central auditory system."	"Multimedia devices such as tablets, smart phones and now smart watches and glasses are commonly used in noisy environments by millions of Canadians. These devices include many speech processing algorithms, such as speech coders or automatic speech recognizers (ASR), whose performances are seriously affected by the presence of noise. For example, an ASR can identify 85% of the words correctly in a noise-free environment; however, this percentage can drop to 31% with a signal-to-noise ratio (SNR) of 10 dB. In order to limit this decrease in performance, speech enhancement (SE) modules, which aim at reducing the noise level without affecting the speech quality, are included in these devices. The performance of these SE modules is largely sub-optimal. In fact, a study having compared 14 of the best SE algorithms reports a maximum subjective score of barely 3/5 for an SNR of 10 dB. In sharp contrast, the auditory system deals very well with noise. In fact, it is fairly easy for humans to follow a conversation in a relatively noisy environment. The long-term objective (+ 10 years) of my research program is thus to develop commercially viable SE algorithms that are inspired by the central auditory system, i.e. the part of the auditory system between the auditory nerve and auditory cortex, with the goal of approaching the excellent performance of the auditory system in the presence of noise. In the short-term (less than 5 years), the main objectives will be to statistically model the representation of noisy vocalizations by the neurons of the central auditory system as well as to develop SE algorithms based on these models. To achieve these short-term objectives, we will first represent neural discharges as point processes and use this representation to develop statistical models of neural coding and decoding; neural coding being the estimation of a spike train given a stimulus, such as a vocalization, and neural decoding, the estimation of a stimulus given a spike train. These models will specifically take into account the presence of noise in vocalizations. Furthermore, we will use the derived models to develop statistical estimators for SE. Since these statistical estimators will be set in a domain closer to the one of the central auditory system, we expect the resulting estimators to be more perceptually relevant and thus more efficient. The recent development of accurate, yet simple, statistical models of neural signals opens a promising research avenue for SE that will be exploited in the current proposal. Moreover, this proposal will allow for the training of multidisciplinary researchers having skills in neuroscience, statistical signal processing and speech processing. Upon completion, this program will improve the performance of SE modules and will therefore allow a much more efficient use of millions of multimedia portable devices such as tablets, smart phones, watches or glasses.""565900,""Plourde, Jinny"
"555312"	"Pomerleau, François"	"Three Dimensional Navigation in Harsh and Remote Environments"	"Remote sensing, 3D mapping, Autonomous navigation, Monitoring, Search & Rescue, Field deployment, Robotics, Laser Rangefinders""567877,""Pomeroy, John"
"555572"	"Popov, Pavel"		"NSERC"
"567636"	"Popovich, Milica"	"Breast screening with microwave pulses"	"OBJECTIVES: This Discovery Grant program proposes development of a low-power pulsed microwave device for breast tissue screening, including design, implementation and testing on patient volunteers. The device will complement existing clinical measurements rather than replace them; the proposed system is intended for regular screening as a tool for monitoring changes and significant departures from a healthy baseline. SCIENTIFIC APPROACH: The primary goal of the program is to develop and test a prototype of a bra-like detection device that incorporates a networked array of microwave sensors which can be used regularly by women at home or in small family clinics. Recent research suggests that low-power microwaves can detect small, early-stage tumors by exploiting the electrical contrast between malign tumors and healthy breast tissue in the microwave frequency range (100MHz – 10GHz). If a tumor is present, then part of an incident electromagnetic wave, launched into the tissue, scatters. The microwave propagation loss in the fatty tissue of the breast is sufficiently low to allow for this scatter to be detectable by receiving antennas on the skin surface. The antenna elements are inexpensive and small, so they can be woven into the fabric of a bra, a structure that is familiar, comfortable and simple to use. Acquired signals will be processed with sophisticated algorithms, yielding results that can alert the physician to potential abnormalities. METHODOLOGY: Several clearly identified components of the proposed prototype will be examined concurrently. 1. Antenna design and layout. The antenna-sensors need to be small, inexpensive and printed on a flexible substrate. In addition to targeting favorable characteristics of individual antennas, the number of sensors, as well as their relative layout will be optimized within spatial and hardware constraints. The substrate and the medium which couples the breast to the antennas need to be chosen in accordance with average electrical properties of the breast tissue. These challenges will be addressed through thorough simulation, fabrication and testing. 2. Microwave pulse generation. The signals fed to the transmitting antennas will be custom-shaped for selection of the frequency range of interest, as well as optimal usage of antenna capabilities. 3. The switching mechanism. Antenna elements act as transmitters one at a time, while the rest of the network operates in the receiving mode. Each scan involves a set of sequential measurements as the antennas take turns in their transmitting and receiving roles. This task includes careful consideration of circuit connections which will minimize signal loss and maintain signal integrity. 4. Sequential anomaly detection. In collaboration with experts in the signal-processing field, sophisticated algorithms will be developed to detect any abnormalities and significant changes. Regular feedback will be collected from medical experts to assess the range of changes which correspond to normal tissue changes due to hormonal cycles. The algorithms will be tested on synthetic signals (obtained through electromagnetic simulation with comprehensive MRI-derived human breast models) as well as on signals measured with realistic breast phantoms and volunteers. 5. Overall system design for patient comfort. This includes investigations of overall patient position (standing or prone), softness of materials in contact with skin, and minimizing the scan time. CONTRIBUTION TO TRAINING HIGHLY-QUALIFIED PERSONNEL: The multi-disciplinary nature of our program allows it to be a training platform for experts in a range of areas applied to a healthcare-centered problem: electromagnetic simulation analysis, antenna design, microwave engineering and signal processing.""544901,""Popowich, Frederick"
"566220"	"Potter, Mike"	"Applications and Innovations for Computational Electromagnetics"	"Understanding the complex interactions of electromagnetic fields in devices and materials is critical in many application areas, ranging from high-speed circuit design to imaging and non-destructive testing. With the exponential growth in computing power experienced over the past several decades, there has been an ever-increasing demand for the tools that computational electromagnetics can provide to tackle engineering problems. Although there are dozens of methodologies for approaching such problems, the Finite-Difference Time-Domain (FDTD) method has become one of the de facto standards, because of its ability to robustly handle interactions of fields with complicated structures and materials. The applicability and efficiency of the method continues to be a vibrant research area, and in particular the method is being continually expanded to apply to a greater range of larger and more complicated modeling scenarios. The aims of the proposed research are to further improve and extend this method, and thus make it an even more indispensable tool for modeling of scenarios with complex material models and/or large structures. The work covered in this proposal consists of two main research areas. The first is the utilization of efficient wave sources in the FDTD method for modeling in complex environments. The second is to investigate the continued augmentation and innovation of the FDTD method on alternative grids for complex material modeling. The first area of research aims to overcome the restrictions placed upon modeling by complicated antenna geometries, by using the concept of equivalent sources to scale down the required computational resources (e.g. memory and processing power). In the FDTD method, equivalent sources can be used to accurately and efficiently represent radiating sources, thus obviating the need to model the source explicitly, allowing for considerable reduction in computer resource requirements, hence allowing for performance of quicker and/or larger simulations. This is particularly important where forward modeling is utilized in multiple iterations as part of an inverse scattering procedure. Such inversion procedures are used in applications where it is desired to build a map of the actual structure of the environment, for example for mapping underground structures to identify oil reservoirs (of particular importance for Canadian energy industries), or for building a tomographic map of the structure of the breast in order to identify malignant tumours. By improving the modeling capabilities of the underlying simulation tools we are inherently enhancing the capability to deal with more and more complicated and realistic practical problems of interest. The second area of research is intended to overcome the restrictions placed upon modeling by the original FDTD method, which is based upon a rectangular (Cartesian) layout of field components on a 3D grid. This choice of grid layout inherently introduces approximations and errors into the modeling, which can be partly mitigated by choosing a higher spatial resolution in simulation and/or better numerical appoximations, but at the cost of higher computational resources. Recently we have shown that both types of errors can be significantly reduced simply by structuring the FDTD method around a different grid from the outset, called the Lebedev grid. The grid also allows us to model complicated materials with higher accuracy compared to existing methods. In particular, augmentation of the method will allow us to model the propagation of waves in the earth-ionosphere system more accurately. This system is of partiuclar importance to Canadian reseachers involved with Space Physics and atmospheric/magnetospheric modelling.""548644,""Pottinger, Rachel"
"566413"	"Prodic, Aleksandar"	"Digitally Controlled Low-Power Hybrid SMPS"	"Switch-mode power supplies (SMPS) are highly efficient power processing devices providing regulated voltage or current for virtually all electric devices today. Dc-dc and ac-dc low-power (LP) SMPS, processing power from a fraction of watt to several hundreds of watts are widely used in computers, communication devices, portable electronics, and numerous other applications. For example, in modern cell phones more than 20 different SMPS supply various functional blocks. In those applications, the LP SMPS are among the largest contributors to the overall volume and the weight of the device. As such, they are one of the main obstacles in further minimization of the devices. Most of this volume, by far, is taken by the inductors and capacitors of the SMPS. The size of these reactive components is dictated by topological limitations and requirements for high power processing efficiency. The existing LP SMPS almost exclusively rely on fairly basic 2-switch and 4-switch power stage topologies, such as buck, boost, and their simple derivations. These topologies put stringent physical limits on the possibilities for the minimization of reactive components and for efficiency improvements. The dominance of these fairly rudimentary topologies is a direct consequence of the traditional use analog controllers in LP SMPS. The analog solution provide simple and cost-effective implementation but are ill-suited for the regulation of advanced converter topologies with a larger number of switches or/and operating modes, which provide more possibilities for volume reduction and efficiency improvements. This is mostly due to very limited ability of inflexible analog controllers to implement multi-mode control laws required for regulation of more advanced topologies. The ultimate goal of this research is to develop new topological and complementary control solutions that will overcome existing physical limitations and allow further minimization of the LP SMPS. To achieve the main goal, the new generation of supplies, named hybrid SMPS, will combine several different power transfer and processing techniques, which are usually used separately, inside a single power converter structure.""556547,""Prodic, Aleksandar"
"567095"	"Prusinkiewicz, Przemyslaw"	"Mesoscopic models of plant development"	"A central question of developmental biology is how organisms acquire their form. Computational models play an important role in addressing this question. Their relevance is due, in part, to the self-organizing character of many developmental processes, which is difficult to analyze and understand without the use of models. Models of plant development constructed to date can be broadly divided into two classes. Microscopic models aim at explaining the spatio-temporal patterns of morphogenetically active substances that control growth. These models operate at the scale of molecular processes taking place in individual cells. Macroscopic models relate local growth to the global form of entire plants and operate at the level of large plant components. Examples include the elements of the branching system (apices and internodes) and organs such as leaves, flowers, and fruits. In principle, microscopic and macroscopic models are complementary, but their correspondence has been tenuous, creating a gap in the understanding of development. Addressing this gap, I propose to devise a conceptual and mathematical framework, modeling methods, and software for modeling plant development at the intermediate, mesoscopic level. This level is characterized by spatial scales larger than individual cells but smaller than whole plants, highly non-uniform growth resulting in radical changes in size and shape, feedback between patterning and growth, and the emergence of new structural elements, including differentiation of vasculature, through self-organizing patterning. I propose to harness the complexity of mesoscopic development using a two-prong approach, where the essential patterning properties of molecular and cell-level processes are distilled into geometric constitutive rules, and the development of large-scale patterns, forms and structures is characterized in terms of these rules. Theoretical notions and computational methods will draw on ideas from computer science, mathematics and physics, and will be applied to biological problems rooted in experimental data. Case studies will include the development of compound leaves, flowers and inflorescences. The results are expected to contribute a unifying perspective to developmental plant biology, and advance methods for realistic modelling of plants in computer graphics.""562451,""Pruski, Kris"
"567794"	"Qian, Li"	"Efficient and Versatile Fiber-based Quantum Photonic Sources"	"Quantum photonic sources are to quantum technology what lasers are to optical technology. In the 50 years after the invention of laser, lasers have emerged from a laboratory curiosity to a necessity in everyday life. Its applications cover everything from DVD players to industrial cutting and welding, from laser surgery to long-distance communication. Indeed, the Internet would not have been possible without lasers. Analogously, quantum photon sources will likely play an equally important role as lasers do in not too distant future. They produce photons exhibiting quantum correlations that cannot be mimicked by classical particles. The quantum correlations can be harnessed to perform functions impossible to achieve classically, such as unconditional security in communication, efficient algorithm in quantum computing, and ultra-high resolution in quantum imaging.  A basic quantum photonic source produces entangled photon pairs, which, broadly speaking, are pairs of photons (or light particles) prepared in a special quantum state that makes them exhibit correlated properties. For example, for polarization-entangled photon pairs, the measurement of one photon’s polarization state reveals that of its partner, even when they are physically far away from each other—a phenomenon once described by Einstein as “spooky action at a distance”.  It is this “spookiness”, or non-intuitiveness, that inspired the unconventional usage of the quantum resource. Entanglement-based quantum key distribution (QKD), for example, utilizes the correlation of the photons sent to two remote parties and allows them to share a common set of random cryptographic keys. It guarantees the security of the keys since any eavesdropping attempt would result in a detectable deterioration of correlation. Entangled photons are also used to construct many other types of quantum photonic sources, such as heralded single photon sources, multi-photon entanglement sources, etc. They are essential tools for quantum computing, quantum communication, quantum metrology, and quantum lithography.  Here we propose to develop an efficient and versatile quantum photonic source using optical fibre. Entangled photon pairs can be generated from higher energy photons in a nonlinear optical material. Optical fibre is typically inefficient for this purpose due to its weak nonlinearity. We propose to use poled fibre, one that has been subject to a high electric field, for efficient entangled photon pair generation. Compact, robust, fibre-based sources are highly desirable for both QKD and metrology applications that use fibre for transmission or photon delivery, because they remove the need for beam alignment, eliminate coupling losses to and from fibre, and facilitate turn-key operation perfect for field deployment.  Quantum technology will play an important role in the 21st century. Quantum photonic sources are essential for many quantum applications. This proposal aims to take an important step forward in the development of quantum technology for practical applications.""563778,""Qian, LinFengVictor"
"566398"	"Qiao, Sanzheng"	"Algorithms for Lattice Basis Reduction and Applications"	"Lattice basis reduction, founded by Minkowski, has been successfully applied to numerous areas, such as Multi-Input Multi-Output (MIMO) signal detection, the GGH (Goldreich, Goldwasser, Halvevi) cryptosystem decryption, Global Positioning Systems (GPS) and symbolic computation. In MIMO detection, although the problem size is small, the real-time system demands quick response, On the other hand, in cryptography, the problem size is large, but reasonably long computation time can be tolerated. In either case, algorithms that are fast for small problems and their complexity does not grow fast as the problem size increases are necessary. The focus of this program is efficient lattice basis reduction algorithms that produce good quality results and their applications. They can benefit Canadian researchers and industry in communications by reducing bit error rate, in security by improving cryptography systems and in symbolic computation by providing fast polynomial factorizations. There are different notions of lattice basis reduction, among which the Minkowski reduction is the strongest, in other words, it defines a basis consisting of shortest vectors. The other notions are approximations of the Minkowski reduction. In particular, the famous and widely used LLL (Lenstra, Lenstra, Lovasz) reduction method is fast and produces good approximations of the Minkowski reduction in practice for small size problems. All existing lattice basis reduction methods, like the LLL algorithm, are based on reducing the vector lengths of a given basis for a lattice. Recently, we proposed a completely new approach: Jacobi-type methods based on improving the orthogonality between given basis vectors for a lattice. In this program, starting from our recently developed generic Jacobi method for basis reduction, we will first improve the generic algorithm and analyze its convergence and complexity. It is known that computing an optimally reduced basis is a non-polynomial time problem. Our goal is to develop polynomial time Jacobi-type methods that compute good approximations of an optimal solution. Practically, we also investigate techniques to improve running time for small to moderate size problems. We then investigate applications in lattice reduction aided MIMO detection and GGH attacks. Moreover, Jacobi-type methods are attractive, because they are inherently parallel. High performance can be achieved by implementing the methods on GPUs. In this program, we also explore parallel lattices basis reduction algorithms and their GPU implementations. We strive for the best. The efficiency of our methods will be measured by both theoretical complexity and practical running time. The quality of our methods will be measured by the commonly used orthogonality defect and the condition number of the computed reduced bases. The challenge is that the well-known LLL algorithm, which is regarded as the best practical lattice basis reduction algorithm, will be used as our comparison benchmark. The long-term goal of the program is to develop a reliable software and transfer the technology to communication, security, and software industries, contributing to wireless communications, computer security, and softwares like Maple and Sage for research and education in Canada.""553855,""Qiao, Wenlian"
"566020"	"Raahemifar, Kaamran"	"Unsafe Driver Behavior Detection Using Novel Dictionary Algorithm"	"Accidents happen on the city and highway roads for many reasons. Some of the factors contributing to road accidents include the emotional state, fatigue, and inattentiveness of the drivers and pedestrians. The short-term goal of this research program involves with driver’s facial expression recognition by analyzing their eyes, eyebrows, and lips. A camera network installed inside the car is utilized to take still images. Image processing techniques are used to extract the facial objects and features necessary for emotion detection. The long-term goal of this research program is to expand the in-car camera system with smart-camera networks installed on intersections to alert the drivers and pedestrians of potential dangers due to fatigue or inattentiveness. The novelty of this research is the use of an enhanced dictionary approach in image processing where new atoms are introduced. Emotion/fatigue recognition is the identification of various expressions based on a database of input images when an image is passed through a trained algorithm. One application of emotion/fatigue recognition is in identifying the upcoming physical reaction of an individual based on his/her state. Fear, anxiety, distraction, anger, inattentiveness, and fatigue could compromise the body’s balance, and impact a healthy individual's stability during standing and walking. They also negatively affect the individual’s reaction time. Fatigue is a known cause of many driving accidents resulting in injuries and death. Therefore, fatigue and changes in emotions detected via human's facial expression could alarm an upcoming accident. In this research, we analyze drivers’ facial expressions to identify potentially dangerous conditions and alarm drivers and pedestrians involved in the scene. Typically, recognition algorithms consist of three main steps: 1) the acquisition step in which the artifact of an individual while expressing a state is detected; 2) feature extraction and representation in which the extracted components are represented in several different ways based on the selected feature extraction method; and 3) expression classification step in which using the extracted features, the algorithm determines the best suited emotion of the participant. One of the challenges in these algorithms is to achieve a high level of recognition rate, a low level of misdetection (false alarm) rate, as well as high sensitivity and specificity rates. Noise could result in false recognition; e.g., a neutral face could be mistaken as a sad face, or a calm voice could be identified as a disturbed voice. Dictionary learning, especially when combined with other signal processing algorithms, has proven powerful in feature extraction. We have shown that dictionary algorithm yields better results when other nonlinear atoms are introduced into DCT (discrete-cosine transform) dictionary. However, this enhanced approach has not yet been applied to emotion/fatigue detection and that is what this research program aims at. The deliverables of this research program are: 1) An enhanced image processing algorithm that detects the emotional state of the driver inside a car with high classification rate, 2) object orientation detection for intersections where cars direction, and pedestrian inattentiveness are identified, 3) communication protocol between in-car camera system and intersection smart-camera system, and 4) warning system for both drivers and pedestrians. By developing a technique to detect the unsafe behaviors of drivers and provide them with proper warnings, this project promotes road safety and accident prevention. Fewer accidents leads to reduced cost of treatment (including rehabilitation and incident investigation) and reduced loss of productivity (or absenteeism).""563060,""Raahemifar, Kaamran"
"566295"	"Rakai, Logan"	"New Technologies for Very Large-Scale Electronic Design Automation"	"The field of electronic design automation is concerned with creating software capable of automating the process of designing electronic devices. The electronics industry is known for exponential increases in design complexity and performance of resulting circuits [1]. At the same time, new technologies are being developed in other fields to solve difficult problems, for example, machine learning, MapReduce programming models, and high performance computing systems. The research program outlined in what follows is aimed at developing new ways to solve very large scale problems in electronic design automation using new technologies.  Major challenges being faced in the field are related to problems arising from scale. For example, the very large-scale problems arising out of creating circuits with billions of components. Industry is constantly feeling pressure to quickly create devices for competitive producers of electronics including computers, phones, and tablets. Automation is the key to being able to design quickly, yet the most challenging designs require substantial intervention from designers to configure the software for each design. Many iterations of a design can take place before converging to a final design meeting product specifications. The design process may take between several months to over a year for cutting-edge circuits. Hence, the targets may not be met in time to meet the producer’s deadlines. Failing to meet a deadline can equate to big losses and loss of future customers in business. These factors all go to support research into solving large problems at scales never previously seen before. The ultimate goal of the research program is to develop several pieces which will be the beginning of a highly scalable design automation tool. The tool will be available through a cloud service where the number of computing cores is elastic and practically limitless. Consumers would be able to have designs of arbitrary size and pay for more or less computing resources depending on how quickly they need the circuit fabricated. Having such a tool would greatly benefit industry and the projects described in this proposal will bring this vision closer to reality and inspire other researchers to pursue a similar goal. The projects discussed in the proposal have a direct impact on the multi-billion dollar electronic design industry. However, the research also involves topics that are of strategic importance to Canada. This is evidenced by the overlap with NSERC's strategic grants target areas in information and communication technologies that emphasize a close connection with industry and recognize market potential of research, collaboration, technical and financial implications, and realizing value of the research within five years. In particular, the categories of Next Generation Computing Platforms: cloud computing, From Data to Knowledge to Action: unstructured pattern recognition, and advanced applications in artificial intelligence. The proposed research program also has the support of the department and aligns with the University of Calgary’s strategic research plans to become a top five Canadian research university by 2016. The proposed research program aligns with the Eyes High strategic vision in the categories of balance, curiosity, collaboration, communication, globalization, and excellence. Taken together, they support programs invested in new lines of inquiry with a global impact and encourage interdisciplinary collaboration.""556573,""Rakheja, Subhash"
"566179"	"Riecke, Bernhard"	"Enabling Effective Human Spatial Cognition, Orientation and Behaviour in Virtual Environments"	"GOAL: To investigate what constitutes effective, robust, and intuitive human spatial orientation and behaviour. This fundamental knowledge will be applied to design novel, more effective human-computer interfaces and interaction paradigms that enable similarly effective processes in computer-mediated environments like virtual reality (VR), immersive gaming, and multi-media.  MOTIVATION: While modern VR simulations can have stunning photorealism, they are typically unable to provide a life-like and compelling sensation of moving through the simulated world, thus limiting perceived realism, behavioural effectiveness, user acceptance, and commercial success. Moreover, VR users frequently get disoriented because the supported spatial behaviour is still clumsy and unnatural. APPROACH & SHORT/MID-TERM OBJECTIVES: I propose that investigating and exploiting self-motion illusions (“vection”) might be a lean and elegant way to overcome such shortcomings and provide a truly “moving experience” in computer-mediated environments, thus enabling more affordable-yet-effective simulations for broader audiences. My team recently provided the first evidence that such embodied self-motion illusions can indeed facilitate perspective switches and spatial orientation, thus providing similar benefits as actual self-motion, but without the cost and effort involved in having to physically move the user. This research program will corroborate and further investigate the functional/behavioural significance of self-motion illusions for a wider range of spatial orientation/cognition tasks that would normally be difficult to accomplish without actual self-motion. To this end, my team will perform experiments with human participants in VR to investigate and optimize multi-modal and higher-level contributions and interactions for spatial orientation and self-motion perception/illusions, while minimizing reference frame conflicts. Specifically, my group of trainees will: (A) Research and utilize higher-level and multi-modal synergistic benefits to enhance vection; (B) Design and evaluate user-powered motion cueing interfaces; (C) Investigate the functional significance of vection using three complementary experimental paradigms; (D) Design transitions into VR; (E) Develop novel experimental paradigms; and (F) Integrate findings into our theoretical framework. Multi-modal, naturalistic and immersive VR provides the unique opportunity to study human perception and behaviour in reproducible, clearly defined and controllable experimental conditions. LONG-TERM GOALS: To investigate how we can best employ self-motion illusion and research-guided interface design and development to enable life-like, robust and effortless spatial cognition, orientation and behaviour in VR and other immersive media. SIGNIFICANCE: This research program will lead to a deeper understanding of human spatial cognition, perception and behaviour that will enable us to design more effective human-computer interfaces and interaction metaphors. These will provide improved test beds for evaluation as well as enable and inspire further research. Thus, combining fundamental and applied research perspectives will allow us to identify the essential parameters of perception/action and will pin-point the “blind spots” that enable the brain to be tricked when simulating VR. This will enable the creation of better cost-effective virtual solutions for numerous immersive and tele-presence applications such as driving/flight simulation, architecture walk-throughs, immersive gaming and recreation, space exploration, engineering, emergency training, minimally invasive surgery, and video conferencing.""563243,""Riecke, Bernhard"
"566905"	"Rose, Jonathan"	"Automatic FPGA Interconnect Synthesis and Understanding FPGA Architecture"	"The fast pace of technological advancement in recent years has been driven by the ability to fabricate digital chips with ever-increasing capability. Each new generation of chip fabrication technology uses smaller transistors and provides exponential improvements in computing capacity. It is less well known that the sophistication and investment required to design and build those chips has *also* been increasing at a near-exponential rate, threatening this progress in the future. Fortunately, the rise of pre-fabricated, user-programmable chips, known as Field-Programmable Gate Arrays (FPGAs), make it possible to use this fantastic technology with far less risk and sophistication. Indeed, the vast majority of all digital hardware design work is done using FPGAs; only in the very high volume systems (such as those in computers and mobile devices) are custom-built chips are used. Everywhere else, some form of programmable device is used - either a computer processor of some kind or an FPGA. There are two key barriers to the wider use and adoption of FPGAs that we address in this research: First, advantages of pre-fabricated FPGA-based hardware must be compared to the other kind of pre-fabricated but programmable digital chips: computers, which are programmed with software. It is much easier to create software than hardware and there many more people capable of writing software. There has long been an effort in research and industry to make the creation of hardware easier, but many of these efforts trade that ease for quality of result. However, it is that performance and energy-efficiency of hardware that are its key advantages over software running on processor. Our goal in this research is to find other ways of making the hardware design process easier, without losing the quality of the result. One of the hardest things to do in the design process is the design of interconnection between hardware computational units. Each such link must be built appropriate to the performance needs of the computation. We propose to build, in stages, a tool that helps the designer more automatically create and vary that interconnect in the face of specific performance requirements. Our ultimate goal is to automatically optimize the interconnect design, and thus release the designer from the burden of re-implementing it many times. With this capability, we believe it will make the designer's job much easier, and enable high-level tools to more successfully help with other parts of the design process. The second barrier to the wider adoption of FPGAs is the cost of the flexibility required to make a pre-fabricated device usefully programmable. The cost can be prohibitive in high volume applications: simple programmable logic is ten to thirty times the cost of non-programmable logic, as our group has reported in a widely-cited paper. The second thrust of this research proposal is to explore a fundamentally new way of synthesizing circuits into FPGAs, as a way to more deeply understand the minimum amount of flexibility actually required. We plan to do this by building a synthesis approach that combines many of the previously-separated steps that are used to take an engineer's design and put it into the FPGA. At the same time, we will carefully vary the amount of flexibility available, reducing it from its current amount. Our goal is to gain insight into ways to reduce flexibility and therefore cost. Finally, our research group is well-known for the only open-source synthesis tool chain for FPGAs, used throughout the world in FPGA architecture and CAD research and startups. We plan to continue to improve the capabilities and sophistication of this software.""555654,""Rose, Rhiannon"
"555255"	"Rosen, Sanae"	"Improving performance, energy consumption and data usage of mobile phones by accounting for network trends and predicted future downloads when scheduling application traffic"	"mobile networking, prefetching, cellular networks, wireless networks""567979,""Rosenbaum, RShayna"
"566345"	"Rosenberg, Catherine"	"A Computer Engineering Approach to the Smart Grid"	"The concept of the Smart Grid is born from the convergence of electrical energy systems and the latest Information and Communications technologies including broadband wireline and wireless networks, sensors, data collection and processing. The objectives are to reduce our carbon footprint, improve the infrastructure cost efficiency (in particular by reducing peaks) and put the customers in the loop by integrating disruptive technologies such as distributed generation, demand side management, energy storage elements, and new loads (e.g., electrical vehicles). The purpose is to convert the existing grid which is 'energy rich, control poor, and information poor' into an 'energy frugal, control rich, and information rich' system. The result will be a large scale heterogeneous distributed system made of hundreds of thousands of intermittent sources, two way flows, and a set of elastic loads and storage elements which has a lot of similarities (and some striking differences) with the Internet. This proposal aims to address several of the fundamental challenges and system issues facing the Smart Grid by taking full advantage of the experiences (successes and failures) drawn from the Internet. As an example, the Internet robustness comes from its simple layered architecture and the use of distributed control mechanisms to prevent congestion and these mechanisms are high potential candidates for enabling demand side management. Internet researchers have had to design for the unknown, both the 'generation' of content and the demands are random, and new applications come regularly to test the robustness of the Internet design. The existing grid is relatively centralized and predictable, while the Smart Grid will be highly distributed and stochastic making the design for the unknown a necessity. In the existing grid, the electric utility controls the supply so that it follows the (quasi-uncontrolled) loads. One of the main paradigm shifts linked to the Smart Grid is supply following, i.e., the fact that due to the high penetration of new intermittent energy sources (e.g., wind and solar) and the need for cost efficiency, loads will need to be controlled to match variable generation while minimizing the impact on the consumers (by taking advantage of their inherent flexibility) or 'impedance matching' via energy storage will need to be performed. Our long term objective is to address the challenge of supply following by considering two (complementary) approaches, one based on energy storage and one based on characterizing and taking advantage of load flexibility. More specifically, we will address the following 5 medium-term objectives: a) Characterizing and quantifying load flexibility using techniques from stochastic calculus. b) Defining mechanisms and protocols to manage load flexibility using techniques from distributed algorithms, control, and convex analysis. c) Fast time-scale supply following using an energy storage aggregator using optimization and real-time distributed algorithms techniques. d) Studying a realistic supply following scenario involving several energy storage aggregators using optimization and real-time distributed algorithms techniques. e) A clean-slate approach to the smart grid: from new service models to an enhanced architecture. The outcome of this research will significantly advance the state of the art on supply following and the Smart Grid in general. It will be key to the development of Smart Grids in Canada and elsewhere. It will help Canadian industries (which are already important global players in the power sector) to continue to lead in this area that is critical to our economy. The projects will also provide excellent HQP training opportunities at the PhD and Master's levels.""546294,""Rosenberg, Lisa"
"567804"	"Roy, Langis"	"MM-Wave Active Antennas for Communications Applications"	"The wireless industry continues to push for higher speed transmission, lower product size and cost, and greater product functionality. At the same time, increasing demands are placed on wireless devices, which are expected to be aware of their radio-wave environment and adapt to it in terms of operating frequencies, power levels and radiation characteristics. Such requirements, coupled with the realities of high-volume commercial applications, translate into several component-level technological challenges that today are barely being met: smart, frequency-agile or multiband RF circuits and antennas, increased integration levels of RF transceivers and antennas at limited cost/performance degradations, acceptable DC/RF conversion and operating efficiencies, to name only a few. These difficulties are severely compounded when operating frequencies move toward the mm-wave band, as they eventually must to avoid spectrum crowding and to permit emerging applications such as short-range multi-Gbps wireless links (i.e. 60 GHz and above). A number of exciting new materials and RF technologies appear promising, at least individually: nanometric (65 nm, 45 nm) CMOS for low-power active circuitry, GaN HEMT for high-efficiency power stages, flexible polymer-based and low-temperature co-fired ceramic (LTCC)-based substrates for passives, antennas and packaging. This work focuses on active antennas and smart antenna systems that intimately combine these and other active, passive, sensing and packaging components with novel radiating structures. The proposed research addresses in several ways the major challenges associated with the development of next generation mm-wave wireless communicators and sensors. Successful realization of high-performance mm-wave active antennas using advanced system-on-chip and system-on-package approaches will have a tremendous impact on the wireless communications industry: terminals may be produced more competitively than ever before, as they will benefit from the advantages of reduced size and weight, better reliability, increased bandwidth and functionality, and lower cost (by incorporating functionality ""for free"" in the package).""564651,""Roy, Langis"
"566806"	"Ruskey, Frank"	"Research in Combinatorial Algorithms"	"Combinatorial algorithms are a core foundational area of computer science. The area is focused on algorithms that operate on finite structures, particularly those that have precise mathematical descriptions. The objective of this research is to identify some problems and structures that are fundamental, find clever ways to solve the problems algorithmically, write code to implement them efficiently, and analyze them mathematically. Also, my grad students and I are not afraid to pursue in considerable detail some side-angle problem that arises in these steps, if such problem seems somehow intellectually interesting and challenging. This proposal focuses on five areas: Gray codes and de Bruijn cycles, nested recurrence relations, Venn diagrams, tatami tilings, and bobbin lace. Gray codes and de Bruijn Cycles: Search spaces are often highly-structured and huge; we intend to continue our development of efficient algorithms for searching, exhaustively and otherwise, in these spaces. Of primary interest are combinatorial Gray codes, which are exhaustive lists in which successive combinatorial objects differ by only a constant amount. Such Gray codes are a necessary precursor to the most efficient of all generation algorithms, those in which only a constant amount of work is done between successive objects generated. Gray codes and de Bruijn cycles have many applications, including computational biology, position detection on rotating axles, combination lock breaking, etc. Nested recurrence relations: Recurrence relations are a fundamental tool of computer science and mathematics. In recent years, we have begun trying to better understand ""nested recurrence relations"" (NRRs), which have received scant attention in the past as compared with the traditional non-nested recurrence relations. The prototypical NRR is Hofstadter's recurrence: Q(n) = Q(n-Q(n-1))+Q(n-Q(n-2)), popularized in the Pulitzer Prize winning book ""Godel, Escher, Bach: An Eternal Golden Braid"". The key syntactic features of this recurrence are that it only uses addition, subtraction, and composition --- and the depth of nesting of the composition is at least two. The recurrences we intend to study all have these key features. We will classify them according to whether they are decidable or not, and provide combinatorial bijections for them whenever possible. Venn and Euler Diagrams Most people are familiar with small ""Venn"" diagrams, and their use in conveying set relationships and explaining syllogisms. Our research is focused on finding symmetric Venn diagrams, both in the plane and also on the sphere, and on exhaustive listing of diagrams with a small number of curves.  Bobbin Lace: Bobbin lace is an old art form, dating back to at least the 16th century, for making fine lace fabric patterns. Given the regular and often symmetric qualities of these patterns, amazingly, there seems to never have been an attempt to precisely catalog and understand the possible patterns, although some ad hoc observational listing and classification has been done. Our aim is to provide a firm mathematical and computational base for bobbin lace patterns. Tatami Tilings: A tiling of an orthogonal region with rectangles is said to be tatami if no four rectangles meet. Such a restriction has a long history in the arrangement of tatami mats on the floors of Japanese rooms. Previous to my work in this area, there were only a couple of mentions of them in puzzle books and in architectural journals. This is somewhat surprising, since the tatami constraint is perhaps the most natural local constraint to place on a tiling. We will continue our investigations into the properties of these tilings and their generalizations.""549818,""Russell, Anthony"
"567510"	"SafaviNaeini, Reyhaneh"	"Securing Data in Future Computer Systems"	"Protection and control of digital data are increasingly threatened in two fundamental ways. First, the computational assumptions that underlie the security of today's cryptosystems will likely to be substantially weakened or completely broken in the near future. In particular, the advent of quantum computing will break the computational assumptions that are the basis of the security algorithms that are the backbone of secure communication: RSA encryption, and Diffie-Hellman (key establishment) protocol. Second, massive data publication and sharing will result in unwanted and uncontrollable inferences. Using search engines and data analysis techniques could reveal information that data owners had intended to protect, making it increasingly hard to maintain individual privacy and to implement laws on freedom of information that require protection of sensitive information. Protected data publishing (PDP) systems work on the assumption that removing terms from documents, or modifying parts of it, will provide the required protection. It has been shown that with sufficient related data, this assumption will not be valid. The long-term objectives of my research program are: To contribute security models for data protection that are inspired by real life scenarios; to develop theories, tools, and techniques to analyze and understand them; and to design systems with verifiable security properties that last through the coming decades. My research will use two complementary approaches, cryptographic and non-cryptographic. A cryptographic system with information theoretic security does not rely on any computational assumption and its security is proven through mathematical theorems that will not be affected with advances in computer algorithms, or development of new models of computation such as quantum computers. A large class of data protection problems, however, cannot be solved by cryptographic systems. I will use probabilistic interpretation of data and information theoretic measures to propose models, theories and protection systems for such problems. I will implement the algorithms and experiment on real data to verify the models and test the theories. Using the two complementary approaches will enable me to explore and contribute to a wide range of emerging problems, with the unifying thread being the use of information theoretic measures, arguments, and proofs. My short term objectives are: 1. Cryptographic systems for long-term data security. Propose models that capture two-party and multi-party data communication scenarios, analyze the models, and propose constructions with optimal performance, provable security, and efficiency. 2. Non-cryptographic systems for PDP. Explore information leakage, model and analyze the leakage in PDP, and propose protection systems to control the leakage. Evaluate the models and constructions, and test theories using experiments on real data. The proposed research is essential for long term security of digital data. Next generation information communication infrastructure will operate in an environment of massive data and abundant computation, and require cryptographic systems that remain secure irrespective of the computational power of the adversary, and tools and systems that can control information leakage in published data. The need for cryptographic technologies for future has been recognized by standardization bodies such as ETSI (http://www.etsi.org/news-events/events/648-crypto-workshop2013), and the need for technologies for controlling unwanted inferences is the today's need of individuals and institutions, which will grow in the coming years.""554597,""SafaviNaeini, Safieddin"
"567978"	"Sahraoui, Houari"	"Learning from examples to improve Automation in Model-Driven Engineering"	"The model-driven engineering (MDE) received much attention in recent years due to its promise to reduce the complexity of the development and maintenance of software applications. However, and notwithstanding the success stories reported in the past decade, MDE is still at the early stages of adoption. One major obstacle to the adoption of MDE is the difficulty to automate many activities of this development paradigm. Automation is a keystone and a founding principle of MDE. In this paradigm, domain-specific modeling languages are combined with transformation engines and generators to produce various software artifacts. Defining modeling languages, writing transformations, and maintaining consistency between the involved models and other artifacts are typical tasks that are difficult to automate. This difficulty comes mainly from the lack of knowledge in some specific domains. This is essentially the case for the definition of meta-models and transformation mechanisms. Difficulty of automation can also be related to various inconsistencies introduced by some manual tasks as in the case of model maintenance. To help in the improvement of automation in MDE, we propose to use examples of artifacts that define the inputs and outputs of the task to automate. Depending on this task, examples could take different forms. For model transformation or refactoring, for instance, examples are pairs of source and target models. Similarly, in metamodel definition, examples could be models that are labeled as valid/invalid. The automation of a task is viewed as an optimization process that derives the automation knowledge that best conforms to the examples at hand. Depending on the size of the problem, i.e., search space, such a process could use an exhaustive or heuristic search. The above-mentioned idea will be explored for different MDE tasks. These tasks include model transformation, model refactoring, transformation specification, and precise metamodel definition.""553631,""Saidi, Saeid"
"555697"	"SalvailBérard, Adam"	"Québec"	"CANADA"
"566573"	"Samet, Saeed"	"Distributed and Scalable Privacy-Preserving Data Mining Techniques for Big Data"	"It is impossible to ignore the importance of preserving privacy especially in the era of Big Data in various fields, such as health, business, government and social networks. With the immense growth in the ability to store data, the increased computing power, advances in data analytics, and very large increases in the number of devices and sensors connected to the internet and dedicated networks, there has been an increase in privacy and security risks. Typical privacy-preserving techniques used with small data sets, such as de-identification, access control, secure computation and data encryption cannot be simply used with Big Data. Therefore, it is crucial to create a balance between beneficial uses of Big Data and individual privacy. Dealing with privacy issues in the research area of Big Data, like other aspects of Big Data such as data collection, storage, analysis, and result dissemination, has become a challenge. This research program plans to propose and develop new privacy-preserving techniques and extend the existing ones in data mining and statistical analysis methods that are scalable and incremental, such that they can be practically applied on Big Data, while minimizing the negative effects of applying these techniques on the overall performance, the accuracy and utility of the extracted knowledge. The findings and outputs of this research program will be applied on genome-environment-associations in type 2 diabetes to uncover gene-environment interactions associated with this highly common disease as proof of concept and test using real data. The long-term objective of this research is to develop scalable privacy-preserving methods and protocols for data mining algorithms on Big Data. The research will focus on practical methods and techniques for privacy-preserving protocols on both simulated and real data (type 2 diabetes). The findings will be useful for comparison in similarly complex applications in health, business and government. In order to utilize the type 2 diabetes dataset it will be necessary to preserve the individual’s privacy while allowing meaningful data mining and computational operations. The results will extend the set of secure protocols to cover statistical analysis and data mining methods, and the proposed techniques will be applicable in other areas of health, business, and government where Big Data are used. Therefore, the focus of the short-term objectives will be to propose, design and implement efficient privacy-preserving tools, using new and existing privacy-preserving techniques applied to simulated and type 2 diabetes data. The specific short-term objectives of this research are to (1) indicate which steps (from data gathering to dissemination of results) of Big Data require privacy protection and where currently available privacy-preserving techniques are used; (2) identify the statistical and data-mining techniques that are currently used on genetic data; (3) develop privacy-protected data-mining and computational procedures and algorithms for these applications and (4) test these privacy-protected algorithms on simulated Big Data and type 2 diabetes datasets.""553691,""Samia, Yasmine"
"567780"	"Sartipi, Kamran"	"Intelligent and Mined-knowledge Driven Consultant Services for User Behavior Recovery in Large Distributed Systems"	"An increasing number of organizations are integrating their public services to become more competitive in providing broader and more easily accessible services to the public. The resulting federated organization must relax its internal regulations and access control policies to provide compatibility of services that collectively form a trust model of service usage. Such trust models jeopardize the public’s precious data assets. The complex and changing nature of usage patterns and the high costs of violating a trust model by accessing sensitive private data, create serious threats to the integrity of distributed information systems that are the backbone of public services. The proposed research program aims at providing a new generation of intelligent decision support services, namely “Consultant-as-a-Service” that effectively assists and guides a broad range of users that can be categorized as: i) customers of organizations who are not familiar with the large variety of organization’s services to the public; and ii) system administrators of large distributed systems who are overwhelmed by the number of system users whose behavior in using the system services must be monitored and controlled. Most large organizations are operated by outdated legacy systems that are not compatible with advanced technology for secure communication, and protocols for user authentication and authorization. Therefore, the system vulnerability problem is becoming more serious and may drastically impair the quality of service provisioning. This issue may cause vast economical costs for the organizations whose private data assets have been compromised. For example, the current state of distributed medical imaging systems and their central repositories in most provinces is based on a trust model, which makes it vulnerable. Such cases have been the main motivation for the approach in this proposal. The proposed consultant service is knowledge driven and closely assists the security administrators to compose detailed and complex patterns of usage and provides solutions that can answer to a variety of user behavior questions, which will save valuable time and effort that must be invested for such discoveries (if possible at all). Example queries are as follow. What is common among users who accessed the system around the traffic rush hour? Which policy rules are sensitive to the current state of system usage? What is the frequent behavior pattern of a specific role or user in the system? Is there any behavior similarity among users in specific and separated locations, and in what aspect they are similar? Etc. Furthermore, there are huge possibilities to identify anomaly behavior of some users by close investigation of the identified common behavioral patterns, in which the existence of some actions cannot be justified by the related policy rules. The proposed research will train three Master’s students who will become expert in different skills such as: applying data mining techniques on different data assets; working on advanced decision support techniques; challenging with high complexity of optimization search algorithms and constrain pattern matching; and designing pattern languages. Two PhD students will be trained in this research, which will provide theoretical and practical solutions to novel user behavioral modeling and designing intelligent decision services. All research students will have access to an advanced cloud based data centre that provides an experimental distributed medical imaging system to design, develop and evaluate their research works.""551568,""Sarty, Adam"
"555927"	"Seaborn, Katelyn"	"Mixed Reality Games for Motivating Older Mobility Device Users: Towards a Human Factors Model of Performance and Self-Actualization"	"human factors, mixed reality games, gamification, human-computer interaction, performance, mobility device use, interface design, usability, serious games, mobility""554913,""Seabrooke, Sara"
"567823"	"Sensinger, Jonathon"	"Exploration of optimal prosthesis feedback information using computational motor control"	"Better robotic prostheses can dramatically improve the quality of life for persons with an upper limb amputation, many of whom reject existing devices because they have trouble controlling them in the same intuitive, subconscious way that they controlled their intact arms. Prosthesis control is difficult because amputees experience great uncertainty both with respect to whether their device will respond appropriately to their control signals and whether sensory feedback cues accurately reflect the actual movement. Researchers have focused on improving isolated aspects of control, for example by improving filters or mimicking able-bodied sensory cues through haptic devices, but these approaches have minimally reduced the uncertainty of prosthesis control. Human interaction with a prosthesis is a multifaceted, time-varying problem that is difficult to solve. What is missing from robotic prosthesis research are principled methods for optimizing control strategies and sensory cues that take into account behavioral choices people are known to make in the face of high uncertainty. Our unique approach is to use an optimization strategy that incorporates the behavioral decisions that humans intuitively make in order to deal with uncertainty. For healthy subjects, computational motor control models based on human behavioral data describe very well how subjects learn, estimate, and control. This is even true for cases that are analogous to prosthesis use, such as mapping non-intuitive joints to abstract degrees of freedom or signal-dependent noise. This suggests that those models could also predict how an amputee learns to control a prosthesis using their noisier control signals and limited sensory feedback. Building models and calibrating them with experiments to make them predictive will allow us to study how different decoder and feedback designs would affect behavior. This model-driven approach promises faster and more efficient prosthesis design. We plan to quantify the uncertainty that amputees attribute to various sources (their control signals, the prosthesis, and the world); to develop novel controllers that reduce the uncertainty of control; and to provide haptic sensory cues that work synergistically with available sensors and control strategies to reduce uncertainty. The proposed research is innovative because it poses the control problem in a broader context that incorporates the highly sophisticated behavioral decisions that humans make in optimizing their control strategy and sensory cues. This approach is able to integrate multiple effects in ways that were not possible using previous approaches. For example, our approach naturally incorporates the fact that people prefer to use less exerted effort to accomplish a task, but tolerate more effort during portions of movement that require greater precision (e.g. final portion of a trajectory). On the other hand, our approach does not favor high-certainty haptic cues if those cues provide redundant information to existing sensory cues such as vision, or if the haptic information does not reduce the uncertainty of controllable system dynamics. Due to the large sources of control-signal noise present in amputees, our work will lead to improved techniques within the fields of computational motor control and optimal control. This research builds on our team’s extensive experience in the design and control of upper-limb prostheses and our collaborator’s experience in the field of computational motor control. Achievement of the proposed aims will contribute to the field of robotic control and to such diverse fields as human-robot interaction, perception, manipulation, and exoskeletons, and will provide a rich platform for education at all levels.""555411,""Sensinger, Jonathon"
"555855"	"Seyed, Alemayehu"	"Exploring Tangible Interaction Techniques for Collaborative Exploration and Decision Making with 3D Data"	"human-computer interaction, mobile computing, multi-surface environments, ubiquitous computing, interaction design, tangible interactions, computer-supported cooperative work""556655,""SeyedainBoroujeni, Setare"
"566495"	"Sheikholeslami, Ali"	"Transceiver Circuits for Optical Backplanes"	"Optical links have long been known as lighter, cleaner, and faster alternatives to copper cables for long-haul data communication where the distance between communicating nodes is in kilometers. In the last decade, however, as the sheer weight and volume of the copper cables have increased substantially, optical links have replaced copper cables in communication over shorter distances, such as a few meters between computers in data centers. Today, optical links are competing against copper at distances as short as a few centimeters, such as those between a microprocessor and a memory on a printer circuit board (PCB), for speeds in excess of 25Gb/s. At these speeds, the copper traces attenuate the electrical signal so much that it is extremely hard to recover the correct data at the receiver end no matter how much electronics one includes at the two ends of the link. On the other hand, the attenuation of an optical channel is known to be negligible at these distances even at far higher data rates. What has prohibited so far the wider use of optical backplanes is mainly the cost of electrical to optical and optical to electrical interfaces and the surrounding electronics to compensate for various imperfections of these interfaces. This is in addition to the assembly cost of the chips on board as much more precise alignments are necessary. This proposal relates to the innovation of designs (devices and circuits) for optical/electrical interfaces, and the electronics surrounding these interfaces. In particular, we propose designs for optical devices such as the ring modulators, photo-detectors, and optical filters, and electronic circuits such as trans-impedance amplifiers (TIA), limiting amplifiers, equalizers, and clock and data recovery circuits. The long-term objective of this research is to demonstrate a fully integrated transceiver for optical backplanes. The short-term objectives are to design, fabricate, and test individual building blocks of the complete transceiver. As part of this research, we also develop a system-level model (in Matlab or Simulink) that includes all the building blocks, along with their main sources of non-idealities. This model will be used to identify the link performance as a function of key design parameters. Accordingly, we determine the sensitivity of the link performance to each design parameter in order to identify the performance limits of the architectures. Once we determine the final architecture, we will move towards implementing one block at a time in circuit level. We will test each block individually before moving on to higher levels of integration.""562803,""Sheikhzadeh, Mehdi"
"566393"	"Shi, Wei"	"Photonic integrated circuits for high-capacity optical communications"	"Over the past decade, we have witnessed an incredible increase of Internet growth (greater than 40% per year) driven by millions of new users and numerous emerging on-line applications such as Google, Facebook, Youtube, Twitter, Amazon, etc. This expansion would be impossible without the physical support of high-speed optical communications. As the Internet continues its explosive growth, more complex optical systems are needed to provide higher data transmission rates, indicating dramatically increased costs and difficulties in implementation. To solve this dilemma, we are developing nanometer-scale optical devices that allow us to integrate a large number of optical functions on a single chip. Owing to their extremely small footprints and high-level integration, ultra-high-speed optical signals can be generated with minimized cost and power consumption. In particular, these devices are based on silicon, a material on which billions of microelectronic chips have been fabricated. This is exciting because of the great potential to leverage the most sophisticated manufacturing facilities, which are already established for microelectronics, to scale complexity of optical communication systems. This research, if successful, may revolutionize telecommunications, leading to giant leaps of Internet speed.""561261,""Shi, Wei"
"566328"	"Shih, Ishiang"	"Research on Crystalline Compound Semiconductors and Transparent-Conducting Oxides"	"Part A: Monocrystalline CuInSe2 -based Semiconductors for Solar Cells  The first generation commercial solar cells in the market are predominantly manufactured using bulk polycrystalline Si or monocrystalline Si. Although the thermal stability of the current Si bulk solar cells is excellent, the energy pay back time is still long. The relatively long energy pay back time is mainly due to the indirect bandgap requiring a large substrate thickness of 200 µm or more for sufficient absorption and the high growth temperatures. The energy pay back time of the second generation thin film solar cells must be reduced for large scale terrestrial applications. The way to achieve the low energy pay back time is to adopt a semiconductor with a direct band gap and high optical absorption coefficients. There are two main material systems being developed for the second generation solar cell fabrication: CuInxGa1-xSe2 and CdTe. Using these semiconductors, the amount of material usage and energy consumption during the cell manufacturing can be reduced. In the applicant’s laboratory at McGill, research work has been performed on the Bridgman growth of monocystalline CuInSe2 (CIS) and CuInGaSe2 (CIGS). Both CIS and CIGS have very large optical absorption coefficients in the solar spectrum and a thin film with a thickness as small as 0.4 µm instead of 200 µm is capable of absorbing 98% of above bandgap photons in the solar spectrum. The performance of thin film cells containing trace of Na (due to inter diffusion from soda lime glass substrates) is much better than those without Na. Unfortunately, it is more difficult to study Na effects in CIS and CIGS thin films and the causes of performance improvement have not been conclusive. Therefore, research work will be performed in this project to study effects of Na on the microscopic and electronic properties of monocrystalline bulk CIS and CIGS. Research work will be performed to prepare ingots from melts containing different amounts of Na. In addition, research work will be made to prepare photovoltaic cells on the CIS and CIGS substrates containing Na. Special attention will be paid to the effect of Na on defect density for the CIGS surface and CIS-CIGS /CdS interface face. We plan to obtain Na results which can allow thin film solar cells to reach efficiencies more than 22%. Part B: Transparent and Conducting Oxides for Electronic Applications  Transparent conductive oxides (TCO) are often microcrystalline thin films which have high transmission in the visible wavelength range and high conductivity. Known TCOs include F- or Sb-doped tin oxide, Sn- or Zn-doped indium oxide and Al-, B- or Ga-doped zinc oxide. The resistivity of the TCO films is limited to a value slightly above 10-4 ohm-cm due to a relatively low mobility (~10 cm2/V-sec) at high doping density of more than 1021 cm-3. The high doping concentration has caused a decrease in optical transmission at least in the long wavelength region. In order to improve the performance of devices involving the TCOs, it is desirable to develop thin films with high electron mobility so that the doping concentration can be reduced to obtain the same level of resistivity or to decrease further the resistivity. In the present project, modulation doping will be explored in the conductive oxides in order to increase the mobility to substantially greater than 100 cm2/V-sec while maintaining the high doping concentration. The improved TCOs will be deposited on the CIS and CIGS substrates developed in part A to form solar cells. We also plan to achieve thin film transistor devices with exceptional switching performance compared to the existing technology.""561759,""Shih, JamesChingFeng"
"567812"	"Shirmohammadi, Shervin"	"Mobile Cloud Gaming Systems and Virtual Environments"	"Games have indeed turned out to be the “killer app” that has brought virtual environments to the mass market. Online games, which enable millions of players to interact with each other over the network, are now widely used not just for entertainment, but also for socializing, business, commerce, scientific experimentation, and many other practical purposes. Literally millions of people spend their time and money in game worlds such as Second Life, Eve Online, Guild Wars, World of Warcraft, or many others. The computer gaming market is therefore huge, and although the industry itself at just over 30 years old is quite young, it has already surpassed the much longer-established film and music industries, generating more revenue than Cinema since 2009 and more revenue than DVD/BlueRay since 2011. Mobile gamin stats are equally impressive. In fact the growth of mobile gaming is happening so quickly that industry observers believe mobile games will change the basis of competition in the gaming industry. Cloud gaming, the newest entry into gaming technology, leverages the well-known concept of cloud computing to provide online gaming services to players, including mobile players who have more computational or download restrictions than players with dedicated game consoles or desktop computers. The idea in cloud gaming is to process the game events in the cloud and to stream the game to the players. Since it uses the cloud, scalability, server bottlenecks, and server failures are alleviated in cloud gaming, helping it become more popular in both research and industry, with companies such as OnLive, StreamMyGame, Gaikai, G-Cluster, OTOY, Spoon, CiiNOW, and others providing commercial cloud gaming services already. In this project, we focus on mobile cloud gaming and we study, design, and develop adaptive, scalable, and energy-aware mobile gaming technologies that will help reduce both the amount of battery consumption in mobile devices and bandwidth generation by the cloud, leading to longer playing time and higher gaming quality for mobile players.""552076,""Shives, Michael"
"566282"	"Sirouspour, Shahin"	"Model Predictive Control Strategies for Mixed Telerobotic/Autonomous Robotic Systems"	"In recent years, technological and scientific advances in a number of disciplines such as mechatronics, manufacturing, information and signal processing, controls, and artificial intelligence have fuelled a rapid growth in traditional and new applications of robotics. Robotic systems are increasingly used in industrial automation, disaster recovery, search and rescue, hazardous material and waste handling, mining, space and underwater operations, and medicine. Notwithstanding all these advances, robots may not still be able to operate fully autonomously in many complex uncertain task environments, and do often require some form of human supervision/control.  This research is concerned with human-in-the-loop robotic systems. These are systems in which human(s) and robot(s) work cooperatively to accomplish a task. The overall goal of the research is to bridge an existing gap between the two fields of autonomous robotics and telerobotics. Traditionally, research in these areas has followed two mostly separate paths. Autonomous robotics systems have mostly comprised of fully autonomous robots. Telerobotics systems, on other hand, have mainly involved system configurations in which one operator fully controls one robotic manipulator. This research pursues a new paradigm in system design and control in which elements of telerobotics and autonomous robotics are combined. In this new paradigm, the operator(s) control aspects of the task that would benefit from human’s unique cognition and decision-making capabilities. Meanwhile, robot(s) assist the operator(s) by autonomously controlling more structured aspects of the task to improve precision and reduce cognitive load. The proposed research will seek a general framework for control and coordination in mixed autonomous robotics/teleorobotics, applicable to a broad class of system configurations. The control strategies will be based on a novel multiple-level control architecture. At one level, optimization-based model-predictive controllers that deal with autonomous aspects of the task will produce commends, which will then be passed to another level of control. This next level controller will combine autonomous and conventional teleoperation control and will resolve any potential conflicts between the two based on user-defined task priorities. Uncertainty in operator(s) future actions and other elements of the task environment will be considered in making optimal control decisions. This will be achieved using particle-based estimation and optimization techniques. It is expected that the real-time computational requirements of the resulting estimation and control algorithms will exceed capabilities of state-of-art desktop computers. Parallel implementation of the algorithms on Graphic Processor Units will be pursued to help achieve the timing requirements using inexpensive, off-the-shelf, graphics cards. The outcomes of this research will help robotics engineers and scientists to design systems that combine benefits of teleoperation and autonomous control. Such systems will be able to operate more effectively in complex unstructured environments, which are common in many robotics applications. The new knowledge arising from this research will find its way into new technologies and products in areas of great importance to Canada such as healthcare, manufacturing and auto industry, mining, and space. Canada is a major player in the field of robotics and Canadian companies in this area will benefit from the new knowledge and also the highly qualified personnel that will participate in the research.""556552,""Sirouspour, Shahin"
"567814"	"Sit, Jeremy"	"Engineered thin-film nanomaterials for enhanced sensing"	"The continually evolving field of nanoscience and nanotechnology relies on understanding and control of the structure and the properties of material at smaller and smaller size scales. The knowledge developed here today leads to applications of tomorrow which can exploit the extraordinary fine surfaces and structures that can now be produced routinely. This research program uses a specialised technique to produce high aspect ratio (thin and tall) columns of material on the order of tens to hundreds of nanometres in diameter. We can control material, column shape, size, density, and surface area. These structural properties in turn affect optical or electrical properties. The research supported by this grant will study how our specialty materials interact with their environment. Specifically, we aim to use our materials as the basis of advanced chemical sensors. For example, as a long-term vision, we will be able to use our technology to detect and measure biomolecules whose presence and concentration in small blood samples can provide critical medical diagnostic information to a doctor who can then order the appropriate treatment for the patient. By exploiting the excellent, built-up expertise and infrastructure already present at the University of Alberta, this research will foster innovation in Canadian science and technology and provide opportunities for training for promising new researchers in the field of microfabrication and nanotechnology.""561243,""Sitkovets, Anton"
"566123"	"Sobot, Robert"	"Implantable Telemetry Systems"	"1) MAIN SHORT AND LONG TERM OBJECTIVES OF THE PROPOSED RESEARCH PROGRAM: by building upon our existing achievements and current state of the work in progress, my first practical objective is to develop micro-sized implantable wireless telemetry systems at cubic millimetre scale, for example, small enough to fit inside a mouse body. My motivation to achieve this particular objective is threefold. First, by itself the proposed objective is extremely difficult but not impossible to reach, which challenges my engineering and academic curiosity. Second, a genetically modified mouse is one of the most important and often used models by biomedical researchers, thus there is practical and immediate need for such systems. Third, design of such telemetry systems is exceptionally challenging engineering task, however once we master the design of telemetry systems at a cubic millimetre scale, the same system is then relatively easily adapted to interface with other types and combinations of sensors. 2) THE SCIENTIFIC APPROACH: my research program is primarily driven by practical engineering problems, which is to say that it addresses complex problems in real contexts. In this program I aim to integrate known and hypothetical design principles to provide feasible solutions to these complex problems. This kind of development and research takes place through continuous cycles of design, verification, analysis, and redesign phases. That is, in my program I have adopted the design-based research approach. As oppose to the traditional predictive research approach (i.e. the hypothesis, experiment, theory cycles) in the design--based research approach my starting point is analysis and definition of a practical problem. Indeed, my current research is inspired by the following question: is it possible, and if yes how can we design a short-distance wireless implantable telemetry system that is small enough to fit into a few cubic millimetres volume, for example into belly of a small animal subject such as a laboratory mouse? The problem is to design an ultra-small wireless telemetry system capable of simultaneously supporting multiple sensors. Currently available data indicate that three main limitations of the currently available telemetry systems are: a) due to their physical size they are limited to subjects with large bodies (e.g. humans), and other telemetry applications that are not space limited; b) they rely on the implanted energy source such as a battery, and c) they serve only one kind of sensor (e.g. either pressure or temperature sensor). Very important component of this program will be in the training of HQP at all levels who will have an opportunity to learn and exercise new and relevant skills, such as theoretical analysis of physical phenomena (e.g. interaction between EM waves and living tissue), microelectronic circuit design (e.g. RF transceiver and signal processing), and the system level analysis with practical implementation.. 3) THE NOVELTY AND EXPECTED SIGNIFICANCE OF THE WORK TO FIELDS IN THE NATURAL SCIENCES AND ENGINEERING: this research program will provide unique contributions in the form of ultra-small integrated RF implantable telemetry system and relevant theoretical work. At first, it will be used for cardiac monitoring telemetry and, eventually, when it is interfaced with various sensors it may become part of a wireless sensor network that could be embedded, for instance, into crop fields, various constructions, and into a human body. Thus, the micro wireless network could enable, for example, real-time monitoring of growing crops, bridge integrity, or the human health. Relative to the other published works, the proposed telemetry system is unique with respect to its size, complexity and the power consumption.""567256,""Soddu, Andrea"
"566454"	"Stuerzlinger, Wolfgang"	"High-Performance Spatial Hybrid User Interfaces"	"Interaction with three-dimensional (3D), spatial data is increasingly common. Spatial data are important for many sectors, including interior design, architecture, urban planning, industrial planning and design, simulation, training, animation, the game and entertainment industry, and the new “maker” movement. Yet, user interfaces for spatial data are complex, difficult to learn, or limited in functionality. My research program investigates better spatial user interfaces for 3D content. The new spatial hybrid user interfaces are a combination of user interfaces, where the user manipulates virtual objects quickly in the air, with the best of desktop user interfaces. Working towards the vision of spatial user interfaces that are efficient as 2D interfaces, the research program involves three main thrusts: characterization of basic operations, accurate interaction, and new technologies. The first thrust investigates how well current, basic spatial interaction techniques, such as 3D rotations, docking, and travel, perform in terms of speed and accuracy. Moreover, the research also investigates how fast transitions between in-air and desktop interfaces are and how reliable gestures are as a substitute for buttons and keys. While in-air manipulation seems attractive, the available experimental data points to a lack of precision. Thus, we investigate hybrid combinations of in-air and desktop interfaces in the second thrust, where the desktop interface is used for fine-tuning of the final result. Such combinations will yield hybrid spatial user interfaces for 3D content creation and editing that are both fast and precise. Moreover, we study new techniques for resolving depth ambiguities and switching between mono- and stereoscopic viewing. The final thrust of this research program investigates new tracking technologies for fingers and pens, with the aim to further increase tracking accuracy. The fundamental contributions of my research program will be accurate quantifications of the throughput of existing and new spatial interaction techniques, new methodologies for evaluation of interaction methods, novel design guidelines, as well as new technologies. Together, the work constitutes a major step forward towards high-performance spatial user interfaces that are still easy to learn. Better spatial user interfaces will increase the accessibility of 3D modeling for non-specialists, which in turn will accelerate the adoption of 3D content manipulation and creation by the general public, e.g., driven by the new maker/3D printing movement. Moreover, 3D content creation and gaming are core Canadian strengths. Canada will benefit from the research program presented here through many globally competitive HQP in user interface design and 3D graphics, as well as a potential startup. Also, the broad applicability of the results will increase interest in science and technical disciplines.""566455,""Stuerzlinger, Wolfgang"
"566450"	"Suen, Ching"	"Error Reduction in Handwriting Recognition with applications"	"The objective of this program is to conduct advanced innovative research in computer recognition of handwritten characters, words, and symbols. Handwriting is one of the most challenging subjects in the field of pattern recognition due to the infinite varieties of character shapes and qualities. Although handwriting recognition systems have the potential of reading bank cheques, envelopes, archival documents, utility payment slips, income tax returns, and other business forms, very few are actually used at the moment, due to the high error rate of the current recognition systems. Indeed, banks and different companies are spending billions of dollars each week to enter handwritten data manually into the computer. In the past, most research was focused on high recognition rates with less emphasis on the most difficult and costly problems of error rates. This research picks up this challenge by investigating different methods of reducing the error rate to increase the reliability of recognitions, to produce a new generation of recognition systems for full deployment in practical environments. This research aims to increase the intelligence and capability of computers so that they can read, at high speed and with great accuracy, the information written on various types of documents where billions of dollars are being spent each week to manually enter the handwritten data from documents into the computer. Based on decades of experience plus a world renowned reputation in (a) conducting intense research in handwriting recognition, (b) having developed numerous high performance recognition systems, (c) having guided more than 190 graduate students, post-doctoral fellows, and visiting academic/industrial scientists, and (d) playing a lead role in this field and constantly interacting with prominent researchers around the world, the applicant proposes the following program to minimize the error rate and maximize the recognition score: 1) To continue to investigate the causes of substitution errors and the drawbacks of current recognition systems, 2) To explore and evaluate a variety of configurations of error-reduction schemes with a cascade of  uncertainty-removal modules, 3) To create very large databases and introduce new training techniques to separate good and bad samples, 4) To discover distinctive features and vital parts of individual characters derived from eye-tracking and perceptual studies. 5) To select dynamic sets of complementary features for various stages of multi-stage recognition systems to produce  the most reliable and the best system to recognize handwritten data in different languages, English, French, Arabic,  Chinese, etc. 6) To integrate the error-reduced recognition systems with the high-performance systems, and discover the critical paths  of optimizing their overall performance and trade-offs, 7) To apply the results to conduct large-scale experiments on reading different kinds of real-world business documents,  word spotting, e.g. custom's declaration forms, taxation forms, bank cheques, utility bills, and archival documents  of historical value, 8) To Introduce this new breed of hybrid classifiers which can identify confusing character/word shapes, minimize costly  substitution errors and maximize performance, so that the newly developed systems can be used in practice to save  billions of dollars and a huge amount of manpower. 9) Expand this project to recognize other types of patterns such as irises, faces, palmprints, and pedestrian patterns. .""563207,""Sugar, Lorraine"
"567670"	"Sutton, Richard"	"Temporal-difference algorithms for reinforcement learning and artificial intelligence"	"Predicting long-term aspects of one’s input stream is a ubiquitous problem facing intelligent agents both natural and artificial. You predict the weather on the weekend to plan your barbecue party, you predict the long-term trend of the stock market when you decide to invest, and you predict where the tennis ball will be when you move to meet intercept it. As you watch the tennis ball travel through the air you continually make a series of predictions and learn to make more accurate predictions in the future. Simultaneously, you predict how your weight will shift, how your limbs and racket will move, your probability of returning the ball effectively, and hundreds if not thousands of other things. Your brain performs an amazingly dense parallel computation using rich sensory input to make large numbers of predictions many times a second while simultaneously updating them to tune them to the current situation. We do all this effortlessly, so it is easy to overlook the enormous demands on our information processing system. The learning algorithms that we employ to do all this are of course not completely known, but the closest analogs in the field of artificial intelligence—the learning methods most suited to the special engineering requirements of making these kinds of predictions efficiently—are known as temporal difference (TD) learning algorithms and are the focus of this research project. The proposed research pushes the frontiers of the theory, practice, and application of TD learning algorithms. Part of the challenge is to learn with both statistical efficiency and computational efficiency, and to balance the two when necessary. The computational complexity of existing TD methods scales linearly with both the number of predictions made and the number of features that are used to make them. Each prediction-feature pair is assigned a numerical weight that is adjusted by the learning process. At each time, the weights of all the features present at that time are added up to produce the prediction that approximates the correct answer. One complication is that the correct prediction depend on what you do. If you swing quickly you may intercept the tennis ball whereas if you are slow you are more likely to miss it. A long standing problem that we have recently solved is that all earlier linear-complexity TD methods were unable to learn about a way of acting unless you actually acted in exactly that way. This is inefficient in that the data should in principle inform the predictions for all ways of acting that are similar to what you did. This is called “the problem of off-policy learning,” and it has been open from 1995 until we started to solve it in 2009. Our solution, however, is only the beginning. It has many refinements and generalizations that remain to be done, and it opens up new opportunities and capabilities, including hybrid methods of various kinds. We plan to develop TD learning into a very general and capable form that may someday be able to do all that people and animals can do. One way we test our algorithms is by running them on small robots to see if they can learn effectively at scale and in real time. Our largest demonstration to date updates and learns 6000 predictions, each a function of more than 6000 features, every 1/10th of a second. This robot learns more diverse predictive facts about its sensorimotor interaction with the world than possibly any other robot. If successful, this research will affect and enhance all the different areas in which decision making is being automated, including communications networks, financial markets, self-driving cars, user interfaces, environmental controls for homes and factories, power systems, and control systems of all kinds.""555700,""Sutty, Sibi"
"555512"	"Svajlenko, Jeffrey"	"Benchmarking Experiments for Source Code Similarity Detectors"	"Computer Science, Software Engineering, Benchmarking, Clone, Clone Detection, Similarity, Software Quality, Software Refactoring, Software Re-engineering, Software Analysis""558514,""Svatos, Adam"
"566633"	"Tait, Robert"	"Thermal micro-systems for gas monitoring"	"The work described in this proposal focuses on gas sensors suitable for use in air quality monitoring, although the technology developed could be applied to a range of other applications including environmental monitoring, health services, industrial control, automotive, food processing, and safety. The advantages offered by this technology over existing devices include low power, compact size, and fast response. The results of this work will be of immediate interest to manufacturing companies using microfabrication techniques, sensor and monitoring companies, and end users. The operation of the sensors in this work relies on absorption of infrared light by individual gas species in a mixture to determine gas composition. Infrared light sources, detectors, and filters, can all be microfabricated and potentially integrated together to form a gas detector. This will potentially make devices smaller, less expensive, and require less power, enabling battery powered operation. This proposal builds on considerable expertise at Carleton University in micro-electro-mechanical systems (MEMS), sensors, thermal actuators and thermal detectors in order to improve current gas sensing technology. The nature of the work will include development of MEMS thermal isolation structures for hotplates, infrared (IR) sources and IR sensors. The technology will be suitable for integration with electronics for temperature control, source modulation, and detector readout. Canadian companies and industry find it necessary to look abroad for expertise in gas sensing. This work will help supply local expertise to domestic sensor and monitoring companies and industries. This activity will support Canada’s obligation to monitor and maintain healthy air quality for all citizens.""554941,""Taj, Rafiq"
"566167"	"TavakoliAfshari, SeyedMahdi"	"Haptic Telerobotic Control Systems: Analysis and Design for High-Fidelity Interaction"	"Haptic feedback provides the humans who operate machines (e.g., planes, excavators, or robots such as the Canadarm or Mars rovers) with a sense of touching objects they are not actually touching by hand but are manipulating by the machines. To recreate for the sense of touch what closed-circuit TV recreates for the sense of sight, haptic feedback produces the illusion of touching the objects by applying forces to the human operator hands through user interfaces (joysticks). This is a useful capability because haptic interaction is the human’s most basic way to understanding an environment and effecting change in it. Haptic feedback allows the human operator of the machine to handle objects more gently, safely, reliably, and precisely. This is of paramount importance if the “machine” is a surgical or rehabilitation robot and the manipulated “object” is the soft tissue or disabled limb of a patient while a surgeon or therapist acts as the machine “operator”. This proposal concerns enhancing the quality (“fidelity” or “transparency”) of haptic interaction, which is critical to the safety and successful execution of various manipulation and sensing tasks, for machines called “teleoperation systems”. In a haptic teleoperation system, a human operator controls a robot via a user interface in order to handle and “feel” objects potentially located at a remote distance (equivalent to broadcast TV in the above analogy). Teleoperation systems are useful machines when a human has to interact with a hazardous environment (e.g., undersea or toxic fields) or an inaccessible/remote environment (e.g., outer space). Undersea teleoperation for monitoring oil and gas pipelines and the inspection of subsea structures eliminates the high cost of human divers and the risk to life. Teleoperation for nuclear waste handling or mine clearance minimizes the risk to life. Space teleoperation minimizes the number of expensive manned missions required for equipment assembly, servicing and repair. In such applications, bilateral teleoperation (i.e., with haptic feedback for the operator) is preferable over unilateral teleoperation (i.e., without haptic feedback). In the proposed research, we will focus on teleoperation systems for surgery and rehabilitation although the research outcomes are applicable broadly. Our proposed research on surgical teleoperation enables surgery on the beating heart in the presence of haptic feedback for the surgeon so that he/she does not apply too much force on the heart tissue. Teleoperation can facilitate in-home telerehabilitation as an alternative to hospital-based rehabilitation of physically disabled patients. In this context, our proposed research will enable time-sharing of one therapist among several patients so that the therapist can shift his/her attention from one patient to another as teleoperated robots at the patients’ homes learn to continue the therapies safely in the absence of the therapist. The proposed research will also answer how two or more humans or human hands can haptically collaborate in controlling the same machine such that execution of a given task takes place more easily or optimally. For instance, we will investigate how an assistive robotic arm mounted on a power wheelchair can be collaboratively teleoperated from two joysticks held by a disabled person’s two hands, which may have limited but complementary motions. While useful in many application areas, the proposed research leads to important enabling biomedical engineering technologies. In addition to the groundbreaking innovations, this research will benefit Canada by training students in areas pertaining to the development of advanced haptic technologies including robotics, human-machine interaction, and systems control.""558168,""Tavares, Ana"
"567105"	"Taylor, Joshua"	"Control and economics of power systems with renewables"	"Non-generator resources (NGRs) like energy storage and demand response are conceptual solutions to the intermittency of renewable energy sources. NGRs have great potential to enhance the electric power system's efficiency and reliability, but will also add considerable new complexity through scale, uncertainty, and economics. Indeed, a demand response program may contain upwards of 10^5 loads, each subject to uncertainty from modeling inaccuracies, limited communications, and random factors like human behavior and weather. NGRs in general have starkly different characteristics than conventional generators, such as faster ramping capabilities, hard energy capacity constraints, and consequently dynamic states of charge. These factors mean that NGRs cannot be treated like conventional generation resources. In particular, power system operators must (i) use large load aggregations as though they were individual NGRs, and (ii) pay NGRs with regard to their unique physical characteristics so as to encourage proper participation in current markets and future investments. Indeed, the persistence of electricity market abuses like the 2001 California electricity crisis and the more recent JP Morgan trading scandal signifies the importance of sound economic mechanisms based on physical models. This research program will address these challenges by developing fundamental algorithmic and economic frameworks for effectively utilizing NGRs. New methodologies for managing uncertain load populations will be built on online learning theory and load aggregation techniques based on polytope theory. For instance, multi-armed bandit index policies will be derived for simultaneously utilizing and improving estimated models of loads. Since NGRs will be significant sources of power system regulation, markets must account for both the dynamics of NGRs and the power system. Toward this end, tools from optimal control and dynamic game theory will be employed to design contracts and pricing mechanisms that are based on physical models of NGRs and power system regulation, and to identify potential vulnerabilities to gaming and market abuse. The research will produce a broad set of tools for optimizing NGR utilization in technical and economic dimensions, in turn facilitating the integration of renewable energy sources into the existing power infrastructure. The resulting benefits will include (i) new theoretical insights and research directions in power system operation and economics, (ii) training of highly qualified personnel for careers in Canadian power industry and academia, and (iii) reduced environmental impacts.""544084,""Taylor, Keith"
"555999"	"Toker, Dereck"	"Ontario"	"CANADA"
"567076"	"Toueg, Sam"	"Algorithms, abstractions and models for distributed computing."	"We plan to work on fundamental problems, abstractions and models in distributed computing. This research will encompass both message-passing models (which are suited to geographically distributed systems such as peer-to-peer, mobile, or cloud systems) and shared-memory models (which are suited to multicore systems). One goal is to derive better algorithms and abstractions that simplify the design or improve the performance of distributed systems. Another goal is to compare, and where possible unify, some of the many models and related results in this area. We now briefly summarize some of the proposed work towards these goals. Linearizable wait-free shared objects are a powerful abstraction for building asynchronous shared-memory distributed systems: they behave as if they are accessed sequentially, and every non-faulty process that invokes an operation on a wait-free object is guaranteed to get a response even if other processes are slow or crash. Implementing such objects, however, can be difficult and inefficient, and this has led to the study of objects that have weaker (though still useful) semantics but have easier and more efficient implementations. In this vein, we propose to continue our work on abortable objects, i.e., objects where operations that interfere with each other may abort without taking effect. This is reminiscent of the simple and attractive behaviour of transactional memory, database transactions, and abortable mutual exclusion --- techniques in which a process can, under contention, ``bail out'' of the computation without leaving a trace. Our preliminary work on abortable objects showed that abortable objects are inherently different from ordinary ones and need to be studied on their own. We propose to continue this work to better understand abortable objects, and to compare them to ordinary (i.e., non-abortable) objects in terms of power and implementation efficiency. Several fundamental problems that cannot be solved in fully asynchronous systems with failures can be solved in systems with additional properties, such as partial synchrony, failure detection, and fair schedulers. Recent research suggests that many of these systems are closely related (e.g., several failure detectors have been shown to be equivalent to fair schedulers) but considerable work remains to be done to better understand their similarities and differences. We will continue to investigate and compare these systems in an effort to unify and better understand the many models and results in the area. In many distributed systems, some strong assumptions, e.g., that processes are synchronous and do not crash, can hold for relatively long periods of time. Furthermore, algorithms that make such strong assumptions can be very efficient, but they may fail in the relatively rare cases when these assumptions are violated. To take advantage of this, one can first run a ``primary'' algorithm that is very efficient because it relies on strong assumptions, and then, in the rare cases when the primary algorithm fails, fall back on a less efficient ``backup'' algorithm that relies on weaker assumptions; this is the basic idea of ``speculative computing''. In preliminary work based on this approach, we derived speculative algorithms for the fundamental problem of consensus in systems with process crash failures. We propose to extend and generalize this initial work in several directions, for example to solve problems other than consensus, to tolerate more than just crash failures, and to investigate the efficient implementation of shared objects based on speculative computing.""549792,""Touret, Nicolas"
"567457"	"Tremblay, Christine"	"Cognitive optical networks enabled by coherent technologies and filterless concepts"	"Recent developments in coherent modem performance and digital signal processing (DSP) technologies have stimulated the exploration of novel agile network architectures. The applicant's group has proposed a passive wide area network (WAN) solution, called the filterless optical network, as a cost-effective and reliable alternative to active optical switching network solutions. The proposed optical network architecture is based on the premise that the need for agility can be provided by wavelength tuning at the transmitter and wavelength discrimination by the coherent receiver, in much the same way as agility is achieved in radio networks. Filterless coherent networks essentially offer a passive broadcast medium in which passive non-filtered optical splitters and combiners are used at some nodes for interconnecting the fiber links. The proposed research program builds on both the theoretical research on filterless optical networking and the advanced optical layer test bed developed by the applicant’s group. The program is aimed at proposing novel approaches for coherent optical network design and control, based on filterless and cognitive concepts, and at demonstrating, both theoretically and experimentally, the applicability of these concepts in real-world applications and the impact on cost, performance and energy consumption. In the first phase, the project will explore cognitive optical networking concepts for the design and for the control plane of filterless networks as a potential means to better route traffic and assign resources through quality of transmission, network status and energy consumption awareness. The impairments that are specific to coherent transmission in a branch-tree filterless architecture will be measured and integrated in our filterless design and control plane tools. In the second phase, the project will concentrate on the design of candidate cognitive flexible optical network architectures. A small scale filterless network will be assembled. Specific experiments will be designed for validating these novel networking concepts and evaluating the dynamic performance and benefits of using cognition, in terms of resource utilization, impairment awareness and signal quality evaluation. The originality of the proposed research project lies in the filterless optical networking concept itself, which is looking increasingly like a very serious candidate architecture for future high capacity optical networks, and also because novel cognitive network architectures, enabled by coherent technologies and energy efficient design practices, will be explored from both the theoretical and experimental perspectives, thereby allowing the use of the available bandwidth in new ways, maximizing network flexibility. Some additional innovations are: new design methods and a control plane based on flexible and cognitive optical networking concepts; new design tools and a simulator adapted to coherent networks; cost-effective multicast architectures for content-delivery networks; and new applications, particularly given that filterless networks are likely to exhibit lower line cost and latency, higher reliability, smaller footprint and less node equipment complexity than optical switching networks based on conventional active photonic switching and optical dispersion compensation elements. The proposed research program will be realized in an expected time frame of 5 years. In total, 3 Ph.D. students and 4 Master's students will be involved in the research program.""552758,""Tremblay, Christine"
"567019"	"Trescases, Olivier"	"Advanced Power Electronic Converters for Improved Energy Efficiency in Photovoltaic and Automotive Applications"	"Power Electronic Converters (PEC) have played a pivotal role in the steadily increasing global energy efficiency over the past 35 years, across transportation, industrial, commercial and residential sectors. PECs are the key enabling technology for efficiently harvesting electrical energy from renewable sources, and delivering it to the modern electrical loads that we rely on for a high standard of living. The power electronics component market represents approximately $20 Billion today and is rapidly growing. From 1990 to 2010, the efficiency gains achieved using PECs alone have eliminated the need to build an estimated 930 one-Gigawatt coal-fired power plants, saving $ 2.794 Trillion globally. Global awareness in climate change, compounded by the steadily rising demand for energy, underscores the need for cheap yet robust low-carbon technologies. The mass deployment of technologies such as Photovoltaics (PV) and Electric Vehicles (EV) will greatly alleviate our future global energy challenges, however today their cost/performance ratio remains far too high to be truly disruptive on a large scale. The high cost of PV systems remains the critical obstacle to achieving a penetration that exceeds 10% of the energy mix by 2030 in North America. While EVs consume nine times less energy than conventional vehicles, they represent only 0.4 % of the market in Canada. The long-term objective is to demonstrate PEC innovations, at the device, circuit and system level, to push these and future low-carbon applications beyond early adopters over the next 10 years. The dominant research trend in PECs is towards 1) increased switching frequency to reduce the size, cost and weight of the components, 2) deploying advanced digital control schemes to make PECs more modular, adaptive, re-configurable and reliable and 3) leveraging new Smart Power IC fabrication technologies to integrate sensing, power-stage and control functions on-chip. The next frontier of power electronics lies in the proliferation of new wide-bandgap power devices using Silicon Carbide (SiC) and Gallium Nitride (GaN), with the ultimate goal of providing unprecedented power density and energy savings. The grant will fund three synergistic projects with 12 HQPs, while providing valuable training in clean technologies for Canadian Engineers. The application focus of PV and EV is chosen based on the potential to achieve a high impact in modern society’s carbon footprint. The first project focuses on highly integrated 600 V GaN based PECs operating at multi-MHz switching frequencies. The second project is anchored around UofT’s one-of-a-kind custom EV prototype, which has a top speed of 130 km/h and a 210 km driving range; a record for this vehicle class. The work will focus on 1) a hybrid energy storage system comprised of ultracapacitors and lithium batteries with GPS-based power-mix optimization and 2) a new self-learning thermal management approach for the battery pack, specifically optimized for the cold Canadian climate. Finally, the third project extends our past work on Distributed Maximum Power Point Tracking (DMPPT) in PV applications to an exciting new aerospace technology: the Solarship. The Canadian-made buoyant electric aircraft was conceived to deliver supplies in remote areas where roads, fuel and electrical infrastructure is sparse or non-existent, making conventional vehicles unpractical. The Helium filled wing creates buoyancy and increases the payload. Two electric motors are supplied by a lithium battery, which is charged using the large wing-mounted PV array during flight. Specialized PECs based on the partial-power-processing concept will be developed to maximize the solar energy harvesting in this weight sensitive application.""550139,""Trevani, Liliana"
"567986"	"Tripunitara, Mahesh"	"Strengthening the Foundations of Access Control"	"Access control comprises the techniques and mechanisms by which we ensure that only authorized principals are able to perform certain actions, such as read and write, on resources. It is an essential component of the security of deployed systems, and is also an active area of research. From the PI's experience over the past few years, which includes collaborative research with industry, he has learned that the foundations of access control need considerable work. This is the focus of this proposal. The proposed research fits into the PI's longer term vision of making computer systems, on which all of us increasingly rely for even our basic needs, as secure as is feasible. The PI proposes to address three fundamental topics. One is forensics, with which we answer questions about past states of a system. Forensics is important because preventive security techniques often fail. Access control systems are an important context in which to perform forensic analysis; however, the forensic analysis problem has not been posed as such in prior research. The PI proposes to precisely pose and investigate a broad class of forensic analysis problems in the context of access control. One of the outcomes of this work will be goal-directed logging, so only essential logs are maintained that lend to efficient analysis. A second topic that the PI proposes to research is the secrecy resilience of authorization policies. Authorization policies are themselves resources that need to be protected because portions of them (e.g., whether a user has a certain privilege) may be sensitive to disclosure. The central question that the PI proposes to answer in this context is: are some authorization policies inherently more secrecy resilient than others? The PI proposes to evolve a notion of secrecy resilience that has intuitive appeal, and explore several research directions, such as whether it is possible to increase the secrecy resilience of a policy without changing its effective authorizations, and whether one can build Role-Based Access Control (RBAC) policies that have a desired secrecy resilience. The third topic that the PI proposes to research is the foundations of testing implementations of authorization and access control systems. This is a topic on which the PI has conducted some recent work in collaboration with industry partners. There are several research problems that the PI proposes to address in this context. One is the identification and development of an appropriate syntax and associated semantics to express authorization systems for the purpose of testing, and the properties for which we would like to test. Another is a theory that relates such declarative properties with procedural traces, instances of which are to be exercised on the system under test. The PI proposes also to develop techniques for automatically generating trace instances using existing tools such as model checkers, and tying that to the process of exercising the trace instances. All of this work will result in a complete testing ecosystem for real world authorization and access control systems. The proposed research is of value to Canada, and will complement the PI's other research, including those he performs in collaboration with industry partners. It will be high-impact in three ways. It will train Highly Qualified Personnel (HQP) in the important area of computer security, it will result in high-quality research publications in prestigious and selective journals and conferences, which in turn will give graduate students valuable exposure to the larger research community, and it will provide the PI and other researchers greater avenues to form research collaborations with Canadian industry partners by way of applying the proposed work to their real world problems.""545609,""Trites, Andrew"
"566522"	"Trudel, Sylvie"	"Improvement of requirements quality and software process efficiency"	"The proposed research program is in the field of software engineering, more specifically in the study of software requirements written in natural language, their quality and their measurability. It is well recognized that natural language requirements are defect-prone and a number of techniques aim to verify and validate the requirements quality through the identification of defects early in the software life cycle for efficiency purposes. Two research axes are related to quality and measurability of functional requirements, more specifically: 1) Specification methods, and 2) Verification methods of requirements written in natural languages. Our research program is aligned with these two axes. Indeed, our research goal is to first improve the quality of functional requirements by using functional size measurement (FSM) as a means to identify defects in these requirements. Then, our long-term goal is to improve requirements specification methods by including FSM early in order to avoid certain types of defects that could have a negative impact on the software engineering process efficiency. The long-term goal of our proposed research program is to improve methods for specifying requirements in order to prevent defects in the requirements. Our hypothesis is that improving requirements quality using FSM will have a positive impact on the software engineering process efficiency. Software process efficiency is measured by comparing the project effort with the software Functional Size (FS) resulting from that process and can be expressed as a number of hours spent per functional size unit. FSM is normally applied on requirements once they are completed. By moving the measurement earlier within the requirements phase, it would prevent some defects and contribute to requirements of a higher quality, hence better measureable software engineering process efficiency. To achieve this goal, we propose the following research objectives: Objective 1. To design techniques for defect identification in completed software requirements using FSM, in order to improve requirements quality. Objective 2. To investigate early defect detection within the requirements engineering phase and its relationship to the software engineering process efficiency. To achieve these objectives, research projects are defined in collaboration with industry partners and research groups and laboratories. The proposed research program is a natural evolution from progress reported in my recent Ph.D. thesis.""558970,""TrudelGuy, Catherine"
"566501"	"vanBreugel, Franck"	"Hunting for Bugs in Source Code of Video and Computer Games"	"The aim of our research is to develop tools to detect bugs in video and computer games. Rather than applying well-known techniques to games, we focus on fundamental problems that are encountered when hunting for bugs in games. Our anticipated solutions will not only impact the game industry but also other application areas and academia. Testing is the most commonly used method to detect bugs. It is particularly effective for deterministic code, i.e., code that for a fixed input gives rise to a single execution. However, in the presence of nondeterminism, i.e., when there are different potential executions, testing is less effective. Running a test multiple times provides no guarantee that different executions have been checked. Also, if a bug has been found, it may be difficult to reproduce. Therefore, to detect bugs in nondeterministic code, methods that complement testing are needed. Model checking is an alternative to testing. Rather than checking a single execution, as is done in testing, model checking attempts to systematically check all potential executions. However, the number of potential executions can be exponential in the number of nondeterministic choices in the code. The fundamental problem of dealing with such a huge number of executions is known as the state space explosion problem. Randomization and concurrency, which are key ingredients of today’s games, both give rise to nondeterminism. Therefore, model checking seems a viable alternative to testing for finding bugs in games. Although numerous model checkers have been developed, we are not aware of any that can verify properties of the source code of games. Our overall aim is to develop the first such model checker. Our main objective is to combat the notorious state space explosion caused by a combination of randomization and concurrency. We focus on model checkers that work directly with the source code. We initially restrict our attention to Java, currently the most popular programming language. Although C++ is most widely used for games, many, in particular online games, are written in Java. For example, according to Guinness World Records, RuneScape, implemented in Java, is the world's most popular free massively multiplayer online role-playing game with more than 200 million registered accounts. The Java code of games contains native calls, i.e., calls which invoke code that is written in a language different from Java. For example, calls related to graphics, networking and sound are usually native. Model checking Java code with native calls usually involves modelling the native code in Java, which is time consuming and error-prone. To make our tools of use to game developers, native calls should be handled automatically. This is our second major objective. Players interact with a game by means of the video game controller, the mouse, the keyboard, etc. In order to model check a game, we need to incorporate a model of the players. Ideally, the models should be built automatically from the source code of the game, so that our tools are easy to use by game developers. Our third objective is to automate the generation of models of the players as much as possible. The Canadian video and computer game industry is among the largest in the world. It contributes almost two billion dollars per year to the Canadian economy. Since our tools will allow game developers to find bugs automatically, they will improve the quality of the games and increase the productivity of game developers. We expect our results to also impact other application areas and academia. For example, tools to find bugs in communication protocols implemented in Java toolkits, which contain numerous native calls, should benefit from our techniques to handle native calls.""558688,""VanCappellen, Philippe"
"555891"	"Vaughan, Thomas"	"Analysis of Methods to Determine Anatomy Geometry Intraoperatively for Computer Assisted Interventions"	"computer assisted surgery, computational geometry, intraoperative planning, mosaic arthroplasty, suface reconstruction, image processing, registration, segmentation""549464,""Vavasis, Stephen"
"566412"	"Veneris, Andreas"	"Theory and Methodology for Performance-Driven Automation in RTL and Testbench Debugging"	"The semiconductor industry has products reaching all aspects of commercial and consumer markets domestically and internationally. It consistently creates smaller, faster and more powerful integrated computer Very Large Scale of Integration (VLSI) chips which fuel the accelerated demand of the end products. Chip companies are challenged to design increasingly complex devices while remaining cost competitive. Computer-Aided Design (CAD) tools are continuously improving their efficiency to mitigate this cost. In the past decade, the effort to verify the correctness of these systems has increased disproportionately and takes as much as 70% of the total design cycle, a trend coined the verification gap. This is also confirmed by the 3:1 ratio between the number of verification engineers versus that of designers in the semiconductor industry, a trend that has been projected to increase almost two-fold by 2015. When verification fails and the design is proven to be incorrect, debugging follows to identify the source of the error and fix it. Today, this is a predominantly manual task. As technical roadmaps and research studies indicate, the resource-intensive manual debugging has become the most significant component of the verification gap taking as much as 32% of the total verification time. That is, for a typical design cycle of 18 months, engineers today spend 4-5 months in manual debugging. Evidently, this introduces disproportional non-recurring costs and it may jeopardize the chip delivery day. Consequently, it comes as no surprise that debugging was recently pronounced the central theme in verification R&D for the next 5-6 years by the CEO of Synopsys, the largest Electronic Design Automation CAD tool vendor. Our group was the first to recognize this emerging trend ten years ago. In 2005 we proposed the first automated debugging theory and methodology that today is used and referenced by peer research groups and industry worldwide. In this proposal we leverage this investment to investigate new theories for formal CAD tools and respective methodologies for the modern chip verification/debug cycle. These tools will utilize robust engines and proprietary data mining algorithms to exhaustively analyze the big data generated by verification to ensure correctness of the design. The end result will be the theory and a CAD tool that can be deployed in any industrial semiconductor site to aid the engineers during debugging. This ambitious and multi-disciplinary project promises major theoretical advances and practical applications that instill benefit in CAD for VLSI but also in other fields of science such as software verification, data mining and artificial intelligence. Further, due to the continuous growth of semiconductor industry, there has been a shortage of Highly Qualified Personnel (HQP) in digital VLSI verification/debug that remains strong at all levels of education (PhD, MASc, BASc). Hence, it comes as no surprise that there continues to be much industrial, political and educational attention within Canada devoted in training of HQP in related areas. Everybody involved with the development of the technology will gain knowledge in advanced CAD concepts, practical VLSI verification, hands-on experience and strategic planning. Our pioneering R&D environment and collaboration with the industry will offer them an unparalleled experience in innovation. Graduate students will publish in prominent scientific conferences/journals with strict acceptance criteria and they will gain international visibility. As shown by the post-graduation employment success of our alumnus, this HQP will gain knowledge at the forefront of technology and they will exercise it in a way beneficial to the Canadian society and economy.""557150,""Vengallatore, Srikar"
"556026"	"Vining, Nicholas"	"Applications of Ricci Flow to Computer Graphics"	"mesh processing, topology, geometry, riemann surfaces, ricci flow, computational geometry, computer graphics""550555,""Viola, MariaGrazia"
"566835"	"Wang, Xiaoyu"	"Control and Operation of Distributed Energy Storage in Active Distribution Networks"	"Renewable energy technologies, together with advances in energy efficiency, are being used globally to reduce greenhouse gas emissions and to relieve the energy crisis brought on by the depletion of fossil fuels. Representing a growing part of the smart grid model for the electric power industry, renewable distributed generation (RDG) units including photovoltaic (PV) panels and wind turbines will be connected to the power distribution systems in increasing numbers. RDG units are small, modular, electrical energy generation units located close to where electricity is used. They are used as an alternative to or an enhancement of the traditional electric power grid to provide high-quality, reliable electricity at the utility customer sites. RDG technologies may also be beneficial in delaying electric utility infrastructure upgrades. Existing power systems, however, impose a number of constraints on RDG interconnection because of safety, economics, and lack of knowledge and tools to support large-scale integration of RDG. Significant research is needed to mitigate the gap between the characteristics of RDG such as variability, uncertainty and distribution, and the restricted grid operation and regulation standards of utilities. The proposed program will investigate and develop optimal control and operation technologies for distributed energy storage (DES) to ensure compatibility of RDG with the power systems. The focus will be on employing DES to mitigate the voltage regulation and power dispatch issues caused by PV interconnections in distribution power systems. The outcomes of this program will provide solutions for the addressed RDG interconnection problems so that the utilization efficiency of RDG can be maximized and RDG can be better integrated into the grid, which will incent more and more RDG to be used by utilities and customers. The program will also bring up-to-date knowledge and tools to Canadian utility companies and prepare them well to meet the widespread applications of RDG technologies. In addition, the trained highly qualified personnel through the program will gain the knowledge and skills needed by Canadian industry and academia to accelerate the smart grid realization in Canada.""552982,""Wang, Xihua"
"566537"	"Ward, Rabab"	"Compressive Sensing Applications to Biomedical Engineering"	"Compressed Sensing (CS) has been successfully applied to MRI. While MRI remains an intersting venue for CS research, other biomedical applications could also benefit from CS theory. This proposal explores the application of CS to new research problems in MRI, X-Ray Computed Tomography and energy efficient EEG signal transmission. These problems are novel but all fall under the broad category of CS. The outcomes will benefit the biomedical engineering community as well as strengthen research in CS as a whole. Dynamic MRI Reconstruction in real-time involves “fast“ processing of many frames per second. Existing reconstruction techniques are offline and only useful for analytical tasks which can be done posthumously, like medical diagnosis and neurological studies. There are other major applications which require real-time reconstruction – image-guided surgery and tracking / monitoring applications. Real-time dynamic MRI reconstruction is a hard problem and has received limited focus so far. There is a need to address this problem and develop efficient, robust and accurate techniques for real-time reconstruction. Reducing Ionizing Radiation for dynamic X-Ray CT results in 30,000 cases of cancer a year and about 15,000 deaths from cancer in the USA. CT is not safe .The problem is aggravated in dynamic CT, as the patient is subjected to more ionizing radiation compared to static scans. We will develop new techniques to address the dire need to reduce the ionizing radiation in CT. Energy efficient EEG transmission for Wireless Body Area Network (WBAN) : In Canada 14.1% of the population is above the age of 65. They should be able to live with dignity; with minimum dependency on others. At the same time they need to be monitored for health conditions. From EEG signals it is possible to infer a variety of health problems. We envision a system where the EEG signal will be acquired at the subject’s location and then transmitted to a healthcare unit for monitoring and analysis. Since the battery life is limited in WBAN applications we will design sampling and transmission protocols that are energy efficient. The MRI, X-Ray CT and EEG problems are tied by a common goal – how to reconstruct the underlying signal from a reduced number of measurements. Thus each problem will be first recast in the CS framework and methodolgies for its solutions will be developed. Real-time dynamic MRI reconstruction is presently solved via two approaches. The first uses dynamical system models like Kalman Filtering, which is not computationally or memory efficient. The other uses CS. This yields more accurate results but remains too slow for real-time performance. We propose to combine the two in a prediction-correction framework. In the prediction step a dynamical model will be used to estimate the frames; in the correction step the predicted estimate will be refined using CS. Offline dynamic MRI reconstruction is well studied but for dynamic CT, only a handful of studies exist. We will leverage our expertise in dynamic MRI techniques to suit the needs for CT. We plan to model the dynamic CT frames as a Casorati matrix. This matrix is sparse in transform domain and will also be low-rank. The novel Casorati matrix model will enable us to exploit both transform domain sparsity as well as its low-rank structure to obtain results with lesser ionizing radiation. The problem of EEG transmission over WBAN is not a mature topic. Signal processing researchers used CS to reduce the number of samples to be transmitted– for power communication Communication theory experts tried encoding techniques for efficient transmission whereas researchers in sensor networks used novel switching techniques to save energy. We want to address the problem as a whole (end-to-end) .""557351,""Ward, Wendy"
"567360"	"Watrous, John"	"Theoretical aspects of quantum information and computation"	"Present-day computers are classical computing devices: each component of a classical computer has a definite logical state before and after each step of a computation, and computations proceed according to rules dictated by sequences of deterministic (or sometimes randomized) logical operations. According to the theory of quantum information, however, which offers an abstraction of the information-theoretic aspects of quantum mechanical systems, classical computations represent only a limited subset of the computations that can potentially be implemented by physical devices. Much like electrons in atoms exist in superpositions that cannot be described definitively within the context of Newtonian physics, quantum computers can exist in superpositions of logical states, and their computations can proceed along multiple computation paths simultaneously that may constructively or destructively interfere with one another. The theory of quantum computation studies the powers and limitations of this computational paradigm. The main objective of my research is to better understand the nature of the computations that can potentially be implemented by quantum computers, as well as the nature of interactions among multiple quantum computers in cooperative and competitive settings. I am also interested in fundamental aspects of quantum information, and in the development of mathematical techniques that are useful for reasoning about quantum information and computation. A primary subject of my work is quantum computational complexity theory. Principal goals of this subject are to identify and understand relationships among classes of computational problems defined by quantum models of computation, and to relate these models and classes to ones defined by classical computational models. This includes the study of a variety of models and classes of computational problems, including models that abstract the notion of a single quantum computer programmed to solve computational problems as rapidly as possible; of models that describe interactions among multiple quantum computers in both distributed and cryptographic settings; and classes of problems defined by placing resource constraints and other limitations on quantum models. The quantum interactive proof system model is one example of a quantum computational model that has been studied within quantum computational complexity theory -- this model has been the subject of much of my previous work, and is central to the research I intend to pursue in association with this research proposal. Powerful mathematical techniques from different areas of mathematics have been applied to problems in quantum information and computation. I am particularly interested in techniques from combinatorial optimization, convex analysis, matrix analysis, and the study of operator algebras. Semidefinite programming and the matrix multiplicative weights update method represent two examples that have been important in some of my recent work. I intend to continue to investigate the uses of these and other methods within the study of quantum information and computation. Quantum information has the potential to bring a transformative change to the way we build and use computers, communicate privately and implement cryptographic protocols, and study the nature of quantum physical systems. If it is successful, this research project will lead to a better theoretical understanding of quantum information and computation, to new mathematical methods that are useful in its study, and possibly to new ways that it can be used.""551037,""Watson, Alexander"
"555639"	"Watson, Diane"	"Evaluating User Experience with Input Technologies using Low Cost Measures"	"Human Computer Interaction, Input Technology, User Experience, Low Cost Measures, Software Design""558714,""Watson, Emily"
"566483"	"Wei, Lan"	"Low-Dimensional-Material-Based Integrated Systems for Energy Storage, Conversion and Delivery"	"Miniaturization and on-chip integration of energy storage, delivery and conversion circuitry is the key to unlocking a new generation of electronic applications, from battery-less wireless sensors to low-cost photovoltaics. Indeed, the bio-sensing and bio-medical implications of self-powered chips alone are staggering. However, given that conventional geometric scaling of front-end Si CMOS and back-end metals are coming to an end, miniaturization of such circuitry remains will be impossible. This proposal aims to breakthrough diminishing scaling returns by using low-dimensional materials (LDMs) such as 1-dimensional (1D) carbon nanotubes (CNTs), 2-dimensional (2D) graphene and 2D transition metal di-chalcogenides (TMDs) sheets. With diameter <2nm (CNTs) or thickness <1nm (2D LDMs), LDMs offer the smallest possible material dimensions. This gives LDMs superior electrical and optical properties versus traditional materials, which is exactly what is needed for miniaturization of energy-efficient circuits. Specifically, this proposal develops LDM-based energy storage and conversion systems across two phases, with Phase I (Years 1-3) focused on the underlying physics of three fundamental technology-level challenges and Phase 2 (Years 4-5) focused on integrated circuit-level solutions. PHASE I: The first technology challenge is to miniaturize passive devices so they can be integrated on-chip. This is not currently possible because large value capacitors (>1 nF) and inductors (>100 nH) are so large that they must be located off-chip. LDMs have potential to overcome this, with high-potential nanostructures including CNT bundles/grapheme spirals and high-density CNT forests/bilayer grapheme/multi-layer graphene-h-BN-graphene structures. However, for LDMs to achieve their potential, problems like the unacceptably low quality factor caused by poor LDM/metal contact must be addressed. The second technology challenge is to step-change the size and performance of active devices. With conventional Si CMOS scaling increasingly limited by leakage and parasitics, 2D TMDs are high-potential due to their thin body and reasonable bandgap. However, compact models that capture sufficient device information to be used for rapid circuit simulation and chip design do not yet exist, and many key TMD properties are still unknown (e.g. frequency-dependent noise characteristics and transport linearity). The third technology challenge is to address the significant parasitic capacitances that can severely degrade post-layout performance of highly miniaturized circuits. This can be done by adopting “parasitic-aware” circuit design techniques that close the gap between pre-layout design and post-layout implementation and potentially convert parasitic “waste” into useful passive components. PHASE II: Once the underlying technology challenges of practical LDM-based passive and active devices are addressed, Phase II will integrate the learning and produce circuit-level models and designs. This will include co-optimization of device structures and circuit topologies to ensure that the unique properties of LDMs are fully utilized and device structures are optimized for target applications. The end result will be physical prototypes that prove LDMs can practically miniaturize energy storage and conversion circuitry for a myriad of applications. Furthermore, by integrating a comprehensive library of active and passive technology models and co-simulation tools, this program will be provide a foundation that future application-specific research can build from.""548152,""Wei, Li"
"555987"	"WilliamsKing, David"	"Understanding Cross-Codebase Backdoors"	"security, online privacy, malware, backdoors, cyberwarfare, electronic intelligence, open source""559602,""WilliamsKing, Kent"
"566711"	"Woelfel, Philipp"	"Algorithms and Complexity of Distributed Computation"	"Distributed computing systems ranging from simple desktop PCs or even cell phones with multi-core CPUs to high-performance computer clusters and systems ""in-the-cloud"" have to deal with problems that arise from concurrency, asynchrony, and failures. In these settings, the design of time and space efficient algorithms, proof of their correctness, and analysis of their complexity are challenging tasks. Algorithms and data structures for fundamental tasks in distributed systems are critical for the future development of efficient and fault-tolerant software. Lower bounds and impossibility results can guide researchers and software developers in their efforts to build efficient software systems, and can help hardware designers to decide what hardware primitives are needed. In my research program I will devise new efficient algorithms and data structures for shared memory systems. I will try to find solutions for fundamental problems such as mutual exclusion, consensus, or the efficient and randomized implementation of strong primitives (e.g., compare-and-swap) from weaker ones. I will complement new algorithms with proofs of lower bounds in order to determine the time and space complexity of these problems. I will also investigate new methods for measuring the quality of randomized and deterministic shared memory algorithms and the complexity of problems. The classical worst-case analysis is often too pessimistic to predict the actual performance of programs in practice, and sometimes leads to complicated algorithms. Therefore, novel ways of theoretically predicting the performance of algorithms need to be developed and evaluated. Results of this research program will yield a better understanding of the system requirements of synchronization problems. Algorithms resulting from the research can be used as building blocks in software development, and lower bounds can help software and hardware developers making design decisions. Research on novel analytical performance measures for shared memory algorithms will directly and indirectly yield new solutions to synchronization problems, and improve the performance of systems relying on synchronization algorithms.""560756,""Woerlen, Natalie"
"566814"	"Wong, Kon"	"Statistical Processing on Signal Feature Manifolds"	"Statistical Processing on Signal Feature Manifolds Signal processing is the extraction of information from physical measurements. Based on linear vector space theory which treats the observed signal as a vector, highly sophisticated techniques of detection, estimation, classification, optimum signal design have been developed and effectively applied to engineering systems such as radar, sonar, communications, speech, etc. As technology in signal processing advances, signal features are subject to direct processing. One such feature rich in second order statistics information is the power spectral density (PSD) matrix. However, direct processing of PSD matrices had not yielded expected successes in signal classification. This led the applicant to realize that PSD have structural constraints and thus, they form a manifold (multi-dimensional surface). Thus, measurements of these features must be carried out along the surface of the PSD manifold, i.e., using the Riemannian distance (RD). Intensive research led the applicant to arrive at several closed-form expressions of RD for measurement on the PSD manifold and these were tested on classification of EEG signals resulting in dramatic accuracy. The proposal here describes a research program aiming at developing processing techniques based on the geometry of the PSD manifold and on the use of the RD derived by the applicant. Through the development of such techniques founded on new concepts and through the use of new tools, it is expected that revisiting existing application areas will lead to deeper insights and to superior algorithms for signal processing. The following important areas are chosen for exploration: 1. Statistical Properties of the PSD Matrices – A major necessity in signal processing is the statistical properties of the signals. Processing on the manifold is no different. This project investigates the PSD matrices and explores their fundamental statistical properties. These will greatly facilitates ensuing analyses and development of processing techniques of the PSD matrices. 2. Detection – For engineering systems such as radar, sonar, communications, this is a fundamental requirement. We propose to explore the detection of signals based on the direct processing of the PSD matrices. New techniques will be developed using: a) The probability distributions obtained in 1. above to establish a likelihood ratio test (LRT) for detection on the manifold. b) The RD between the different classes of PSD matrices and detect by comparing the relative closeness to the classes. 3. Linear Estimation and filtering – Using the Pythagorean and the projection theorems, parameter estimation and filtering are made possible in a linear vector space. Due to the structural constraints of a PSD matrix, the “linear” weighting operation here has to be specially defined. Using such a “linear” combination of PSD matrices, we seek to establish equivalent concepts on the PSD manifold and explore the possibility of optimum estimation and filtering of the PSD matrix. 4. Signal Design – Signal bank design using the PSD matrix with Euclidean distance has been investigated in recent years. Here, we propose to employ RD as the (more accurate) measure for discrepancies between PSD matrices, and use a special set of orthonormal functions as basis to solve the problem of optimum signal design by PSD. The above proposed projects represent a fundamental departure from the traditional signal processing approach, shifting from the vector space to the manifold. With the more accurate measurement of RD, the algorithms so developed are also expected to be more accurate. Investigations into extending these processing techniques to other features will also be considered.""555347,""Wong, Lawrence"
"567660"	"Yang, Herbert"	"Static and Dynamic Image Analysis and Synthesis"	"Computer vision (CV) focuses on using mathematical methods to extract the shape, structure, semantic information or other intrinsic properties of a scene. The goal of computer graphics (CG) is the reverse process of generating desired images, photographic or artistic, using computer algorithms. CV and CG are intimately related. Hence, research results in one area can often be applied or extended to the other area. My long term research goal is to develop a process or processes that can facilitate the interchange of results between CV and CG. During the last 5 years, when my students and I were developing an underwater 8-camera array system for Neptune Canada, we discovered that many previous works in underwater imaging have simplified incorrectly the refraction effect. Since I am developing the next generation of underwater 3D vision system with Venus Canada and since there is very little work in physically correct underwater computer vision algorithms, I plan to focus my efforts on issues related to underwater imaging by developing physically correct methods. In the short term, I plan to focus on the following objectives within the next five years: 1) to develop a physically correct framework for developing underwater computer vision algorithms, 2) to use the framework to develop physically correct underwater computer vision algorithms, in particular, camera calibration and 3D reconstruction, 3) to use the framework to develop new algorithms in synthetic aperture imaging and in structured light techniques for the underwater environment, and 4) to extract attributes or features that can help to animate fluid more realistically. To achieve objective 1, I plan to investigate different mathematical frameworks that can address the issues of refraction. The goal is to develop a set of mathematical tools similar to what has been done for land-based systems. For objective 2, I plan to use the framework developed in objective 1 to derive select computer vision algorithms, in particular, camera calibration and 3D reconstruction. As well, I will demonstrate the validity of these algorithms experimentally. The next problem that I will investigate is in 3D reconstruction. The success of land-based computer vision algorithms is due to the discovery and the use of constraints, e.g. epipolar, optical flow, left-right consistency, to name a few. In the underwater environment, I will extend constraints used for land-based system to the underwater environment and to develop new ones. The plan for objective 3 is to investigate two major computer vision topics, namely, synthetic aperture imaging (SAI) and structured light. For SAI, I will extend my recent work to develop one for the underwater environment. One advantage of SAI is its ability to see through occlusion, which is very useful when some portion of a camera is occluded. An underwater structured light system will provide much improvement to the accuracy of the correspondence process. Realistic rendering of fluid is closely related to the above mentioned work. On land, using computer vision algorithms, extracting the intrinsic properties of a scene can enhance the quality of rendered images. By the same token, for the underwater environment, the extracted attributes of the underwater environment will enhance the quality of rendered images. Since there is very little work done in underwater computer vision and computer graphics, my work will certainly have significant short as well as long term impact to the research community.""566884,""Yang, HongChuan"
"567737"	"Yang, LaurenceTianruo"	"A Tensor-based Data Representation and Processing Framework in Cyber-Physical-Social Systems"	"The booming growth and rapid development in embedded systems, wireless communications, sensing techniques and emerging support for cloud computing and social networks have enabled researchers and practitioners to create a wide variety of Cyber-Physical-Social (CPS) Systems that reason intelligently, act autonomously, and respond to the users’ needs in a context and situation-aware manner. The CPS systems are the integration of computation, communication and control with the physical world, human knowledge and sociocultural elements. This is a novel emerging computing paradigm and has attracted wide interests from both industry and academia in recent years. Generally, CPS systems collect massive data (Volume) from the physical world by various physical perception devices (Variety) in structured /semistructured/unstructured format and respond to the users’ requirements immediately (Velocity) and provide the proactive services (Veracity) for them in physical space or social space. These collected big data are normally high dimensional, redundant and noisy, and beyond the processing capacity of computer systems. With the rapid development of CPS systems, a novel data representation and processing framework should be urgently devised to cope with the increasing large scale and high dimensional data. This proposal describes a data representation and processing framework in CPS systems based on tensors, a type of high dimensional matrix widely used in many applications. First of all, we project the CPS data onto a 6-order tensor which includes time, spatial location, cyber resources, as well as users, and establish a unified data representation model with the integration of cyber, physical and social spaces in CPS systems. Then, the incremental distributed Higher-Order Singular Value Decomposition (HOSVD) scheme based on the effecient Lanczos method is proposed to perform the dimensionality reduction and quick preprocessing on the offline/online streaming tensor-based CPSS big data. The scheme will be suitable for any devices and any processor/machines easily participate in big data processing at any time. New scheduling algorithms for various machines/processors and devices’ task mapping will be investigated, as well as the corresponding case studies in some applications such as smart home and traffics to validate the feasibility and flexibility of the proposed framework.""559125,""Yang, Liang"
"567700"	"Yang, Victor"	"Optical coherence tomography, optical topographical imaging and fluorescence guided surgical laser ablation"	"Biophotonics technology and imaging modalities are being used in clinical applications based on the target specifications, imaging parameters, and experimental protocols. Relevant and significant translation of these technologies into the clinical world can be achieved and applied in meaningful medical practices. The goal of the research proposed in this application is to develop intraoperative optical coherence tomography and fluorescence image guided surgical laser ablation. Short term goals include establishing depth controlled laser ablation in the context of surgical procedures and improve real-time assessment of ablation results, improving on existing optical topographical imaging surgical navigation technique for accurate intraoperative targeting, and designing and developing a combined optical coherence tomography and fluorescence imaging platform for high-resolution real-time structural and functional imaging in the surgical setting. The long-term goal for this research project will be to utilize laser ablation, optical topographical imaging, optical coherence tomography, and fluorescence imaging in a surgical setting to improve the existing surgical protocols. The targeted clinical benefits will include the fields of neurosurgery and otolaryngology. Medical laser ablation is an established technique for the removal of tissue during surgical procedures. Dr. Yang and his laboratory have investigated using ultra-short pulsed laser ablation in hard tissues with inline coherent imaging (ICI, a derivative of optical coherence tomography) as a non-contact real-time feedback method to remove material within a localized target volume. There is great potential to utilize laser ablation in surgery with depth control, especially for neurosurgeries that require increased precision with extra care. Non-ionizing radiation, such as optical topographical imaging (OTI) of a surgically exposed area, can be combined with 3D data with real-time feedback for surgical navigation. OTI can be combined with medical laser ablation to increase the system response speed and achieve faster complete topographical image acquisition and registration cycles with robust performance using visible spectrum light, in a feedback loop to control laser ablation’s lateral positioning. The speed improvement will complement the accuracy that OTI will provide for laser ablation. Optical coherence tomography (OCT) and fluorescence imaging are two existing imaging modalities used in the research laboratory and in the clinic for disease detection, analysis and diagnosis. There is great potential for the combined usage of both imaging modalities for direct surgical benefits such as tracking/navigation and tissue ablation. OCT is an imaging technology that uses near infrared light (800 ~ 1550 nm) interferometry and offers near-histological resolution (ranging from submicron to a few microns) for non-invasive or minimally invasive imaging. Fluorescence imaging is an imaging modality that involves the absorption and emission of light by fluorophores of a target specimen to form an image of the detected signal by a fluorescence microscope. The development and implementation of OCT and fluorescence imaging for surgical guidance will only enable greater accuracy in optical tracking and laser ablation in the surgical suite. OCT and fluorescence guidance can be applied in the medical field and become standard operating room technology for neurosurgeons during laser ablation surgeries.""557555,""Yang, Victor"
"567560"	"Yanushkevich, Svetlana"	"Biometric Intelligent Interfaces"	"The proposed research program envisions building a unified framework for an ambient biometric sensing and identification environment. The framework will integrate emerging accurate and perceivably latent biometric sensing, intelligent image/signal processing, and advanced decision-making support. The framework will result in control interfaces that provide seamless scans of gestures, face and facial expression, and body, for identification and continuous tracking of a user's actions. This will enable the combination of the concept of Natural, Intuitive and Immersive (NII) interfaces, and the security of the access to the NIIs, through enhanced usage of biometric technologies. This will not only permit integration of multiple NII in security access control applications, but will also reinforce security and privacy of related NII applications, such as contactless interfaces for personal devices, computer systems, and situational awareness environments. The middleware of the framework will be built upon a platform that uses an Application Programming Interface, and performs local processing for low power, as well as sensor processing in parallel with application processing for high performance, using Graphic Processing Units. We will address this technology issues by integration of biometric sensors with intelligent context-aware support. This will involve multimodal data storage and processing using content-based multimodal data analysis and indexing. This will be addressed in this research program via syntactic and semantic analysis of multimodal input and multimedia data, in order to provide intelligent, or context-aware, retrieval of stored multimedia. The other challenge is a trade-off between low power/cost of implementation and low precision of the sensors. Ways to resolve this problem include using multi-sensor approach, as well as embedding more intelligence in sensors to compensate for quality. This proposal will deploy the most advanced RGB-Depth sensors, such as the multi-sensor approach currently embodied in PrimeSense’s Carmine, Microsoft's Kinect that perceives and identifies objects, face and full body motions using depth data from objects at 0.8 -3m distance, as well as the Leap 3D Motion control that allow recognition of users' finger gestures with superior precision (1/100 mm) at 0.2-0.8 m. The project will be implemented in the Biometric Technologies Laboratory at the University of Calgary, and is based on the previous and current projects, involving modeling of biometrics such as fingerprint, iris and face, facial expression recognition, face recognition in both visual and infrared spectra, as well as creating the prototyping modules for a new-generation of situational awareness biometric systems, based on multi-sensor and multi-modal structure and Bayesian decision-making. The ultimate vision of this research program is to bring this biometric platform to the level of NII that will influence all levels of the underlying technology, enable the anticipated applications (in human-personal device interaction, security, health care and education), and create a whole host of future applications waiting to be discovered.""563581,""Yao, Christine"
"555403"	"Ye, Winnie"	"An Automated Photonic Probe Station For Nanophotonic Device Characterization

"	"This proposal seeks support to build an automated photonic probe station to perform automated testing of multiple nanophotonic devices simultaneously. Recently, the successful launch of commercial silicon foundry services assisted by CMC Microsystems gives Canadian researchers inexpensive and reliable access to photonic multi-project wafers. As a result, a large number of prototyping devices are ready for testing. The proposed RTI system will expedite the testing procedure of active and passive devices significantly by allowing automated and parallel testing.The automated photonic probe station is well suited to the testing of ultracompact integrated circuits, with ground-breaking applications in computing, telecommunication, biomedical, and renewable energy. The proposed system is essential to support the research program of Dr. Ye, the Canada Research Chair (Tier II) in Nano-scale Integrated Circuits for Reliable Opto-electronics and Sensors. Dr. Ye's research team has been actively involved in designing innovative waveguide-based devices for optical switches, modulators, polarization rotators, sensors, as well as solar cells. Measurements to date have been performed manually, device by device. There is a timely need for an automated optical probe station.  Any delay in the acquisition of the equipment will have an irreversible and negative impact on Dr. Ye's leadership in silicon photonics. Furthermore, the delay will cause financial burden to Dr. Ye's research program due to the unavailability of such an automated system in the local region. Finally, the requested equipment will enable effective training for the next generation of highly-skilled personnel (1PhD/2MASc/2RA), and contribute significantly to building Canada's international leadership role in photonics.""557067,""Ye, Winnie"
"567472"	"Yoon, YoungKi"	"Simulation-Based Predictive Analysis and Optimization of Multi-Layer 2D Flexible Nanoelectronic Devices"	"The electronics industry has changed dramatically over the last decade, shifting its focus from high performance to mobile applications; today’s technology drivers typically target low-power, lightweight, transparent and flexible functionality. In this regard, a new class of thin, 2D layered nanomaterials is favorable, offering numerous opportunities for emerging electronic devices. Like highly confined conventional 3D semiconductors, electronic properties of layered materials change substantially with the thickness of material (i.e., the number of layers), but in a quite different manner such that the change of band structure is beyond the simple physical confinement effects. In addition, unlike single-layer materials, the transport properties of a multi-layer system are significantly affected by interactions between the neighboring layers. Furthermore, different combination of 2D materials, particularly those that include artificial lateral heterostructures (e.g., graphene and hexagonal boron nitride), may enable new functionality. Such novel 2D materials are promising for future electronic devices specifically on plastic substrates due to their thinness and flexibility. However, our understanding of multi-layer flexible electronic devices is still in its infancy and our current fabrication and engineering methods for these devices are far from optimal. Therefore, the proposed Discovery Grant program will pursue critical new fundamental understanding of the basic scientific and complex engineering problems underlying multi-layer 2D flexible nanoelectronics through highly efficient computer simulations. The program will build upon the applicant’s recent research in quantum transport simulations for emerging devices based on various nanomaterials including nanowires (1D), graphene (2D) and confined InAs (3D). From the simulation viewpoint, the investigation of multi-layer 2D nanoelectronics calls for fundamentally different approaches from single-layer or confined 3D semiconductor devices. Therefore, the investigation of quantum transport in multi-layer systems, especially in the presence of out-of-plane strain will indeed be groundbreaking in this field. In pursuing the program's overall goals, several shorter-term objectives will be addressed over the next five years, each of which will advance the state-of-knowledge on layered material electronics and provide a unique training environment for imparting leading edge skills in computational nanotechnology research: (1) To obtain fundamental understanding of layered-material flexible electronics with external stress through atomistic quantum transport simulations; (2) To provide accurate predictions and ultimate optimization of such nanodevices; (3) To develop a highly efficient parallel code to quickly solve large-scale diffusive transport problems of 2D flexible electronics; (4) To calibrate theoretical models with experiments. Outcomes of this research program will provide deep insights into multi-layer 2D flexible electronics, laying critical groundwork for the future, ultra-portable and flexible electronic devices. Currently global semiconductor industry has a $300 billion market per year and the development of this research program will bring huge economic benefit to Canada’s IT industries as the source of information is shifting rapidly from desktop to mobile devices. In addition, this research will help position Canada at the forefront of nanoelectronics research through HQP training; two PhD and three MASc and one Undergraduate Co-op students will be trained to acquire unique skills of numerical simulations including non-equilibrium Green’s function method, and graduates from this program will be highly sought after by both research organization and industries.""560468,""YorkLyon, Anna"
"566416"	"Yu, Eric"	"Agent-Oriented Modeling for a Dynamic and Evolving World"	"Successive waves of advances in software and information technologies (IT) have transformed society in recent years. Our work, play, and social interactions are now intricately mediated by IT systems. Yet, newly emerging technologies are about to trigger another round of transformation. Low-cost sensor networks, the Internet of Things, location-aware and mobile devices, and advanced data analytics and business intelligence, when used in combination, will bring about a new digital revolution in “sensing and interpreting”. When coupled with existing IT infrastructures which are highly developed for executing pre-defined procedures and interactions, the emerging enterprise-wide IT architectures will help organizations become much more responsive and adaptive to their increasingly rapidly changing environments.  This research project aims to develop a modeling framework to help conceptualize, analyze, and design the complementary application of diverse software and IT systems to achieve business and organizational goals in highly dynamic and rapidly evolving environments.  In information systems and software engineering, system designers and architects rely crucially on appropriate models to conceive, describe, analyze, and manage complex systems. The field of Requirements Engineering has developed models and techniques that help bridge a “business” level understanding of a system to its technical design and implementation. In recent years, the field has expanded from an earlier focus on detailed specification of individual systems to encompass an enterprise-wide scope, connecting from business strategy to policies and plans, to business processes, to technology systems. In this project, in recognizing the rapid adoption of a new crop of “sense-and-interpret” technologies, we will develop a new modeling framework to support the new digital revolution, one in which software and data will offer orders-of-magnitude advances in sensing and interpreting, just as past IT advances had led to orders-of-magnitude gains in executional efficiency.  This new phase of the digital revolution is also enabled by advances in software architectures which offer greater flexibility and adaptability such as cloud and service-oriented computing, as well as those with self-adaptive capabilities. The framework to be developed in this research will guide the development of enterprise-wide architectures that will leverage sense-and-interpret technologies to exploit the adaptability offered by the new software architectures, overcoming the rigidities common in traditional IT systems. The framework will help system designers and enterprise architectures rethink the boundary between design-time and run-time, and the re-allocation of responsibilities among humans and automated systems. The new revolution could also transform software engineering processes, as some knowledge-intensive activities (including requirements engineering activities) may migrate from design-time to run-time to leverage near-real-time input from the environment. The project will build on, adapt and substantially extend the i* agent-oriented modeling framework, a leading framework for goal-oriented requirements engineering. It highlights the need to analyze strategic interests and social relationships among semi-autonomous actors in the context of complex software and IT applications. The i* framework originated from the applicant’s PhD thesis, and is the basis for an international standard. The proposed research will draw upon concepts from diverse disciplines ranging from strategic management to software architecture to socio-ecology. Case studies with Canadian industry will be used to derive framework requirements and to validate results.""555380,""Yu, FeiRichard"
"567704"	"Yuan, Fei"	"Design Techniques for Remote Calibration of Passive Wireless Microsystems"	"Passive wireless microsystems (PWMs) harvest their power from RF waves. The absence of bulky batteries minimizes the physical dimension and cost of these microsystems, and removes the need for maintenance. As a result, PWMs can be embedded in products or implanted in living bodies permanently. Attributive to small size, wireless accessibility, programmability, low cost, and maintenance-free operation, PWMs have found a broad range of emerging applications including implantable bio-MEMS pressure sensors, retinal prosthetic devices, intraocular pressure monitoring, capsule endoscopy, pressure sensors for wireless arterial flow characterization, temperature sensors for human body and environmental monitoring, pH sensors, identification tags in logistics automation, and product authentication, to name a few. Because PWMs are subject to the effect of process spread, supply voltage fluctuation, and temperature variation (PVT), they must be calibrated prior to their intended operation. The physical inaccessibility of these microsystems mandates that their calibration be conducted wirelessly from the base station hereafter referred to as remote calibration. The long-term objective of this research project is to develop PWMs for emerging applications in healthcare, product authentication and counterfeiting, object tracking, assess management, security, etc. and capitalizing research findings as technology transfer to Canadian industry. The short-term objective of this prject is to develop new design techniques, circuit topology, and silicon realization for the remote calibration of PWMs so as to ensure their proper operation and reliable communications with the base station. The research project targets the calibration of the following key blocks of PWMs: power harvest, system clocks, analog-to-digital converters, voltage and current references, and voltage regulators. Time-mode approaches that scale well with technology and offer many intrinsic advantages as compared with voltage-mode or current-mode approaches will be fully explored and utilized in both the design and calibration of these blocks. This research project targets issues that are critical to the deployment of PWMs in emerging applications especially those where PWMs are physically inaccessible. As the techniques to be developed are applicable to general PWMs, the findings will be of a great importance and interest to the scientific community and industries. The project will also provide a rich learning environment for students by engaging them in the development and design of PWMs with boldly focused applications.""565187,""Yuan, Fei"
"567208"	"Zariffa, José"	"Novel sensors for adaptive neurorehabilitation systems"	"My research is concerned with developing technology that can help restore function after neurological injuries, such as spinal cord injury (SCI) or stroke. My current focus is on innovative signal and image processing solutions for monitoring and assessing the activity of the nervous system. These measures are needed as feedback control signals to create neurorehabilitation systems that can adapt to changing conditions both in the user’s body and in the external environment. In particular, implanted systems have used functional electrical stimulation (FES) of motor neural pathways to restore movement, but these devices currently rely on pre-defined patterns of stimulation and thus can only produce crude, fixed movements. The objective of the research program proposed here is to overcome this limitation by developing sensors that can provide the feedback signals needed to create closed-loop implanted FES systems. The intact nervous system achieves effective motor control only by relying on a combination of proprioceptive (limb position), tactile and visual information. My specific aims are therefore: 1) To develop an implantable neural interface capable of monitoring the sensory information in peripheral nerves, such as tactile signals and limb position information. This interface will be based on multi-contact nerve cuff technology (cylindrical electrodes that wrap around a nerve, with recording contacts located on their inner surface). In one study, we will optimize the design of the electrodes and the signal processing algorithms, using first a novel computer model of a peripheral nerve, later followed by in vivo validation. In a second study, we will explore a new class of signal processing algorithms designed specifically to extract information from multi-contact nerve cuff recordings by exploiting spatiotemporal relationships in the electrical activity of the nerve. These projects are the first proposed attempt to tailor a multi-contact nerve cuff specifically to the monitoring of sensory activity. These devices have not yet been used for this purpose, but are a promising avenue because they combine suitability for chronic use in humans with improved selectivity compared to single-channel cuffs. 2) To develop a wearable sensor capable of providing visual information about the user’s environment and their interactions with it. This will make it possible to incorporate visual information into the control algorithms for closed-loop neuroprostheses. We will focus here on upper limb function. Computer vision technology will be used to analyze in real-time video from an ear-mounted wearable camera recording the user’s point of view. We will explore image processing and machine learning algorithms capable of parsing the visual information in the immediate neighbourhood of the hand with the goal of detecting environmental interactions (e.g. grasp attempt, success or failure, grip type used). No previous solution has been proposed to incorporate visual information into the control of FES. Our proposed study is the first to address this gap. Significance: The proposed research will produce novel sensor technologies that will be essential if we are to develop closed-loop neuroprosthetic systems capable of restoring natural movements to individuals with neurological injuries. Innovations in the natural science and engineering will include a new class of algorithms for processing neural signals, new computer vision techniques tailored to wearable cameras, and improved computer modeling of bioelectric activity. The research will ultimately lead to improved independence and quality of life after stroke and SCI, and reduce the economic impact of these conditions on the health care system.""561594,""Zarifi, Omar"
"566738"	"Zemel, Richard"	"Machine learning to understand images and text"	"Machine learning research aims to build computer systems that extract useful information from data. Considerable progress has been made in the two main areas of learning. Supervised methods, which rely on having target labels for each input, have proved to be very powerful and widely applicable, particularly on classification problems, in which each input example is assigned to one of a small set of classes. These methods generally scale poorly with the number of training examples and classes. Unsupervised methods utilize unlabeled data, where no targets are provided, and attempt to construct representations useful for many input-output mappings. These methods scale readily but are not as powerful, because without labels considerable structure must be imposed to make the algorithms work. This proposal focuses on the development of learning methods that utilize both labeled and unlabeled examples, addressing two fundamental problems. The first is extracting informative summaries of structured objects, which contain several inter-dependent components. Two very common structured objects are images and documents. A sample task in this domain is scene analysis, which entails picking out multiple items in visual input. For example, a video of a busy street recorded digitally as a stream of color pixel-maps may be re-represented in terms of the cars and pedestrians and their motions. This is an essential and difficult problem, involving a combination of low-level image properties with high-level, object-specific knowledge. The second problem is rapid learning. The human ability to acquire a visual concept from a few examples, and to recognize instances in a complex scene, poses a central challenge to the fields of computer vision, cognitive science, and machine learning. Improving machines' ability to acquire new concepts readily has applications not only to vision, enabling analysis of scenes containing relatively novel objects, but also to the domain of online interaction, as a rapid learning system can build an accurate model of a user with just a few questions, or limited knowledge of the person's web-surfing history or product preferences.""556722,""Zemel, Richard"
"567096"	"Zhang, Xiupu(John)"	"Exploration of InP Quantum Dot Coherent Frequency Comb Lasers and Mode-Locked Lasers"	"Indium phosphide (InP) is one of the key semiconductors for production/integration of active and passive semiconductor devices, and large-scale photonic integrated circuits. Recently, quantum-dot (QD) semiconductor lasers, an emerging research area and one of the key InP devices in optical communications, and optoelectronic and photonic devices, have many advantages when compared to conventional quantum well (QW) lasers. Use of QDs leads to a broader optical gain spectrum, lower temperature sensitivity, higher saturation output power, smaller chirp and smaller linewidth enhancement factor, as well as the independent multi-wavelength operation. Perhaps more importantly, one-section multi-wavelength QD lasers can become coherent frequency comb sources and mode-locked lasers when the laser driving current is beyond a certain level when the QD lasers are fabricated by chemical beam epitaxy. Moreover, the QD model-locked lasers have higher repetition rate, ultra-narrower pulse width, higher output power and lower timing jitter than any other two-section mode-locked lasers. Currently, we are collaborating with National Research Council (NRC) on InP QD lasers (including many new QD lasers). Examples are the QD mode-locked lasers having repetition rate of 10-100 GHz and pulse width of down to 295 fs, 403-437 GHz repetition pulses using external cavities, tunable terahertz generation of 1-2.2 THz using tunable external cavities. In addition, we have developed a physical model that has been successfully applied to the analysis of QD laser mode-locking. This five year research project is proposed to develop comprehensive physical models for QD laser modeling and design, which will not only be first of its kind but also an important tool for QD laser design optimization. We will also investigate novel and new designs of QD laser physical structures for achieving lower threshold for lasing and mode-locking, optical gain spectrum > 20 nm, modulation bandwidth > 20 GHz, relative intensity noise < -140 dB/Hz, and RF linewidth < 1 KHz etc.. This is a real challenge to researchers. Physical models will be created using the commercial software applications like Crosslight and Silvaco technology computer aided design (TCAD) integrating with our innovative theories and models. Users of this new platform can deploy the basic functions directly from the commercial software applications while we can dedicate our new, sophisticated tools for the QD lasers models. Upon using the new tool to model and optimize the physical designs, NRC will then help us fabricate them with regard to the different QD laser criteria. This novel research will expand and strengthen research and training in InP lasers and photonic integrated circuits, and will deliver a comprehensive study of QD laser modeling and design which will be an asset to Canada and the world. The developed lasers can have applications to medicine,optical sensing, advanced optical /wireless communications and THz generation. Moreover, the Canadian economy will have the needed “high-tech skills”, the “sophisticated level of education” trained HQP who will be job-ready. There will be no skills “mismatch” . Our well-rounded HQP will be sought after by NRC, Ciena in Ottawa, JDSU in Ottawa, EXFO in Quebec, Ericsson Canada, TeraXion in Quebec, OneChip in Ottawa, CRC in Ottawa, Infinera in Ottawa, OZ Optics in Ottawa, Alcatel-Lucent in Ottawa, Excelitas Technologies in Montreal, and ART Research and Technology in Montreal etc, where some of my HQP are working. This project will foster internationally competitive research directly contributing to Canada's science and technology needs.""550985,""Zhang, Yan"
"555362"	"Zhuang, Yanyan"	"A Global Testbed for Programmable Exploration of End-User Devices"	"Computer systems design, Distributed computing, Mobile cloud computing, Performance isolation, Security and privacy, Virtualization, Ubiquitous data access, Mobile sensor network, Statistical learning, Data analysis""550958,""Zhuo, Min"
"566424"	"Zulkernine, Mohammad"	"Build and Watch: Towards Intrusion-Aware Software Systems"	"Security is of prime importance in the design and implementation of software systems. Nevertheless, in most software development processes, security issues are not addressed from the beginning of a software development life cycle and sometimes even are looked at after the deployment of the software. As a result, these software systems remain vulnerable to various attacks. The growing complexity of software systems requires more serious attention of software engineers and security analysts to monitor software systems for their security. However, the main focus in most secure software development methodologies is not specifically on integrating or monitoring security aspects along the development phases. This research program will focus on building intrusion-aware software systems and then watching or monitoring those at runtime. The ultimate goal of this research program is to develop methodologies allowing the generation of more secure software systems by enabling them to monitor intrusions automatically in the modern distributed environments. The proposed program will be carried out to achieve this goal using two complementary approaches of development and monitoring: i) Security requirements specification-based; ii) Security pattern-based. A specification method will be proposed which can be used to specify operational behavior of the system both under normal condition and when it is under known attacks. The same specification will be utilized at runtime for identifying the deviation from the specification. Tools will be built for intrusion scenario description and automatic signature generation from the scenarios. Finally, software monitoring systems will be developed to detect any intrusions into the software systems by analyzing the run-time behavior of the systems with respect to the generated signatures. Security patterns will be used to realize security requirements in software design and the violation (absence or presence) of these patterns will be detected by automatic analysis of the patterns employing the observed runtime information. As security patterns are the components of design and correspond to security requirements, any violation of these patterns indicates the violation of the corresponding requirements. The methods and tools derived from this research program will be assessed on case studies from real world security-critical applications. The outcomes of this research will play a vital role in bridging the gap between software engineering and security engineering practices for modern, complex, and distributed software systems. Security of software systems is more crucial now as it is the basis for the security of new computing paradigms and environments such as Cloud and mobile. One of the unique aspects of this research program is to provide a balanced training facility for five graduate students in both software engineering and security engineering principles preparing them to build secure software products.""562390,""Zumer, Jeremie"
"566009"	"Özsu, MTamer"	"RDF Data Management"	"Graph data are of growing importance in many applications including the semantic web, social network analysis, bioinformatics, and physical communication networks. Graphs naturally model complicated structures in these fields, such as the relationships among people in a social network or the protein-protein interaction networks. The size and complexity of these graph data raise significant data management and data analysis challenges. My broad research scope is the study of these problems. In this discovery grant, my focus is on the graph structures that arise from models of Web resources. The Resource Description Framework (RDF) is the standard (proposed by W3C) by which Web objects are commonly modeled. RDF is a self-descriptive data model that is suitable for machine understanding and interpretation, and, therefore, expected to facilitate the “semantic web”. W3C has also defined a query language, called SPARQL, for accessing RDF repositories. RDF data sets have started to proliferate and grow. For example, Yago and DBPedia extract facts from Wikipedia and store them in RDF format to facilitate structural queries over Wikipedia; many local governments are now encoding the resources they provide to citizens in RDF format as part of the e-government initiatives; biologists have built elaborate RDF data collections (BioRDF and Uniprot RDF) for community sharing of experimental data; and Linked Open Data (LOD) initiative has been growing (as of September 2011 - which are the latest available information - over 31 million triples [tuples]) as a web data integration platform. Consequently, managing and analyzing large and distributed RDF datasets have emerged as an urgent and important concern. My group’s approach to RDF data management and analysis differs from many of the existing approaches that map, in one way or another, RDF into a relational representation and convert SPARQL queries into SQL. Although this has the advantage of leveraging mature technology, it gives rise to performance and modeling mismatch problems. We model an RDF dataset as a graph (which is the native model for RDF) and also represent a SPARQL query as a graph. Consequently, query execution reduces to graph matching. This approach has modeling and performance advantages. Within this general approach, I intend to study the following issues over the next five years: 1. Efficient storage structures for RDF graphs. 2. Efficient and effective query processing and optimization techniques for SPARQL queries (including aggregation queries that are now part of the SPARQL standard). 3. Distribution of RDF graphs and evaluation of SPARQL queries over distributed RDF stores. 4. Web data querying and integration using RDF, which requires some reasoning capability over RDF data (so called OWL 2 entailment regime). The methodology that will be followed includes algorithmic studies, development of prototype systems, and extensive experimentation. Successful completion of this research will result in the development of efficient and effective techniques for RDF data management, and web data integration and querying through RDF. Since RDF technology is now widely deployed (including by various levels of government at a number of countries), the results will have significant impact both technically and societally.""556726,""Özsu, Tamer"
