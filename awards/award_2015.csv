"id"	"researcher_name"	"application_title"	"application_summary"
"589576"	"AlHaddad, Kamal"	"Convertisseurs électroniques de puissance à haute densité, haut rendement et à faible repercussions en termes d'harmoniques et d'interférences électromagnétiques"	"Il est prévu que plus de Soixante-dix pour cent de l'énergie électrique produite transite par des convertisseurs statiques de l'électronique de puissance, d'où l'importance majeure d'optimiser de ces unités de conversion dans le but de mieux utiliser les ressources énergétiques disponibles et à développer. L'évolution rapide des technologies de l'Électronique de Puissance durant la dernière décennie a rendu son utilisation très abondante, tant dans le secteur industriel moderne que dans la production, la transmission, la conversion et le stockage de l'énergie électrique. Dans la plupart des applications telles que la traction électrique, les lignes de transport haute-tension à courant continu, les micro générateurs à énergies renouvelables (éoliennes, panneaux photovoltaïques, piles à combustible), les entraînements électriques à vitesse variable, les chargeurs de batteries, les centres de communication, etc., les grandeurs électriques sont modifiées dans le but de répondre aux besoins de la charge. Cette conversion est naturellement assurée par des interfaces électroniques de puissance à base de semi-conducteurs. <br />Les objectifs visés par cette recherche sont:<br />Concevoir la nouvelle génération de convertisseurs électroniques de puissance à haute densité de puissance capables d'opérer à haut rendement énergétique et générant le minimum d'interférences électromagnétiques et de pollution harmonique.<br />Développer un nouveau concept de réalisation des nouvelles topologies de convertisseurs de puissance basée sur l'intégration des cellules commutables multiniveaux de tension, multicellulaire permettant de réduire les contraintes sur les semi-conducteurs de puissance tout en utilisant les nouvelles technologies SiC et GaN.<br />Développer de nouvelles techniques de modélisation en vue d'une simulation en temps réel et d'une commande utilisant la technologie FPGA direct des structures complexes de convertisseurs électroniques de puissance dont le nombre de semi-conducteurs dépasse la centaine.<br />Développer des techniques de modulation et de commande avancée des structures complexes permettant de commander les interrupteurs de puissance avec des FPGA associé à une isolation optique afin de minimiser les pertes par commutation tout en assurant une performance dynamique inégalée. <br />Former un personnel hautement qualifié, à la fine pointe de la technologie aux trois cycles universitaire et ceci en impliquant les étudiants de baccalauréat, de maîtrise et de PhD dans la conception, la réalisation expérimentale, et la mise au point de ces nouvelles technologies à déployer. <br />Assurer le transfert des technologies développées dans le cadre de ce projet d'envergure vers les partenaires industriels constitue un volet très important du programme de recherche et de formation des étudiants en plus de la publication des résultats dans les IEEE Transactions et dans des conférences IEEE<br />""578903,""Alhajj, Reda"
"589822"	"Ali, Shirook"	"Design, Optimization, and Fabrication of an Advanced Infrared Energy Harvesting System"	"The increasing energy demands of a growing world population and depleting fossil fuel reserves indicate the urgent need of seeking alternative renewable energy resources. The sun is an abundant source of clean energy whose radiation spectrum consists of ultra-voilet, visible and infrared (IR) regions. This program is about harvesting the Sun's IR energy and transforming it into usable power. The proposal will deliver world class nano-systems through ambitious nano-antenna array design built using nano-science materials and packaging. The proposed system is smart in that it can optimize its performance by sensing the environment and tracking the IR sources via the developed algorithms. The program grant will provide the opportunity to build the foundation for this long-term research effort. <br />The short-term goals of this research program fall into three main streams: First, modeling and design of nano-antennas. Maxwell's differential equations will be updated to include dispersive permittivity and permeability. Adjoint sensitivities for dispersive materials will be implemented. With these, broadband sensitivities will be available to the investigation with minimal simulation overhead regardless of the number of parameters of interest. The second stream, use the developed modeling tools using them in investigating different conventional (dipoles, bow-tie, and patch antennas) and non-conventional antenna structures (fractal and multi-feed antennas) as well as testing different materials for their packaging. The developed nano-arrays will be fabricated at the McMaster Micro- Nano-Systems Laboratory (MNSL), a $22.25M state-of-the-art facility funded by the Canada Foundation for Innovation (CFI). Different fabrication processes will be examined. This is to search for optimized tolerance and cost for mass production. <br />The third stream includes investigating the elements of the nano-system together to examine tracking algorithms for optimized harvesting capabilities. The systems performance will be measured in metrics such as the reflection and transmissions spectra of the electric field, the antenna EM radiation pattern(s), and power spectra of the nano-antenna where the output of the model is in units of emissivity vs. wavelength. <br />The outcomes of this project will benefit Canada in providing much needed technology for energy harvesting. This will have a significant impact on the strategic information technology sector of the Canadian economy. Active student involvement and training are central to the applicant's program. The applicant has strong ties to industry. This grant will enable the training of HQP offering them advancements in career opportunities. The applicant strongly believes that this research proposal will bring intellectual property, knowledge, and potential commercial deployment.""568408,""Ali, Syed"
"590241"	"Allili, MohandSaid"	"Segmentation sémantique et analyse des images et vidéos: approches par apprentissage et sélection de caractéristiques et applications"	"<p><span style=""font-size: 12.8000001907349px; line-height: 1.5em;""></span></p><p>Le besoin croissant de systèmes visuels autonomes génère un grand défi pour le développement d'algorithmes capables d'analyser et d'interpréter efficacement et automatiquement le contenu des images et des vidéos. Ceci est d'autant plus important vu la croissance fulgurante du nombre d'images et de vidéos et l'émergence de nouvelles applications pour aider l'humain dans ses tâches quotidiennes (ex. gestion des grandes bases de données d'images/vidéos, surveillance, robotique, navigation autonome de véhicules, etc.). Parmi les défis posés pour ces applications, la segmentation sémantique des images et vidéos est l'un des plus importants. Dans les images, le problème est lié à l'identification des catégories d'objets présents dans les images ainsi que leur segmentation. Dans les vidéos, en plus de la reconnaissance et la segmentation des objets, le problème est lié aussi à l'analyse et la reconnaissance des activités générées par ces derniers.</p><p>Malgré les développements remarquables en recherche des dernières années, la plupart des algorithmes actuels demeurent loin d'atteindre la capacité du système de vision humain pour la segmentation sémantique. Mon programme de recherche vise à développer des approches novatrices et originales basées sur l'apprentissage automatique et la sélection de caractéristiques, pour analyser, extraire et représenter l'information sémantique des images et vidéos. Pour les images naturelles, les approches combineront l'analyse du contexte, de la saillance et de la forme des objets pour guider la segmentation sémantique. Pour la vidéo, mon programme se focalisera sur l'intégration de la segmentation d'objets et la reconnaissance de leurs activités. En particulier, des approches tenant compte de l'analyse du contexte et des interactions entre objets seront développées pour l'analyse des activités individuelles et de groupes.</p><p>Mon programme de recherche apportera des avancées majeures dans les domaines de la vision artificielle et du traitement des images et de vidéos. En particulier, il mènera au développement de nouvelles approches pour l'analyse et l'extraction automatiques du contenu sémantique des images et vidéos. Il aura aussi des retombées pratiques sur le développement de nouvelles applications en amont avec les besoins de l'industrie des technologies de l'information et de la société Canadiennes. En particulier, mon programme mènera à des approches qui amélioreront la gestion des grandes bases de données d'images/vidéos (ex. recherche et l'indexation sémantiques des images/vidéos), la sécurité (ex. surveillance vidéo et détection de spams multimédias) et la télédétection (ex. segmentation de la végétation, des eaux, etc.). <br /><br /><br /><br /></p>""572719,""Allingham, John"
"591250"	"Allison, Robert"	"Role of stereopsis in locomotion and navigation: Interaction and interfaces"	"The overall theme and long-term goal of this research program is to develop a model of stereoscopic surface perception and its role in visually guided action. Stereoscopic depth or stereopsis arises from processing of the differences or `disparities' between the images in the two eyes and is important for the perception of surface shape, environmental layout and material properties. Such information is, in turn, important for the planning and execution of visually-guided movements including locomotion. The focus of the current proposal is to provide a better understanding of the perceptual cues humans use to make judgements of their environment that support their locomotion. We then apply this knowledge to help evaluate and develop more effective 3D user interfaces.<br />Recent studies (from our lab and others) and anecdotal reports demonstrate that binocular vision and stereopsis can provide substantial perceptual benefits at distances relevant to the moment-to-moment control of locomotion. In the current proposal I address the role of stereopsis in guidance of locomotion in real and computer-mediated environments with these short-term objectives: <br />1)   Investigating the role of stereoscopic vision in <em>p</em><em>erceptual decision making</em> including in the perception of our self motion and of the layout of the world through which we move. <br />2)   Understanding how these perceptions affect the <em>control and guidance</em> of locomotion, path planning and navigating.<br />3)   Assessing the implications in information display and for aiding <em>virtual reality (VR) and augmented reality (AR) locomotion.</em> We will develop design rules, guidelines and techniques for generation of stereoscopic imagery to support AR locomotion.<br />Given that the role of stereopsis in locomoting, orienting and navigating has not been well studied, in a series of foundational experiments we will extend our work on scene perception to the control of locomotor action. In particular we will examine the role that binocular vision plays in performing real world tasks such as ramp walking and walking on rough terrain. These experiments will be enabled by availability of state-of the art facilities such as our unique wide-field stereoscopic display.<br />This basic research will provide insight for developing effective 3D user interfaces, particularly in VR and AR. We will extend our work on the role of stereoscopic cues in the perception of self-motion and cognitive mapping in the real world and apply it to abstract and naturalistic 3D locomotion. Based on these experiments, we will recommend ways to augment real and virtual environments to support and extend these capabilities. The research will enable safer, more effective and more natural user interfaces for low vision aids, vehicular displays and other applications. Highly-qualified researchers will be trained on skills in human-computer interaction and visualization that are highly sought after by Canadian industry.""574385,""Allison, Robert"
"589362"	"Aravind, Alex"	"Development of Synchronization Algorithms and Simulation Systems"	"<p>A concurrent program is a set of sequential programs that can be executed in parallel and cooperate to a common goal. As concurrent programming is becoming a mainstream software development approach, it is among the most important topics in computer science. Therefore, it is our view that it is highly beneficial for Canada to take the lead in both research and the training of students in this emerging field. </p><p>Recent developments in hardware and software indicate that concurrent programming is inevitable in future software development.  It is gaining practical importance in many real-world applications such as event-based implementation of graphical user interfaces, real-time systems, multi-user games and virtual worlds, sensor networks, embedded systems and devices, etc. As indicated in a recent study, simple microcontrollers  produced by PIC, ARC, Altera, Marvell, and Freescale, are used in low-cost devices like basic cell phones, cameras, printers, music players, toys, etc., and billions of these devices are created annually and will continue to exist for years to come. Therefore, it appears that we are entering into the concurrent programming era. </p><p>The chief goal of the proposed research is to develop, verify and prove, and conduct performance studies on synchronization algorithms for concurrent systems. A concurrent system is a set of processes (or threads) whose executions either interleave or overlap in time. Concurrent systems span a wide spectrum of computing systems.  Although, in theory, numerous combinations of concurrent systems  are possible, only the systems where processes interact with one another warrant attention, and they often require careful investigation and deep understanding before implementation. We plan to focus our study on synchronization issues. More specifically, we are interested in developing and studying solutions for the mutual exclusion (ME) problem and its variations. ME is a fundamental synchronization problem, and most other synchronization problems inherit ME or its variants in some form. Our interests lie in designing and studying solutions (concurrent algorithms and constructions) for shared memory systems without using any special hardware, compiler, or operating system support.</p><p>Among concurrent systems, many software systems are life critical (such as medical systems) and some are high assurance (such as online financial systems and e-commerce). As software has become an integral part of our lives, the advancements in this domain will have ubiquitous influence. Therefore, concurrent programs designed for such systems must be thoroughly verified for synchronization issues and studied for performance before they are implemented and used in practice.  The prime motivation for our research is to ensure that the concurrent systems developed are safe, reliable, and efficient, well before implementation. </p>""584817,""Aravind, AlexAlagarsamy"
"590452"	"Arbel, Tal"	"Probabilistic Inference in Computer Vision and Medical Imaging"	"The objectives of the multidisciplinary research program proposed are to develop new probabilistic formulations for problems in computer vision applied in particularly challenging medical imaging contexts, such as those presented in neurology and neurosurgery, where they have the potential to lead to significant advancements in terms of robustness, accuracy or speed over traditional techniques. First, new sophisticated, multi-level probabilistic graphical models will be devised for a wide variety of open and difficult pathology detection and segmentation tasks in the domain of medical imaging. New, high level, irregular grid (i.e. non-lattice) random fields will be developed to properly represent contextual (spatial and textural) information.  Inference will iterate across voxel, lesion and context level random fields until convergence. Development of new graphical models for these domains will require departures from popular neighbourhood models, innovation in design of appropriate models for node and edge information, as well as appropriate inference techniques. In addition to theoretical contributions machine learning, medical image analysis and to computer vision, the resulting models should significantly improve a wide variety of difficult and important pathology detection and segmentation tasks in real, clinical domains, such as Multiple Sclerosis lesion segmentation, the segmentation of sub-classes of brain tumours, and breast cancer detection. Through collaborations with a local company, my research team will have access to the largest dataset of real, multicenter, clinical trial, manually annotated, patient MRI images of any medical image analysis group in the world in which to test the methods. <br />The research program also includes the development of new fast multi-modal image registration techniques for time-sensitive domains (e.g. image-guided neurosurgery), where, by embedding learned context-specific, probabilistic feature/pixel selection into the registration strategy, we hope to show significant speedups without compromising accuracy.  Probabilistic techniques will be developed for the analysis of neurological images, focusing on learning the variability of cortical folding patterns in different sub-populations (e.g. age, IQ) and to automatically discover biomarkers of neurological disorders, such as MS and Alzheimer's patients. This will lead to innovations in terms of robust feature learning and clustering in contexts with large variability.""582566,""ArbelaezMoreno, LinaMarcela"
"590567"	"Atkins, Stella"	"Novel human computer interaction methods and algorithms for medical applications"	"<p> </p>Efficient and effective human-computer interaction is essential for real-time medical applications. My program of research is to develop image processing algorithms and human computer interaction (HCI) techniques, with the long-term applied goal of expanding the role of smartphones into mobile healthcare applications. In the short-term, I will perform research in two distinct, complementary research areas: image analysis algorithms and eye tracking. Two important medical areas will provide context and data for this fundamental research: dermatology (skin) image analysis, and laparoscopic (keyhole) surgery training. With wearable eye trackers and augmented reality displays integrated into smartphones, these research areas will merge, by developing eye tracking and pupil responses as novel interaction controls for smartphone apps.<br /><strong>Image Analysis</strong>: Mole images acquired with a magnifying dermoscope attached to a smartphone camera can show several textural features just below the skin surface, not visible to the naked eye. Sophisticated dermoscopy image analysis algorithms will be developed and extended for application in a mole imaging smartphone app, to provide detailed quantitative analysis about these textural features. Image registration algorithms will be developed to identify changes in skin lesions over time. Interactive functions will be investigated to allow users to identify where the image is taken on the body and to monitor changes over time.<br /><strong>Eyegaze tracking</strong>: In laparoscopic surgery training, the surgeon looks at the surgery scene through a remote monitor display, and computer simulators are often used to teach novices expert strategies for eye-hand coordination. Eyegaze responses can differentiate novices and experts in surgery training, and I plan to develop new training methods based on so-called ""gaze training"", which encourages novices to adapt the eyegaze strategies of experts during eye-hand coordination tasks. Surgery training simulators will be developed for smartphones enhanced with eye tracking and hand motion sensors, employing novel interaction methods based on eyegaze glances, blinks and fixations. Furthermore, pupil responses, which provide insights into the physiological state of the user, can be used to monitor the state of the user while performing surgery simulations.<br />This research will advance and merge the fields of image analysis and human computer interaction.<br /><p> </p>""579097,""Atkinson, Adam"
"591010"	"Aycock, John"	"Tools and frameworks for computer security alternatives"	"<p>Our society is heavily reliant on our computers, devices, and networks, but there are still any number of problems making our computers secure, even- after decades of computer security research. Given the sheer volume of research in computer security, relatively little of it has translated into new, improved defenses that are deployed for the protection of end users. Arguably, computer users have seen stagnation or at best marginal improvements in their defenses, while at the same time the number of threats has increased dramatically. The long--term goal of my research program is to find novel methods for security that break from this status quo, radically in some cases, to find entirely new alternatives to the current state of computer security that users experience, and make users more secure. Practically speaking, I will be realizing this long--term goal through the construction of software tools and frameworks, meaning that the research work is not just seen in academic publications, but will result in software usable by others.</p><p><span style=""line-height: 1.5em;"">    In the shorter term, for the tenure of this grant, I will be looking at advancing my long--term goal in three areas: code obfuscation (protecting computer code), program execution monitoring and sampling (watching computer code), and computer systems (new ways to create and express computer code). These areas and the initiatives within them are not arbitrarily chosen. One guiding principle that underlies the security alternatives I'm proposing is how to employ the abundance of resources (e.g., computing power, network bandwidth, Internet--connected users) we either have presently, or will have in the next 5--10 years. A number of initiatives are being undertaken as a deliberate strategy of risk mitigation; this is a necessary consideration because what I am proposing is higher-risk work and not just incremental research.</span></p><p><span style=""line-height: 1.5em;""></span>    One novelty here is in identifying and challenging deeply- engrained assumptions related to security, e.g., anti--virus software must work a certain way, a solution must work on x86/ARM/Windows/Mac..., computer programming must be done a certain way. Proffering self-same defenses is simply not working, and effort needs to be expended on seeking revolutionary approaches to security rather than incrementally less-bad ones. The security of our computers must be improved, and leading the way with new approaches instead of following others is a good investment for Canada to make.<br /><br /></p>""571201,""Ayers, Paul"
"590855"	"Bagheri, Ebrahim"	"Dynamic Runtime Software Architecture Adaptation"	"<p>There has been a paradigm shift in the way data is constantly produced, procured and analyzed specially with the prevalence of social computing infrastructure. It has already been reported that 90% of all the data in the world has been generated over the last two years. Predictions indicate that the data growth rate itself will be exponential in the next few years. The unprecedented growth of data volumes has introduced exciting new challenges for the software platforms that deal with such voluminous amounts of data. </p><p>From a software development perspective, software applications need to exhibit desirable characteristics such as scalability, elasticity, and reliability in order to meet the minimum requirements associated with data volume and data growth rates. While the community has acknowledged the pressing need for innovative software platforms, a growing number of solutions do not address issues of scalability, elasticity and reliability at the software architecture layer, offloading these important considerations onto the data management layer through techniques such as data partitioning, data replication and distribution models such as MapReduce; therefore, entangling issues of data management with application scalability and elasticity. </p><p>This proposed research program on runtime reconfiguration and architectural adaptation of software platforms will develop architectural models, algorithms, techniques and tools that will efficiently handle and address challenges associated with the novel characteristics of data such as volume, variety, velocity and veracity. The main objective will be to develop dynamic runtime reconfigurable and adaptive software architectural models that would automatically react and adapt to varying data production and consumption patterns.</p><p>The concrete objectives of our work in this research program will be to: a) enable real-time architecture adaptation by developing model verification and validation techniques that automatically evaluate architectural alternatives without prior rule setting; b) provide scalability and elasticity by performing analysis on sampled low-overhead application profiling data that would point to architecture-level application partitioning or merging requirements; c) support runtime cost-efficient rollback of software architecture into already profiled and stable architecture checkpoints in order to proactively prevent or reactively resolve an undesirable quality or functional state; and d) enable predictive adaptation reliability estimation through modeling historical behavior of software applications as they correlate with measurable software architecture metrics.</p><p>The proposed program provides a solid foundation for HQP training including 2 PhD and 4 MASc students who will gain expertise in state-of-the-art software engineering, self-adaptive systems, and big data applications. </p>""577651,""Bagheri, Pooya"
"578453"	"Baillargeon, Sophie"	"Québec"	"CANADA"
"590463"	"Bajcsy, Jan"	"Reliable Information Transmission Techniques for Multi-Terminal Communication"	"<p>This proposed research program focuses on increasing data transmission speeds in future wireless and optical communication systems through the use of advanced coding, modulation and signal processing techniques. The program objectives focus on design and application of communication and signal processing techniques that can better utilize both the existing and emerging communication spectrum. </p><p>As opposed to traditional communication systems, where data are transmitted at so called Nyquist rate which is equal to <span style=""line-height: 1.5em;"">twice the available communication channel bandwidth, this research program focuses on Faster-than-Nyquist communication, where  </span>data are sent over a physical communication channel at rates exceeding the Nyquist rate. The Faster-than-Nyquist transmission approach has an inherent potential to significantly increase the speed of communication, but exceeding the Nyquist rate limit <span style=""line-height: 1.5em;"">leads to very severe </span>inter-symbol <span style=""line-height: 1.5em;"">interference that prevented practical development and utilization of these communication systems until very recently.  </span></p><p>In particular, multi-terminal, multi-user and continuous-time structure of the physical communication channels will be modeled when Faster-than-Nyquist transmission is utilized. The obtained new channel models will be for communication in one-to-many (uplink), many-to-one (downlink) and many-to-many (relay, collaborative and/or multi-antenna) settings, which have not been studied for the Faster-than-Nyquist transmission by others. The obtained channel models will be  analyzed for their channel capacity and interference structure, then exploited in order to achieve order of magnitude improvements in obtained communication bandwidth utilization (spectral efficiency) and user data rates. Furthermore, design of c<span style=""line-height: 1.5em;"">oncatenated error-control codes (e.g., turbo, low-density parity check, Raptor and/or polar), advanced modulation techniques for faster-than-Nyquist transmission, iterative turbo signal processing and multi-antenna/port technology will result in system architectures that can practically achieve envisioned goals of system level improvements.</span></p><p><span style=""line-height: 1.5em;"">The obtained research results and techniques will lead to better communication spectrum utilization and faster communication speeds. This work  and its applications will be of interest and benefit to telecommunication equipment and infrastructure manufacturers in Canada (e.g., Ericsson Canada, Huawei, Ciena, Blackberry) and will also provide benefits to telecom service providers (e.g., Bell Canada, Videotron, Rogers, Telus). Throughout the proposed research program, graduate students (5 PhD and 10 MEng level) and 10 research undergraduate students will be trained, acquiring cutting edge knowledge and research skills in communication systems,  signal processing and coding techniques that will enable them to obtain future employment in industry and academia.</span></p>""590406,""Bajcsy, Michal"
"589802"	"Baranoski, Gladimir"	"Predictive Simulation and Rendering of Material Appearance Changes Induced by Invisible Light"	"<p>Computer graphics remains one of the most pervasive areas of computer science, with applications spanning from the production of movies for educational and entertainment purposes to the visualization of scientific and medical data. The pursuit of realism has been a driving force for innovative research developments in this area. In order to generate images with a high degree of realism, it is necessary to accurately simulate light and matter interactions that affect material appearance attributes such as colour, glossiness and translucency.<br /><br />Realistic rendering applications are largely centered at the visible domain, with the resulting images usually depicting materials in their typical original state. However, real materials are subject to prominent changes in their visible appearance attributes, notably due to exposure to invisible light. For example, natural processes affecting the visible appearance of human skin, such as tanning and freckling, can be induced by ultraviolet light, which can also trigger the emission of visible light (fluorescence and phosphorescence phenomena) by man-made and naturally occurring materials such as minerals and chlorophyll.<br /><br />The main goal of this research program is to advance the state of the art in the modeling of material appearance through the predictive simulation of natural processes triggered by exposure to ultraviolet and infrared light. Although it will be grounded in the computer graphics field, the proposed work will have a substantial interdisciplinary component associated with the evaluation and dissemination of its outcomes. In order to be predictive and reliable, simulations of light and matter interactions need to provide physically correct results. Accordingly, we will evaluate our simulation results through comparisons with actual measured data. We will also make our simulation algorithms and supporting biophysical data available online to enable the reproduction and expansion of our findings.<br /><br />Due to the scientific importance of material appearance changes caused by exposure to invisible light, this research program will lead to relevant contributions not only to computer graphics, but also to a wide range of disciplines, from applied optics to remote sensing. By having predictability and reliability as its main guidelines, it will provide reproducible outcomes that one can build on efficiently in order to tackle open questions in these disciplines and practical problems affecting the quality of life worldwide. Examples of such applications include, but are not limited to, the design of new technologies for the automatic screening of severe medical conditions, the development of effective procedures for the early detection of food quality problems, and the formulation of accurate methods for the remote assessment of soil contamination and crop yield. Hence, it will benefit both researchers and the public at large.</p>""588446,""Baranova, Elena"
"589965"	"Barlage, Douglas"	"Wide Band Gap Semiconductors for Flexible Power Applications"	"<p>This discovery grant proposal describes a comprehensive research and education program of wide-band gap semiconductors for power applications in diversified mediums. Two semiconductors of focus are gallium nitride (GaN) and zinc oxide (ZnO), both which offer very distinct advantages over silicon (Si) over their respective application space. The development and commercialization of these material systems are the underlying motivation for this work.  The GaN device offers performance advantages on traditional single crystal substrates that currently cannot be realized with Si or silicon carbide (SiC).  The envisioned goal is seeing the discrete MOSFET component to its completion for eventual implementation in large-scale power systems. The immediate advantages of GaN  is the potential higher speed operation.  Power management systems operating at higher frequency allow the use of smaller passive components with higher efficiency thus leading to a positive environmental impact.   <br />The ZnO device does not offer the raw performance advantage of the GaN device because of the lower inherent mobility.  However, its potential application space is much wider due to its strong transistor performance with locally deposited semiconductors. We first achieved this using a source-gated FET (SGT) with comparable performance to state-of-the-art and commercially available indium gallium zinc oxide (IGZO) thin-film transistors (TFTs).  Recently, we also built a novel transistor architecture that can be described fully as a ""thin-film bipolar tunneling emitter junction transistor"" (TJT). The first iteration of the TJT fully built below 150 °C outperforms all known TFTs with respect to power management. The ZnO approachs are currently being tested for integration into biomedical applications.  This benchmark achievement, will enable high current and power applications for flexible and printable devices previously not possible.  <br />Students are held to one common theme that enables exploration into a broad variety of applications within this multidisciplinary program.   Students participate in theoretical and fabricated device design with the aid of industry standard design and simulation packages.   Within the successful program proposed here, theory is used alongside measurements of fabricated devices to determine the impact of new material systems on electronic systems.  Currently, as part of this program we will continue to explore a number of circuits in power (GaN) and biomedical (ZnO) applications with this unique device architecture. Further research enables a wide variety of applications in the area of energy harvesting.  The approach mimics the process in high-tech manufacturing development.  In this process, students are immersed in solid state devices, nano physics, material science and advanced circuit design principles to address the needs of today's high-tech engineering market place.  </p>""577269,""Barlow, Martin"
"590301"	"Barrière, Caroline"	"Analyse d'expressions multi-mots dans le contexte du Web sémantique"	"Le Web sémantique vise l'échange d'information structurée, mais n'impose qu'un standard syntaxique (le format RDF - Resource Description Framework) et non un standard sémantique, même s'il promeut l'adhérence à des ontologies de haut niveau telles SKOS (Simple Knowledge Organization System) ou FOAF (Friend-of-a-Friend). Il existe donc d'énormes problèmes d'hétérogénéité sémantique entre les diverses bases de connaissances (RDF datastores) du Web sémantique, empêchant cet échange d'information.<br /><br />Aussi, le Web contient une quantité importante d'informations dans des textes non structurés qui seraient d'autant mieux caractérisés et utilisables s'ils étaient indexés sémantiquement à l'aide des instances et concepts définis dans les bases de connaissances (BCs). Le problème d'hétérogénéité s'accentue, car ces textes utiliseront probablement des expressions différentes de celles mises comme étiquettes dans les BCs. Les étiquettes, tout comme les références textuelles, peuvent être des mots simples, ou des expressions multi-mots. C'est à ce phénomène, soit les expressions multi-mots, que nous nous intéressons particulièrement. Par expression multi-mots, nous référons aux entités nommées (tel « New York Stock Exchange »), aux termes spécialisés (tel « data acquisition system »), ainsi qu'aux syntagmes nominaux complexes (tel « rising crude oil price »).<br /><br />Les problématiques sous-jacentes à une meilleure compréhension de ces expressions sont : (1) les variations dans les modes de représentation dans diverses ressources (terminologiques, lexicales, BCs), (2) les défis de mise en commun des ressources, (3) la compositionalité régissant la construction d'expressions multi-mots complexes à partir de mots de base et (4) l'utilisation de formes variées de ces expressions en corpus.<br /><br />Le présent programme vise à développer des méthodes d'analyse, de compréhension et de représentation d'expressions multi-mots dans le but de faciliter la mise en commun d'information textuelle non structurée et d'information provenant de ressources, en particulier des BCs du Web sémantique. L'étude des expressions multi-mots est une avenue de recherche à la frontière entre le traitement automatique des langues, la sémantique, la linguistique, la terminologie et la représentation des connaissances. Ayant ouvré dans ces domaines au cours de ma carrière de recherche, le présent programme permettra une centralisation de l'expérience acquise antérieurement vers un enjeu ciblé.""583924,""Barron, Boris"
"591001"	"Bauer, Michael"	"Policy-based Autonomic Management in Data Centers"	"As data centers and systems grow, become more heterogeneous, support increasingly diverse applications and computing environments, system administrators face daunting challenges in managing the systems and applications in order to meet operational requirements, e.g., as embodied in service level agreements (SLAs). Concomitant with these challenges, data center operators face increasing costs of power for both computing and environmental systems as well as increasing environmental legislation.  Consequently, operators must try to optimize several competing objectives: minimize power consumption, reduce emissions/carbon footprint, manage thermal output, meet performance and availability constraints, ensure security, etc. Addressing these challenges is multifaceted, requiring: novel hardware approaches, improved software, conservation strategies, and increased automation of data center management.<br /><br />The proposed research focuses on the development and evaluation of automated methods for data center management. ""Automated"" in this context means that there are one or more autonomous or semi-autonomous computational agents that control elements and behavior in the data center and relieve or augment human administrators in meeting or managing tradeoffs among the objectives of the operator.  The long term goal of the research is to identify, develop and evaluate automated approaches for the management of data centers or collections of systems. In particular, we will investigate the use of policy-based autonomic management as a means of automating the management of data centers comprised of multiple heterogeneous systems. The data center, then is comprised of many different elements, such as, processors (cores), compute nodes, network devices, systems, applications, etc., that require different means of control and require management actions at different time scales, e.g. in microseconds for a processor to hours for an entire computing system.  It is unlikely then that a single manager would suffice or even be realistic.  Therefore, automated approaches at this scale must consider multiple managers, their interactions and how they coordinate.   The research will also evaluate and assess various energy aware techniques that have been proposed for data centers and systems, measure the impact and overhead of alternatives, evaluate the trade-offs of different management strategies and techniques, and propose new, novel methods.<br /><br />Data centers are predicted to consume 8% of the worldwide electricity supply and contribute 2.6% of the global carbon output by 2020. We expect the proposed research to have a direct impact on future management and operations of data centers. Given Canada's focus on a digital economy, adoption of these approaches are central to cost efficient data centers in Canada and can help make Canada a leader in data center expertise and technology.""590014,""Bauer, Robert"
"589438"	"Baysal, Olga"	"Leveraging software analytics to maximize developer productivity during software maintenance."	"<p>Software practitioners make technical and business decisions based on the understanding they have of their software systems. This understanding is grounded in their own experiences, but can be augmented by studying various kinds of development artifacts, including source code, bug reports, version control meta-data, test cases, usage logs, etc. Unfortunately, the information contained in these artifacts is typically not organized in the way that is immediately useful to developers' everyday decision making needs. To handle the large volumes of data, many practitioners and researchers have turned to analytics.</p><p>This research program aims to explore how we can systematically uncover the benefits of software analytics in supporting software developers' daily decision-making and improving their awareness and effectiveness when they build and maintain software systems. This program will investigate techniques for gathering, preprocessing, transforming and modelling the data from various development artifacts (bug reports, commit history, source code, maintenance tasks, test suits, developer discussions, etc.) with the purpose of highlighting useful information and drawing conclusions from it.</p><p>This project is a part of of a long-term research agenda with the goal of improving the understanding of how developers create, evolve and maintain software systems, and investigating how the practice of software maintenance can be improved.<br />Software is made by people; and thus, we believe that by better understanding how people create, use and reason about software systems, we can enhance their effectiveness as they develop software. By making tools and development environments more analytics-based, we can improve individual performance and productivity. </p><p> </p>""592521,""Bazalova, Magdalena"
"589931"	"BenDavid, Shai"	"Utilizing unlabeled data for machine learning tasks - theoretical analysis"	"<p>Mainstream machine learning tools depend on the availability of human annotated training data. Nowaday, in what is called the ``big data"" era, applications of machine learning have access to very large amounts of un-annotated (a.k.a. unlabeled) data. Consequently, there is vast interest in designing machine learning tools that can utilize such big pools of raw, unannotated data to reduce the need for human intervention in the learning process. Various recently arising machine learning paradigms address this issue. These include Clustering, Active Learning, Semi-Supervised Learning, Domain Adaptation and Transfer Learning, as well as Learning from ``Weak Teachers"" (like supervision obtained via crowdsourcing). <span style=""line-height: 1.5em;"">To cope with such scenarios, learning practitioners have developed heuristics that, while apparently working reasonably well in practice, are not supported by existing mathematical analyses. </span></p><p><span style=""line-height: 1.5em;"">In the past couple of decades, machine learning provided a resounding demonstration of the impact of theoretical analysis on the development of practical applications. Algorithmic paradigms like Support-Vector-Machines, Decision-Trees and Boosting grew from theoretical models into popular and vastly applicable software packages. Can the success of theoretical analysis of machine learning be extended to the modern big-data-minimal-human-intervention scenarios? The proposed research aims to provide basis for such developments by building mathematical support for those emerging machine learning and data mining paradigms. </span></p><p><span style=""line-height: 1.5em;""> Some examples of recent initiatives taken by my team in that direction include a program that aims to provide tools for guiding users that wish to cluster big data sets on how to choose appropriate clustering algorithms and parameter settings. Such choices are critical to the success of clustering applications, and yet have so far been done in an ad hoc fashion. The PhD thesis of my student Margareta Ackerman took first steps in this direction </span><span style=""line-height: 1.5em; font-size: 12.7272720336914px;"">but there are still big challenges to overcome, both in terms of developing such tools, and in terms of raising the awareness of the data mining community to the significance of the matching between clustering tasks and the algorithms employed to address them. Another project addresses the task of utilizing ``weak supervision"".  The use of annotation by novice supervisors to help collect training data for classification prediction tasks has been drawing research attention recently due to the growing popularity of using crowdsourcing. In contrast with much of the current research in this direction, we consider the scenario in which the weak supervision is used in cooperation with human supervision, aiming to reduce  (rather than eliminate) calls to the expert. We are developing novel mathematical models to zoom in on the instances for which novice-generated labels cannot be trusted and need to be scrutinized by a human expert.</span></p>""589814,""Bendayan, Reina"
"589635"	"Benkoczi, Robert"	"Discrete optimization with applications in wireless networks"	"Discrete optimization is essential in telecommunication. The ability of a network to serve its demand depends on how well network operators deploy and manage their resources. Facility location problems are of particular interest as they are concerned with the placement of facilities to optimize an objective function such as providing equitable service to clients, minimizing the transportation cost, etc. In wireless networks, facilities are the nodes that form the network backbone and the objective function represents qualitative network parameters.<br /><br />This proposal aims at advancing the research on discrete optimization and facility location to support the development of cutting edge wireless technologies. One such area of application is the design of Cognitive Wireless Networks (CWN). CWN are characterized by dense and overlapping network deployments that co-operate to use the scarce radio spectrum more efficiently. For example, WiFi deployments and cellular networks could co-operate to deliver the data packets for a telephone conversation in the best and most efficient way possible. As the nodes themselves must decide on the best strategy for co-operation (the nodes are ""cognitive""), advances in discrete optimization algorithms for problems modeling the physical characteristics of wireless networks, become indispensable.<br /><br />Three concrete research areas are identified.<br /><br />1) Algorithms for weighted colouring (WCP) - investigating the impact of interference and channel assignment in time division multiple access (TDMA) networks: a common approach to modellng interference in wireless networks uses network flow problems in communication graphs with edge costs and capacities. Although extensively used in routing protocols, network flow problems are not as expressive as WCP when modelling one of the most precious resources in TDMA networks, time. There is a surprisingly small number of different strategies to solve WCP.This proposal identifies a series of algorithms that target special graph topologies and employ powerful techniques such as linear and integer programming. Dealing with uncertainty is a third aspect overlooked, so far, in the literature that this proposal addresses (see also point 3 below).<br /><br />2) Covering with variable capacity is a new problem that emerged from my earlier work on multi-hop cellular networks concerning code division multiple access networks. The proposal contains several directions that expand my current research, including specialized algorithms for restricted network topologies and using more general radio interference models.<br /><br />3) Robust facility location deals with problems with uncertain input, a common occurrence in networks. At present, very little is known about robust problems with multiple facilities. I propose to begin the research on multiple facility location problems by considering the 2 facility case with different objective functions.<br />""591856,""Benmokrane, Brahim"
"590899"	"Boone, François"	"British Columbia"	"CANADA"
"590133"	"Bremner, David"	"Geometric aspects of optimization"	"<p><strong>Overview</strong></p><p>Geometry is an important tool for efficient optimization algorithms; conversely geometric flavoured optimization problems are common in applications ranging from manufacturing to machine learning and statistics.  These two facts motivate my research focus on algorithms for high dimensional geometric objects, including point sets, hyperplane arrangements, and especially <em>convex polyhedra</em>.  Convex polyhedra (or just <em>polyhedra</em>), natural generalizations of the classical Platonic and Archimedian solids, are the fundamental mathematical objects in linear constrained optimization, and widely used as approximations in more general optimization problems.</p><p><strong>Geometry in optimization</strong></p><p>Many polyhedra arising in optimization exhibit a high degree of symmetry.  Recent solvers are ableto factor out certain simple kinds of symmetry; a natural question is how to exploit the more generalsymmetries known to occur.  For industrial branch-and-bound solvers this requires extremely fastmethods for testing these more general equivalences.  Generalizing recent <em>core set</em>algorithms requires understanding how the structure of the candidate solutions (the <em>core set</em>)changes for more general symmetries.</p><p>The study of <em>extended formulations</em> starts from the observation that the obvious integer programming formulation of many problems results in an exponential sized polyhedron, while many of these problems have a simple <em>extended formulation</em> of polynomial size that projects onto the obvious formulation.  The recent discovery of impossibility of such a projection for Edmonds' matching polytope motivates our more general approaches to extended formulations that relax the projection requirement while still representing the entire computation in the polyhedron.</p><p><strong>Geometric optimization problems</strong><br /><br />Certain <em>classifier training problems</em> can be interpreted as finding the maximum norm point in a convex polyhedron.  The general norm maximization problem is known to be extremely difficult; for this reason I plan to focus on certain features of the polyhedra that arise in classification algorithms, and on developing methods for <em>rounding</em> these polyhedra to easier ones.</p><p>Various <em>data depth</em> measures have been developed by statisticians to measure the centrality of a point (measurement vector) with respect to a data cloud.  Unfortunately many of the depth measures with the most pleasing statistical properties are difficult to compute.  On the other hand, measures such as lens depth depend on only a small number of data points in any dimension, and are thus relatively trivial to compute.  I plan to investigate to what extent the easy to compute depth measures can be used to approximate the difficult ones, either analytically or as a bounding mechanism in some kind of exact algorithm.</p>""569397,""Bremner, Murray"
"590407"	"Brzozowski, Janusz"	"Complexity Measures for Regular Languages"	"<p>Complexity is an important issue in Computer Science: How much time will it take to solve a problem, how much memory space? One would prefer a precise answer, but that is difficult to get. Lower and upper bounds are useful, and lower bounds are often difficult to find. I work on the complexity of problems in the simplest, most common formal model of a deterministic finite automaton (DFA), using the number of states of the DFA as a complexity measure. I study the complexity of operations on DFAs; this gives a lower bound on the time and space complexities of operations.</p><p><em>A DFA </em><em>M</em><em> </em>has a finite number of <em>states </em>and <em>inputs; </em> the set of inputs is the<em> alphabet</em> <em>A</em> of <em>M</em>. A state is a value in a memory cell and an input, a <em>letter of A,</em> is an action by a user of the DFA. When a letter is applied to a DFA <em>M</em> in its present state, <em>M</em> moves to a next state. A sequence of letters is a <em>word</em>, and a set of words is a <em>language</em>. A DFA has an <em>initial state</em>, and a finite number of <em>outputs</em>; one output is associated with each state. The simplest model has only two outputs, ""yes"" and ""no"". If a word <em>w</em> is applied to a DFA in its initial state, the DFA reaches some state <em>q</em> at the end of<em> w</em>. If <em>q</em> has output ""yes"", <em>w</em> is <em>accepted</em>; otherwise it is <em>rejected</em>. The set of all words accepted by a DFA <em>M</em> is the <em>language L(M)</em> of <em>M</em>. Each DFA performs a specific task of accepting only a certain language.</p><p>For two different tasks, DFAs<em> M<sub>1</sub></em> and <em>M<sub>2</sub></em>, it may be necessary to satisfy the conditions of both tasks; then we need a DFA that accepts the <em>intersection</em> of <em>L(M<sub>1</sub>)</em> and <em>L(M<sub>2</sub>)</em>. If we need to satisfy either<em> M<sub>1</sub></em> or <em>M<sub>2</sub></em>, we need the <em>union</em> of <em>L(M<sub>1</sub>)</em> and <em>L(M<sub>2</sub>)</em>. If we first have to satisfy <em>M<sub>1</sub></em> and then <em>M<sub>2</sub></em>, we need the <em>concatenation</em> of <em>L(M<sub>1</sub>) </em>and<em> L(M</em><sub>2</sub>), which consist of words composed of a word of <em>L(M<sub>1</sub>) </em>followed by a word of <em>L(M<sub>2</sub>).</em> The language of words constructed by concatenating any number of words of <em>L(M<sub>1</sub>)</em>  is called the <em>star</em> of the language. In some cases it is useful to store the words of a language in the reverse order; then we need <em>reversal</em>.</p><p>A language <em></em> is <em>regular</em> if it can be constructed from finite languages by using only union, concatenation and star; equivalently, if and only if it can be accepted by a DFA. The<em> state complexity </em>of a regular language is the number of states in the minimal DFA accepting that language. The state complexity of an operation is the maximal state complexity of the language resulting from the operation expressed as a function of the state complexities of the arguments. I study state complexity and related measures of complexity of regular languages.  Operations with more than one basic operation, e.g., the union of a language with the reverse of another language, are called <em>combined operations</em>. </p><p>I hope to give implementers of operations on finite automata and regular languages not only a large catalog of known results, but also a theory -- firmly based on the theories of semigroups and groups -- which should permit them to more easily estimate the complexity of combined operations in the class of regular languages or one of its subclasses. </p><p> </p><p> </p><p> </p>""576028,""Brzozowski, Matthew"
"590944"	"Bui, Tien"	"Applications of Sparse Representation, Low Rank Approximation and Dictionary Learning to Image Processing and Pattern Recognition"	"<p>      Many problems in image processing and pattern recognition can be modeled by using an energy minimization framework in the context of the variational principle. On the other hand there are other approaches to image processing based on partial differential equations (PDEs) and on the theory of multi-resolution analysis. In the last few years, there has been tremendous progress in the development of the multi-resolution analysis, wavelet transforms, directional filtering and sparse representation. Both the energy minimization and the multi-resolution theories have tremendous successes in image processing and related areas. However, there has been little work on the combination of these theories in a mathematically coherent manner. We plan to study models that can take into account both approaches by forming energy functional that contains the texture component represented by filters, and the cartoon-like components represented the boundary curves. Innovative combination of these models with machine learning methods will lead to new powerful techniques for image processing, pattern recognition and computer vision.</p><p>     Recent research in harmonic analysis and mammalian vision systems has revealed that over-complete and sparse representations play an important role in visual information processing. Sparse representation and related topics such as compressive sensing, low rank approximation, and dictionary learning are very appealing to many problems dealing with big data, for example in uncontrolled face recognition problem. We plan to investigate the use of these methods in information retrieval in images, video, audio and text. Investigation of a sparse representation framework for the detection of unconstrained handwritten keywords in digital documents is a novel problem. In medical imaging, while compressive sensing has been successfully applied to MRI, other biomedical applications could also benefit from these theories. In soft biometrics, face recognition under various uncontrolled conditions such as illumination, pose, expression, and occlusion is a most challenging problem. Finally, filling in missing information in digital images known as in-painting is an area that could benefit from this research particularly when the in-painting problem is in medical or biometric images.</p><p>     Our work is expected to have applications in many areas including document processing, biometrics, medical imaging, computer vision and others. The significance of this work is in new, efficient and intelligent methods for pattern recognition and image processing.</p>""591038,""Bui, Tuan"
"589932"	"Bulatov, Andrei"	"The complexity of the constraint satisfaction problem and its variants"	"<p>The constraint satisfaction problem (CSP) provides a general framework in which it is possible to express, in a natural way, a wide variety of problems. The aim in a CSP is to find an assignment of values to a given set of variables, subject to constraints on the values which can be simultaneously assigned to certain specified subsets of variables; in the counting constraint satisfaction problem the objective is to find the number of such assignments. The CSP is therefore very important in a number of applications, both theoretical and practical. In theoretical applications we look for new algorithmic ideas that can help to solve problems that could not be solved before, to improve the existing algorithms, to design more general and uniform algorithms, and to find evidence that certain problems are not efficiently solvable in the general case. In practical applications, constraint and satisfiability solvers are now standard tools in a wide range of areas from scheduling to hardware verification. Advances in the study of the CSP will speed up such solvers and expand their area of applicability.</p><p> This project will contribute in both aspects of constraint problems, although its main focus is on the theoretical side. One of its main goals is to precisely understand which CSPs can be solved efficiently and design an efficient solution algorithm. This is one of the long standing open problems in computer science, but there is evidence that we may be approaching its resolution. Another component of the project is the study of counting constraint satisfaction problems. This area has unlikely connections to statistical physics, where such numbers affect the properties of large ensembles of particle, and, in particular,  help to locate phase transition thresholds, at which, for instance, liquid turns to gas. Finally, this project proposes to study the efficiency and to design new algorithms in the framework that is closer to practical circumstances than the standard worst case approach. It is known that practical problems tend to have a certain property called the power law distribution, and we plan to mathematically study the behaviour of various algorithms on instance of the CSP satisfying this property.</p>""568948,""Bulgak, Akif"
"590271"	"Cai, Lin"	"Cooperative and self-organizing wireless networks for machine-to-machine communications"	"<p>The growth of the Internet will accelerate in the following decades thanks to the demand of machine-to-machine (M2M) communications. Wireless networking technologies are a key enabler for future M2M networks. There are still several critical challenges of the growth of M2M wireless networks, including the scalability, reliability, complexity and efficiency concerns given the ever-increasing demand, limited wireless resources, limited energy and computing power of wireless devices, and unsustainable energy costs. <br /><br />To tackle these challenges, the proposed research is aiming to exploit new cooperative networking opportunities both horizontally and vertically, and develop scalable, flexible, reliable, efficient, and robust network solutions with self-organizing network (SON) functionalities for large-scale, high-density M2M wireless networks. With SON functionalities, wireless devices can automatically recognize the network environment, exchange information with other devices, adjust and optimize their protocol configurations and parameters to ensure coverage, service quality, and energy and spectrum efficiency in a cooperative fashion. Horizontal cooperation is among multiple communicating peers and vertical cooperation refers to a cross-layer approach to designing and optimizing reconfigurable protocols in various layers of the network protocol stack. The SON architecture facilitates advanced cooperative networking solutions, and the combination of them will lead to a new paradigm of  how future wireless networks will be built, managed and used.  <br /><br />Specifically, we will investigate and develop new solutions in (a) cooperative and topology-aware modulation and physical-layer network coding supporting self-optimization, (b)  cooperative and adaptive topology control and routing supporting self-organization, and (c) reconfigurable medium access control (MAC) for high-density, cooperative M2M networks. During the research, in addition to the widely-employed queueing theory and stochastic optimization tools, we will leverage and further develop analytical tools in stochastic geometry, percolation theory, modern control and optimization. Experiments will be conducted using a combination of simulation, emulation, and prototyping testbeds to validate the analysis and investigate the feasibility and performance of the proposed solutions.<br /><br />The proposed multi-disciplinary research will provide a rich environment for the students and researchers involved to learn and further create new knowledge and networking research tools that will be useful by other researchers and engineers. The proposed cooperative and self-organizing networking solutions will contribute to the success of future machine-to-machine networks that can underpin many new services and applications, and thus improve the competitive edge of the Canadian economy.</p>""579293,""Cai, Lin"
"590861"	"Cai, Yang"	"Assistant Professor"	"<p>Recent years have seen a convergence of ideas and research goals between Computer Science and Economics. Computer Science enabled the development of a new genre of computational platforms that require economic as well as engineering thinking for their proper design and study; primary examples are those systems created and enabled by the Internet. Economics brought to the discussion quantitative models and tools useful to analyze these systems. At the same time, it became clear that the computational nature of these models and tools is crucial for them to be used to study systems with thousands or even millions of interacting individuals. Motivated by this realization, computer scientists have taken in the past decade a fresh, computational look at Game Theory and Economics. One major focus is Algorithmic Mechanism Design, in which we seek for computational efficient systems that are so cleverly designed that users' selfish behavior helps advance the designer's objectives. The goal of this proposal is to enrich Algorithmic Mechanism Design from two directions that - to the PI's opinion - has not been sufficiently explored in the literature. We aim (i) to understand how the complexity of an auction affects its performance, and (ii) to apply Algorithmic Mechanism Design to systems beyond auctions.</p><br /><p>Algorithmic Mechanism Design has mainly focused on optimal auction design for various objectives so far. However, the optimal auction is usually complicated and thus hard to implement in practice. The first half of this proposal aims to understand how the complexity of an auction affects its performance, and propose new simple practical auctions with provable performance guarantees. I will employ tools from Theoretical Computer Science and Applied Probability (1) to search for simple and (nearly-) optimal auctions in certain basic and fundamental multi-item multi-bidder settings, (2) to identify settings where the optimal auctions have simple formats, and (3) to study the computational complexity of finding a Bayes-Nash equilibrium in simple non-truthful auction games for the combinatorial auction problem. The second part of this proposal aims to expand the applicability of Algorithmic Mechanism Design to settings beyond auctions. I will import concepts, ideas and techniques from Economics (1) to design mechanisms for data science - incentivizing workers to provide high quality data for statistical estimation with low cost, and (2) to design computational efficient mechanism for fair resource allocation. </p>""576625,""Caillié, Brice"
"590841"	"Caloz, Christophe"	"Millimeter-wave and Terahertz Real-Time and Space Processing based on Metamaterial Technology"	"<p>Today's exploding demand for faster, more reliable, and ubiquitous radio systems in communications, instrumentation, radar, imaging and sensors poses unprecedented challenges in microwave (mw), millimeter-wave (mmw) and terahertz (THz) engineering. Recently, the predominant trend has been to place increasing emphasis on digital signal processing (DSP). However, while offering device compactness and processing flexibility, DSP suffers from intrinsic drawbacks, such as limited speed, high-cost analog-digital conversion, high power consumption, and poor performance at high frequencies, leading to serious technological bottlenecks. For instance, in wireless communications, a 1,000-fold increase in data capacity demand over the 5 next years has been predicted, and it is widely recognized by industrial experts that current technologies will be largely insufficient to meet such requirements! Therefore, novel paradigms and disruptive technological solutions are urgently needed.<br />The proposed research program proposes a radically different approach to address the aforementioned challenges by leveraging metamaterial technology. Inspired from ultrafast optics, it will develop mmw and THz real-time analog signal processing, consisting in manipulating high-frequency signals in their pristine analog form and in real time to realize specific processing operations, as an alternative to DSP-based processing.  This is essentially temporal dispersion engineering, where linear components such as metamaterial-based ""phasers"" (term borrowed from acoustics) and nonlinear components called ""companders"" (portmanteau for compressors and expandors) will be elaborated to control the phase and group delay of mww/THz systems in unprecedented fashions. <br />As electromagnetic waves propagate both in time and space, spatial dispersion engineering appears to be a natural processing complement to its temporal counterpart. The proposed program will thus develop, this time inspired from optical analog signal processing, novel types of mmw and THz antennas and surfennas (metasurfaces operated in the monochromatic regime), such as for instance vortex wave antennas and generalized refraction surfennas. Finally, the program will investigate devices operating simultaneously as temporal and spatial processors, namely ""phasennas,"" ""compandenas"" and ""surfphasennas.""<br />The proposed time/space/spacetime processors will lead to a great diversity of applications, such as real-time/space Fourier transformers and spectrogram analyzers, real-time convolvers/correlators, nonreciprocal devices, dispersion encoders, vortex multiplexers, maxwellian spatial systems, and space-time cloaks, with application areas as various as communications, wireless energy transfer, sensing/imaging, instrumentation, security/defense, biomedicine, nanoelectromagnetics and astronomy.</p>""576642,""Calvé, Thierry"
"577682"	"Card, Dallas"	"Ontario"	"CANADA"
"589816"	"Chan, Sonny"	"Visual and interactive computing for surgery and medicine"	"<p>In this data-rich era, there is much untapped potential to use computing to help us interpret and understand our observations of the physical world. In the domain of medicine, we now have an unprecedented ability to image the human body and generate repositories of rich, three-dimensional image data. Despite advances in imaging technologies, most physicians today still examine images as if they were on a sheet of film, severely limiting the knowledge and understanding they can efficiently extract from the data. The central theme of the proposed research program is the investigation of visual and interactive computing techniques that will improve the way clinicians, scientists, and trainees are able to understand, interpret, and utilize medical images.</p><p>The proposed research aims to advance knowledge in computer science which will enable creation of interactive virtual environments to permit exploration and touch-based manipulation of realistic, three-dimensional models derived from medical images. Drawing a motivating example from the application of surgical planning, imagine if an accurate and realistic virtual model of a patient could be constructed from pre-operative image data. What if the surgeon could interactively manipulate the model to reveal pertinent anatomical structures and their spatial relationships in 3D? What if the surgeon could use a familiar instrument to perform a virtual dissection, and feel contact and resistive forces during the operation while the anatomical model responds in a physically realistic manner?</p><p>To achieve these capabilities, scientific challenges in a number of core areas of computer science must be addressed. These include (1) image analysis and geometric modelling, (2) real-time volume rendering and visualization, (3) haptic rendering and touch-based interaction, (4) collision detection and physically-based simulation, and (5) design and evaluation of interactive virtual environments. Research along the five stated directions forms the basis of investigation for the proposed research program.</p><p><span style=""line-height: 1.5em;"">Progress in each research direction would contribute to scientific knowledge in that field, and that knowledge can have applications in a variety of disciplines. Advances in all focus areas, in concert, would have the greatest significance in realizing the vision of enhancing medical education, diagnosis, treatment planning, and computer-assisted intervention through use of visual and interactive computing. Ongoing collaborations with practicing clinicians grant us access to real-world data, and at the same time allows us to validate the correctness and utility of the methods developed. The research problems to be addressed are driven by real needs, ensuring that results would benefit applications with practical value, and potentially increasing the likelihood of technology translation or commercialization.</span></p>""570354,""Chan, Timothy"
"591284"	"Chapman, Glenn"	"Digital Imager Enhancement and Investigating Grayscale Photomasks Technologies for Fabrication of Micro-optics"	"New microfabrication techniques have had a dramatic impact on society, by transforming microchips into digital imaging sensors and micro-optics. From scientific instrumentation to commercial photography, imaging sensors are now used to integrate sensors with micro-optics and control electronics onto single chips. This proposed research combines the two areas of opto-electronics (the combination of electronics and optics) and nanofabrication, the technique used to build the micro-optics.<br /><br />1) In opto-electronics my research group focuses on digital pixel arrays in cameras. Imagers develop a growing number of defects over time that degrades image quality and reduces the sensor life. Using commercial camera images, and software analysis algorithms, we study defect development to identify faulty pixels characteristics, and pinpoint the causal mechanism. Research will improve developed formulas that show the trend towards larger sensor area, higher sensitivity and shrinking pixel size creates exponential growth in both defect strength, and development rates, which can reach 1000's of defects per year. We will investigate applications such as novel ways to improve image repair based on software-correcting pixels that use knowledge about the defect parameters, creating parameters to give better metrics for choosing cameras in high radiation environments like the Space Station, and using these defect patterns and parameters to create a camera-unique ""fingerprint"" for forensic identification in security and criminal investigations.<br /><br />2) In nanofabrication, we investigate the fabrication of grayscale photomasks, which are a technology critical to cost-effective 3D structures. Existing commercial grayscale masks are either very limited in transparency control or accurate but highly expensive. We are developing bimetallic thin films of Bismuth, Indium and Tin, forming a thermal resist which oxidizes and becomes transparent under laser exposure. With equipment that varies the laser power on these films, direct-write grayscale photomasks will be developed which the control transparency to a high accuracy (256 steps) in a low cost mask. Since mask transparency is only one limiting factor in the vertical control ability of microstructure, we will then combine this new mask with new fabrication methods to transfer these patterns into the creation of very accurate 3D structures for micro-optics. The micro-optic application requires highly precise shapes to create the proper optical images. Our research also intends to apply this method to the creation of micro-optics for applications in improving camera pixels, creating less distorting thin Fresnel lenses, and a thin Gabor lens for heads-up displays. In microfluidic designs, we will create 3D shapes in microchannels with unique fluid mixing properties and integrate micro-optics into fluorescence based lab-on-a-chip systems.""578155,""Chapman, Jacqueline"
"591016"	"Chechik, Marsha"	"Abstraction and Automation for Reasoning about Complex Software"	"<p>     Modern software development is a complex and messy business. Requirements are likely incomplete. New development uses complex existing libraries, tools and components that can fail. Multiple development teams proceed on different schedules, making it difficult to assure that their artifacts will ""talk to each other"" as intended. Every time a change is made, expensive quality assurance needs to be performed to ensure that software still does what it was supposed to. Such problems are made even more dire because many companies, especially in automotive, electronics, aerospace and defense domains, need to maintain families of software product variants with similar yet not identical functionality (e.g., supporting different types of customers and/or software platforms).<br />   I believe that many of the above problems can be aided by the availability of industrially scalable automated reasoning and analysis tools that enable users to answer essential questions about their artifacts. For example, they can determine the impact of a proposed change, ask ""what if"" questions about their designs, determine whether the different features are going to interact, check whether desired properties of a component would hold in any environment, etc. Creating such tools requires expertise both in formal verification and in software engineering, as many of the challenges are in improving usability of the analysis tools for SE practitioners, performing analysis at the ""right"" level of abstraction, enabling analysis earlier in the development lifecycle where complete information is not yet available, reusing results of analysis between product variants, etc.<br />   The goal of this project is <span style=""text-decoration: underline;"">to develop precise, effective and scalable automated techniques</span> to solve a variety of complex software engineering problems. The emphasis will be on <span style=""text-decoration: underline;"">enabling analysis of artifacts with incomplete information, handling product lines, and analysis reuse.</span> I intend to leverage my in-depth knowledge of verification tools such as model-checkers and static analyzers as well as strive to establish the right abstractions to facilitate reasoning. Success of this project will significantly expand the scope of applicability of automated reasoning and analysis in practice.</p>""578248,""Chee, Ryan"
"589766"	"Chen, Lawrence"	"Silicon photonics:  enabling optical and microwave signal processing for broadband communications beyond the next decade"	"<p>Digital technology and the internet have changed the way we interact:  a digital culture embracing video-oriented services for communication, education, entertainment, healthcare, and business has emerged.  The overall mission of this research program is to deliver disruptive technologies to power the fiber optic and optical-wireless telecommunications infrastructure necessary to support this digital culture.  We will achieve our mission through research on two inter-related topics:  Theme 1-silicon photonics for optical signal processing and Theme 2-silicon photonics for microwave signal processing.</p><p>The long term objective of Theme 1 research is to provide innovative solutions for signal processing based on silicon photonics that address issues and challenges associated with the parallel transmission of data signals, i.e., space-division-multiplexing, which is recognized as the only means to increase transmission beyond known capacity limits of single mode fiber and avoid bandwidth exhaust in fiber optic communications.  </p><p>In a parallel effort, the long term objective of Theme 2 is to develop integrated microwave photonic systems in silicon photonics for optical-wireless communications that address the unprecedented increase in consumer demand for wireless gigabit service.  The devices will realize important functionalities in microwave systems that are complex or otherwise impossible to achieve using electronics.</p><p>In the short term, we will design fundamental building blocks in silicon photonics and investigate the performance limitations of the various approaches for optical and microwave signal processing.</p><p>The information and communications technology (ICT) sector has long been central to Canada's economic development and ICT industries have accounted for 7.5% of the Canadian GDP growth since 2007.  The ideas, results, and technologies generated from the proposed research program can be adopted by ICT industries through partnerships, student internships, and collaborations.  The research program will provide excellent training for four students per year in signal processing; optical communications; numerical modelling; device design, fabrication, and testing; and in systems experiments.  They will be in an excellent position to make immediate contributions and have an impact in the ICT sector.  Graduates from the current research program have been placed in relevant industries, including Ciena and MPB Communications.</p>""574536,""Chen, Leanne"
"590393"	"Cheung, Jackie"	"Towards Generalized Natural Language Generation with Distributional Semantics"	"From smartphones to smart cars, robotic assistants and beyond, the common goal of many emerging technologies is to extract useful information about the world in order to give users greater control over their surroundings. The field of natural language processing provides techniques to extract meaning from text, reason with it, and finally produce text and speech in order to relay information back to the user.<br />Recent successes in personal assistant applications and news summarization systems have raised the demand for natural language generation (NLG) systems that can interactively produce feedback to the user. Yet many existing methods only work well on highly restricted domains and tasks, as typified by a GPS navigation system with pre-programmed templates for generating driving directions.<br />My research program aims to produce a generalized computational account of NLG that is sensitive to differences in the topics, language types, and goals of an application. This will require an analysis of the parameter space of NLG systems in order to characterize their diversity, as well as a correspondingly adaptive and expressive semantics that can support the reasoning and inferences that are necessary for NLG.<br />For the former requirement, my research group will develop techniques for NLG that take into account the desired usage of the system in its broader context. For example, a news text summarization system might emphasize factors such as brevity and formality, whereas an interactive educational game might emphasize other factors such as simplicity and interest.<br />For the latter requirement, my research program will investigate distributional semantics (DS), a data-driven approach to modelling meaning that can be trained without human annotation effort. My research will focus on using DS to model the meaning of entities such as Quebec or Android, as well as events such as the Olympic Games of 2016, which is required in order to reason about the important information that should be expressed by NLG.<br />The impact of this research will be to move NLG from being confined to highly restricted and domain-specific scenarios to being integrated with interactive systems in a natural and pervasive manner. The tools, techniques, and framework contributed by this research program will demonstrate that NLG systems that are supported by expressive, data-driven semantics can be developed and tailored for education, entertainment, or business.""585312,""Cheung, Jackie"
"589539"	"Cheung, Teresa"	"Translating large-scale brain imaging technologies into deployable mobile technologies"	"<p>Brain wave technologies such as electroencephalography (EEG) provide an online record of different brain functions. While this low-cost, portable technique has been tremendously useful in recording brain signals for single time point diagnostics at the scalp-level it is difficult to monitor the whole brain because using EEG alone to compute brain activity can be inaccurate. This makes it difficult to distinguish signals from different areas of the brain. At present, analysis and targeting of specific regions of the brain can be accomplished only using expensive, access-limited neuroimaging devices such as functional MRI (fMRI) and magnetoencephalography (MEG) combined with anatomical MRI scans. Even though, EEG and MEG signals originate from the same neurophysiological processes, there are important differences. Magnetic fields are less distorted than electric fields by the skull and scalp, which results in a better spatial resolution of the MEG, allowing signals to be localized with more accuracy. The high cost and lack of portability of fMRI and MEG limits their use in point-of-care settings where portability or frequent imaging can benefit outcomes.</p><p>Our research will develop software that uses the high spatial information from MEG, MRI and fMRI combined with EEG to find methods to extract EEG signals that have better spatial accuracy. These new techniques will enable EEG to be used to monitor the whole brain, which will open up new diagnostic avenues. These rapid imaging technologies have the advantage of reaching more people while providing individualized care. The ability for rapid imaging technologies to provide a reliable monitor of brain function change would provide considerable insight to guiding on-going treatment and rehabilitation of brain injury and related neurological conditions. For example, portable whole brain imaging can help athletes in high-risk-to-concussion sports by providing a means to do baseline testing before injury as well as provide assessments during recovery back to baseline after injury. The development of a novel monitoring device fulfills a growing need for ""online"" brain monitoring and will advance biomedical engineering and signal processing techniques by taking advantage of potential market segments that are not yet serviced.</p>""573532,""Cheung, William"
"590556"	"Christara, Christina"	"Numerical Methods for Partial Differential Equations: Algorithms and Software on Innovative Computer Architectures"	"Partial Differential Equations (PDEs) are the basis of many mathematical models of important physical and technological phenomena.<br />This project involves the development and analysis of numerical methods for PDEs, and the development, testing and evaluation of mathematical software for the solution of PDEs on a variety of computer architectures.<br />Two of the main components of a computational scheme for PDEs are the discretisation technique for the continuous problem and the solution method for the resulting set of discrete algebraic equations.<br />Models of physical phenomena often involve linear elliptic Boundary Value Problems for PDEs, the discretisation of which, in turn, gives rise to large sparse linear systems of algebraic equations.<br />Other models may involve time-dependent PDEs, which often require the solution of large sparse linear systems at each time step of the time-discretized problem.<br />In developing and studying computational methods for solving large-scale PDE problems, two key issues have to be addressed -- namely, the accuracy and the efficiency of the computations.These mainly depend on<br />(i) the convergence properties of the discretisation method;<br />(ii) the computational complexity of the linear solver;<br />(iii) the implementation of the discretisation method and solver; and<br />(iv) the ability to exploit parallelism to a degree proportional to the size of the model.<br />This last factor becomes particularly important when the size of the mathematical model (i.e., the number of discrete equations) is very large.<br />This research includes the following components:<br />(a)<br />Development and analysis of high-order PDE discretisation methods, such as spline collocation methods,<br />and low computational complexity solvers, such as FFT methods, multigrid schemes,<br />domain decomposition techniques and hybrid approaches, with a scalable degree of parallelism.<br />Discretisation methods and solvers are first developed for simple model problems, then extended to handle more difficult problems,<br />such as problems with layers, rough behaviour, ill-conditioning, discontinuities, etc.<br />(b)<br />Implementation and testing of the proposed methods for solving large models on parallel machines with many processors.<br />This includes the performance evaluation of methods and machines for solving PDEs<br />in terms of parallel time and memory complexity, communication complexity (on distributed memory machines),<br />memory access latency (on GPU machines), speedup, utilisation, load balancing and scalability.<br />(c)<br />Application and testing of the proposed methods in the solution of problems such as financial derivatives valuation and medical applications.<br />These areas are strategically important having a direct impact on the economy and the development of other related fields of science.<br />""572444,""Christen, Andreas"
"590601"	"Chrostowski, Lukas"	"Silicon photonics integrated circuit design"	"<br />Silicon photonics is an emerging field in optics, whereby CMOS electronics foundries are used to manufacture optical chips, known as photonic integrated circuits (PICs). This program focuses on solving some of the major engineering problems associated with silicon photonics, with the aim of enabling the long-term success of this technology. While the applicant is involved in several specific research and development projects in silicon photonics, this program seeks to identify the common needs for a variety of disciplines (computer communications, optical telecommunications, wireless-photonic systems, sensors) and develop solutions suitable for these applications. <br /><br /><br />The major research topics to be pursued in this program, which aim to address many of the existing project challenges, and the anticipated future requirements are: <br /><br /><br />1) design methodologies for optical circuits - the electronics industry has developed elaborate design tools over the past 30+ years. Design automation is critical to build circuits containing thousands to billions of transistors. Similarly, silicon photonics technology is rapidly advancing whereby it is now possible to integrated tens to thousands of optical components in a photonic integrated circuit (PIC). An improved design flow is required for sophisticated PIC design.  <br /><br /><br />2) Components development and optical circuit modelling - there are many components required to build photonic circuits, and we are focusing on building a Library including optical interfaces, high-speed modulators, detectors, optical filters, and suspended optical waveguides for nano-opto mechanical switches and high efficiency thermal tuners. These components need to be designed and optimized, characterized, and circuit models for these components need to be developed. In many cases, there are already excellent performance components available (SiEPIC Library), but the key challenge is to develop compact photonic component models for circuit modelling. <br /><br /><br />3) manufacturing issues - one of the major challenges in silicon photonics is that the extremely small optical features lead to a high sensitivity to fabrication variations. Quantifying the manufacturability variations is the first step; next these variations need to be included in the circuit design methodology; finally, methods to build circuits immune to these variations need to be developed. <br /><br /><br />4) packaging - a final challenge of silicon photonics, which unless solved will lead to the catastrophic demise of the technology.  Not only must low-cost solutions be realized, but packaging needs to be taken into account in the design methodology, to enable the large volume applications such as disposable biosensors and optical communications within computers.""579283,""Chrostowski, Lukas"
"589923"	"Coady, MYvonne"	"Distributed Clouds: Modularity and Deployment Methods"	"<p>Clients using cloud services are often caught unaware of some of their drawbacks.  These include: (1) the necessity to explicitly replicate services to accommodate zone failures, (2) the inability to have fine grain control over location of data, and (3) the fact that large megadata centers may reside in areas subject to vastly different privacy laws. Despite these drawbacks, clouds provide affordable, effective, dynamically scalable infrastructure vital to meet the needs of growing user populations and massive data sets. Almost every dimension of society relies on today's cloud services, including research, government, industry, healthcare, education, citizen science, and entertainment.<br /><br />The proposed research program innovates with new methodologies for modularizing and deploying applications that will rely on a hybrid of cloud computing and distributed systems.  Key research initiatives will involve establishing accuracy when dynamically simulating system-level resources to reveal critical characteristics in deployment strategies, tradeoffs in the software engineering costs and benefits encountered, and practicality and sustainability in the context of real-world workloads.  Requirements will be derived from current needs that are of national and international significance.  The proposed research program will include interdisciplinary teams from two different organizations at the University of Victoria, CanAssist and Oceans Networks Canada.  These organizations are currently identifying the needs of monitoring systems in the context of both healthcare and emergency response, and are committed to exploring solutions that will overcome the drawbacks associated with today's clouds.<br /><br />By revealing more of the underlying platform and offering developers this ""break in the clouds"", cloud-based applications will have more control over not only the placement of data, but the quality of service provided by the supporting infrastructure. The key challenge will be to determine if these changes can be introduced to the existing cloud paradigm, without overwhelming application developers with complexity.</p>""586581,""Coady, Yvonne"
"589467"	"Collins, Christopher"	"Mixed-Initiative Visual Text Analytics: Data-driven Views and Analytic Guidance"	"<p>Visual analytics is the science of combining data processing, information visualization, and software to support people in the process of data analysis. The strength of visual analytics comes from having computers do work appropriate to their capabilities, such as counting, clustering, and bulk processing of data, while human analysts do the work of hypothesis formation, reasoning, evidence gathering, and decision making. The traditional way to start analyzing data with visualizations is to ""overview"" the data using high-level visual summaries. <span style=""line-height: 1.5em;"">However, as data scales have grown, overviews are increasingly challenging to design in an effective way. Overviews of very large data, such as all the Tweets in a city over a year, are often too cluttered to reveal anything interesting. So, rather than starting with an overview, it may be better to suggest a starting point for analysis, such as a single Twitter user, or tweets from a single day. The proposed research will harness computing power to create data-driven suggestions of starting points for analysis and guidance for next steps.</span></p><p><span style=""line-height: 1.5em;"">There is a trade-off around how far to take the role of computers in analyzing data. On one side, automatic highlighting or hiding of data details introduces potential risk of decisions being biased by algorithms. I argue that burdening analysts with visualizations which do not provide analytic guidance is equally inappropriate, as it ignores the ability of computational systems detect trends and regions of potential interest automatically. Analysts faced with general overviews may not know where to go, and may experience frustration as they start an investigation. By suggesting an interesting place to start, an analyst may more quickly achieve a state of deep engagement in analytic tasks. Thus, this proposal presents a program of research to investigate the nexus of the human-computer relationship in visual analytics: where should the control lie, what sorts of guidance are possible and helpful, and when should guidance be provided? In all of these investigations, I imagine retaining a human-in-charge paradigm, where suggestions and guidance can be ignored or deactivated, similar to the `autocomplete' functions on a mobile phone. </span></p><p><span style=""line-height: 1.5em;"">The questions in this research will be investigated using text and document data. Text data is economically and socially important, and generated in enormous volumes daily, through emails, business reports, books, news, legal proceedings, and more. Through the research process, 10 students will receive specialized training in highly sought-after data science skills. This </span><span style=""line-height: 1.5em;"">research will introduce new visual analytic practices, achieving a closer coupling of analysts and algorithms.  The outcomes promise to improve Canadians' ability to gain insights from economically and socially important large-scale datasets in domains from business intelligence to health informatics.</span></p>""579146,""Collins, Christopher"
"590532"	"Cook, ChristopherPaul"	"Automating Dictionary Construction for Better Natural Language Processing"	"Language technology is an important component of many everyday software systems, such as predictive text entry on smartphones, spelling checkers in word processors, and automatic translation tools. Many language technology systems rely on dictionary-like resources that contain specialized information about words.<br /><br />Language is always changing. For example, new words, such as ""mansplain"" and ""staycation"", are coined every day. Established words regularly take on new meanings, such as the new senses of ""post"" and ""wall"" related to social media. New expressions also emerge, such as ""flash mob"". Dictionaries must therefore be continually updated to include these new usages.<br /><br />When information is missing from a language technology system's dictionary, the system won't work as well as it could otherwise. Keeping dictionaries up-to-date is therefore essential to building high quality language technology.<br /><br />Dictionaries are also important tools for language learners and translators, and are valuable in terms of cultural heritage. To be most useful for these purposes, dictionaries must also be kept up-to-date.<br /><br />Traditionally, lexicographers manually analyze large collections of texts to identify relevant facts about how words are used, and subsequently write dictionary entries. However, a staggering amount of text is written every day, particularly through online sources such as the web and social media. It is simply not possible for lexicographers to keep up with it all.<br /><br />How then, can dictionaries be kept up-to-date?<br /><br />Automatic methods are required. This research program will first consider how to best build massive document collections from the web. Drawing on recent linguistic theory and research in statistical machine learning, new natural language processing techniques will then be developed to analyze these documents to automatically ""learn"" the meanings of words and idiosyncratic expressions. New methods will then be developed to automatically discover how the meanings of words and expressions have changed. This research will have a particular focus on social media, because it contains an abundance of new words and usages.<br /><br />This research will enable the construction of higher quality dictionaries. This will have broad benefits for language technology systems, because they often rely on dictionaries. It will also benefit traditional dictionary users, including language learners and translators.<br />""584868,""Cook, ChristopherPaul"
"590457"	"Corless, Robert"	"Design and Analysis of Algorithms for Structured Nonlinear Problems"	"          Modern science relies very heavily on computer simulation of scenarios that behave according to theoretical models. If the simulations perform as expected in the ""testing"" phase, then confidence in the theory underlying the model is bolstered. If the simulations manage to predict real-world behavior, especially if it is surprising (or might potentially cost lives or money if unanticipated) then they are of social benefit, not just academic importance.  But carrying out such simulations on a computer often involves solving a potentially large number of nonlinear equations. Doing so reliably and efficiently enough that predictions can be made in real time can be (depending on the subject area) extremely challenging. This project aims at producing new algorithms to make such solutions both more reliable and more efficient.<br />          The nonlinear algebraic problems that these algorithms are intended to solve occur in very many branches of engineering and science. Examples include fluid-structure interaction, computer-aided geometric design (CAGD) for manufacture, and what is coming to be known as ""Algebraic Biology."" Fluid-structure interaction is important in wind engineering of tall buildings, large bridges, transmission wires, cables and pipes. CAGD is a principal tool for manufacture of automobiles, aircraft, ships and other structures. The economic impact of those industries in total is in the billions in Canada alone. Algebraic Biology might be even more important in the future; already the design of new organisms is worth billions.<br />    Specifically, this project seeks to create numerically stable and efficient algorithms for certain structured mathematical modelling problems. We term this general approach the creation of ""Domain Specific Algorithms."" The main tool to be used for these algorithms is the replacement of the problem with a structured eigenvalue problem to be solved by standard or special-purpose iterative numerical methods. The structured matrices to be used come from my earlier work on companion pencils and linearizations from the Lagrange, Hermite and Birkhoff interpolational bases.<br />""589263,""Cormier, Gabriel"
"590909"	"Daneshmand, Mojgan"	"Miniaturized Reconfigurable Microfabricated Waveguide Circuits"	"The information and communications industry in Canada is an area of strategic national interest that is enjoying phenomenal growth, especially in the wireless sector. For example, wireless and communications equipment manufacturing in Canada has experienced a 12.5% average annual growth from 2002-2011, according to Industry Canada statistics (NAICS category 33422) [1]. The market continues to be fueled by increased demand for mobile applications and devices, multi-media and satellite services, and wireless sensors. Radio frequency systems are a critical and substantive portion of any of these technologies, and are responsible for efficiently receiving, filtering, and sending the high frequency signals at the front end of their transceivers. <br />Recently, wafer-level thin film and micro/nano technology-based systems have been used to surpass the limits of conventional devices and pave the way for miniaturized integrated systems with superior performance. There are three unique features of micro/nano technology that attract a high level of attention in the radio frequency community. These characteristics are miniaturization, multiplicity, and microelectronics compatibility. Miniaturization is clearly an important part of micro/nano technology, allowing the construction of components and subsystems that are both extremely small in size and high in performance. Multiplicity is inherent in micro-machining processing, which allows hundreds or thousands of components to be created concurrently and as easily as one. Micro-electronic compatibility allows the monolithic integration of micromechanical systems with microelectronics on a single chip, with benefits to cost, size, reliability and performance. <br />In this research, the benefits of micro-technology will be used to advance millimeter-wave front-end systems. These benefits arise from the favorable scaling behavior of mechanical systems to micro scale dimensions and their ability to transmit radio frequency signals. We will develop high quality miniaturized millimeter-wave reconfigurable wafer-level waveguide systems that incorporate microelectromechanical components. The ultimate goal is to have structures with small form factor and the capability of adapting the transfer function to demand. In many cases, such components would not only substantially reduce the size, weight, power consumption and component counts, but also promise superior performance and additional functionality.""575024,""Dang, DinhDong"
"590956"	"Das, Olivia"	"Performance and Availability Evaluation of Software Systems"	"<p>In today's world, performance and availability of software systems are two critical revenue impacting factors. Poor performance of software or an hour of its downtime can result in significant financial loss to businesses.</p><p>Increasingly, software applications are being deployed in clouds because cloud computing offers several advantages. For example, it relieves the application service providers from buying and maintaining data centers thereby reducing the operational costs; it allows dynamic scaling of computational resources as required on a pay-per-use basis; and, it promotes easy deployment in multiple geographic locations at minimal cost. Moreover, in mobile domain, cloud computing can facilitate offloading of intensive computations from resource-constrained mobile devices to resource-rich cloud, thereby reducing battery power consumption.</p><p>Although the aim of cloud computing has been to reduce computational cost, unfortunately the concept has been hit by performance and availability issues. Examples abound: Frequent outages of Microsoft's Azure is one. Rackspace's cloud service degradation is another. The onus is therefore on the software providers to ensure that their softwares meet the performance and availability requirements in presence of such issues. One way to achieve this could be through utilizing multiple clouds. However, managing applications in multi-cloud environment is a costly and complex task and hence calls for efficient strategies.</p><p>The goal of the proposed research is to develop novel modeling techniques, tools and algorithms that would help in evaluating performance (response time or throughput) and availability of software systems utilizing multiple clouds. Such techniques and tools will assist the software providers to plan in advance and manage the computational resources in a cost-effective manner.</p><p>The proposal includes (i) development and verification of novel models to evaluate performance and availability of applications in clouds; (ii) development of tools to aid in the modeling process; (iii) optimal deployment and provisioning of applications in multi-cloud environment that would minimize cost, and would meet their performance and availability requirements; (iv) computation offloading of mobile apps to improve their performance and reduce the battery power consumption of the mobile device. </p><p>The outcome of this research effort, if successful, will help improve the quality of software systems and reduce business cost. Such developments in Canada can help strategically position the software industry here and help advance it globally.</p>""576536,""Das, Rony"
"590048"	"DeCorby, Ray"	"Hollow waveguides and micro-cavities for optofluidics"	"The proposed research lies at the increasingly important intersection between integrated optics, micro-electromechanical systems (MEMS), and microfluidics. Our long-term aim is to demonstrate complex `optofluidic systems on a chip' by targeting close integration of optical devices (e.g. waveguides, resonant cavities), electromechanical elements (e.g. electrical and magnetic control structures), and microfluidic or atom delivery channels and reservoirs. <br />Underpinning the proposal is our previous development of a novel MEMS-like, buckling self-assembly process, which enables us to fabricate low-defect, air-core structures on a silicon-based chip. These air-core networks can encompass microfluidic channels, low-loss optical waveguides, spectral dispersion elements, and high quality micro-cavities. <br />Building on this, our main objective is to develop arrays of air-core optical micro-cavities, monolithically integrated with hollow waveguides and microfluidic delivery mechanisms. Open-access optical micro-cavities of this kind can be infiltrated with liquids, gases, or atoms, and have great potential to address key needs within both the optical sensing and information processing fields: <br />i. Optical sensing in lab-on-a-chip systems - Close integration of microfluidics with optical detection devices (micro-cavities, micro-spectrometers, etc.) is widely sought. The ultimate goal is the realization of powerful, low-cost, portable, and widely distributed sensing and analysis devices. The proposed work has strong potential to enable progress in this regard, and could have implications for the health, energy, and environmental monitoring sectors. <br />ii. Quantum information processing - Quantum networks are expected to enable great advances in computing and secure communications, and will also yield insights into the fundamental nature of quantum mechanics. Interaction of atoms and light within optical resonant cavities, sometimes termed cavity quantum electrodynamics (CQED), is considered a leading candidate technology to achieve these goals. To date, there is no practical approach to the implementation of large arrays of high-finesse, open-access micro-cavities on a chip. The proposed work has strong potential to address this need. <br />In summary, we will develop new optical integration technologies, and will apply these technologies to applications in sensing and information science. <br /><br />""576254,""DeCorwinMartin, Philippe"
"590769"	"Dehghanian, Vahid"	"Cooperative Multi-Radio Positioning"	"<p>The location of a handheld wireless device has been traditionally estimated by Global Navigation Satellite System (GNSS), e.g. GPS.  This has been highly successful as evidenced by the integration of GPS receivers into most mobile devices, such as smartphones and tablets.  However, the accuracy and availability of GNSS positioning does not meet the requirements of many existing applications.  For instance, a robust, accurate positioning system with seamless outdoor and indoor coverage is highly needed to increase safety in emergency responses.  In addition, many emerging applications, such as, automated warehouse management, unmanned vehicles, and robotic control, require meter-level positioning accuracy which is not available through GNSS.  This has resulted in a mounting impetus for the development of sophisticated signal processing for mobile localization that enables such accurate and reliable wireless localization under various indoor and outdoor conditions.  In addition to accuracy and availability, a practical mobile positioning system must be designed with stringent cost, size, and power consumption requirements such that it can be implemented into commercial handsets.  One economic and readily applicable method is based on exploiting the location information that is embedded in the received signal strength (RSS) from multiple wireless sources such as Wi-Fi, cellular networks, FM radio, Bluetooth, and etc.  This opportunistic approach is made possible by the key RSS measuring feature, which is ubiquitously integrated into the existing mobile devices.  However, the majority of these wireless infrastructures are designed for purposes other than wireless location, therefore, mobile localization based on heterogeneous wireless signals is exasperated by weak signal levels and incidental variations of the transmitted power.  One approach that is currently being considered by industry and academic institutions is cooperative positioning (CP).  CP is based on exchanging location information among neighboring mobile devices with the outcome of improved positioning accuracy.  The development of specialized statistical signal processing that enables such processing is the primary long term objective of the proposed research.  These algorithms will specifically take advantage of the physical motion of the mobile handset, which is a part of its typical usage mode, to further reduce location estimation errors.  The plausible outcome will be highly innovative mobile localization algorithm, which is accurate, yet inexpensive and power efficient such that it can be implemented into the mobile handsets of current vintage.</p>""577174,""Dehghanpour, Hassan"
"577746"	"Denton, Emily"	"Nova Scotia"	"CANADA"
"590339"	"Descoteaux, Maxime"	"Ontario"	"CANADA"
"590183"	"Dew, Steven"	"Micro- and Nanoscale Component Assembly Using Biomolecular Recognition"	"<p>     Many exciting applications of nanotechnology will require the integration of very small, separately fabricated components such as embedded processors, energy storage/generation units, sensors and actuators, and communications modules.  Collectively, these can create tiny, complete autonomous nanosystems that can sense and respond in a distributed fashion for diverse and powerful applications in health, security, the environment, industry or consumer use. A key unmet need, however, is the ability to assemble these separate components even when their total size is micron-scale or smaller. At this scale, conventional robotic systems are impractical, and self-assembly is needed. This proposal is for a research program to develop methodologies for the self-assembly of micron-scale components using biomolecular recognition. Molecules such as complementary DNA strands or biotin with avidin combine in a highly specific manner and can act as ""smart glue"" to cause micron-scale chips to automatically assemble in their assigned locations from an aqueous solution. The smart glue can be applied to chip and substrate with high resolution using standard nanofabrication techniques. Recent work by our group has validated this concept of programmed biomolecular attachment, but significant effort is needed to understand the capabilities of the process to determine whether it can be viable as a manufacturing paradigm. Specific tasks will be to investigate sensitivities to process parameters, design and test different biomolecular structures, and develop techniques to improve position and orientation accuracy of assembly. This will involve both experimental and modeling techniques.</p>""583056,""Dewan, Ian"
"591275"	"Dickinson, Sven"	"Perceptual Grouping and Shape Abstraction"	"Object categorization continues to be one of the most important challenges facing computer vision. When a particular target object is searched for in an image (the object detection problem), the target provides a strong shape prior to help segment and interpret the image features.  But for the more general task of object categorization from a large database, no such target object is available. Instead, we must rely on a set of object-independent, mid-level shape priors that reflect the regularities of our world -- regularities identified by the Gestalt psychologists 100 years ago. The problem can be formulated as follows.  Given an image of a cluttered scene and no a priori knowledge of scene content, our first task is to group together image features that belong to the same object. This is the classical problem of perceptual grouping, which has been largely ignored by the vision community in recent years due to the prominence of the object detection problem, in which stronger object-level priors subsume weaker mid-level priors.  One of the primary objectives of this proposal is therefore to model these mid-level shape priors, such as symmetry, continuity, junctions, and closure, and to develop algorithms that can detect these regularities and use them to segment a scene into causally-related collections of image features, each corresponding to a different object.  <br /><br />Ultimately, we would like to recognize the objects that these feature collections depict. But herein lies the problem when dealing with categories that exhibit high within-class variation. The granularity of the features comprising a categorical model is very coarse, whereas the granularity of the image features comprising a collection is very fine.  This disparity is often referred to as the semantic gap in computer vision. We must therefore abstract (or regularize) the groups of local image features in order to ""lift"" them up to the level of the coarse, prototypical features that make up the models in the database. This raises many critical research questions also addressed in this proposal: 1) how do we model the abstract shape of an object?; 2) what are the invariant parts whose detection can help us determine what object we may be looking at? (a problem known as object indexing); 3) how can we use knowledge of a vocabulary of abstract parts to drive the abstraction process that will help us recover such parts from an image?; and 4) should these parts be 2-D or 3-D? <br /><br />Our research program addresses the two important and closely related problems of perceptual grouping and shape abstraction on multiple fronts. Without any knowledge of scene content, the ability to recover from an image a set of abstract, part-based image features yields a powerful indexing mechanism that can prune a large database down to a small number of promising candidates which, in turn, can provide strong, top-down priors to segment and detect the objects in a scene.<br />""576298,""Dicks, Naomi"
"590504"	"Ding, Chen(Cherie)"	"Selection and Recommendation of Data Analytic Services in the Cloud"	"Big Data is an emerging trend and phenomenon. With more and more data becoming available from various sources, there is an increasing demand on analytic services for the purpose of understanding the data in a better way. Cloud Computing provides a platform to provision these analytic services. In this proposal, we would work on the selection and recommendation of data analytic services in the cloud. Compared to the traditional web service selection, there are several unique challenges for cloud-based analytic services which have not been sufficiently studied and which we address in this proposal; the challenges include the data-directed selection process for analytic services, the identification of meta-features appropriate for Big Data and feasible for measurement, the pairing of analytic services with other supporting cloud services (e.g., infrastructure, storage, data) to offer an end-to-end solution to users, and the ranking of the solutions. Our objective is to design and implement a data analytic service selection and recommendation system in which analytic service is selected based on the given dataset and the historical QoS values of different services on different datasets. A composite service combining analytic software service with other required component services is then recommended based on the predicted end-to-end QoS values. To begin with, we will select a problem domain such as social network analysis. For this chosen domain, we identify the meta-features of its datasets, proper data sources, and representative analytic algorithms and software services, and then we will study the problem of algorithm selection for this type of Big Data. In the next step, we will work on the service vertical composition and end-to-end QoS prediction for composite services. Finally we will build a cloud service marketplace with selection and recommendation components and evaluate our system. The key features of this proposal are as follow: 1) with the proposed selection system, users will be able to select an analytic service, which is best for the given dataset as well as the application domain, and compose a complete solution with recommended supporting cloud services; 2) we will train the HQP to gain a deep understanding on topics such as service selection and recommendation, algorithm selection and meta-learning, analytic service for Big Data, QoS prediction, and service vertical composition; in addition, they will gain hands-on experience working on system building and systematic experimental methods. Our research agenda also fits in perfectly with Canada's Open Government initiative. Since many datasets have been made available online through this initiative, our proposed system can help ordinary citizens select the appropriate analytic services in order to create value and have a better understanding of the otherwise hardly comprehensible raw data.""580425,""Ding, LinaWanlin"
"589718"	"Dingel, Juergen"	"A Toolkit for the Analysis of Models of Real-Time Embedded Software"	"With the number, size and complexity of real-time embedded systems increasing, the development costs for the software they run have become a concern in many domains including automotive, telecom, avionics, and medical. Growing pressure by governments and regulatory bodies to adopt more rigorous validation, verification, and certification techniques further exacerbates the problem.  With their emphasis on abstraction and automation, model-based development techniques and tools, based on, e.g., the Unified Modeling Language (UML), have the proven potential to allow for the efficient, effective development of high-quality complex embedded software and are increasingly adopted in industry.  <br /><br />However, the current state-of-the-art provides insufficient support for the analysis of models, i.e., for automated checks allowing developers to detect flaws in the model early in the design. For instance, for UML-based models as used in parts of the telecom industry, analysis capabilities offered by existing tools are incomplete and unable to adapt to the needs of specific domains, companies, groups, or users; model-level analyses investigated in academia often also fall short in this respect.<br /><br />Inspired by the success of modern analysis toolkits for<em> source code</em> such as GrammaTech CodeSonar and Coverity Code Advisor<em> the long-term goal of the proposed research is the development of an open, customizable analysis infrastructure and toolkit for industrial UML-based models of real-time embedded systems</em>; more concretely, the toolkit would consist of an Application Programmer Interface (API) containing the data structures, routines, guidelines, and examples necessary for the implementation of a wide range of industrially relevant analyses of these models.<br /><br />To achieve this goal, we will leverage (1) the background and knowledge collected during our past work on the formal semantics and verification of UML, (2) our industry connections to industrial users of model-based development techniques, and (3) the increasing interest in and availability of open source tools for model-based development in general, and the creation of the PolarSys Eclipse Industry Working Group in particular. More precisely, we will first conduct two case studies on model-level analysis; these case studies will be carried out in collaboration with industry (e.g., Ericsson Canada) to ensure relevance. Then, the insights gained on the required features and requirements for the analysis toolkit will be used to design and implement the toolkit. In the final step, the toolkit will be evaluated and refined. <br /><br />The proposed research will advance the state-of-the-art in the foundations and industrial use of model-based software development. Moreover, it will give several students (5 PhD, 3MSc, and 5 BSc students) the knowledge and skills they need to become sought-after graduates and make valuable contributions to Canadian industry.<br />""584040,""DingsAvery, Chantal"
"577937"	"Dumoulin, Vincent"	"Ontario"	"CANADA"
"589981"	"Dussault, JeanPierre"	"Solutions numériques de modèles d'équilibre"	"Mon programme de recherche s'articule autour de l'étude d'algorithmes d'optimisation mathématique continue. Mes travaux actuels portent sur la robustesse, fiabilité et efficacité des algorithmes, en particulier pour des problèmes comportant diverses formes de dégénérescence. Je suis activement impliqué dans des projets d'applications en reconstruction tomographique et en apprentissage machine, sources de motivation pour l'étude de modèles de tailles énormes. L'acquisition comprimée et l'optimisation éparse sont également apparentées à ces applications.<br /><br />L'objectif général de mon programme de recherche vise à une meilleure compréhension et l'amélioration d'algorithmes d'optimisation et se regroupe en quelques thèmes. Les méthodes de second ordre (Newton) étant prises comme référence, deux objectifs  sont d'exploiter des variantes d'ordre supérieur et d'utiliser  des méthodes d'ordre un dans les algorithmes de poursuite de trajectoires. Pour les problèmes de taille énorme, des algorithmes plus simples d'ordre un sont souvent privilégiés. <br /><br />Donc, une bonne partie de mes préoccupations sont d'ordre théorique et visent l'avancement des connaissances sur les algorithmes d'optimisation non-linéaire continue. Néanmoins, une source constante de motivation provient des applications. Il subsiste toutefois un écart considérable entre les besoins appliqués et la théorie des algorithmes. Par exemple, dans l'entrainement de réseaux de neurones profonds, on utilise de manière routinière des variantes de l'algorithme de gradient incrémental ou de l'algorithme de descente de gradient stochastique. On tente également d'accélérer les calculs en regroupant les données en mini-lots. Or, la théorie mathématique de ces algorithmes est très peu développée pour de tels problèmes non-convexes. De plus, les applications utilisent souvent des variantes algorithmiques connues pour ne pas converger vers la véritable solution optimale. Malheureusement, les variantes que l'on peut valider mathématiquement ne sont pas forcément aussi efficaces en pratique.<br /><br />Tout en poursuivant l'avancement des connaissances, je me préoccupe donc également de combler l'écart entre la pratique et la théorie. Les retombées de mes travaux auront un impact sur les applications notamment dans les problèmes d'imagerie et dans les problèmes d'apprentissage machine.<br />""584268,""DussaultFrenette, Olivier"
"590315"	"Eagleson, Roy"	"Design and Evaluation of Augmented Reality and VR-based Task Simulators"	"The proposal is part of an overarching research programme on the design and implementation of 3D visualization techniques for task-based training simulators. The Engineering Design process that we follow is classical, in that it includes a formal task analysis as part of the requirements elicitation stage, along with analysis at the information-processing level before software architecture and hardware deployment lead to implementation. Our approach, in addition, stresses the development of systematic user-centred evaluation methodologies, using paradigms adapted from pure Science-based research in human perception, cognition, and action. In other words, we consider the design of task-based simulators to be an enterprise within the Software Engineering domain of Human-Computer Interface Design.  We are working with the following partners on these projects: <br /><p>Digital Extremes, Inc has established a collaboration with our lab to extend their proprietary ""Evolution Engine"", a scenario-based content creation tool for video game development, to develop tools for task-based simulators using their game engine. The scenarios that we are developing are in part consistent with a collaboration that we have established with the NRC on their simulator platform. This system makes use of physics-based modeling and haptic interfaces in addition to an immersive VR-based display. We also plan on extending to augmented-reality based trainers, through a collaboration with Christie Digital Displays; making use of their Projection-Mapping algorithms, and projector-based systems for Augmented Reality environments. </p><p>We are establishing an approach in which a formal Hierarchical Task Analysis (HTA) phase is being conducted on the specific tasks being trained within simulator-based curricula. The task analysis is important for two reasons: First, the HTA description is compatible with UML Harel Statecharts, which are representations that can be adapted using standard patterns on a variety of platforms. This approach allows us to formalize the requirements of the main software classes in the simulator systems, in terms of the input-output requirements for the View classes, the Controller Classes, and those of the Model. It allows us to examine the task requirements from an information-processing standpoint, in terms of graphical display outputs, and user interface input events.</p>This facilitates a behavioural description of the system which span the abstraction hierarchy so that low-level technical skills performance can be addressed by extending the classical Fitts paradigm which respects the speed-accuracy tradeoff, and also to generalize to higher levels of the abstraction involving 3D spatial reasoning, path planning, and executive control of sub-tasks.   Curriculum uptake on non-technical skills can thereby be assessed based on task completion times and error rates as analogous metrics.""574139,""Eames, Brian"
"591015"	"Elbiaze, Halima"	"Energy-efficient algorithms for next generation open networks"	"<p>The remarkable growth in Internet traffic is expected to generate many new research challenges to address the new requirements in terms of volume and speed of exchanged data information. Indeed, telecommunication networks and the Internet are being re-designed to be energy efficient and enable smarter and sustainable growth.</p><p><span style=""line-height: 1.5em;"">Contrary to existing solutions, an </span><strong style=""line-height: 1.5em;"">original </strong><strong style=""line-height: 1.5em;"">advanced holistic approach</strong><span style=""line-height: 1.5em;""> for energy aware networks is proposed involving </span><span style=""line-height: 1.5em;"">both</span><span style=""line-height: 1.5em;""> data and control plane. The </span><strong style=""line-height: 1.5em;"">innovative goals</strong><span style=""line-height: 1.5em;""> of our solution consist of: (i) investigating </span><span style=""line-height: 1.5em;"">local</span><span style=""line-height: 1.5em;""> and </span><span style=""line-height: 1.5em;"">distributed</span><span style=""line-height: 1.5em;""> algorithms and techniques for energy efficient network operations, (ii) studying dynamic, scalable, ad-hoc and optimized resource allocation that balances the </span><span style=""line-height: 1.5em;"">trade-offs</span><span style=""line-height: 1.5em;""> between energy consumption and network performance, as well as differentiated quality of service (QoS) requirements and (iii) exploring </span><span style=""line-height: 1.5em;"">smart standby</span><span style=""line-height: 1.5em;""> dynamic power saving strategies to be implemented in both data plane and control plane.</span><br />Besides, information and communication technologies (ICT) industry faces a fundamental <strong>paradigm shift</strong> led by the increasing adoption of software-defined networks (SDN) and network functions virtualization (NFV). Hence, one <strong>important issue </strong><span style=""line-height: 1.5em;"">that will be </span><span style=""line-height: 1.5em;"">addressed </span><span style=""line-height: 1.5em;"">in my research program is exploiting the potential energy efficiency improvements provided by SDN and NFV in future networks.</span><span style=""line-height: 1.5em;""><br /></span></p><p>On the other hand, optical networks are widely recognized as a future proof and cost-effective transport technology for supporting the growing traffic volume and services variety at very <span style=""line-height: 1.5em;"">low energy footprint</span><span style=""line-height: 1.5em;"">. However, elastic optical networks (EONs), that may meet the future bandwidth requirements, increases considerably the energy consumption. Furthermore, new generations of data center networks have to provide higher bandwidth efficiency, lower latency, increased flexibility and lower cost. Optical networks interconnection solution is recognized as the best candidate to satisfy these needs. Thus, this proposal brings a </span><strong style=""line-height: 1.5em;"">special emphasis</strong><span style=""line-height: 1.5em;""> on the energy-efficient design of EONs taking into account (i) physical layer impairments, (ii) routing, spectrum allocation, and modulation format and (iii) dynamic network operation.</span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">The proposal will provide fundamental </span><span style=""text-decoration: underline;"">knowledge advancement</span><span style=""line-height: 1.5em;""> </span><span style=""line-height: 1.5em;"">and </span><span style=""line-height: 1.5em; text-decoration: underline;"">practical </span><span style=""text-decoration: underline;"">contributions</span><span style=""line-height: 1.5em;""> </span><span style=""line-height: 1.5em;"">to the area of energy efficient strategies for the next generation of high speed data networks and will provide excellent </span><strong style=""line-height: 1.5em;"">training</strong><span style=""line-height: 1.5em;""> </span><strong style=""line-height: 1.5em;"">opportunities </strong><span style=""line-height: 1.5em;"">for eight graduate and four undergraduate students. Indeed, this research proposal aims to provide necessary highly qualified people for Canada's economic growth and increased competitiveness. Also, investigating fundamental aspects using the requested discovery grant is expected to exert a leverage effect in order to obtain collaborative or industrial research funds for research projects that focus on practical or implementation aspects.</span></p>""573199,""ElBoussaidi, Ghizlane"
"590577"	"Elder, James"	"Recurrent computations for the perceptual organization of shape"	"Imagine a visual world of random blotches of colour and texture, like an abstract painting.  This is a world without shape, and it illustrates how our capacity to see structure and recognize objects is determined by our ability to perceive shape.  In the proposed project we will employ a novel combination of psychophysical and computational methods to determine how the human brain extracts and represents 2D and 3D shape information from contours in complex imagery, and will develop improved computer vision algorithms for object segmentation and shape processing based upon these insights.<br /><br />The seemingly effortless way in which we perceive shape belies a daunting complexity.  As you gaze around you now your visual world is likely a complex clutter of partially occluded objects, variegated lighting and shadows.  These complexities fragment objects into perceptual shards that the brain must correctly group together in order to compute accurate representations of shape.  This process of perceptual organization is a combinatorial problem of exponential complexity, yet the brain solves it reliably and efficiently, vastly outperforming current computer vision algorithms.   This impressive performance appears to derive from the brain's ability to fuse multiple local and global grouping cues within a recurrent hierarchical neural architecture.  A major deliverable of the proposed work is a detailed and testable computational model of this neural circuit informed by a key set of new psychophysical experiments.<br /><br />Natural shapes generally sweep out low-dimensional curved manifolds in a high dimensional shape space.    While most effort in computer vision has focused on discriminative methods for separating these manifolds, a fully generative model is required to support perceptual grouping and many other tasks.  The challenge is thus to identify efficient but fully generative models of shape.  Prior work has been challenged by topological instability, but our recent formlet theory of shape solves this problem.  A second deliverable of the proposed work is a fully probabilistic graphical formlet model of 2D shape, evaluated psychophysically as a model for human shape representation.<br /><br />While most studies of 3D shape perception focus on depth cues to shape such as stereopsis, or surface cues such as shading, the 2D shape of the bounding image contour is also known to strongly influence 3D shape perception.  A third deliverable of the project is a new probabilistic model for estimation of 3D shape from the bounding contour, represented using a 3D extension of our formlet theory.<br /><br />These results will substantially advance our understanding of shape processing in human visual cortex.  The algorithms developed here will form the basis for applications in automatic video analytics for traffic surveillance and for 2D to 3D film conversion.""577150,""Elder, James"
"590833"	"Elliott, Robert"	"Estimation and filtering of hidden semi Markov models, event based filters and stochastic control."	"The central objective of the work will be the derivation of new algorithms for signal processing, in particular the estimation of information hidden in noisy signals. This will build on previous contributions by the applicant. Three directions will be pursued. The first will develop new estimation algorithms for hidden semi Markov models, extending our previous work on hidden Markov models. The second will consider event based filtering problems when the observed process is received only when the hidden signal changes by more than a specified amount or when the signal process crosses certain levels. The third will discuss the optimal control of a noisily observed Markov chain using backward stochastic differential equations.<br />In addition to signal processing, speech processing and other areas, an important application of hidden Markov models has been to genome and protein sequencing. However, when modelling discrete sequences, for example in discrete time or in  biological applications, if the hidden process is a Markov chain its (random) occupation time in any state is a geometrically distributed random variable. In many areas of applications, including queuing theory, reliability and maintenance, survival analysis, performance evaluation, biology, DNA analysis, and genome sequencing, it seems more general occupation times should be considered. This leads us to consider semi-Markov models. We shall consider 'hidden' semi-Markov models, that is situations where the semi-Markov chain is not observed directly but modulates a second, observed, process.  We shall develop for these models the results found in our book and previous publications.  <br />In the digital world, continuous-time signals must be sampled. Traditionally, they are sampled uniformly in time. The term 'event-based sampling' can refer to the traditional uniform time step sampling, (sometimes called Riemann sampling), but it usually means that samples are taken in response to a priori defined events, such as when the signal changes by more than a specified amount, (send-on-delta), or when it crosses specified levels, (Lebesgue sampling). The objectives of the work will be to obtain new implementable filters. The theory of event-based sampling is more involved but there are many practical benefits including cheaper sensors, reduced communication costs and less data to process.  <br />The third line of research will use our recent results on backward stochastic differential equations to investigate partially observed stochastic control problems. The adjoint process is described by a backward stochastic differential equation; for partially observed problems this is a backward stochastic partial differential equation. Initially we consider this problem for the control of a partially observed Markov chain where a system of backward stochastic ordinary differential equations will give criteria which determine an optimal control.""572986,""Ellis, Bobby"
"590492"	"Englehart, Kevin"	"Myoelectric Control of Powered Upper Limb Prostheses"	"<p>The loss or congenital absence of a limb is a major disability that can have considerable physical and psychological impact on the life of an amputee.  It is estimated that there are more than 3 million upper limb amputees globally, with the rate of incidence growing steadily. Better robotic prostheses can dramatically improve the quality of life for persons with an upper limb amputation, many of whom reject existing devices because they have trouble controlling them in the same intuitive, subconscious way that they controlled their intact arms.</p><p >Myoelectric prostheses use the electromyogram (EMG) signals (the electrical signals generated during muscle contraction) from residual limb muscles to control motorized arm joints. The use of EMG offers a non-invasive means of establishing a natural interface to the neuromuscular system to control the lost functions of the limb.<br /><br />Although significant advances have been made in building lighter, stronger and more versatile prostheses over the last 20 years, little progress has been made in viable control of these prostheses. Individuals who have used myoelectric prostheses clearly indicate that it is the reliability and dexterity of control that is the most significant factor in acceptance of these devices.<br /><br />The objective of this research is to deliver robust, dexterous control to myoelectric prostheses. Although the applicant, and others, have demonstrated great success in myoelectric control using pattern recognition and regression based methods in controlled laboratory settings, the ability to have these methods succeed in user's homes and work environments requires advances in robustness and dexterity that will translate into a meaningful improvement in user experience. This will be accomplished by introducing innovative modeling and signal processing paradigms, including novel methods in pattern recognition, adaptive learning, and nonlinear regression. </p>""585897,""Engler, Steven"
"589819"	"Ergun, AyseFunda"	"Streaming Algorithms for Structural Trends"	"<p>Over the past years, computer science has been responding to the ever-increasing data size by exploring new ways and models of computation.  This proposal focuses on the theoretical aspects of this effort by exploring streaming algorithms. </p><p>A streaming algorithm reads its input left to right over one or more passes. This model corresponds to reading data from sequential storage, as well as processing as it is generated on the fly. The main limitation is that the space use must stay well below the input size. To comply, the algorithm  typically condenses its data into a small summary, losing many details and obtaining an output that is only approximately correct.</p><p>Most of the streaming research is in discovering statistical information, such as element frequencies, which is independent of the order of the elements in the stream.  In this proposal, we propose to focus on and develop tools for the less well-studied problems of discovering ""structural"" trends such as monotonicity, periodicity, etc, which rely on the order of the elements in the input.</p><p>Previous work as well as our preliminary results indicate strong relationships linking structural trends in streams to one another.  For instance, one can discover periodicity by using pattern matching, and discover palindromes by using periodic properties.  Our main goal is to understand these relationships and help build a common ""vocabulary"" of tools, similar to the sketching algorithms discovered over the past 15 years for statistical problems, that will apply to the general problem of discovering structural trends.  Example problems include various versions of monotonicity and common sequences, frequent substrings, smoothness of the input, local periodicity, squares (and cubes) in streams, etc.</p><p>Problems on the ordering of the input occur in fields such as bioinformatics and networks.  However, most existing models are too simple to be of use in an applied setting.  Complications that simple models cannot handle include noisy data, slow changes in the input over time, the involvement of difficult subproblems (such as dealing with edit distance in a stream environment), etc., which makes the algorithms design messy.  It is the goal of the PI to understand (feasible) applications and build their needs into the model in a realistic, clean way.  For instance, while finding perfect palindromes in a stream is reasonably easy, finding those that might have a few incorrect characters is open.  In order to solve such problems, we propose to consider and build tools that are custom-designed for handling real world complications.</p><p>We also propose to work on recent models that use streaming implicitly in an ad hoc manner (such as the MapReduce application) that would benefit from more theoretical rigour.  </p><p>The ultimate goal of this research is to better our understanding of big data processing in the theoretical sense, as well as make it more amenable for real applications.</p> <p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p><p> </p>""579149,""Ergun, AyseFunda"
"582261"	"Evans, Robert"	"Ontario"	"CANADA"
"590297"	"Fair, Ivan"	"Variable-length constrained sequence coding"	"<p>This research focuses on the continued development and application of variable-length constrained sequence codes in digital transmission and storage systems.<br /><br />Encoding and decoding of data is an indispensable part of all digital communication systems, including both transmission and storage systems.  These coding techniques fall into the general classes of:<br /><br />Source codes:  used to represent the source information in as compact a form as possible;<br /><br />Constrained sequence (CS) codes:  used to generate encoded symbol streams that satisfy the constraints of the channel and attempt to ensure proper operation of practical demodulation circuitry, and<br /><br />Error control codes:  used to detect and correct symbols that have been incorrectly detected by the demodulator.<br /><br />When compressing data with a source code, variable-length (VL) coding techniques are almost always used owing to their ability to efficiently represent the data with as few encoded symbols as possible.  In contrast, practitioners of both constrained sequence codes and error control codes continue to focus on the development and implementation of fixed-rate codes.<br /><br />However, many new communication systems are inherently based on the use of VL sequences which enables the use of simple and efficient VL CS codes.  Packet-switched transmission networks are prime candidates for VL CS codes, as are storage systems that use VL compression techniques.<br /><br />My research will focus on (1) the continued development of and (2) the application of variable-length constrained sequence codes for digital transmission and storage systems.  The starting point of this work will be the state-independent VL CS coding technique recently developed in my group whose implementation is simpler, and which offers higher efficiency, than any other class of CS codes published to date.<br /><br />Extensions of a fundamental nature to this exciting work will include:<br /><br />(1a) extending the current state-independent coding techniques to support state-dependent encoding<br /><br />(1b) developing simplified search technique for optimal codes<br /><br />(1c) determining appropriate techniques to mitigate error propagation during decoding<br /><br />(1d) optimizing word mappings to limit error propagation during VL CS decoding<br /><br />(1e) integrating VL CS codes with error control codes, and<br /><br />(1f) extending the development of binary VL CS codes to systems with multidimensional signaling alphabets.<br /><br />We will also apply these principles to optimize the design of VL CS codes for the important applications of:<br /><br />(2a)  recording codes for flash memory, and<br /><br />(2b)  spectral shaping codes to avoid narrowband interference in single-carrier digital video transmission systems.<br /><br />This research will therefore continue the development of a new approach to construct efficient constrained sequence codes and will demonstrate their design and application in new and evolving digital communication systems.</p>""581763,""Fair, Kathryn"
"590141"	"Fels, Sidney"	"Creation and Evaluation of Novel Media Experiences"	"<p>This proposal has two threads investigating novel means of leveraging human perceptive and cognitive abilities to create and evaluate next generation media experiences. Two media experiences will be investigated: a novel 3D display and interactive video in an educational context.  <br />For the first thread, I plan to advance the state of the art of 3D perspective-corrected spherical (<strong>3DPS</strong>) displays. Touch and gesture input, high-resolution, and multi-person functionality will be added to a spherical, 3D display targeting two aspects of behaviour in 3D: interaction and collaboration. Having high-resolution using multiple projectors with low-latency feedback and fast refresh rate, enables the sensation of an object being physically in the sphere. With this phenomenon, along with gesture and touch interaction on and around the spherical display, the potential for high-fidelity, collaborative 3D interaction can be explored. Leveraging off existing work on interaction with volumetric, cylindrical and cubic displays, new interaction semantics for pointing, selecting, translating, rotating, and sculpting content will be explored. As the display supports multiplexed video, novel co-located and remote collaborative, 3D interaction semantics will be created and evaluated.</p><p>The second thread advances the state of interactive video (<strong>IV</strong>) authoring, navigation and sharing in educational environments. The main insight pursued matches interface design for spatio-temporal video content with peoples' cognitive and affective mechanisms, as and applies it to educational video contexts. Current video players target mostly linear viewing of relatively long video content leading to poor mechanisms for organizing partially viewed video or salient content. However, with online video content becoming ubiquitous and shorter, consumption has become more non-linear and the amount of video viewing has increased by at least an order of magnitude. Finding particular videos, or salient content within videos are significant challenges, especially as existing mechanisms tend to be geared towards semantic memory rather than episodic.  Furthermore, user generated content suffers from a model that separates authoring from viewing and sharing, impeding video content creation by non-experts, such as educators and learners. Exploring additional mechanisms, such as affective, temporal and spatial cueing will address this challenge. </p>""589630,""Felty, Amy"
"591059"	"Filizadeh, Shaahin"	"Novel paradigms for transient simulation of modern electric-power systems"	"<p>    Electric-power systems have been the backbone of the modern civilization for over a century. During this time, they have evolved from small and isolated installations to massive systems often interconnecting countries or even continents. Computer modeling and simulation has been an indispensable practice in the design and operation of modern power systems as, arguably, the only viable means to tackle their complexity.</p><p>   Modern power systems contain an increasing number of power-electronic converters. These converters serve to transmit large amounts of power (e.g. high-voltage dc transmission systems), improve the grid's operating conditions (e.g. flexible ac transmission systems), or interface renewable sources to the grid (e.g. wind or solar power). With the accumulative presence of power electronics in the grid, conventional simulation algorithms and platforms are facing escalating limitations in properly representing the transient and dynamic behavior of the power grid. This is because existing power-system transient simulators have been designed mainly for conventional power systems with rotary machines and without many power-electronic converters. </p><p>   The proposed research focuses on the development of next generation power-system transient simulation algorithms and platforms that are designed specifically for the emerging grid in which power-electronic converters are heavily embedded. The core of the research is based upon the notion of <em>extended-frequency dynamic phasors</em>. These mathematical constructs provide a great deal of flexibility and selectivity in computer modeling and simulation of complex power systems. A simulation platform built upon extended-frequency dynamic phasors will enable simulation of power systems with controlled resolution, over extended periods of time, and on advanced and specialized computing hardware, all far beyond what is presently possible. </p><p>   This timely research will be directly beneficial not only to the manufacturers of power-system simulation hardware and software, but also virtually all utilities, equipment manufacturers, and other stakeholders who rely on accurate computer simulations to design and operate power systems. Its main benefits will include superior selectivity and accuracy, controlled computational burden, and platform flexibility for simulation-based studies. The research will train 11 HQP (3 Ph.D.'s, 3 M.Sc.'s and 5 USRA students). The research and its output, both the knowledge to be generated and the HQP, will be important contributors to secure Canada's stance as the global leader in the power-systems simulation industry.</p>""591428,""Filleter, Tobin"
"589778"	"Fofana, Issouf"	"Évaluation précise de l'état des transformateurs - solutions pour la gestion des transformateurs vieillissants"	"Les transformateurs de puissance constituent pour les exploitants des réseaux électriques et les industriels de très gros investissements. Leurs défaillances sont très coûteuses, principalement à cause de la non-disponibilité du service électrique qu'elles entraînent. Il est donc d'intérêt de fiabiliser leur fonctionnement sur une période aussi longue que possible. De nombreux parcs de transformateurs approchent aujourd'hui de la fin de leur durée de vie théorique. Associé au fait que la demande en électricité ne cesse de croître, les risques de voir les plus vieux équipements tomber définitivement en panne et entraîner d'importantes coupures d'électricité augmentent de jour en jour. Ce qui soulève des inquiétudes tant au niveau de l'approvisionnement énergétique, de la sûreté du public, de l'environnement qu'au niveau des investissements.<br />L'objectif du projet de recherche proposé est non seulement d'améliorer nos connaissances actuelles du vieillissement des isolations, mais surtout de développer des outils d'évaluation rapide et précis de l'état des transformateurs, facteurs clés d'une exploitation efficace et sûre. Nous prévoyons décrire le comportement physique du système d'isolation en réponse à divers contraintes diélectrique par une modélisation multi-entrées, basée sur les méthodes inverses. Ces études nous diront également comment prédire la durée de vie restante des transformateurs vieillissants. Ces travaux de recherche fourniront aux ingénieurs et planificateurs d'entretien, des outils d'analyse pratiques à la prise de décision et planification à long terme sur une base solide scientifique. L'application de la recherche proposée offrira non seulement des avantages économiques importants, mais a également comme conséquence le potentiel significatif de garantir la distribution d'électricité aux utilisateurs. La formation avancée d'ingénieurs et chercheurs sera un des résultats significatifs du programme proposé.""585024,""Fogel, Stuart"
"591096"	"Friggstad, Zachary"	"Improved Mathematical Programming Techniques for Approximation Algorithms"	"<p>A striking number of problems in discrete optimization are, unfortunately, computationally intractable. These problems stem from issues faced by our complex society: coordinating vehicles in a transportation network, compiling code to create efficient executable programs, determining placements of fire or ambulance stations to improve response time. More precisely, many such problems are NP-hard meaning we do not have, nor do we expect, any efficient algorithms to solve these problems optimally. To cope with this difficulty, we focus on devising efficient algorithms that find near-optimum solutions.</p><p><span style=""line-height: 1.5em;"">The subject of this proposal is designing improved approximation algorithms for NP-hard optimization problems, primarily by devising and analyzing new mathematical programming approaches. The goal is to provide new polynomial-time algorithms to compute solutions whose costs are within some proven explicit bound of the optimum solution. The fact that these algorithms will be based on mathematical programs will also help articulate the connection between theoretical computing science and practical heuristics, as linear and integer programming techniques are often used to devise algorithms that perform well experimentally yet lack proven guarantees on their worst-case performance.</span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">My proposed research includes modelling discrete optimization problems as mathematical programs and then relaxing some constraints of these programs to get models that can be solved efficiently. Typically, this is a linear or semidefinite programming relaxation of an integer program. Once this relaxation is solved, the solutions are carefully rounded in a way that obtains a feasible solution to the original model while preserving the objective function value as much as possible. This is already known to be one of the most effective ways to design approximation algorithms; eight chapters of the recent book ""The Design of Approximation Algorithms"" by Shmoys and Williamson are devoted to this method.</span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">In particular, applications of mathematical programming techniques to vehicle routing and resource allocation problems will be investigated. In vehicle routing problems, a strong linear programming approach has recently proven useful in addressing problems with multiple vehicles and I propose to further explore these techniques to address fundamental routing problems. Additionally, I will investigate the so-called unsplittable flow problem in trees which represents the frontier of our understanding in how to approximate resource allocation and packing problems. In particular, I expect that understanding the effectiveness of a relatively new linear programming model should either lead to improved approximations or tighter lower bounds.</span></p><p><span style=""line-height: 1.5em;""><br /></span></p>""573279,""Frigon, Alain"
"589275"	"Frize, Monique"	"Development and Knowledge Translation of Decision-Support Tools"	"The<strong> </strong>main goal is to continue to develop information technology systems that can provide knowledge that enhances decision-making by users for the various problems under study. The decision-support tools use a systems engineering approach, combined with web services, to provide relevant information in an easy and intuitive manner; this phase of our research program focusses on delivering knowledge to physicians and patients/parents in a perinatal care context. It is important to note that the methods developed in this research program can be applied to non-clinical databases like weather and climate measurements to predict severe weather patterns, or changes to the stock market, and help classify outcomes for non-medical applications. The major part of this research resides in the domain of systems, computer, and software engineering; the evaluation of the results (usability studies) use software engineering methods. The application of these systems to a medical environment represents around 10 % of the research proposed. The objectives are to develop prediction tools to identify the risk of premature birth in pregnant women, delivery mode (vaginal or Caesarian), and Apgar Score (state of infant at birth); and improve our risk prediction models for intensive care infants regarding mortality and a number of complications, using real-time data collected from up to twenty infants simultaneously. No currently available approaches predict these outcomes. We plan to identify the variables that best predict the various outcomes being studied as well as incorporate warnings and alarms when notable changes occur. The next step will be to present the information to physicians and parents of infants in an easy, secure manner using web services, which can be accessed locally or from remote locations, by several users. The characterisation of risks and the comprehensive obstetrical and perinatal information need to be integrated in an original manner to provide a useful tool for physicians and patients. The risk estimation results, along with response to treatments and patient prognosis, will offer a unique clinical tool not available in current research or commercial systems. There is a considerable level of interest by physicians for our approaches. The research is entirely novel and the team has considerable and complementary expertise: Dr Bariciak, neonatologist, for clinical aspects; Dr Gilchrist, a Computer Scientist, for software questions; Dr Frize, for biomedical engineering and research methodology. The estimation tools will help to intervene earlier and improve patient outcomes. The principal applicant has trained more than fifty highly qualified personnel over twenty years; this application will expose research students to a multidisciplinary environment in an area of great demand for Canada. All former research students found relevant employment after degree completion.  <br /><br /> ""568746,""Froda, Sorana"
"590832"	"Gaboury, Sébastien"	"Reconnaissance d'activités à base de capteurs de nature RFID et électrique aux fins d'assistance cognitive"	"Un grand nombre de recherches en intelligence artificielle (IA) et particulièrement en habitats intelligents s'intéressent au monitoring d'activités humaines en temps réel pour l'assistance des personnes âgées atteinte d'une déficience cognitive. La problématique repose sur le fait que l'assistance est conditionnelle à la reconnaissance (et/ou l'apprentissage) de l'activité en cours d'exécution (ou à réaliser). Par conséquent, l'élaboration et l'implémentation de systèmes de reconnaissance d'activités de la vie quotidienne (AVQ) fiables et robustes constituent un élément clé pour permettre le déploiement de systèmes d'assistance cohérents et efficaces.<br /><br />La plupart des algorithmes de reconnaissances d' AVQ utilisent des données provenant de capteurs intrusifs (cameras, microphones) et/ou de capteurs binaires (contacts électromagnétiques, détecteurs de mouvements infrarouge,etc.). Bien qu'elles fournissent la plus grande expressivité, les caméras sont intrusives (respect de la vie de privée) et le traitement de leurs données est complexe. Il en est de même pour l'utilisation de microphones. Afin de trouver un compromis entre l'expressivité de l'information et l'intrusion, plusieurs chercheurs s'intéressent à l'exploitation de technologies alternatives telles que les capteurs RFID (Radio Frequency IDentification) de type passif et les capteurs de signaux électriques fixés au panneau principal d'alimentation d'une résidence pour la reconnaissance d'AVQ. <br /><br />L'exploitation de ces technologies exige de relever plusieurs défis scientifiques. D'abord, les données extraites par ces deux types de capteurs sont très bruitées. Ainsi, le développement d'algorithmes de débruitage est nécessaire pour permettre l'utilisation de ces données. Ensuite, ces données doivent être interprétées. Par exemple, la conversion des forces de signaux provenant d'étiquettes RFID passives en une position (2D) requière la création d'algorithmes de localisation et de traçabilité capables de gérer l'imprécision liée à cette technologie. Du côté du capteur électrique, l'identification des ouvertures/fermetures des appareils à partir des signaux électriques au panneau principal nécessite l'utilisation de techniques mathématiques avancées d'analyse du signal permettant l'extraction des signatures (puissance active, réactive, harmoniques, etc.) de chaque appareil mais aussi le développement de modèles algorithmiques qui tiennent compte de la variabilité de ces mêmes signatures. Enfin, une désagrégation du signal électrique est souvent inévitable pour discerner les appareils en fonction simultanément.<br /><br />Pour les cinq prochaines années, je prévois m'attaquer aux différents défis scientifiques mentionnés précédemment afin d'étendre nos algorithmes de reconnaissance d'activités et d'apprentissage basés sur l'exploitation des données provenant de capteurs RFID et électrique.<br /><br />""589214,""Gabriel, David"
"589884"	"Gad, Emad"	"Modeling and Simulation of Deterministic and Stochastic Nano systems"	"<p>The relentless downscaling drive in the integrated Very Large Scale Integration (VLSI) circuit technology (from the deep submicron and down to the nano-scale level) has increased the level of challenges in predicting the system performance or estimating its basic figures-of-merit. The main reason for that difficulty stems from the increasing uncertainty that surrounds the actual (i.e., post Silicon-layout) values of the design parameters. Design parameters, in this context, refer to a wide range of circuit design parameters including  geometrical and structural parameters, process parameters or electrical parameters. Traditional modeling and simulation approaches have treated those parameters as deterministic variables with well-specified numerical values. However, with the above-mentioned uncertainty surrounding their post-fabrication or actual values, those parameters must be regarded as random variables that are characterized with certain probability density functions (PDF), such as Gaussian or Uniform PDF. With this new development, system performance metrics are no longer deterministic functions in its design parameters, but need to be treated as stochastic processes whose full characterization can be best approached through calculating their PDFs. This goal of computing the PDF of a system is generally known in the mathematical literature as Uncertainty Quantification (UQ).</p><p>The proposed research program presented by the applicant will tackle the UQ problem on three different fronts. The first front explores specific areas where this problem has a direct impact on the design methodologies and the cost of design iteration. <br />On the second front, advanced approaches based on recently discovered mathematical formulation will be investigated and generalized for reducing the computational cost that arises in the problem of UQ. The third front will focus on developing novel methodologies to take advantage of utilizing alternative computation platforms based on massive parallel architectures that are offered by modern General Purpose Graphical Processing Units (GPGPUs).</p><p>In addition to addressing the problem of UQ, the proposed research will continue to build on the success of the high-order stable method that the candidate proposed recently for the simulation of VLSI circuits. The new phase of the proposed research in this regard will aim at including the full range of transistor device models at the deep submicron and the nano-scale levels. The main underlying objective in the proposed approach is to develop a fully automated technique to convert device models, to the novel graph-based rooted tree structure that are more suitable for the high-order stable methods. </p><p> </p>""583825,""Gad, Kareem"
"589797"	"Goebel, Randolph"	"A framework for the formalization of interactive visual analytics"	"<p>The process of visualization is about transforming data into pictures.  The computer science study of visualization is about the development of the theory and practise of transforming data into pictures from which humans can draw inferences (we use the word ""picture"" to include all manner of visual media, from pencil dots on a page to 3D video).</p><p>Because the variety and breadth of the world's data is enormous, a visualization process must necessarily filter, compress, or otherwise reduce the scope and complexity of data in order to transform it into sensible pictures.  So one foundational challenge of visualization is to ensure that data to picture transformations preserve those properties that best help humans draw ``appropriate'' inferences about those data.  For example, a bar chart that represents the number of hockey players by country of origin should make it easy to see which country produces the most hockey players.  In fact, one measure of a good picture is that it leads most if not all humans to draw the same conclusions, and that it avoided introducing visual anomalies (e.g., a Necker cube).</p><p>A major challenge is about how to build transformations that preserve important properties of the base data (e.g., numbers in a spreadsheet) when turning them into a picture (e.g., a histogram).  Since visualization can't merely represent every base data point in a picture, some aggregation of data is required before producing a picture.  For example, if our hockey player country of origin turned ages of played into averages, one could likely ""see"" which players were on average younger, but not find out which was youngest or oldest.</p><p>The important visualization research challenges are about how to reduce data volumes to visually manageable forms, how to determine what aggregate properties are important to preserve in the transformation (these typically become domain specific, e.g., transforming weather data and hockey player data into images is unlikely to produce the same kind of pictures), and how to evaluate alternative transformations by confirming the inferences that most humans draw from such pictures.  A more recent challenge included amongst these foundation ones is that provided by modern touch screen technologies: in addition to preserving data properties, reducing ambiguity, and confirming preferred pictures for efficient human inference, the question of visual manipulation begs the challenge of what are appropriate repertoires of picture manipulation, and how well they can help reveal data properties within pictures.</p><p>Overall, the challenge of visualization research is to consider property preservation, avoidance of ambiguity, confirmation of best inference support, and appropriate repertoires of picture actions to improve human understanding of data. All advances help provide higher value exploitation of all forms of scientific and business data.</p>""571464,""Goel, Ashvin"
"590724"	"Gross, Warren"	"Integrated Signal Processing Systems for the Internet-of-Things"	"The Internet-of-Things (IoT) is the interconnection on a massive scale of autonomous smart objects, enabling innovative applications in health, infrastructure, environment, agriculture, and industry. The goal of the proposed research program is to develop signal processing hardware for smart objects in the IoT addressing two critical challenges: <em>energy efficiency</em> and <em>security</em>. We will investigate fundamentally new approaches addressing the very tight cost, size, and power budgets of smart objects in the IoT that cannot be met by conventional architectures and methodologies by developing neuromorphic (""brain-like"") low-power signal processing hardware. The proposed methodology is organized around three research thrusts: i) stochastic computing for low-power signal processing in smart objects, ii) associative memory for secure network processors in the IoT, and iii) polar codes for secure, low-complexity wireless IoT communications. Stochastic computing is the design of hardware that computes with random pulse trains, resulting in remarkably simple logic circuits that offer a natural way to trade-off power consumption with the precision of a computation. Another brain-inspired structure, associative memory, integrates memory with searching and maps naturally to very efficient logic-in-memory architectures. Both of these techniques are inherently fault-tolerant and therefore able to gracefully accommodate errors induced when reducing the supply voltage to minimize power. Polar codes are a breakthrough capacity-achieving error-correction technique for digital communications that can provide secure wireless communications, with a low-complexity decoding algorithm. This work has the potential to introduce a disruptive paradigm for low-power signal processing systems design. In the short term, our approach has the potential to greatly reduce power consumption of critical signal processing components for smart objects in the IoT, demonstrate the advantages of polar codes, driving them towards standards, and train highly qualified personnel to be employed in Canadian industry. In the long term our work will help demonstrate the potential of neuromorphic architectures for emerging computational problems.""586874,""Grosse, Roger"
"590842"	"Gupta, Manisha"	"Novel flexible optoelectronics for sensing applications"	"<p>Traditionally, semiconductor devices including optoelectronic devices like lasers and light emitting diodes are fabricated on hard inflexible substrates; these have been used for a large number of applications including computers, portable electronics and biomedical applications. However, flexible electronics is an increasingly important area as it can be used for large scale displays which can be rolled up into small rolls, portable power sources using photovoltaics, conformed on skin for sensing, large scale organic light emitting diode (OLED) lighting and many other applications. The biggest advantage of the flexible electronics devices is their application in areas where traditional rigid electronics cannot be used like disposable sensors, displays, and cheap solar cells.<br />Optical materials play an extremely important role in our day-to-day life with their utilization in items of everyday use like lighting, displays, etc. Flexible light sources have several advantages including low power consumption; they can be applied in general lighting and portable electronics along with biophotonics and biomedical applications. The biomedical applications include therapeutic applications like nerve stimulation, photodynamic therapy and LED based biosensors.<br />This research program will focus on the development of novel flexible optoelectronics solutions for lighting, photovoltaic and sensing applications. It will combine using organic and inorganic materials to fabricate flexible electronic and optoelectronics components. The approach that will be pursued to develop flexible optoelectronic devices will be to use low temperature growth techniques like pulsed laser deposition to grow optical materials directly onto flexible substrates. These substrates are biocompatible, which will enable biosensing in addition to other sensing applications. The research program will involve developments in optics, materials and devices to develop flexible optoelectronic devices, biosensors and biophotonic applications.<br />The major advances in the field of flexible electronics that will be accomplished by this research program are the integration of flexible electronics with optical components, the development of tunable light sources that will provide multifunctionality to the devices, and the focus on developing viable, efficient and reliable hybrid devices integrating inorganic and organic components.</p>""570772,""Gupta, Radhey"
"589957"	"Haccoun, David"	"Communications cognitives sans fil efficaces et techniques de codage de haute fiabilité"	"Résumé de la proposition -Un problème fondamental des communications modernes consiste à concevoir et réaliser des systèmes et techniques puissantes , efficaces et pratiques pour assurer une grande fiabilité de l'information transmise quel qu'en soit le type et quelque soit le canal de transmission ,terrestre ou par satellite Une telle fiabilité nécessite l'utilisation de techniques puissantes pour corriger les erreurs de toutes sortes dues au bruit dans les canaux. Dans ce projet une attention particulière est portée sur une classe de codes puissants et efficaces appelés codes convolutionnels doublement orthogonaux, sous leur forme non récursive (dénotés CDO) et récursive (dénotés CDO-R) ,ainsi que sur leurs techniques de décodage itératif et leur performances d'erreurs. Une autre problématique reliée à l'accroissement des utilisateurs concerne la pénurie de spectre de fréquences disponible dont l'utilisation est réservée à des utilisateurs licenciés appelés Utilisateurs Primaires( dénotés PU)..Une grande partie du spectre licencié étant sous-exploitée,. les systèmes dits radio-cognitifs,( dénotés CR) permettent d'implémenter un accès efficace au spectre licencié par des utilisateurs non licenciés, appelés Utilisateurs Secondaires (dénotés SU ) et ce, sans dégrader les performances des PU. Dans ce projet une approche novatrice utilisant la technique de l'étalement de spectre est analysée pour faire cohabiter les signaux primaires (PU) et secondaires (SU) Naturellement cette technique devra assurer une qualité de service donnée aux SU sans dégrader celle des PU. Le projet est donc essentiellement articulé autour des deux domaines complémentaires : codage et décodage par codes CDO-R , et communications Radio Cognitives efficaces utilisant l'étalement spectral. Pour les codes CDO-R, il portera sur l'analyse, et l'évaluation des performances de leurs décodage itératif, ainsi que sur leurs applications pratiques . A cet effet, des solutions aux problèmes importants de minimisation de la latence de décodage et de la terminaison des trames de données seront proposées et analysées Pour les Communications Radio Cognitives, l'étalement spectral des transmissions SU sera développé afin d'établir des communications SU adéquates non tributaires de la présence de blancs ou trous dans le spectre des PU, et ce, sans dégrader qualité de service des PU .Une procédure novatrice qui consiste à effectuer l'étalement de spectre par codes convolutionnels de très faible taux de codage est utilisée afin d'ajouter un gain de codage au gain de traitement dû à l'étalement, En plus de s'inscrire dans les courants actuels des recherches en communications sans fil pratiques et efficaces, le projet apportera des solutions originales importantes dans la simplification du codage et décodage itératif , ainsi que dans les systèmes et techniques de communications CR pratiques et efficaces..""587836,""Haché, Alain"
"590777"	"Han, Jie"	"Toward Energy-Efficient, Bio-Inspired Circuits and Systems for Error-Resilient and Biomedical Applications"	"Many of us get our computers, laptops or phones replaced every few years. Technology advances due to the continuous miniaturization of electronic devices, such that a larger number of devices can be packed into a single chip, while the cost has been kept relatively stable. This trend has slowed down, however, and is predicted to end in less than a decade. A transistor, the basic functional unit in a circuit, is now sized in just a few nanometers, that is, a few billionth of a meter. At such a small scale, it is difficult to fabricate all transistors uniformly and make them operate reliably. The current method to ensure a reliable operation is to apply a larger power than it is often necessary, so electronics still consume a lot of energy. On the other hand, many computer applications, such as multimedia, voice recognition and web search, do not always require a fully accurate result and a ""good-enough"" result is often sufficient due to many factors such as human perceptual limitations. This class of applications is considered imprecision-tolerant or error-resilient. One objective of this research program is to address the energy-efficiency and error-resilience issues in nanometer-scale electronics by developing new and innovative computational structures that employ approximate, stochastic and the brain-inspired neuromorphic computing techniques. These new techniques allow computing systems to trade off quality for energy. <br /><br />As a transistor is to a computer, a cell is the basic unit of life in a biological system such as a human body. The advances of computing techniques also provide opportunities to help address some emerging biomedical issues. For example, computational models have been used to help understand how a gene network functions in a cell. Validated computational results provide additional knowledge to our understanding of biological systems. The other objective of this research program is to apply computing techniques to the modeling and analysis of biological networks by exploiting the similarity between an electronic circuit and a biological network. Such a network can be a genetic network or a signaling pathway in a cancerous cell. The obtained results will help to investigate gene intervention-based therapeutic methods for some genetic diseases such as cancer. <br /><br />This research program addresses some of the fundamental and challenging issues faced by the information technology industry and the biomedical research community. It will take a truly interdisciplinary approach to leveraging the interactions between computer engineering, computational biology and biomedical engineering. This program will also facilitate the training of highly qualified personnel (HQP) with skills highly demanded by the high-tech sectors in Canadian industry. Such skills will be crucial to the long-term growth of the economy, and thus will be of significant economic importance to Canada.<br />""578388,""Han, Jiho"
"590226"	"Harutyunyan, Hovhannes"	"Optimal message dissemination problems in graphs"	"<p>Nowadays, Computer networks, from local area networks to the Internet, have become essential in many aspects of modern society. The main purpose of these networks, is to share and spread information. Efficient communication becomes particularly important when a computer network supports a distributed file or database system, where large amount of information needs to be disseminated among the computers in the network. The performance of the information dissemination often determines the efficiency of a whole network or a parallel system. Many different information dissemination problems for efficient message dissemination are considered in practical networks. Therefore, based on this differnt theoretical models for message dissemination are considered  by placing constraints on the amount of information available for each processor, number of senders and receivers, length of the message, capacity of the links and and different processors, transmission or processing delay, the number and the different type of faulty links, the number of messages, etc.  New challenges appear with the advancement of wireless and ad-hoc networks. Efficient communication in such networks require the design of message dissemination algorithms where the nodes do not have knowledge of the global network. Moreover, the nodes only have a limited knowledge of their local neighborhood. Considering the size of internet and other large-scale networks make efficient communication under the above constraints very challenging. All these problems therefore require careful mathematical analysis and algorithmic tools to overcome these challenges. My research proposal will investigate all the above research challenges. The proposed research program will be a continuation of my research work done at Concordia university since 2001. The research work will span from investigating pure combinatorial properties of the main communication parameters to efficient algorithm design and their actual implementation on commonly used interconnection networks in the literature and also extensive simulations on large practical networks generated by well-known network simulator NS-2. </p><p>The proposed research program will investigate message dissemination problems in computer networks for parallel and distributed computing. Different parallel and distributed primitives such as routing, broadcasting, multicasting and gossiping will be considered under various information dissemination models. </p><p> </p>""577537,""Harvard, MichaelJustin"
"590334"	"Hendren, Laurie"	"Compilers, Tools and Languages for Scientists"	"<p>Computation and programming are becoming increasingly important aspects of the work done by scientists.   However, scientists face many challenges and do not necessarily have the proper languages, compilers, and software tools to efficiently and safely accomplish their goals.  A 2010 article in Nature, entitled ""Why scientific programming does not compute"", describes the increasing importance of programming, the challenges faced by scientists, and the consequences when the software developed is not up to a proper standard.</p><p>Other important current trends in programming languages and computer architectures also impact on this problem.   Dynamic programming languages such as MATLAB, R and Python are becoming increasingly popular as application programming languages for scientists,  and JavaScript is becoming increasingly popular as a way of programming on the web.   On the architecture side, scientists have at their disposal a wide range of devices including workstations, laptops, and mobile devices, as well as access to many remote devices via the web.   Furthermore, almost all devices now support some combination of multicore CPU and GPUs.    <br /><br />The overall objective the proposed research is to develop new compiler techniques, software development tools, and domain-specific languages which will help scientists program effectively and which will make the best use of computer resources.  Our proposed research will build upon our recent work on the McLab project (www.sable.mcgill.ca/mclab),  which has concentrated on compiler/VM tools and techniques, language extensions, and tools for MATLAB.  As has been the case historically in our research, we will develop new ideas,  implement these as open source projects,  and test them on a variety of benchmarks from a wide variety of scientific disciplines.</p><p>We have identified five related research directions:  (1) we will support automatic translation of MATLAB programs to JavaScript, so as to allow Scientists to easily run their code on the web; (2) we will continue our work on effective optimization and code generation for multicore CPUs and GPUs; (3) we will leverage static and dynamic program analysis to develop programming tools for scientists, (4) we will continue our work on benchmarks for scientific computations, expanding to a variety of dynamic languages; and (5)  we will work on new domain-specific programming languages in order to ease the programming burden on scientists.<br /><br />The proposed research is intended to be used by both compiler researchers (new techniques and open source tools),  and scientist end-users.   The research is novel in that it is targeting an increasingly important area of enabling programming for scientists,  and in generating efficient code for both modern CPU/GPUs and web-based systems.</p>""592027,""Hendren, Laurie"
"590854"	"Heywood, Malcolm"	"Permutation based task transfer for genetic programming"	"<p>The general context for this research proposal is that of genetic programming (GP) as applied to learning decision making policies for agents operating in environments with delayed payoff (or reinforcement learning). The specific focus of the proposal lies in developing a framework for systematically scaling GP to more difficult versions of tasks with delayed payoff than have previously been considered. In particular we are interested in scenarios in which solutions for a simpler initial `source' task are then `transferred' to a more difficult but related (target) task; or a form of transfer learning. The insight of this work is to make use of the capability of GP to identify solutions that make use of subsets of state variables. This then provides the basis for redeploying solutions discovered under the source task such that more difficult tasks can be solved. The potential benefits of adopting such an approach are that: 1) it is not necessary to continuously rediscover policies from scratch; 2) increased success in / or better solutions to the ultimate target task; and 3) lower computational overhead as measured against finding solutions to each task.<br /><br />Two specific target domains will be used to illustrate the approach: 1) learning policies to play soccer in the continuous valued simulated 2D world of RoboCup keepaway; 2) learning general policies for solving the 3 by 3 Rubik cube. The keepway soccer task represents a benchmark for multi-agent learning, hence has a history of previous results as well as posing tasks of incrementally increasing difficulty. The Rubik cube task has had little previous history as a benchmark for learning algorithms of any form. Instead solutions have taken the form of deploying some form of exhaustive search. Both tasks represent examples of complex task domains that have very large state-spaces (potential number of legal states), but possess underlying properties (regularities) that GP should be able to discover specific instances of. The basic hypothesis of this research is that once an instance of a context dependent strategy is identified for solving some subset of an initial task, then we should be able to use this as the basis for generalizing to many more instances of the task through a deterministic process of variation in the policy's references to the state variables. Naturally, for the process to scale, we need to avoid artificially introducing pathologies into the search. In short, the sum of initial policies from which a later policy is constructed needs to exceed the mere sum of its parts.</p><p>Success in these objectives would provide a general framework for scaling GP to a wide range of tasks with delayed payoff. Such tasks are of widespread interest to the GP community because they represent some of the most expensive, if not the most expensive, set of task domains for applying GP to. Moreover, the two task domains are widely acknowledged to be of a particularly challenging nature.</p>""589041,""Hibbard, Glenn"
"590012"	"Hinzer, Karin"	"Ubiquitous Solar: Novel solar energy devices and systems costing less than $1/W"	"<p>Solar power is revolutionizing the way we generate and distribute energy around the world. This research will develop the skilled personnel necessary for the rapidly expanding solar industry and demand for solar power, promoting effective knowledge transfer and filling employment needs of existing and future Canadian industry.</p><p>The proposed research will develop new solar cell and system designs capable of <$1/Watt system prices and matching grid pricing in most locations worldwide. Achieving this goal requires lower costs and higher power conversion efficiencies. Lower cost manufacturing is possible by minimizing system complexity, improving system design, and lowering material cost and usage. Higher efficiency increases the power extracted per unit area, reducing costs of land, materials, and other system components. Simultaneous optimization of both cost and efficiency is required to reach grid-equivalent pricing.</p><p>The title `Ubiquitous Solar' refers to the universality of the solution, whether for small- or large-scale deployment, for on-grid or isolated use, and for maximum power generation in low or high sunlight locations. The program will focus on 3 key innovation streams:</p><p><strong>Stream 1 - 50% EFFICIENT PHOTOVOLTAIC (PV) CELLS</strong> (2/5 budget)<br />In any PV system, a 10% increase in solar cell efficiency typically yields a 20% higher return on investment, reducing the cost of electricity. The most effective path to high efficiencies is with a multi-junction solar cell structure. Three design approaches (wafer bonding, spectral splitting, and mechanical stacking) will be evaluated for performance higher than the best devices presently available. Options with the potential to realistically achieve 50% efficiency will be prototyped.</p><p><strong>Stream 2 - NEXT GENERATION CONCENTRATED PHOTOVOLTAIC SYSTEMS</strong> (2/5 budget)<br />Concentrator optics focus large areas of sunlight onto small PV cells. We will study, design and characterise new concentrator systems which integrate the novel higher efficiency cells of Stream 1. We will develop new hybrid systems incorporating regular flat panel cells to complement the concentrated system in regions or conditions where direct normal illumination is intermittent or consistently low. These designs offer the potential to yield the highest yearly energy per peak-power of any system available.</p><p><strong>Stream 3 - NANOGRIDS INTEGRATING PHOTOVOLTAICS AND STORAGE</strong> (1/5 budget)<br />Nanogrids are small managed local systems (AC or DC) that operate separately or simplify connection to the grid. They can include solar cells and local storage (batteries,  capacitors) to ensure continuous back-up during power cuts or low-sunlight periods. We will maximize the proportion of solar power possible on the grid, optimizing the PV units to work smoothly with the grid and storage systems. This will include options for direct DC supply to electronics (e.g., computers) that normally require power adapters.</p><p> </p>""592186,""Hinzer, Karin"
"589608"	"Hossain, Shahadat"	"Computing with Sparse and Structured Matrices: Mathematical Derivatives and Beyond"	"<p>This research is mainly concerned with the design of efficient computational methods for the computation or estimation of mathematical derivatives and related scientific computing problems. Our approach is based on the synergy of combinatorics, graph theory, and numerical linear algebra to solve computational problems where the scale of the problem calls for innovative strategies for algorithm design and their computer implementation. An important component of our research methodology is the identification and exploitation of  information such as structure, sparsity, and concurrency. <br /><br />Modelling and solving scientific problems arising in diverse application areas - from computational finance to meteorology to electricity grids, share a common theme: numerical calculations on matrices that are sparse or structured or both. The MIT general circulation model, MITgcm (a numerical model to study Earth's climate), is an example of the so called ""exascale"" application where, even one simulation run of the underlying model requires computational resources of an unprecedented scale. An essential calculation in such a numerical model is concerned with the evaluation of sensitivity of the model with respect to some model parameters that are unknown or poorly known. Excellent research in algorithmic differentiation (AD) techniques in the recent years enabled scientists to ""automate"" sensitivity calculation for MITgcm computer code. Exploiting information such as sparsity and structure of the underlying problem is crucial in designing effective algorithms for such applications. Moreover, the evolving architectural complexity of modern high-performance computing systems pose considerable challenge for effective software implementation of innovative algorithms. <br /><br />In broader terms, the results from this research  are expected to find applications in scientific and engineering calculations that involve solving system of nonlinear equations or optimization (minimization or maximization) of certain quantities. The training component (for graduate/undergraduate training) of this research will contribute  to the pool of highly qualified personnel (HQP) in Canada. All of the trainees supported by the past discovery grants went on (or currently considering) to undertake graduate studies (at M.Sc. or Ph.D. level) or are actively contributing to the Canadian Industry (e.g., Syncrude, Telus, University of Lethbridge).</p>""576442,""HossainiFard, Navid"
"589910"	"Hu, Alan"	"Automated Formal Verification at the Hardware/Software Boundary"	"Computer chips, computer systems, and computer software are the most complex things ever designed by humans.  For example, even the simple programs written by our students in their first computer science course have far more possible behaviors than there are photons in the universe!  Not surprisingly, it's hard to get all the details right.  In fact, companies now spend the most effort not on design, but on verification -- the task of determining if the system behaves correctly.<br /><br />Formal verification is the use of mathematical logic to aid the verification process.  My research emphasizes automatic formal verification, in which a computer program analyzes the system being designed, automatically finding bugs or proving properties about the design, with minimal human effort beyond specifying what is desired.  Research breakthroughs over the past twenty years have made formal verification indispensible for verifying computer hardware -- all major computer companies use formal verification on their computer chips.  This has been a big success story of academic research creating enormous value for society.  With more recent research breakthroughs, automated formal verification is becoming mainstream for software verification as well.<br /><br />Much of the impact of computing, however, comes not from hardware or software, per se, but from actual, life-changing, ""smart"" products -- the smartphone being the most obvious example, but with the coming Internet of Things, all sorts of everyday objects will become smart and connected.  Smart products are the result of tightly integrated hardware and software, and it's obviously important to get this integration correct.  However, companies are having great difficulty creating these new products, because hardware and software have fundamentally different models of computation, resulting in mistakes when integrating the two.<br /><br />This proposal focuses on developing theoretical and practical tools for verifying the interaction between hardware and software.  In some respects, this combines all of the challenges of automated formal verification of hardware and software, because verification must reason about both.  On the other hand, I believe that there are certain characteristics and design patterns that can be exploited to enable practical, automatic formal verification at the hardware/software boundary.  If successful, this research will generate a large quality and productivity boost for creating any smart product.<br />""582931,""Hu, Amy"
"589394"	"Huang, Jimmy"	"Searching and Analyzing Big Data: Context-sensitive and Task-aware Approaches"	"<p>Using Google is easy. Finding information using Google is another matter. Over the decades, significant progress has been made in Information Retrieval (IR). However, IR is far from a solved problem and many challenges remain. First, most Web search engines take a short text query as input and output a ranked list of documents. The retrieval decision is made primarily based on the current query and document collection. The results for a given query are usually identical, independent of the user or the context in which the user made the request. Second, IR is an interactive process. With the current document-centered retrieval paradigm, interactive retrieval is treated as a sequence of independent simple retrieval decision making steps. However, it has been brought into attention that analysis of task-aware user sessions, which contain a sequence of requests submitted by a user to fulfill an information need, provides useful insight into the query behavior of the user. Third, most of present IR systems use keywords to query and index documents. However, this traditional keyword-based IR model provides little semantic context for the understanding of user information needs, which can lead to mismatch between query and document in search. Ideally, one would like to see the query and document match with each other, if they are topically relevant. Thus, the integration of semantic context according to the user's information need and the user's understanding of the documents into IR systems is needed to improve the IR performance. Fourth, organizations are now increasingly dealing with petabyte-scale collections of data. Context-sensitive and task-aware approaches become more challenging when dealing with big data. Hence, it is important to propose new algorithms and models that can effectively and efficiently process big data and implement the context-sensitive and task-aware approaches in big-data scenarios.</p><p>In a world where data are growing at extraordinary rates, there is a huge demand for searching and analyzing big data more accurately and effectively to discover useful information. This research program tackles the problem of searching and discovering useful information from big text data. The long-term objective of the proposed research is to overcome the limitations of the existing IR methods and formally develop a new retrieval paradigm called context-sensitive and task-aware information search for big data. In particular, (1) we will develop a new theoretical retrieval framework for capturing rich user information and providing personalized search results; (2) we will develop novel task-based retrieval methods for context-sensitive information retrieval to optimize the long-term retrieval utility over an entire retrieval session; (3) we will develop new models for automatically analyzing and searching big data to efficiently extract knowledge and perform semantic matching. </p>""577147,""Huang, JimmyXiangji"
"589875"	"Hung, Patrick"	"Safety Enforcement Framework for Bring Your Own Device in Service-Oriented Architecture"	"Bring Your Own Device (BYOD) is an emerging application distribution model that encourages users to use their own mobile devices such as smart phones and tablet computers to access various online and mobile services in an organization, for example at work through a technical infrastructure or network, to support a workflow as long as the users agree to comply with a given safety policy in the organization. Since the services may contain sensitive information or even trade secrets of an organization, the safety policy usually specifies the security and privacy rules such as Single Sign On (SSO) and Lightweight Directory Access Protocol (LDAP) check-up of authenticity in End User Level Agreement (EULA) format as realistic regulatory measures and assessment. A Gartner report predicts over 30% of organizational BYOD strategies in the world will leverage personal applications and data in 2016 as well as half of the organizations will require employees to bring their own mobile devices for work purposes in 2017. Recently BYOD is being adopted by many traditional industries such as airline companies and even toy manufacturers around the world. From the perspective of the organization, BYOD is typically perceived as less secure and trustworthy because the organization may not be able to exercise as much control over the mobile devices as it may have over organization-issued devices. In addition, the mobile device may be used for both work related and private activities, resulting in both corporate and private data accessed and stored on that same device. From the perspective of users, there may also be concerns regarding the privacy issues of their private data while the device is interacting with the services. In Canada, the Personal Information Protection and Electronic Documents Act (PIPEDA) sets out ground rules for how private sector organizations may collect, use, or disclose personal information in the course of commercial activities. Failing to comply with these legislations may lead to civil and/or criminal penalties and/or imprisonment. Thus the organizations must comply with PIPEDA in order to enact BYOD in Canada. This proposed research program presents a technical framework of a mobile services cluster to publish, find and bind mobile services in an emerging movable and composable structure. The mobile services are based on the technical architecture of Services Oriented Architecture (SOA) and Web services. This research program focuses on the theoretical model with a technical XML-based framework for enforcing and managing a safety policy between the mobile services cluster and mobile devices from the perspective of both organizations and users in this BYOD paradigm. The built models and research results will bring a formal solution to support a secure BYOD paradigm, especially in a mobile services cluster such as Mobile Cloud Computing (MCC), for both organizations and users in Canada.""569782,""Hungr, Oldrich"
"591072"	"Ilie, Lucian"	"Bioinformatics Tools for Genomics and Proteomics Research"	"<p>The genome of an organism consists of the complete set of its DNA. Genomics concerns the understanding of the genome sequence and functions. DNA encodes the genetic instructions for the development and functioning of all organisms, including the production of proteins. Proteomics studies the structure and functions of all proteins. Proteins usually perform their functions by interacting with other proteins. <br />The possibility of obtaining the sequence of the human genome opened unprecedented possibilities for cancer research, genetic disorders, disease control, personalized medicine, etc. Sequencing technologies enable sequencing of genomes at ever decreasing costs but processing and analyzing huge amounts of data is made possible by continuously evolving algorithms and software programs. The ongoing main objective of my research program is the development of advanced algorithms and tools critical in analyzing such big data to uncover essential aspects of the structure and function of DNA and proteins. Short term objectives include improvements in error correction of DNA sequencing data, genome and metagenome assembly, genomic variation detection, whole genome tiling, protein-protein interaction (PPI) prediction, and alignment of PPI networks. <br />These bioinformatics tools will impact many applications in biological and health sciences by enabling and accelerating the discovery process in these areas, helping to improve the health of Canadians. Original and innovative ideas used for building such tools will help advancing the knowledge in several areas of Computer Science as well. <br />The research objectives proposed involve effective HQP training in an interdisciplinary and collaborative environment.</p>""571779,""Ilie, Monica"
"589748"	"Jacobsen, HansArno"	"Accelerating Data Analytics Through Emerging Software-Hardware Mechanisms"	"The objectives of this research are to investigate algorithms, design, and architectures for enabling an efficient real-time event data analytics platform to support multi-query processing over high-volume and high-frequency event streams. To achieve these objectives, we plan to leverage modern hardware mechanisms, such as Field Programmable Gate Arrays (FPGAs) and Application Processing Units (APUs) in our design. We strive to achieve line-rate multi-query processing by exploiting unprecedented degrees of parallelism and potential for pipelining, only available through custom-built, application-specific and low-level logic design. Furthermore, we intend to compare against the use of emerging Graphical Processing Units (GPUs) in our design, evaluating above all performance, but also development effort.<br /><br />The need for efficient real-time analytics is an integral part of a growing number of data management problem scenarios such as business analytics, big data processing, and complex event processing.  Common among all these scenarios is a predefined set of continuous queries and an unbounded event stream of incoming data that must be processed against the queries in real-time.<br /><br />However, the vision of enhancing data analytics computations with FPGAs has a few caveats that make acceleration a challenging undertaking. First, current FPGAs are still much slower compared to commodity CPUs. Second, the accelerated application functionality has to be amenable to parallel processing. Third, the on-/off-chip data rates must keep up with chip processing speeds to realize a processing speedup by keeping the custom-built processing pipeline busy. Finally, FPGAs restrict the designer's flexibility and the application's dynamism, both of which are hardly a concern in standard software solutions.<br /><br />By meeting these challenges in our approach, we propose an FPGA-based real-time analytics platform that supports line-rate processing of data streams over a collection of continuous queries.  We plan to explore this problem space along the following three dimensions: (1) Design high-throughput, custom circuits to implement the relational algebra operators, (2) design multi-query optimization techniques amenable to the features offered by FPGAs, (3) design software-to-hardware multi-query processing techniques that map a set of queries into a global query plan for processing by our custom circuits.""570538,""Jacobson, Jr, Michael"
"591083"	"Janicki, Ryszard"	"Concurrency Theory and Non-Numerical Approximation"	"<p>Both concurrent systems and approximation problems abound in human experience but their fully adequate conceptualization as yet eludes us. Our increasing dependence on ever more complex systems in the management and control of human affairs and activities increases the urgency for developing more adequate and preferable more formal concepts to maintain reliable control over systems we have created.</p><p >The solution of the problem of correct specification of the design and verification of its behaviour becomes crucial, and a satisfactory conceptual apparatus for rigorous specification and verification becomes essential.</p><p >The project will explore theoretical issues involved in the formal reasoning about concurrent systems and approximate reasoning. First general aim is to develop a framework, based on the concept of Generalized Causality modelled by Discrete Relational Structures (developed by Janicki and Koutny) and generalization of Mazurkiewicz traces, to support the design and the verification of sophisticated concurrent systems. The second general aim is to strengthen foundations and improve applications of non-numerical approximation and non-numerical ranking techniques.</p><p >What connects all three topics is methodology, as the scientific method is basically the same, i.e., discrete mathematics, especially set, relation and automata theory and of course formal logic.</p>""587150,""Janin, Amelie"
"589623"	"Jatskevich, Juri"	"Next generation tools for modelling and analysis of evolving power and energy systems"	"Electrical energy systems are fast evolving with rapidly increasing complexity, wider acceptance of DC-based technologies, and integration with communication systems and sensors. Sophisticated computer models are essential for operation of electric grids, planning purposes, design and integration of future renewable energy sources and systems. Thousands of engineers in control centers and research facilities around the world are working full-time developing models and conducting studies. Since the models are used by engineers and researchers many times (often iteratively during the design cycle), both the model accuracy and numerical efficiency (simulation time) are very important. Even a fractional increase in simulation speed will result in very significant savings of the engineering time worldwide. However, models of rotating machines and power electronic components are typically the limiting bottle-neck, and the present modeling tools are falling behind of the needs.<br />Canada has been a leader in developing the state-of-the-art solution approaches and computerized tools that enable design and operation of electric power systems of various scales. The electromagnetic transient programs (EMTP) e.g. ATP, Microtran, EMTP-RV, PSCAD, RTDS, RT-LAB, the Matlab's SimPowerSystems, and Powertech Labs transient stability tools DSATools, etc., all have been originated and/or developed by Canadian researches and are now overwhelmingly used throughout the world as industry-standard tools. The proposed research continues UBC's long-time tradition in advancing the power systems analysis tools. We are developing the most computationally efficient synchronous and induction machines models that achieve constant-parameter interfacing circuits (and constant conductance submatrices) and avoid the limitations and numerical instability of the traditional qd0-models. We are developing a revolutionary parametric approach for automatically constructing the dynamic average-value models of switching converters and machine-converter systems (with capability of including significant harmonics). The new/advanced models are best suited for future simulation environments that will allow intermixing EMTP-type, state-variable-based, and dynamic/static phasors solution approaches. The proposed research will enable the next generation of transient simulation tools with new capabilities and extended range of applications, capable of much faster simulations of large-scale systems and yet preserving high-level of detail and accuracy at the localized sub-systems as required. These tools will have a tremendous impact on how simulations are used by engineers and researchers in the near future. The results of this research are essential in enabling the new paradigms of AC/DC microgrids, alternative energy sources and storage, which together define the evolving future smart grid.""575333,""Jatskevich, Juri"
"590312"	"Jeyasurya, Benjamin"	"Power System Stability Monitoring and Control Using Synchrophasors"	"<strong>Blackout ! Blackout !</strong> We really do not want it. In Canada, we take continuous electricity supply for granted. At the flip of a switch, we<br />turn on lights, heat, air conditioners, ovens, microwaves, washers, dryers, etc. Blackouts cause significant economic loss, affect the<br />well-being, safety and security of a society. Power companies like to reduce (if not eliminate) the frequency of blackouts or implement<br />suitable measures to restore the system as quickly as possible. The challenge is to prevent events from spreading into cascading failure.<br />The blackouts in different part of the world have shown clearly how the power system can become vulnerable, causing significant<br />economic loss. Some of the trends in the electric utility industry that must be considered now are the aging electrical infrastructure, the<br />aging workforce, and the growth of renewable energy sources etc. The tasks required to ensure a secure and reliable supply of electrical<br />energy are very complex.<br />Synchrophasor technology is the most significant electric grid monitoring and control center data improvement tool introduced over the past decade. Synchrophasors are time-synchronized phasor measurements. Phasor measurement units (PMUs) are the most widely used synchronized measurement devices for power system applications. Each PMU typically monitors 6-8 phasor quantities, such as voltages, currents, and frequency, all of which are time-stamp synchronized using a global positioning system (GPS) satellite. With the increasing number of PMU installations in different utilities, there is a need to develop real-time tools which utilize the potential of synchrophasor technology. This is the motivation behind the proposed research. Using a medical analogy, one can think of the conventional measurements (using Supervisory Control and Data Acquistion systems) as X-Ray and PMU measurements as MRI.<br /><p>The new and innovative approach in this proposal is an integrated approach to stability monitoring and control of power systems using available synchrophasor measurements. As an example, when the power system response is predicted to be unstable, remedial actions will be ordered by the stability controller. Time-synchronized measurements provide the starting point for the implementation of these controls. Voltage stability monitoring and control and transient stability monitoring and control will be the key focus of this research.</p><span style=""line-height: 1.5em;"">The significant contributions of this research will be the development of strategies for power system performance enhancement using the capabilities of synchrophasors. This Discovery Grant will provide opportunities for many Graduate students to develop their skills and </span><br />become resourceful in applying synchrophasor technology for electric power utilities. Particularly for Canadian electric utilities the results of the research will be useful in operating the power system with a high level of security.""589232,""Jha, Amitabh"
"590800"	"Johns, David"	"Long-Range Low-Energy Systems for the Internet of Things"	"The internet of things refers to the many devices that will be connected to the internet over the next number of years. According to various sources, the number of devices connected to the internet by 2020 is estimated to be from 26 to 50 billion (the number of all devices connected to the internet in 2012 was estimated to be around 8.7 billion). Many of these new devices will be battery powered (or energy harvested) embedded systems where long-range communcations and low-energy are crucial to their success.  This is particularly true in cases where the devices needs to be small and light where the battery can dominate the size and weight of the device.  In addition, for long-range communications, there will likely be a need for repeaters (devices that relay the wireless information to extend the range) and once again, low-energy operation will be critical.<br /><br />The long term goal of this research is to develop wireless communication circuits and systems that can achieve long-range communication while consuming the lowest energy per information bit transmitted or received.  To achieve this challenging goal, many theoretical and practical obstacles need to be overcome.  For example, when transmitting in a peer-to-peer device situation, there is a tradeoff in improving receiver sensitivity (i.e. having a better noise figure) with reducing transmitter power in order to minimize overall energy usage.  However, since many other circuits (such as a VCO, VCO drivers, baseband circuits, etc) may not scale with receiver sensitivity or transmitter power, one needs to account for the entire receiver/transmitter energy consumption.  In addition, it is often useful to transmit over a narrowband channel to save energy, however, in the case of burst transmission, it takes more time to power up a narrowband channel for either transmission or reception than a wideband channel so the power-up time needs to be accounted for in total energy consumed for the information transmitted/received.  We hope to bring new insights into the system and circuit level implementation for these difficult long-rang low-energy systems.""575645,""Johns, David"
"589929"	"Joslin, Chris"	"Next Generation Video Coding"	"<p>The advancing pace of video technology for entertainment, conferencing, and educational purposes has increased enormously in the last two decades due to the wide availability of high-speed internet and the ability to effortlessly produce digital content. We now have access to high-definition downloadable video, ultrahigh definition digital cinemas, and high-definition and multi-view video streaming content for numerous applications. The current standard for most applications is high definition or HD (1920 x 1080 pixels), with a moving trend towards ultrahigh-definition or UHD (4096 x 2048 pixels, commonly referred to as 4K, and above) devices and content. </p><p >The latest standard available that can handle the compression of this type of content is the High Efficiency Video Codec, or HEVC (more commonly known as H.265) and is the latest in a long line of successful video codecs based on the H.26x methods (or MPEG depending on the development group). As most of the techniques are mainly innovations based on methods developed in the 60's, 70's and 80's we propose to examine the next generation of video coding methods. Within this program of research we are looking at the development of implicit coding techniques (converting sequences into implicit curves and coding their control points into the bit-stream), holistic coding techniques (attempting to code the scene and focusing on background/foreground objects rather than blocks), and pattern-based coding (using pre-existing patterns to various types of information) Here is my summary</p>""589849,""Josselyn, Sheena"
"590213"	"Karimi, Houshang"	"Power Quality Improvement of Smart Microgrids"	"Smart microgrids are promising solutions to integrate renewable energy sources (RESs) into the utility grid and allow the customers to participate in the energy enterprise. In the near future, smart microgrids will contribute to affordable electricity for customers, and will pave the path to rely on an emission-free form of energy. Smart microgrids are usually categorized into ac, dc and hybrid ac/dc, and provide several advantages over conventional power systems, e.g., they provide increased reliability, security, efficiency, allow the exploitation of the RESs through distributed generation (DG) units, reduce losses, and are beneficial for health, the climate, and the world economy. Despite their advantages, there exist several technical challenges associated with the efficient design and use of smart microgrids. Some of the main challenges are power quality (PQ) degradation of the utility grid, and stability issues due to the intermittent nature of the RESs and load variations. Moreover, the proliferation of nonlinear and unbalanced loads in distribution networks results in a number of PQ and stability problems for smart microgrids. Poor PQ results in a multitude of problems for both utility grid and consumers, e.g., power losses, malfunction of sensitive equipment, power interruptions, poor performance of the loads, safety issues, and incur huge costs. For example, a survey conducted by EPRI-CEIDS shows that the North American economy loses <strong>more than 24 billion dollars per year</strong> due to various PQ problems. <br />In this Discovery program, we aim to overcome some of the challenges associated with PQ and stability of ac and hybrid ac/dc microgrids. More specifically, in the proposed research, we will tackle the following tasks:<br />   Proposing high performance robust decentralized controllers for PQ improvement of ac microgrids with electronically- and directly-coupled machine-based DG units,<br />   Developing multifunctional control strategies for grid-connected converter-based DG units in order to compensate for non-integer harmonics,<br />   Proposing robust controllers to improve robust stability and PQ of islanded hybrid ac/dc microgrids in the presence of uncertain nonlinear and/or unbalanced loads, and<br />   Designing nonlinear robust controllers for the interlinking converters of a grid-connected hybrid ac/dc microgrid to guarantee voltage stability and improve PQ. <br />The proposed Discovery program will contribute to the development of renewable energy and smart microgrids technologies. Moreover, through the proposed program, we will contribute to the training of <strong>HQP </strong>which are in high demand by Canadian industries and academia.""586389,""Karimi, Houshang"
"590989"	"Karki, Rajesh"	"Power System Reliability Evaluation Incorporating Renewable Energy and Smart Grid Initiatives"	"The proposed research will develop short-term reliability models for renewable resources with smart grid technology application for electric power systems. The models will be embedded in new methodologies that will be developed to evaluate operating performance for efficient utilization of energy resources while maintaining acceptable level of system reliability. Widespread support for renewable energy has led to rapid increase in large-scale wind power integration in power systems. Significant growth in intermittent generation, increased transmission congestion with highly fluctuating severity levels, and the relative growth in distributed generation with potential wind and solar power are considerable changes in modern power systems. Increasing use of electric vehicles can bring about significant changes to energy use and management in future power systems. These changes have and will create new challenges to system operation in order to provide reliable and efficient use of electrical energy. Smart grid technology is receiving significant attention as potential means of mitigating these problems. The application of smart grid technology in various forms can significantly influence the energy demand profiles, response characteristics to system conditions, and operating philosophy. These transitions further dictate a need to deviate from the conventional practices of system operation, and research and develop new and suitable techniques considering the applications of emerging technologies and potential operational measures. The proposed research will develop methodologies, considering the ongoing system structural changes and initiatives, to increase energy efficiency, maximize renewable energy utilization and maintain acceptable reliability of the overall system. The objective is to provide useful indicators and valuable information to power system operators in order to maximize the benefits from different types of energy sources considering storage opportunities from electric vehicles and demand response initiatives in the changing environment. The developed methods should also be useful to energy policy makers and to system regulators to develop and implement appropriate reliability standards in the changing energy scenario.""575849,""Karl, Jenni"
"589278"	"Karmouch, Ahmed"	"Service Oriented Resource Allocation in Mobile Cloud Computing"	"<p>The success of wireless networks and Cloud Computing technologies is carrying the technological advances to offer complete transparent and elastic services to the ever-growing number of mobile wireless devices. Mobile Cloud Computing is a novel paradigm that promises to provide processing and storage offloading capabilities by transferring computing resources from the mobile devices to cloud services. Challenges come from the fact that design rationale for Mobile Cloud Computing functional components has been focusing on only a very limited scope of key performance factors. Advanced Mobile Cloud scenarios require more complex mechanisms as result of being deployed in more distributed and heterogeneous infrastructures, thus driving the need for a novel design rationale. <br />The long-term objective of this proposal is to develop innovative contributions (e.g., techniques, algorithms, methodologies, and the like) to support efficient modeling and design of offloading decision making coupled with a distributed resource allocation system capable of dynamically composing and self-organizing an end-to-end optimal cloud Infrastructure. The composed infrastructure is optimized for the execution of a mobile application workload. Research effort will include the following four short-term objectives:<br /><strong>(1)</strong> Develop analytical modelling for decentralized processing offloading decision making where strategic software components and algorithms take timely decisions on what, when, and how to start the processing offloading procedures. <br /><strong>(2)</strong> Design the building blocks of  a Mobile Cloud Computing architecture with their respective functionalities,  aiming at (i) accommodating scalability requirements for mobile applications; (ii) providing decentralized decision making mechanisms to mobile devices in order to ease the management load at the cloud computing providers; (iii) minimizing control signaling overhead between mobile devices and the distributed  cloud computing infrastructure; and (iv) realizing the dynamic computation of?oading decision update procedures (monitoring, elasticity). <br /><strong>(3) </strong>Develop a dynamic resource provisioning and allocation based on factors such as the availability of cloud resources, the performance optimization objective and the topology awareness of clouds and mobile devices. <br /><strong>(4) </strong>Develop a working prototype to demonstrate usefulness, capabilities, and performance of the novel Mobile Cloud Computing framework. <br />The outcome of the project will significantly advance the state-of- the-art on resource allocation in Mobile Cloud Computing. It will also have profound impact on the development of industrial Mobile Cloud Computing resource management systems. The research is particularly critical for the sustained growth of mobile applications. Finally, it will provide excellent opportunities for training highly qualified personal.</p>""568715,""Karney, Bryan"
"590352"	"Karray, Fakhreddine"	"Cognitive Based Platform for Advanced Driver Assistance Systems"	"<p>Major advances are made in the design and development of novel ground transportation infrastructure and intelligent vehicles systems. This is an integral part for building the next generation of intelligent transportation systems involving smart roads and highways, connected vehicles, intelligent sensors and actuators, advanced communication systems and autonomous vehicles equipped with the latest smart mechatronic devices and knowledge based software. At the same time, additional stringent requirements are made by the car manufacturers and insurance industries for achieving best possible performance in terms of fuel efficiency, driving safety, comfort and environment protection. Advanced driver assistance technologies are among the most notable advancements made towards these goals. This market alone is expected to grow to $260 billion by 2020, as the technologies become mainstream not only for luxury and executive vehicles, but also for higher volume product. Transportation authorities in several countries have already mandated such systems and large research budgets have been allocated for the development of next generation advanced driver assistance systems (ADAS).<br /><br />The proposed research program will investigate and build novel functionalities for a cognitive based ADAS. The system's proposed capabilities should substantially help enhancing the safety and the comfort of the car's occupants, while continuously monitoring the driver's current state, the vehicle's dynamic characteristics and the surrounding environment. To this end, we will develop a context aware based multi-agent system, which accounts for static and dynamic context information involving all types of sensory information coming from the inside and the outside of the car, along with a continuous update on the driver status and the driving environment. To achieve the highest levels of reliability and to infer the most accurate decision-making process, the system must be able to store a wide variety of information, including knowledge from past situations and should possess capabilities for adaptation and driver behavioral learning. Given the various types of sensory signals involved, which need to be interpreted and analysed in real time, an advanced novel approach for multi-modal data and information fusion will be developed as well. <br /><br />The proposed program will tackle a number of challenges in the areas of cognitive design, context awareness and data/information fusion. It should lead to real opportunities for the advancement of knowledge pertaining to the design of next generation ADAS and the training of highly qualified personnel, who will advance these technologies through their own research programs, while contributing to the strategically important Canadian automotive and transportation sectors.</p>""587984,""Karray, Mourad"
"590464"	"Khademi, April"	"Algorithm Development for Magnetic Resonance Images (MRI)"	"The visual analysis of medical images is subjective (observer-dependent), qualitative, error prone, and is inefficient, which ultimately effects diagnostic accuracy and the ability to conduct large research studies. Image analysis techniques offer a better alternative since they automatically measure image properties in a quantitative, objective, efficient, and reproducible manner. The purpose of this program is to advance the state-of-the-art in image analysis algorithms that operate only on Fluid-Attenuation Inversion Recovery (FLAIR) Magnetic Resonance Images (MRI) to quantify neurological disease in a novel and efficient way.  Although FLAIR MRI are now the preferred modality for neurodegenerative disease detection and analysis, only few image processing methods exist and novel image processing algorithms are much needed. <br />The objectives of this research proposal surround the development of 1) segmentation algorithms for neuroanatomy and pathology in FLAIR MRI in order to extract features from tissues such as volume, texture and shape, and 2) registration of FLAIR MRI to atlases in order to analyze brain images in a spatially normalized space, to study progression of disease in longitudinal data (i.e. change in lesion volume), and also to identify important anatomical landmarks. Considering the neurological disease prevalence, aging demographics, as well as associated economic burden on the Canadian healthcare system, such a program has the potential to positively impact many Canadians.""579607,""Khadour, Danny"
"591190"	"Khan, Ali"	"Intelligent imaging informatics for surgery"	"<p>Brain surgery to treat drug-resistant epilepsy or brain cancer is very challenging, as the surgeon needs to trade-off completely removing the disease, with sparing healthy functional tissue.  Planning how much of the brain to treat requires knowledge of the neurological pathways and abnormal regions, however this can be difficult as the boundaries are not always well defined.  Incorrectly defining the region to treat can carry a great deal of risk; if too small, the surgery will not be effective, and if too large, function may be altered negatively. Magnetic resonance imaging (MRI) has attempted to solve these problems non-invasively, however, the puzzle piece that has been missing is the integration of signal information that is collected during and after surgery. Clinicians commonly rely on this signal information, such as functional recordings from electrodes placed on the brain, or microscopic images of tissue, to confirm what can only be suspected with MRI.  Fusing these biomedical signals with MR imaging closes the loop, and allows us to gain a deeper understanding of how image analysis can be used to precisely highlight the target. <br /><br />Gaining an understanding of this fused biomedical information requires us to address three key challenges: 1) robustly extracting information on what neurological pathways in the brain are functional and need to be avoided, 2) finding spatial correspondence between images taken before, during, and after surgery, and 3) using all this information, built into an informatics database, to outline the target in a new patient and the uncertainty in this prediction. The goal of the proposed research program is to develop sophisticated software tools that address these challenges. This work will be the first to systematically align and fuse functional and biological information from surgery with pre-operative images, and will allow for an unprecedented ability to compare this information at a local scale. The contributions in computational image analysis will also be of significance to the wider biomedical image processing community, as they address many key challenges related to robustness in image registration and how uncertainty is modelled. Ultimately, this work will transform how image-guided surgery is performed, and will pave the way for the delivery of more precise and minimally-invasive treatment.</p>""580834,""Khan, Areesha"
"590378"	"Khendek, Ferhat"	"Model Driven Incremental Design and Validation of Software Systems"	"Software systems are pervasive. They are part of our daily life, they control cars, airplanes, telecommunications networks and services and other home appliances. They are very complex, because of their inherent characteristics: large size, embedded, distribution, concurrency, real-time constraints and deadlines. Such systems require usage of rigorous and formal methods in order to ensure their functional correctness and other non-functional qualities such as real-timeliness. Model Driven Development (MDD) is an approach that has emerged over the last two decades, and which is gaining in momentum. However, the adoption and the application of MDD is hindered by the challenging issues of scalability and formality.<br />The focus of MDD, so far, has been on developing profiles, extending/specializing the Unified Modeling Language (UML) and languages for transformations, only a few techniques have been proposed for its realization.  Moreover, these techniques do not span the necessary phases to link and relate the various artifacts and phases, do not scale or are not formal. The main objective of this research proposal is therefore to contribute to the MDD of complex software systems with techniques for their modeling, design and validation. We propose to investigate a modeling framework that covers several phases of the development process: requirements, design, and validation/testing. The modeling framework will consist of a subset of UML 2 and related profiles like MARTE, SysML and UML Profile for Testing (UPT).  Using this formalized framework, we aim at devising MDD compliant techniques for the incremental design and validation of software systems. Incremental design approaches taking into account functional and non-functional characteristics as well as incremental test case generation techniques will be devised.  These formal incremental techniques will certainly scale and will contribute to the application and adoption of MDD.<br />This proposal aims at devising techniques for improving the quality of software systems and at reducing their development cost. The research issues tackled in this proposal are of high importance for the software research community and industry which includes the telecommunications and automobile sectors.  The proposed research program will train graduate students who will join the Canadian work force and contribute to the Canadian economy in different sectors.<br /><br /><br />""575474,""Khennich, Mohammed"
"578288"	"Kiros, Ryan"	"Québec"	"CANADA"
"589429"	"Kpodjedo, Sègla"	"Recommendation Systems for Evolving Software"	"Within the software life cycle, the maintenance/evolution phase is viewed as the longest and costliest. Software projects become bigger and more complex with each iteration and documentation is often incomplete. Maintainers and re-engineers need tools that use the sheer amount of data to make timely and relevant suggestions. In the broader field of software engineering, Recommendation Systems in Software Engineering (RSSE) have emerged as such tools. The proposed program aims at uncovering insights about a software project's evolution and using those to propose Recommendation Systems for Evolving Software (RSES).<br /><br />Changes performed on a software project have different scopes and motivations that can be relevant several versions later. These evolution decisions are not necessarily documented and range from simple renamings to vocabulary shifts or replacements of libraries. Violations of such decisions can eventually result in serious issues: from consistency and comprehension problems to vicious defects. Much like ""code smells"" is used for symptoms of possible hidden implementation problems, violations of previous evolution decisions qualify as ""evolution smells"", i.e., symptoms of possible modification problems. This program covers two main axes: (i) techniques that help in uncovering changes and evolution decisions and (ii) proposals for effective RSES based on the investigation of evolution smells. We have been working on the first axis and the generic techniques we (and others) have developed are mature enough to serve as tools for preliminary explorations of evolution smells. The second axis will thus be the immediate focus and will inform the subsequent research efforts for improving techniques concerned with retrieving accurate and meaningful software evolution data.<br /><br />For this five-year cycle, the objectives are as follows:<br />1) Collect/study change facts and investigate evolution smells<br />2) Propose recommendation systems for evolving software<br />3) Improve techniques for mining evolution decisions<br />4) Improve approaches for diagram differencing in software evolution.<br /><br />The first objective will be achieved through a systematic literature review and the study of a corpus of projects that will be used to investigate evolution smells. The second will collect and study tools and publications on RSSEs in order to propose a generic architecture and first iterations of RSES. The last two objectives are about improving techniques that can (i) aggregate change facts in an insightful way or (ii) compute the differences between two versions of a software.<br /><br />This program investigates technologies that will help engineers in their tasks and thus brings research on software evolution closer to industrial relevance. Strong and motivated students will be trained and will help contribute to significant advances in the discovery and exploitation of new insights about software evolution""569922,""Kraatz, HeinzBernhard"
"589518"	"Krishnan, Sridhar"	"Non-stationary Signal Feature Extraction and Analysis"	"<p>Signal processing continues to play a fundamental role in many technological innovations and advancements related to speech, multimedia, healthcare, defense, security, telecommunications, Internet,  and energy systems. For the past 15 years, the Signal Analysis Research (SAR) Group at Ryerson University is involved in developing various innovative techniques and algorithms for processing and analysis of speech, audio, multimedia and biomedical signals. The underlying characteristics of signals involved with these systems is that they are complex, typically long duration, difficult to interpret, and have time-varying properties. In order to extract valuable information (features) from these signals and characterize events of interest, and to automatically classify patterns, sophisticated signal analysis algorithms (and analytical tools) need to be designed. The proposed NSERC Discovery Grant research will systematically investigate and develop mathematical methods, algorithms and tools to map 1-dimensional (1D) signals into higher dimensions for automatically extracting signal features at multiple levels, which are otherwise difficult or impossible to extract from conventional techniques. It is envisioned the mathematical transformation of signals to higher dimensions and the subsequent feature extraction algorithms will reveal underlying signal generation/modification mechanisms that could be useful in recognizing hidden/subtle signatures for better recognition and classification applications. The extracted signal features will be further coupled with appropriate machine learning algorithms in providing enhanced and robust recognition and classification performance efficiencies. Automatic feature extraction and analysis has lots of practical applications, and is the foundation of everyday systems encountered in speech, audio, multimedia,  biometrics and many other intelligent systems. The algorithms will be applied to real world datasets collected in our lab and other open source databases. The algorithms and the databases will also be shared with other interested research groups for the benefit of their specific domain of application (e.g., big data analytics in energy or health sector). The research program will also train a large number of highly qualified personnel who could eventually lead technological advancement in various industry and research sectors that are crucial for the societal well-being and economic prosperity of Canada.</p>""592189,""Krishnan, Sridhar"
"590329"	"Krzymien, Witold"	"Efficient Radio Resource Management and Transceiver Signal Processing for 5th Generation Cellular Networks and Beyond"	"<p>The general objective of this proposal is the development of reduced-complexity radio resource management and transmitter/receiver signal processing techniques and algorithms for the design of future broadband wireless systems, cost-efficiently enabling very high throughput, very low latency, spectrally and power efficient access to the backbone network for mobile users of data and multimedia services. The scope of the proposal encompasses several interconnected rapidly developing areas in cellular-network-focused wireless communications research: (1) robust reduced-complexity radio resource allocation and interference management algorithms for  coordinated dense heterogeneous MIMO (multiple-input multiple-output) cellular networks; (2) cost and performance-efficient signal processing and resource management techniques for massive MIMO, robust to implementation imperfections; (3) heterogeneous MIMO networks with massive MIMO at the macrocell tier.</p><p>In a continuing quest to increase throughput of cellular networks the last few years have seen a major shift in the focus of wireless systems research to heterogeneous networks (HetNets) and large-scale antenna systems (massive MIMO). HetNets encompass network nodes of widely varying transmitted powers and communication protocols, such as macro, pico and femtocells, remote radio heads, distributed antenna systems and infrastructure-based relays. Unfortunately, in such multi-tier networks the co-channel interference problem (a fundamental capacity-limiting factor in all cellular networks) is difficult and it becomes even more difficult when multiuser MIMO techniques are considered to further increase network throughput by taking advantage of MIMO spatial multiplexing gains. Mitigation of this problem requires coordination of transmissions from/to network nodes. Since coordination of transmissions over the entire network is unrealistic (and also unnecessary), a promising idea of coordination within dynamic user-centric cell clusters (to mitigate detrimental cluster-edge effects) will be investigated (with focus on the more challenging downlink) together with the related problem of user-node association, while using the tools of random geometry to model HetNets. </p><p>An alternative, but hopefully also complementary to HetNets approach to a dramatic increase in cellular network capacity is to deploy large excess of antennas at network nodes over active users, which would also improve energy efficiency and reduce latency of the air interface. Practical implementation of this massive MIMO concept still faces a number of both hardware and system-level challenges. The proposed work will begin with investigation of merits of robust joint spatial division and multiplexing, incorporation of non-orthogonal multicarrier transmission schemes into massive MIMO and integration of massive MIMO with HetNets.</p>""591045,""Krzyzak, Adam"
"589605"	"Kulic, Dana"	"Motion Analysis and Control for Robot Assisted Rehabilitation"	"<p>Despite their relatively slow cognitive processing abilities, humans have unparalleled capabilities for an extraordinary range of agile, responsive and highly optimized movement.  A key characteristic of human movement is that movement patterns change over time, and can be adapted and improved through training and practice, to respond to changes in physiology  and to adapt to tools.  In this proposal, we aim to develop a modeling approach which can learn the control strategies used by humans during movement and movement adaptation, to improve our understanding of how movement changes over time as a function of training, and develop systems which optimize guidance mechanisms to maximally improve progress. The proposed approach will learn a model of the dynamics of the body, including the constraints on movement, and the control strategy being used during motion execution.  A key innovation of the proposed approach is that robotic systems will be used to test and validate the proposed learning methods.  The learned control strategy and a soft robotic assistive device will then be used during movement training to provide guidance that is coordinated and adapted to human movement.  The proposed research will provide a unique opportunity and research environment for training of highly qualified personnel, who will gain experience in  human motion analysis and modeling, robot learning and control, and human-robot interaction.   The proposed methods have a wide range of potential applications, including understanding and preventing maladaptive movement strategies in workplace tasks, facilitating work training, improving progress during rehabilitation following injury or disease, improving performance and preventing injury during sports training, and prosthesis design and control.</p>""575264,""Kulic, Danica"
"589699"	"Kwong, Raymond"	"Security and Fault Tolerance of Cyber-Physical Systems"	"Critical infrastructures such as power grids, water supply systems, and traffic networks involve networked control systems with distributed sensors and actuators. We refer to them as cyber-physical systems. One key requirement for such systems is security from malicious attacks, often in the form of cyber-attacks aimed at compromising supervisory control and data acquisition (SCADA) systems. The second key requirement is fault tolerance: the ability of the control system to continue functioning despite component failures. Component failures can occur as a consequence of malicious actions by an attacker, but can also occur as a result of hardware breakdown or unforeseen operating conditions.<br />Most approaches to combat cyber-attacks focus on prevention through firewalls. With the advent of smart sensors and stealth technologies, preventing access and detecting intrusion are becoming increasingly difficult. There is an urgent need to develop counter-measures that can combat malicious intruders after intrusion has occurred. Traditional approaches to fault-tolerant control design are also inadequate as they often rely on first carrying out accurate fault diagnosis, followed by control reconfiguration. The system may be damaged or irrecoverable while waiting for diagnostic information. <br />Our proposed research aims to improve the security and fault-tolerance of control systems central to cyber-physical systems. The two main objectives of our proposed research program are: (1) to develop novel methodology to detect malicious intruders and prevent them from inflicting damage to the control system; and (2) to maintain functionality and safe operation of control systems when components fail. <br />The first objective focuses on security at the system level, typically involving SCADA systems describable using discrete event system models. Our methodology will make use of the theory of supervisory control and diagnosis for discrete event systems to perform intrusion detection and to counteract malicious behaviour. The novelty in our formulation is our focus on the control aspects after an intrusion has occurred, on how the feedback loop induces interactions between the actions of the attacker and defender.<br />The second objective focuses on resilience of the control system at the physical level when actuators and/or sensors fail. The novelty of our approach is that we focus on decentralized fault-tolerant control design which does not depend on accurate diagnostic knowledge. We propose to study fault-tolerant control as an integrated problem of distributed diagnosis and control reconfiguration, exploiting redundancies through coupling, and investigate network effects such as cascading failures. <br />Protecting cyber-physical systems is of great national interest. Our proposed research can provide significant improvements on the security and resilience of cyber-physical systems.""590616,""Kycia, Jan"
"591104"	"Lam, John"	"Advanced high frequency medium voltage power architectures for wind energy systems"	"<p>By 2030, wind energy is expected to generate at least 20% of the world's energy demand. The Global Wind Energy Council (GWEC), has forecasted that the total installed wind capacity will grow by 63% in the next 4 years.  A typical offshore wind farm consists of hundreds of mega-watt wind turbines and a medium voltage AC (MVAC) grid that collects the power from the individual wind turbines.  Since the output voltage of the turbine is too low for effective long distance power transmission, a dedicated low frequency medium voltage (MV) step-up transformer is used for each wind turbine to step up the turbine output voltage to the MV level. However, the step-up transformers are bulky; heavy, and typically located in the immediate vicinity of the wind turbines.  In literature, the MVAC grid has been suggested to be replaced by a MV DC grid, so that the MV step-up transformers can be replaced by the MV medium frequency step-up power converters.  However, the conventional MV step-up power converters have low efficiency and poor reliability.  With regards to the existing controller structure used in the MV power conversion in wind energy systems, one controller is delegated to the front-end AC/DC stage for maximum power point tracking (MPPT) to extract maximum power from the wind, while another controller is designated for the DC/DC step-up voltage conversion stage to provide output voltage regulation.  Therefore, the existing two-stage MV wind power conversion has low power efficiency, poor reliability and requires complex control structures. <br />The proposed research program is to develop novel and efficient power architectures that features advanced high frequency MV power converters and digital control techniques for the MV grid in wind farms. The short term goal of this program is to: (i) devise new high frequency soft-switched MV power converters with high voltage gain; (ii) devise new integration techniques to form a new class of single-stage high frequency MV step-up rectifiers with significantly reduced power losses; (iii) devise new digital controller unit that simultaneously performs MPPT, output voltage regulation and protection function for the given wind and load conditions. <br />While proof-of-concept experimental prototypes will be implemented and tested in the laboratory to confirm all the performance, the long-term goal is to collaborate with industry to further develop the devised technology into a commercial product for use in high power wind turbines.  Advanced and novel MV step-up power converters and controllers  developed in this research program will be ground-breaking as they will significantly improve the power efficiency; size; cost and reliability of the existing power converters used in high power wind turbines.  Furthermore, successful development of this novel technology will help in establishing Canada as one of the global leaders in power electronics research for renewable energy.</p>""585041,""Lam, Jonathan"
"590748"	"LaRochelle, Sophie"	"Photonic technologies for high-capacity optical communications"	"<p>In this new millennium, people and intelligent devices are constantly interacting through untethered RF links and reconfigurable networks. These communication networks handle enormous amounts of data that must be harvested from mobile users, processed in highly interconnected data centers, and transmitted on optical fiber communication conduits already burdened with burgeoning traditional data traffic. To enable the information age, telecommunication engineers have been innovating at a rapid pace for many decades, delivering many successive generations of systems with increased data rates, capacity and connectivity. Users are usually unaware of the tremendous technological challenges that this unstoppable revolution poses to scientists designing networks around the world.</p><p>In this research program, we are investigating how to overcome current limitations of high speed communications in optical fiber networks by reinventing the optical fiber channel. Our research proposes to carry data on parallel channels, with the same wavelength, by cleverly designing fibers that multiply the number of cores in single fiber strand or allow transmission on orthogonal modes in a single core. Furthermore, we are studying microstructured fibers, with air holes, to help optimize the fiber properties and extend the current communication spectral window. In the first research axis, our focus is on fundamental fiber properties, such as scattering loss and cross-talk caused by surface roughness, that currently limit the maximum transmission distance of these innovative optical fibers. In the second axis, we examine erbium-doped optical fiber designs to make power efficient amplifiers for these communication links. Finally, in the third axis, we will research all-fiber devices to couple and filter light into the mode channels.</p><p>This research program takes advantage of the colocation of an optical communication laboratory, with state-of-the-art test equipment, and of a fiber fabrication infrastructure that is unique in Canada. This close proximity allows easy access to optical fiber prototyping and provides a unique opportunity to make high impact research in optical fiber communication technologies. Canada has a strong photonic industry, encompassing large telecommunication equipment manufacturers as well as small high-tech businesses that must constantly innovate to maintain their leadership position. In addition to its anticipated research outputs, this research program will provide a high quality training environment for young researchers who will be well prepared to join this dynamic industrial sector.<br /><br /></p>""582422,""Larochelle, Stacy"
"589725"	"Lau, LapChi"	"Linear Algebraic Techniques in Algorithmic Graph Theory"	"<p>Algorithms for fundamental graph problems are important tools in computer science and engineering.  Extensive research has been done in the past decades to develop efficient algorithms, most of them based on combinatorial techniques.  In recent years, new linear algebraic techniques have been developed to make surprising and significant progress in these fundamental problems.  My previous research develops linear algebraic techniques to design both better approximation algorithms and faster exact algorithms for graph problems.  I plan to continue this line of ongoing research and have identified three promising directions to investigate.</p><p>Spectral graph theory: The application of spectral techniques to study combinatorial graph properties has a long history.  The classical result relates the second eigenvalue to the edge expansion of the graph, and it has been used to develop efficient algorithms for graph partitioning problems and to analyze the mixing time of random walks.  Recently, there have been exciting new results relating other eigenvalues to combinatorial graph properties.  This new connection opens up ways to do better analysis of existing algorithms, to design better approximation algorithms and to design faster algorithms for various graph problems.  I believe that this approach will bring new insights into some outstanding open problems including the small-set expansion conjecture, the bipartite matching problem, and the log-rank conjecture.</p><p>Random walks: Random walks are simple stochastic processes on graphs that have a wide range of algorithmic applications.  One classical application of random walks is in designing random sampling algorithms.  Recently, random walks have been used to design sublinear time graph algorithms.  I plan to do a systematic study of the random walks method, to design exact and approximation algorithms and to prove hardness of approximation results.  Also, with our new knowledge about random walks and graph expansion, I would like to revisit the outstanding open problems in random sampling, including the matroid expansion conjecture and the mixing time of the Glauber dynamics for graph coloring.</p><p>Linear algebraic algorithms: Recently, there has been significant progress in designing fast algorithms for fundamental graph problems using linear algebraic algorithms.  A major open problem in this area is to design fast algebraic algorithms for weighted problems.  I also plan to develop this approach to solve additional graph problems and linear algebraic problems, and to study the interplay between graph theory and linear algebra.</p><p>I believe that achieving the goals in this proposal would have major impacts in the theory of computing, as well as in other areas such as machine learning, applied probability, and symbolic computation.  I also expect to have significant contributions to the training of HQPs by acquiring these emerging techniques with my students. </p>""572355,""Lau, Michael"
"590123"	"Leung, Henry"	"Big Data Fusion"	"<p>Data fusion is a mechanism for the data acquired from multiple sources to be gathered and combined in a consistent way to obtain more reliable, less voluminous processed data in a timely manner. It is conventionally used in many sectors involving multiple sensors, such as defense and security, medical diagnostic and surgery, intelligent transportation systems. As we are moving into an era of data deluge, not only is the amount of data greatly increasing, but there is a wide variety of types of data sources, such as sensor measurements and recordings, and cyberspace (e.g. social networks). This incredible growth and availability of both structured and unstructured data causing problems in volume, velocity and variety is termed big data. Current data fusion techniques, however, cannot really handle the challenges posted by big data.</p><p>In this project, we propose a multi-disciplinary approach for big data fusion. We will combine approximation theory in computer science, complexity theory in physics, and distributed and sparse systems in signal processing to develop novel methods for fusing massive amounts of data. In particular, we will focus on the three basic components of data fusion: registration, association and fusion. We propose using system sparseness and motion dynamic to develop a new registration model to align a large amount of devices. To associate or correlate the data, we will develop an approximate multi-frame nearest neighbor data association to speed up the association process and use complex networks to extract the deep correlations in the cyberspace to determine honest relationships. Since big data takes on a variety of formats, we propose to develop a universal data fusion based on first- and second- order statistics to provide a close-to-optimal data fusion.</p><p>The proposed research will focus on applications in three important industries: energy, wireless communications and the environment. The proposed research does not only develop fundamental data and signal processing algorithms for big data, but it also makes data fusion applicable to areas that will eventually involve big data. The big data industry is worth more than $100 billion and growing at 10% a year. By 2018, the United States alone could face a shortage of about 170,000 people in this area. The proposed research will prepare talented people for the rapidly growing market.</p>""581124,""Leung, Jason"
"590728"	"Li, Howard"	"Sensor Driven Multiple Robot Mapping and Exploration"	"<p>In the future, robots will be used to assist rescue crews in search and rescue missions after disasters. Affordable robots can be quickly deployed and used in dangerous situations to locate survivors and send data about their whereabouts to first responders. The proposed NSERC Discovery program focuses on an emerging market: novel robotics mapping and exploration technologies for emergency responders. <br /><br />Robots are widely used to perform tasks that are too dirty or dangerous for humans. Sensing, localization and exploration in unknown environments are key issues to be addressed for real-world robotics applications. Simultaneous Localization And Mapping (SLAM) is a process in which an unknown environment is explored and mapped consistently by a robot. With the rapid development of robotics technologies, robots need to work in teams with other robots to share information and coordinate activities. Significant research is in progress to support the perception and decision-making process for multiple-robot systems. Multiple-robot SLAM has benefits such as being more robust to failures and being faster in exploring and mapping unknown environments. The proposed five-year research program will focus on developing novel sensor driven cooperative exploration and 3D mapping technologies for multiple heterogeneous robots. <br /><br />The following problems will be investigated: sensing, mapping, planning, and control for multiple robots. Multiple robots must sense the surroundings, fuse the data, and interpret them in a useful way. Then the robots need to map the unknown environment and localize themselves in the environment. Next, robots must plan paths in order to explore the area and accomplish the mission. Finally, robots must control themselves so as to implement the decisions.<br /><br />With the support of the previous NSERC Discovery Grant program, the applicant's research group has developed robotics technologies for real-world air, ground, and maritime applications. For the proposed research program, the research group will conduct cutting-edge research on multiple robots and address new challenges. Multiple robots equipped with cameras and laser sensors can provide enhanced situation awareness. The expected outcome of this research program is novel 3D exploration and mapping technologies that can be used to assist emergency responders in rapid information gathering.<br /><br />The results of this NSERC Discovery program will have significant and lasting benefits to the Province of New Brunswick and Canada by providing cutting-edge research on robotics and unmanned systems. The results will be used for emergency response applications. Furthermore, the research will provide training for highly qualified personnel in the areas of unmanned systems, robotics, mechatronics, control theories, mapping, artificial intelligence, and aerospace, which are highly demanded by Canadian employers.</p>""579356,""Li, Huazhou"
"577751"	"Li, Ke"	"New Brunswick"	"CANADA"
"591109"	"Liang, Jie"	"Advanced Signal Processing for Mobile Social Media Sharing"	"<p>Recently, sharing images/videos online has become increasingly popular, thanks to advances in wireless networks, smart phones, and social networks. This leads to explosive growth in the number of online images/videos. To reduce uploading cost and delay and the server storage cost, social network providers downsample the high-resolution images/videos before uploading, which wastes the capability of the camera. On the other hand, when other users view these images/videos, they frequently want to zoom in to see some details, but the quality of existing zooming algorithms is still not satisfactory, especially for panoramic images. <br /><br />Therefore, how to improve the mobile social media sharing quality without increasing the cost to social network users and providers is a challenging and important problem. Fortunately, some new theories have been developed, and various powerful cloud computing platforms have become available, offering potential opportunities to address the challenge.<br /><br />The long-term objective of our research is to develop algorithms and systems to improve multimedia communications experience. In the short term, we will develop and integrate novel theories in the following three closely related topics in graph, sparse, and redundant signal processing, apply them to mobile social media sharing, and develop a mobile phone app.<br /><br />1. Graph transform is a new theory to process data defined on graphs such as those from social networks and sensor networks. It has also been used in image/video coding. However, existing graph transforms do not have fast implementations. In this program, we will design fast graph transforms by generalizing the conventional filter bank theory.<br /><br />2. Compressed sensing (CS) is a new way to sample and reconstruct sparse signals. Currently, the approximate message passing (AMP) method represents the state of the art in CS. Recently we develop a side information-based AMP (SI-AMP) to incorporate the initial estimate of the signal. In this program, we will further improve SI-AMP by exploiting the latest denoising methods. We will also design a general multi-resolution CS scheme, apply SI-AMP to it, and integrate them with the graph transform.<br /><br />3. Dictionary-based representation is widely used in CS. In this program, we will design a quantized dictionary-based representation to reduce the storage cost, which is suitable for mobile social media sharing, and apply it to our CS schemes.<br /><br />4. Mobile app development: we will develop a mobile phone app that allows users to upload and view photos via our app and server. The encoding and decoding of images/videos are based on the algorithms developed above, which will provide better quality than existing schemes.<br /><br />This program will provide solid theoretical training and product development experience to the students involved, so that they can make contributions to Canadian economy in the future.</p>""579299,""Liang, Jie"
"590914"	"LiboironLadouceur, Odile"	"Smallest Integrated Networks for Highest Data Density"	"The exponential increase in data continues to force important societal changes across the world. From scientific simulations to consumer electronics, data processing has entertained us, but more importantly saved lives and influenced economical growth. However, many parts of the world have weak infrastructures, or are unable to afford the high cost of modern computing systems. Indeed, to meet the high demand for data processing and storage, complex systems with thousands of processors are working in parallel interconnected through electrical switches, inherently limiting their long term scalability in terms of performance. With recent advancements in photonic integration, the electrical interconnectivity can be replaced with optical technologies. The proposed program addresses the system integration of photonics towards high data density. The expected research outcome will lead to practical scalability with performance enhancement in modern computer systems. Furthermore, price deflation which comes with the increase in performance makes these modern computing systems more accessible to all parts of the world, including remote areas of the Canadian landscape where energy is not easily available to process large amount of data required such as in the mining industry or in supporting the northern communities.<br /><br />Dr. Liboiron-Ladouceur's proposal is on the investigation of the fundamental limits of the highest on-chip bandwidth density in optically interconnected CMOS-based heterogeneous computing systems. She builds a research methodology upon two of her recent innovations developed through her first NSERC Discovery program on low-power photonic interconnects. In the first invention, she has experimentally demonstrated a high-bandwidth electrical nanoscale waveguide capable of simultaneously supporting modulated optical signals leading to a potentially important paradigm shift in data communication where metal transmission lines can be used electrically for control signals and optically for large data transfer. In her second invention, Dr. Liboiron-Ladouceur has demonstrated the feasibility of integrated interconnection network architectures on a single chip downsizing architecture by a factor of a thousand from m<sup>2</sup> to mm<sup>2</sup> leading to bandwidth density enhancement. The objective of the research program is to develop computer architectures with enhanced bandwidth density in the interconnectivity between processors. The multi-disciplinary research program offers and excellent training platform for a total of 27 students, which includes 10 undergraduates, 9 master's, 5 doctoral, and 3 postdoctoral students. The budget leverages advanced training opportunities in photonic integration offered by an NSERC CREATE program along with contributions from McGill University. Thus, the amount requested represents 45% of the proposed expenditure. <br />""569231,""LiChan, Eunice"
"590941"	"Lisonek, Petr"	"Nonlinear functions, codes and quantum computation"	"<p>The proposed research is divided into three parts.</p><p>In the first part I will study several families of cryptographic functions, which are important components of block ciphers and stream ciphers. These ciphers protect privacy and security of communications on Internet, wireless networks, mobile telephony, and other networks. They also protect data that is stored digitally. Hence, these ciphers are used every day by most citizens of a modern society. My anticipated outcome is the construction of new optimal cryptographic functions, by finding formulas for these functions, or by developing computational methods that efficiently test whether any given function has the desired properties.</p><p>In the second part I will study linear codes. Error control codes detect and correct errors that occur due to noise when a signal is transmitted over a noisy channel, or when data is stored in, and retrieved from, computer memory or disks. Sources of noise are for example interference with other devices, atmospheric electricity or manufacturing imperfections in hardware compounded with its depreciation. Error control codes are critical for the proper function of Internet, wireless networks, mobile telephony and computers. The anticipated outcome of my research are algorithms that exactly determine the error detection and correction capacity of any given code; my algorithms will be significantly faster than hitherto known algorithms. My algorithms will be important not only for practical construction of error control codes but also to researchers in other areas, since many theoretical problems can be reduced to the existence of a certain linear code. I will also examine whether my algorithms may be relevant to attacking McEliece cryptosystem.<br /><br />The third part of my research deals with quantum computing. It is anticipated that certain unique features of quantum physics will enable a dramatic speed-up of quantum computers over classical computers. One such feature is contextuality, and it is demonstrated by Kochen-Specker sets. I will construct new families of Kochen-Specker sets by finding formulas for them or designing algorithms that find them. My algorithms will also prove non-existence of these sets in certain cases. Significant challenges exist with practical engineering of scalable quantum computers; it is anticipated that quantum error-correcting codes will be important in this process. I will construct quantum codes that are better than hitherto known codes.<br /><br /></p>""588410,""Liss, Steven"
"589916"	"Liu, Peter"	"Towards Realistic Surgical Simulation and Training Systems"	"    At present, surgical residents acquire and develop hands-on skills mainly through trials on animals, cadavers and patients. There are, however, obvious disadvantages to these conventional training approaches: animals have different anatomies from humans; cadavers are unable to provide appropriate physiological responses; it is risky when students practice on patients.<br />    With recent advances in haptics and computer graphics, visual reality based surgical simulation is rapidly emerging and increasingly perceived as a valuable addition to, or replacement of, traditional training methods. Simulators can generate virtual patients with realistic human anatomy and physiological responses, potentially providing a risk-free training environment with the possibility of unlimited training over a wide range of pathologies. In addition to these benefits, new and complex surgical procedures can be practiced safely on a simulator before proceeding to patients; students can practice on their own schedules; training sessions can be stored and later reviewed by physicians.<br />    While a small number of companies are starting to offer surgical simulation training systems of varying quality, no simulators are able to provide a sufficiently high degree of resemblance to real-life surgery. There still exist numerous fundamental challenges and technical difficulties, such as the dexterity of haptic interfaces, accuracy of virtual patient models, collision modelling, real-time and immersive visualization of the operating scene, etc. <br />    In this research, we propose to develop principles, algorithms and tools that are essential to realize highly realistic surgical simulation, enabling medical institutions to train surgeons using simulators. Due to the broad scope of potential research topics, we limit our efforts to a subset of problems including: 1. haptic feedback, 2. tissue modelling, and 3.validation criteria. At the end, the anticipated results will be integrated and validated by the development of a cricothyroidotomy simulator.<br />""575040,""Liu, Qi"
"590486"	"Lu, Paul"	"Mechanisms, Models, and Policies for Fast Data Sharing"	"<p>Whether transferring a large digital movie file to your local Internet provider (e.g, Apple iTunes, Netflix, Google Play) or a large data file for a computational scientist (e.g., the Large Hadron Collider and the Higgs Boson), moving that large amount of data across the Internet requires the latest in protocols and technologies.  Behind the scenes, industry and scientists rely on improved networks and software to move gigabytes and terabytes of data in minutes or hours, instead of days or weeks.</p><p>My research centres on improving the networking, software, and related systems.  With my graduate and undergraduate students, we build new software prototypes that are faster and more reliable than the existing systems.  Working with partners like CANARIE (who runs Canada's research network), Cybera (who runs Alberta's research network), and Mover (an Edmonton start-up company in cloud data transfers), we build and evaluate new solutions to large data sharing and transfer problems.</p><p>In the future, if you hear about the next exciting particle physics discovery or if you are able to watch an Ultra HD movie downloaded from the Internet, my research might have played a role in making that possible.</p>""572811,""Lu, Ping"
"589439"	"MacKenzie, IScott"	"Motor and Perceptual Optimization of User Interfaces for Mobile Devices"	"<p>This research program is broadly aimed at developing new, efficient, and provocative user interfaces (UIs) for mobile devices, such as tablets and smart phones.</p><p>The short term goal is to develop new interfaces and evaluate them with users.  The long term goal is to move the most provocative and opportune UIs forward into products that the user community will take up and embrace as novel, useful, and engaging interactions for their mobile devices. </p><p>""User interfaces"" is broadly interpreted as the points of interaction between a user and a device - in this case, a mobile device - used by a human for doing a task or achieving a goal.  The ""motor"" and ""perceptual"" aspects of the research pertain to the human output (motor) and input (perceptual) channels that are engaged as humans interact with mobile computing technology. ""Optimization"" is a reference to the use of empirical research methods to test alternative interfaces.  Design and interaction options are conceived, implemented, tested, and compared in user studies, with better performing options advancing for further optimizing and testing, and poorer performing options discarded.<br /><br />The human side will research the use of hands, arms, fingers, and even the movement of head and eyes. (Speech input to computers is not part of this program of research.) There is considerable on-going research in the way users interact with mobile computing devices with their hands and fingers, and this will continue.  The PI will research touch-based devices, such as smart phones and tablets, that support a variety of touch gestures (e.g., flicks) and multi-touch interaction (e.g., for zooming, chording).   Auditory and tactile feedback will be researched as a means to optimize interaction and to reduce the visual load.  The latter is important for mobile devices since they are typically used in dual-task settings where the primary task (e.g., walking) requires the use of the eyes.<br /><br />Modern mobile devices typically include a variety of sensors, such as accelerometers, gyroscopes, magnetic field sensors, proximity sensors, light, temperature, and pressure sensors, and so on.  Opportunities to engage and optimize information from these sensors in the user experience will be researched.  Furthermore, cameras (sometimes two!) are typically embedded in contemporary mobile devices.  The research will study the use of eye- or head-position information (obtained via the camera) in interaction scenarios.  Gaming is a typically example of an application domain for such data.  The research will include gaming as well as the broad spectrum of interaction techniques required for other applications.<br /><br /></p>""585175,""Mackenzie, Jacqueline"
"578591"	"Maharaj, Tegan"	"Massachusetts"	"UNITED STATES"
"591022"	"Mahseredjian, Jean"	"Accurate computation of power system transients"	"This project is for conducting research on the accurate computation (simulation) of power system transients. The targeted field of research is the computation of electromagnetic transients (EMT) using circuit based models of power systems. The simulation and analysis of EMTs is crucial to the operation of an efficient, reliable and environmentally friendly power system. The EMT simulation tools are used for the determination of component ratings such as insulation levels and energy absorption capabilities, in the design and optimization process, for testing control and protection systems and for analyzing power systems in general.<br />The findings of this project will impact on the accuracy of EMT-type computation methods. Two main aspects are investigated.<br /><br />On the first aspect it is proposed to increase the accuracy in the computation of initial conditions. Such conditions establish the steady-state operating point of a power system before performing transient analysis. Although steady-state initialization methods are currently available, it is proposed to develop new techniques capable to account more accurately for harmonics, control systems and various power electronics based devices. It is also proposed to study methods for finding more than one steady-state solution in the presence of nonlinearities. This research will allow to reduce engineering efforts and delays, and significantly improve simulation capabilities for complex problems, such as geomagnetic disturbances and integration of wind generation.<br /><br />On the second aspect, it is proposed to conduct research on accuracy problems related to interfacing of electrical network equations with control block diagrams, generic mathematical tools, and external models. Currently, such interfacing is typically based on approximations that often result into errors and/or require increased computing timings. In addition to modeling accuracy and flexibility, this research will have an important impact on modeling portability problems between EMT-type applications. This research will contribute methods that will allow accurate open-source and black-box model exchange among different simulation platforms and will facilitate engineering work flow. The proposed research on open-source modeling will lead to standardization and deliver important means for maintaining modeling consistency to research and industry.<br /><br />This research program will impact on Canadian power system utilities by delivering computational techniques capable of solving analysis problems that cannot be solved today. Canada is also a world leader in the development and supply of advanced simulation tools for power systems and this research will contribute new numerical methods that can be advantageously used by related software development companies.<br />""575875,""Mahshid, SaharSadat"
"591162"	"Mao, Yongyi"	"Coding for Flash Memory"	"<p>Due to their low costs and high capacity, flash memories are emerging to become a huge global market, surpassing the conventional DRAM market.   The exponentially increasing demand in digital storage presents tremendous opportunities and challenges in further developing the coding technologies for flash memories. In this project, we will join force with the very young flash coding research community. We will develop novel, powerful, and practicable coding techniques for the design of high-capacity and long life-time flash memories. We will also develop new fundamental understanding on the capacity limits of flash memories under practical engineering constraints. </p><br /><p>To date, flash memory technologies and the associated industrial sector are severely under-developed in Canada. The outcome of this project will lead Canada's research in flash coding to align with world's research forefront, and help to build a strong Canadian research community and industrial sector in this technology area. Graduate students participating in this project will be trained to develop technical expertise and skills that will be in great future demand.</p>""583636,""Mao, Zhijun"
"590627"	"Marbach, Peter"	"Social Networks Analysis"	"<p>Online social networks have revolutionized the way we interact and share information over the Internet. Popular social networking applications include Facebook, LinkedIn and Twitter, which support millions of active users. While being enormously popular, we believe hat that these applications only scratch the surface of what is possible to do  and that there are tremendous opportunities in developing new and more advanced online social networking applications.</p><p>Developing such applications and algorithms poses new and fascinating research problems. The major research challenge is to develop a formal understanding of online social networks both in terms of how these networks are formed and evolve over time, as well as how they can be used to efficiently share and distribute information. The goal of the proposed research is to address these challenges by working towards a theoretical foundation of social networks. As such, the focus of the proposed research program is on creating  mathematical models for how social networks are being formed and used to shared and distribute information.</p><p>We believe that a formal understanding of how social networks are being formed and used to share and distribute information can lead to fundamentally new algorithms both for social networking analysis and the implementation of social networking applications. In particular, the striking feature of social networks is that there exists a strong coupling between the network topology and the information flow within the network. We believe that by quantitatively modeling and analyzing this relation, it is possible to design algorithms that are simpler and are more efficient compared with existing algorithms, and yet lead to (close) to optimal performance.</p><p>The proposed research has the potential to have an impact both on the academic research community and for practical social networking applications. On one hand, creating a mathematical models that can be used derive properties of social network and to derive social network analysis algorithms will deepen the understanding of social networks. On the other hand, this understanding can also be used to create and design novel online social networking applications that go well beyond of what today's social networking applications are capable of doing. In particular, we envision new social networking application that are able to interact with more naturally, and are able to more accurately filter the content that users are interested in. This is in contrast with the large amount of noise that exists in today's social network application such as Twitter.</p>""569345,""Marceau, Danielle"
"589966"	"Marti, Jose"	"Emerging Power Grid Integration in a Unified Simulation Environment"	"     With the emergence of distributed generation and increased online measurements and optimization, the traditional separation in the analysis and operation of the high-voltage transmission system and the lower voltages distribution systems is disappearing. In transmission, the requirement of 60 Hz synchronicity in maintaining angle and frequency stability across an extensive network of interconnections is ""naturally"" aided by the large inertia of the rotating masses of the large thermo and hydro generators. Wind and solar plants, on the other hand, require sophisticated control mechanisms, together with fast switching power electronic devices, to create a ""synthetic"" inertia that allows their coupling and join operation with the 60 Hz main grid.<br />     The 60-Hz synchronicity of the new grid will require a close integration of the traditional grid with inertias in the order of the seconds, determined by the mechanical masses of the large hydro and thermo generators, working together with inertias in the order of the milliseconds, created by fast controllers in wind and solar farms driven by power electronic valves with time constants in the order of the microseconds. Three orders of time constants will be working together. In terms of solution techniques, electrical voltage and current phasor solutions in the main grid will have to ""live together"" with faster changing electrical voltages and currents in the wind and solar plants and with even faster changing voltages and currents in the power electronics converters.<br />     The increased penetration of distributed non-dispatchable Variable Energy Resources (VER) will affect all aspects of the existing grid's design and operation, including technical aspects, economic and business models, and social and environmental impacts. With the new integrated grid, the optimization objectives of high voltage transmission will have to be integrated with the optimization objectives of distribution and customer levels. These three levels of objectives are not always in agreement with each other and individual optimums will have to be combined with global optimums. A fine granularity distributed control system, and a correspondingly extensive layer of ICT infrastructure, will be required to support the extended grid. The increased complexity will make the grid more sensitive to internal failures and more vulnerable to external cyber attacks.<br />     Under this grant, we will advance the development of a unified comprehensive simulation environment that can integrate the complexity and multiplicity of objectives of the 21st century power system grid.""573983,""Marti, Jose"
"590693"	"Mazalek, Alexandra"	"Grasping Science: Designing Tangible and Embodied Interactions for the Computational Sciences"	"The goal of this research program is to design, develop, and evaluate tangible and embodied interfaces that facilitate the manipulation and interpretation of large and/or complex data sets. The specific focus is on interface design to support computational science practices, and the research program thus builds on the PI's ongoing work with biochemical engineering and computational genomics researchers.<br /><br />We propose that the growing area of human-computer interaction (HCI) research known as tangible and embodied interaction (TEI) offers unique opportunities for enhancing research practices and discovery in the computational sciences. This is based on the increasing body of evidence from cognitive science and neuroscience, which shows that sensory and motor processes play an important role in cognitive processes, such as scientific reasoning and problem solving. However, interactions with computational simulations and large/complex data sets currently make only limited use of the motor system, as these interactions remain constrained by mainstream human-computer interfaces like keyboards/mice and, more recently, touch interaction. In contrast, TEI employs physical objects, surfaces, and spaces as controls and representations for digital information, supporting joint engagement of motor, perceptual and cognitive processes in interaction with computational systems.<br /><br />We further propose that the need for innovative interfaces that can enhance discovery processes involving large and/or complex datasets presents an opportunity to drive forward the field of TEI. The TEI community has suggested that the field needs to take on more complex problems and computation. Yet, significant limitations for TEI remain, and the research area has not yet addressed scale and complexity at the magnitude applied in areas of computational science. The contributions of the research program include developing a fundamental understanding of how TEI can support research practices and processes in specific computational science domains; the design, development, and evaluation of prototypes of these interfaces that can be applied in practice; and the formulation of a design framework that can aid HCI designers and researchers in creating systems to support computational science practices.<br /><br />The program will advance the field of TEI by addressing problems that have not yet been addressed by the field due to their complexity and/or scale, but that can benefit from TEI techniques. More broadly, the computational sciences are critical to scientific and technological advancement in Canada, and the design framework developed in this research program will serve as a practical guide for creating tools that have the potential to better facilitate the ability of computational scientists to solve difficult yet societally critical scientific problems in areas such as biochemical systems and genomics.""574363,""Mazar, Adil"
"590289"	"Mazumdar, Ravi"	"Complex interacting networks and systems: Models, analysis, and algorithms"	"Complex systems and networks such as the internet, wireless networks, the cloud, and social networks, are comprised of large numbers of interacting entities (users and resources) that interact in a random way that determine their functionality and the macroscopic behavior of the overall system. To manage the complexity of networks and systems we need to understand their dynamic as well as emergent behavior in order to design algorithms for their stable, efficient, and secure operation as well as for providing user experience. The systems of today are vastly different in scale and the basic issues are not simply traffic issues. The methodologies required are therefore very different that go beyond classical queueing models, optimization,  and  resource allocation. Since the systems are large in terms of users, resources, and are spatially distributed we need to understand the issues of local vs. global behavior and the design of distributed algorithms that do not require too much information. The research will aim at understanding the impact of heterogeneity and the size and population of resources and how these can be used to characterize the performance. <br />The  research will concern itself with the issues of complexity and the stochastic behavior of such networks and systems. The aim is to address the main problems that impact stability and performance. The goal is the design of  simple efficient and distributed algorithms for user experience and system efficiency. The techniques will draw upon stochastic networks, generalized voter models, mean field or McKean-Vlasov analysis, randomized algorithms, random graphs especially the spectral theory, and bandit processes.<br />The research will be pursued with the following principal application thrusts: 1) Wireless networks, 2) Social networks and security, 3) Network performance and algorithms , and 4) Cloud computing and applications.  The aim is to study the interplay between size and emergent behavior, while using averaging to devise  simple algorithms for resource optimization and providing predictable performance to both users and operators.<br />In each of the constituent application areas the research contributions will be in models, their analysis with performance being the focus, and the design of distributed and randomized algorithms for scalability without sacrificing efficiency. The aim is to develop the theoretical frameworks necessary for the design of robust and stable algorithms that will allow resource allocation and optimization. A key component is the training research students to acquire wider variety of tools and techniques that are necessary to deal with the new problems that are not part of classical training which will lead to new courses and monographs. The research likely to enrich the constituent fields from where the tools will be drawn namely stochastic networks, bandit processes, randomized algorithms and graphical models.""588574,""Mazumder, Asit"
"582128"	"McCarten, Isabel"	"Ontario"	"CANADA"
"589651"	"MehmetAli, Mustafa"	"Resource Optimization of wireless access networks, software defined networks and cloud computing for the Internet of Things"	"Next generation Internet, commonly referred to as the Internet of Things (IoT), has the potential to improve our daily lives significantly. The IoT will allow global interconnection of heterogeneous smart objects through mobile-based machine-to-machine (M2M) communications. The list of smart objects includes smartphones, smart cars, homes and cities and IoT is expected to find applications in transportation, healthcare, smart environment (home, office, plant) and social networks. IoT presents many new challenges in the scale of the number of devices to be connected, communications, robustness, security and big data. It is expected that the number of devices to be deployed in IoT to reach trillions. Most of the mobile devices to be connected have limited processing power, storage and battery life. Cloud computing will provide the additional computing power and storage needed by mobile devices. Large amount of raw data will be continuously collected through real-time sensors and stored in clouds for analysis and inference, which will create new knowledge. <br />Wireless communications is expected to play a critical role in IoT as it is to provide access to mobile users, sensor networking etc. The demand for higher data rates and massive number of wireless devices with differing quality of service (QoS) requirements to be connected to the Internet is creating new challenges for next generation (5G) wireless networks. It is envisioned that 5G networks will meet these demands through very dense deployments, moving into millimeter wavelengths spectrum and centralized processing. It is expected that 5G technology will result in hierarchical and heterogeneous network architecture (HetNet). HetNet will have multi-tier cell architecture with cell sizes ranging from macro to small cells. As networks become denser, interference management will become more critical due to multi-cell interference. As a result, centralized control of radio access networks through software-defined-network (SDN) management has been proposed for efficient resource allocation and interference management. <br />A very important goal of this research proposal is development and optimization of network architectures and communication protocols for 5G networks. We will consider optimization of resource allocation, energy efficiency and QoS provisioning in 5G networks that maximizes network utility. We will develop optimization tools for design of the networks as well as heuristic algorithms for online implementation. We will also consider optimization of resource allocation in cloud computing centers under job quality of service (QoS) requirements. <br />The proposed research program will make contributions to 5G wireless networks, SDN and cloud computing centers, which will contribute to the realization of IoT. The proposed research will benefit the telecommunications industry in Canada which is a strategic economic sector.<br />""577871,""Mehrabian, Abbas"
"589456"	"Mercer, Robert"	"Argumentation in scholarly biomedical literature: A computational theory and its software implementation"	"<p>Scientific research articles are written by scientists to argue for a new scientific claim and to persuade other scientists to accept this claim. The philosopher Toulmin has proposed a framework that captures how people use language to convince others. This framework, drawn from observation and analysis of legal arguments and transformed into the experimental science arena, is composed of a scientific <em>claim</em><span style=""line-height: 1.5em;""> backed up with </span><em style=""line-height: 1.5em;"">evidence</em><span style=""line-height: 1.5em;""> drawn from the authors' scientific experiments. Far too much new scientific knowledge is being accumulated in the biomedical sciences for any person to keep abreast of. New computer technologies can help experimental scientists and clinicians cope with this deluge of information by aggregating all of this new knowledge into a useable form.  </span><span style=""line-height: 1.5em;"">My research program will provide some of these new computational technologies. A key to these new technologies will be methods to capture these persuasive arguments in a format that is manipulable by computers. Argumentation is accomplished through the use of a variety of rhetorical devices, all having been well-studied non-computationally by applied linguists and rhetoricians of science: rhetorical methods in (oral and written) language, and the use of citations, figures, tables, etc. in scientific writing. My research program will capture these rhetorical devices that are found specifically in published biomedical research papers in a computational theory of argumentation structures and translate these text-based argumentation structures into Abstract Dialectical Frameworks, a state-of-the-art Artificial Intelligence model of argumentation. To construct the argumentation structure, these components must be identified. This is accomplished using standard supervised Machine Learning methodology which requires analysis of the text using Computational Linguistics techniques to extract text-based features to generate the machine learned model. The text analysis will use the many widely used information resources (ontologies, databases, software tools) that have been developed by Biomedical and Computational Linguistics researchers for this purpose. In addition we will also develop novel methods to extract information from tables. Outcomes of this research program will have profound societal benefits in the fields of experimental and clinical medicine. Having computer-based representations of scientific arguments can lead to significant changes in how biomedical research is performed. Inconsistent experimental results can be detected, unrecognized biomedical relationships can be discovered, and the vast biomedical literature will be synthesized into a more accessible form for clinicians' use.  Clinicians, with this information, will be able to diagnose rare cases more confidently and as part of their clinical practice will be able to choose more appropriate genetic tests to confirm their diagnoses.</span></p>""582723,""Merchant, Brian"
"589424"	"Mezghani, Neila"	"Méthodes avancées d'analyse de données multimodales et de classification de formes en génie biomédical"	"<p>Le développement rapide des technologies de l'information a engendré une augmentation considérable de la masse de données hétérogènes (de nature différente) enregistrées à partir de plusieurs modalités d'acquisitions. Ces données, caractérisées de multimodales et hétérogènes, se présentent souvent dans des formats différents (signaux, images, bases de données, etc.), provenant de sources de données différentes (capteurs de mouvement, imageries radiographiques, caractéristiques démographiques et évaluations cliniques, etc.) ce qui rend l'extraction des connaissances difficile. A l'occasion du 50ème anniversaire de la conférence Methods of Information in Medicine en 2011, les enjeux du forage de données ont été discutés et un appel à la réflexion approfondie sur les perspectives de l'analyse de données et de l'extraction de connaissances dans le domaine biomédical a été lancé (Bellazzi, Diomidous et al. 2011).</p><p>Les données biomédicales se caractérisent par leur grande dimension dans le sens où elles sont constituées d'un ensemble élevé d'observations (par exemple, au cours du temps, durant plusieurs cycles d'activités ou durant plusieurs états). De plus, ces données sont rares dans le sens où les bases de données actuelles contiennent relativement peu d'échantillons. Ceci nous confronte, souvent, à faire des analyses de données dont le nombre de variables dépasse le nombre d'observations </p><p><span style=""line-height: 1.5em;"">Le programme de recherche que nous proposons de mener a un caractère aussi bien fondamental, touchant le développement théorique et méthodologique, qu'appliqué, appréhendant des applications importantes en génie biomédical. Son objectif général est de développer des méthodes avancées d'analyse de données multimodales et de classification de formes et leur mise en application en génie biomédical. Les objectifs visés par la présente demande sont les suivants:</span></p><p><span style=""line-height: 1.5em;""></span>Objectif 1 : Classification de données multimodales hétérogènes et application à l'aide au traitement de pathologies du genou.</p><p>Objectif 2 : Modélisation de signaux de grande dimension et application aux données biomécaniques.</p><p>Objectif 3 : Classification de signaux complexes et application à l'aide au diagnostic de pathologies du genou.</p><p>La réalisation de ces objectifs contribuera directement à l'avancement des connaissances, d'une part,  dans le domaine d'analyse de données et de classification de formes et, d'autre part, dans le domaine de la santé par le développement d'applications d'aide au diagnostic et d'outils d'aide à la décision de traitement de pathologies du système musculo-squelettique. Les méthodes d'aide au diagnostic de pathologie du genou seront intégrées au système d'évaluation fonctionnelle du genou KneeKG, actuellement commercialisé par la compagnie canadienne EMOVI qui vient d'être reconnu par le Fond de recherche québécois en santé comme une des 10 percées scientifiques du laboratoire au patient.<br /><br /></p>""575554,""Mezghani, Neila"
"589750"	"Misic, Jelena"	"Resource management in cloud data-centers"	"<p>Today's data center applications include web search, financial services, recommendation systems and social networking which require request completion within a  fraction of a second and they are usually called ""online data intensive applications"" (OLDI). Those requirements are also known as soft real-time requirements and have significant impact on service providers. For example Amazon reported that every 100ms increase of latency reduces business revenue by 1%.</p><p>The aim of this proposal is to develop algorithms for decreasing response time of applications/jobs running in data centers. In order to achieve that, deep understanding of  multi-layer protocol interactions in data centers  is needed. We will design cross layer protocol stack for data centers where application information is passed to transport protocol, recorded  in switches using SDN and used to schedule the flows in order to meet soft-real time application bounds.<br /><br />Our research activities will include designing priority scheduling of flows based on data skew coming from MapReduce partition/aggregate workflow and designing application admission control with placement of virtual machines. We will also evaluate time needed to set entries in switch forwarding table, and develop schemes for their improvement. We will  deploy  SCTP (stream control transmission protocol) as transport protocol  in data center network and develop schemes for security isolation among applications in data center.<br /><br /><br /></p>""589678,""Misic, Vojislav"
"589419"	"Mitiche, Amar"	"Image motion in computer vision: Variational processing and experimental verification"	"The purpose of this research is to provide a common algorithmic framework under the variational paradigm of the fundamental subjects of image motion processing in computer vision. The subjects are motion estimation, detection, tracking, and three-dimensional interpretation. Motion estimation evaluates the optical velocity field from the image spatiotemporal variations. Motion detection determines the region in an image covered by the moving objects, i.e., it segments the image into a foreground of moving objects and its complement. Tracking is the process of following moving objects through an image sequence, and three-dimensional (3D) interpretation recovers the shape and motion of the objects in space which produced the image motion. Variational methods formulate problems from an optimization viewpoint, i.e., as the optimization of an objective functional which expresses all of the constraints that characterize the desired solution. As a result, they can compute very accurate solutions and do so efficiently. In spite of their manifest advantage, variational methods have not been systematically investigated in motion analysis as there are currently many important problems that have been addressed by traditional heuristic processing schemes. This research will investigate problems currently treated by non-variational methods but will also address important open problems. The investigations will consist of a variational analysis of each problem, which includes the design of the objective functional and an inquiry about efficient means of its optimization, such as parametric graph cuts and the calculus of variations via level sets. The resulting formulations will be verified by detailed comparative experimentation using image sequences from freely accessible databases dedicated to the validation of image motion processing algorithms. The anticipated outcome of this research is a unified body of efficient, easily usable, and well documented algorithms for motion analysis.""588291,""Mitra, Sushanta"
"589621"	"Mohammed, Noman"	"Privacy-Preserving Biomedical Data Sharing"	"The adoption of Electronic Health Record (EHR) systems has accelerated the collection and sharing of health data. EHR databases are often integrated with biorepositories and shared to facilitate genome-wide association studies (GWAS). While such integration and sharing of biomedical data have many social benefits (e.g., personalized medicine), it also raises concerns about individual privacy. A recent survey on privacy attacks demonstrates various techniques for breaching biomedical data privacy, and these techniques question the efficacy of the protection mechanisms that are currently in practice. <br /><br />The aim of this research is to understand, evaluate and contribute to the problem of sharing biomedical data while protecting individual privacy. The research activities are organized along the three related tasks. The first task is to more fully understand the privacy threats and challenges of sharing biomedical data, and to propose appropriate privacy models. The challenge is to thwart privacy attacks caused by inferences between different data types, and to quantify the risk of data sharing for a given adversary model. The second task is to develop data-sharing mechanisms with strong privacy guarantees. The challenge is to provide a reasonable trade-off between privacy and data utility. The final task is to develop distributed mechanisms to securely integrate data from multiple institutions to facilitate controlled and effective data sharing. The challenge is to propose a scalable algorithm that safeguards the raw data stored across multiple institutions against an untrusted data aggregator.<br /><br />The success of biomedical research heavily depends on the availability of large datasets. At present, there is no specific national guideline for sharing biomedical data. The outcomes from this research will help to bridge this gap by providing clear guidance and technical solutions, and will help increase public trust in biomedical research. Further, this research will advance the current state-of-the-art techniques for privacy-preserving data sharing and broaden our understanding of the privacy threats caused by the interconnections of different types of data.""579565,""Mohan, Sibin"
"578284"	"Morrill, Dustin"	"Newfoundland and Labrador"	"CANADA"
"590810"	"Mousavi, Parvin"	"Towards Integrative Data Analysis for Predictive Modeling in Biomedical Computing"	"<p>This research is aimed at developing innovative methods to analyze medical data and understand the mechanisms of actions of complex disease, specifically prostate cancer (PCa). The new era of large-scale biology is characterized by insurmountable amounts of data including medical images, high throughput genetic information, large public databases, and clinical signals. Data abundance stands in contrast to our limited knowledge about the biological events underlying disease. The proposed research seeks to close this gap through a novel framework that uses state-of-the-art image analysis methods, and integrates molecular data and biological information, to formulate more accurate models to characterize cancer.</p><p >Although PCa is the third leading cancer-related cause of death in North American men, if diagnosed early, it can be managed with a 90% long-term disease free survival rate. Diagnosis of PCa requires biopsy, guided by ultrasound. However, ultrasound guidance is not tailored to an individual and has poor sensitivity in detection of cancer, specifically its aggressive forms. There is an urgent need to clearly distinguish between aggressive PCa that requires intervention and indolent PCa that can be put on ""watchful waiting"". The algorithms developed here  will address this challenge by i) augmenting  ultrasound images with information from probability maps indicating the aggressiveness of cancer,  ii) comprehensive analysis of molecular information to stratify disease outcome; and iii) integration of features at multiple resolutions of data to build a comprehensive model of PCa.</p><p >The unparalleled methods for acquisition and analysis of ultrasound signals provide enhanced prediction of the existence and extent of PCa. The innovative models formulated by integration of features of multiple modalities facilitate the discovery of personalized therapies and biopsies. The methods and software built will help the clinicians make better management decisions for PCa to reduce its mortality while balancing it against the risks of unnecessary overtreatment. The computational methods sought will advance research in image and signal processing, machine learning, systems biology and software engineering. Methods are discussed in the framework of Pca; however, they are extendible to other disorders. </p><p >Training of highly qualified personnel is a primary focus of this work; three PhD and three Master's students, as well as five summer undergraduates will be involved in this research program.</p>""573395,""Mousavi, Pedram"
"577795"	"Muise, Christian"	"Ontario"	"CANADA"
"590150"	"Muldner, Kasia"	"Extending educational technologies with user models of cognitive, affective and meta-cognitive student states"	"<p>Personalized instruction is key for improving not only students' <em>cognitive</em> expertise related to the target domain (e.g., mathematics) but also their <em>affect</em> (e.g., interest about the domain) and <em>meta-cognitive skills</em> (domain independent abilities needed to learn effectively). Given that computers are becoming ubiquitous in today's classrooms, there is an opportunity to develop educational technologies that maximize student learning through personalized instruction adapted to a given student's unique needs. The prerequisite for realizing this adaptation is that technologies can recognize student states of interest (e.g., what students know about the target domain, how they feel during the learning process, and their learning strategies). This task is accomplished by the <em>user model </em>component. Traditionally, user models have focused on students' cognitive states. However, all three elements (C<span style=""line-height: 1.5em;"">ognitive, Affective, Meta-cognitive, <em>CAM</em>) impact learning outcomes and so need to be taken into account during the modeling process. A key difficulty in doing so pertains to the fact that there is little if any direct information available to the model on higher level states associated with, for instance, affect or meta-cognition.</span></p><p>The long term goal of my research program is to design and evaluate user models for CAM states in a variety of educational contexts and applications. In the short term, my research agenda entails applying machine learning and expert centric approaches to construct models for CAM states based on data  coming from two sources: (1) students' interaction with the target technology and (2) sensing devices (eye tracking, EEG). The models will be designed for four educational contexts that have the greatest potential to make an impact in terms of HQP training, research contributions, and practical implications, including <em>analogical problem solving</em>, <em>collaborative activities</em>, <em>creativity in open-ended environments,</em> and <em>learning from teaching robotic agents</em>. The outcomes will correspond to user models for each of these contexts that are capable of recognizing CAM states most relevant to the corresponding context, as well as general techniques for devising and evaluating models from interaction and sensor data.</p>""580147,""Mullen, Aleysha"
"589820"	"Müller, Martin"	"Québec"	"CANADA"
"589361"	"Najm, Farid"	"Computer-Aided Design for High-Performance Large-Scale Integrated Circuits"	"<p>In order to enable the design of future advanced large-scale integrated circuits (ICs), we are developing computer-aided design (CAD) tools and techniques to address a number of challenges relating to the verification, optimization, and design of the chip power distribution network (PDN), also simply referred to as the power grid. With increased power dissipation and reduced supply voltage, large microprocessors today draw over 150 Amperes from the external supply! These levels of current are unprecedented in microelectronics, and are a key challenge for design.  Apart from the design issues of delivering well-regulated low voltages at these extremes of current, a key problem for designers is to make sure that the increased voltage drop and/or rise (due to IR-drop and/or Ldi/dt drop) in the chip PDN do not lead to circuit delay errors and functional failures. However, checking the grid node voltages is very time-consuming and expensive, so that it is often incompletely done, or not done at all. We have worked for the last decade on power grid verification, but we now propose to embark on an innovative new research direction to develop new techniques for enabling verification, design and optimization of the power grid. This new direction involves solving the inverse of the traditional verification problem: instead of asking users to provide the circuit currents and reporting to them whether or not the grid is safe, we simply tell them what ranges or combinations of circuit currents would be guaranteed to maintain grid safety. There are many advantages for this approach, such as large improvements in the speed of verification, as well as valuable information for design placement and floorplanning regarding the locations on the chip where high power blocks may be placed.</p>""588604,""Najmanovich, Rafael"
"590351"	"Nicolici, Nicola"	"Systematic and Structural Methods for Post-Silicon Validation"	"<p>The difficulty of guaranteeing that electronic circuits and systems meet the stated or implied goals has been continuously increasing over the past decade. This is mainly due to the exponential dependence of the state space on the growing number of state elements, as well as the excessively large number of clock and power domains needed to facilitate performance improvements under power and energy constraints. Pre-silicon verification is commonly employed to ensure the consistency between the design and its specification. Before tapeout what can be measured is limited by the simulation time and accuracy, and designs are released for manufacturing when the confidence level is deemed sufficient. Manufacturing test is focused on screening for physical defects in each fabricated device; considering that its reference is the design implementation, manufacturing test is not concerned with finding and identifying subtle design errors (or bugs) that have escaped to silicon prototypes. Thus the verification tasks employed during the pre-silicon phase continue on these early silicon prototypes, a term commonly referred to as post-silicon validation.</p><p>Because simulation is 6 to 9 orders of magnitude slower than the actual silicon prototype, it is not practically possible to compute golden responses a-priori. Together with the lack of internal node access, this makes post-silicon validation an intractable problem. Furthermore, state-of-the-art circuits and systems might fail in one experiment and operate correctly in the subsequent ones. This is because subtle design errors that escape to the silicon prototypes are often excited by not-easily-repeatable events. For example, they can be triggered by rare interactions caused by asynchronous interfaces and/or dynamic changes in the circuit's operating mode, e.g., for trading-off power vs performance in response to varying workload or environmental conditions. Considering the above, investigating new systematic approaches that can assist the post-silicon validation tasks can bring significant benefits to the broader electronics industry, in terms of both productivity gains and, more importantly, the quality of the final product.</p><p>As part of this research program, we aim to investigate automated methods for all the key building blocks of a general-purpose post-silicon validation system: trace collection and analysis, event detection, post-silicon stimuli generation and application, and coverage measurement. All of the above will rely on the design structure rather than its functionality. It is our position that a structural approach will provide the much-needed theoretical foundations, which will facilitate seamless portability of automated methods across the semiconductor industry, as well as scalability to the next-generation of integrated circuits which are expected to aggressively trade off power vs performance at run-time. </p>""577718,""Nicolle, Paméla"
"590450"	"Nikolaidis, Ioanis"	"Protocols and Algorithms for a Sustainable Internet of Things"	"We define as Internet of Things (IoT) the massive and ubiquitous deployment of embedded, usually wireless, communication devices, installed into everyday life objects. Estimates of the IoT scale suggest that, in the not-so-distant future, it will consist of tens of billions of devices! The vast deployment scale of IoT devices has the potential to be the cause of a massive e-waste problem looming on the horizon. Yet, if we ask today how the IoT devices and systems should be designed to extend their useful life to multi-decade, and ideally perpetual, operation, we cannot point to a comprehensive answer. <br /><br />The proposed research program addresses the question of what can be done today at the algorithmic, protocol, and systems level, towards making IoT sustainable. The proposal outlines a systematic approach, with tools and methodologies, to build this long-lasting IoT. The systematic approach described consists of four main components: (a) adding failure resilience for various applications during the design stage, as well as, developing generic means to handle failures during operation, (b) enabling the devices to identify when communication is impaired by interference and endow them with mechanisms  to handle broad classes of interference, including as-of-yet-unknown forms, (c) aligning the communication protocol with the application logic, using specialized scheduling, to reduce the energy consumption required to meet the application communication requirements, and (d) harvesting energy from the immediate device environment to attain self-powered operation and autonomy. <br /><br />In the proposed research program, I describe a number of technical approaches to these four directions and plans to empirically validate our methods and tools in the context of our ongoing projects in designing, installing, and programming sensor systems for smart assisted living environments and energy efficient buildings.""583756,""Nikoletos, Nicholette"
"577832"	"Noorian, Zeinab"	"Saskatchewan"	"CANADA"
"581995"	"Parisotto, Emilio"	"Québec"	"CANADA"
"589476"	"Parsons, Jeffrey"	"Using Concept Lattices to Reconcile Semantic Heterogeneity in Data"	"<p>In modern information systems, data capture often cannot be closely controlled by traditional organizational mechanisms intended to enforce common meaning. Additionally, data are no longer used only for established and predetermined purposes. Indeed, one of the key promises of Big Data and Open Data is the opportunity to repurpose data for uses that were not anticipated when systems to collect and store the data were designed. This increasingly diverse information landscape creates significant challenges in integrating or combining information from various sources in ways that retain the semantics (meaning) of the underlying data. Integration difficulties arise from the independence of sources, resulting in a high level of semantic heterogeneity between sources. Achieving high-quality semantic integration is essential to take full advantage of the opportunities afforded by Big Data to support decision-making. </p><p>The proposed research introduces the notion of ""concept lattice"" as a mechanism to provide semantics for data that can be used to integrate heterogeneous independent sources. In a concept lattice, nodes are concepts, interpreted as predicates applied to phenomena (instances) (e.g., Male(John)). A directed arc indicates a ""precedence"" relationship between two nodes;  that is, possessing one property  implies possessing another  (e.g., Voter(John)  -> Adult(John)). In this framework, the semantics of a node is entirely determined by the pattern of (and relationships among) arcs entering and leaving it.</p><p>A concept lattice is a ""lightweight"" conceptual model of a domain, in contrast with traditional approaches that rely on ""heavyweight"" models (global or mediated schemas). Nodes may be considered either classes or properties, depending only on the pattern of incoming and outgoing arcs. This semantic relativism facilitates integration of concept lattices, where a concept may be a class in one lattice but a property in another. Moreover, two semantically equivalent nodes from heterogeneous sources can have different manifestations, indicating different semantic interpretations of the values of a property.</p><p>The proposed research program will build on my earlier work by focusing on four objectives, working with graduate students and academic colleagues. First, we will formally define the core elements of a concept lattice structure, and develop reasoning mechanisms to navigate lattices. Second, we will design and implement a prototype to store and process concept lattices. Third, we will use concept lattices as a foundation to integrate information across heterogeneous sources by defining ""merge nodes"" - nodes from independent lattices that carry the same meaning and can be used to combine lattices. Fourth, we will evaluate the effectiveness of concept lattices in improving information retrieval from heterogeneous data sources.</p>""573493,""PartoviNia, Vahid"
"590663"	"Patel, Hiren"	"Computer Architecture Support for Complex Networks"	"<p>Complex networks play an important role in our lives today. They can be used to gain a better understanding of our actions in the past, predict our actions in the future, and use these predictions to influence outcomes.  For example, product recommendation systems represent a user's interactions with their database of items and past purchase histories to suggest future items that may be of interest to the user.  The potential benefits of using complex networks for inferences are enormous.  They are already in use for the following applications: traffic congestion, infectious disease and security threat analyses. </p><p>A key challenge in inferring properties from complex networks involves performing queries on complex networks efficiently.  This is difficult because of the expected growth in size of future complex networks ranging from hundreds to thousands of petabytes.  This brings about an interesting observation that future computing platforms used to query complex networks will be accessing large amounts of data instead of performing large amounts of computation.  This means that complex network workloads will not be dominated by computation, but accesses to data.  Unfortunately, modern computing platforms are not designed based on this observation, which would result in large query times on complex networks. Consequently, the long-term vision of the research program is to investigate the design of computing platforms for fast and efficient querying of large complex networks.</p><p>Dr. Patel proposes to address three topics in this proposal. The first topic investigates notions of data locality that best represent proximity in complex networks, termed graph-locality.  Dr. Patel believes that queries on complex networks do not exhibit traditional notions of spatial and temporal locality. Hence, discovering graph-locality metrics will provide insights into redesigning the memory hierarchy to exploit these metrics.  The second topic explores data compression for large complex networks to mitigate the amount of data to move between memories in computing platforms.  Since compression of entire networks is impractical, Dr. Patel will discover techniques for selective compression using graph-locality.  The third topic proposes moving computation closer to the data instead of traditional approaches that require moving data to computation for processing.  This clearly follows the observation that future complex network workloads will be data-access-heavy and not compute-heavy. The research methodology takes a balanced approach with empirical, theoretical and implementation components. The research will result in high-quality publications, and eight HQP will be a direct result of it. They will have expert skills in computer architecture, algorithms, databases, systems, and hardware design.  Canadian industries in computing platform design and big-data software design will greatly benefit.</p>""574308,""Patel, Kamala"
"589368"	"Pattabiraman, Karthik"	"Building Error Resilient Software on Next Generation Computing Platforms"	"<p>As computer systems become smaller and faster, their reliability becomes an important concern. One way of achieving high reliability is to design the system with many redundant components, so that it can continue operating even if some of them fail. However, this approach entails significant costs in power and energy consumption, which are very important in next generation computer systems such as smartphones and data centers. Therefore, we need new methods to ensure the reliability of future computer systems that does not rely upon redundant hardware.</p><p><span style=""line-height: 1.5em;"">We propose to tolerate hardware errors at the application software level, by building software that is resilient to hardware faults.  The main rationale for doing so is that few errors make their way up the system stack to affect the application, and even the ones that do are not necessarily important to the application. Therefore, it is much more efficient to protect software applications from (most) hardware faults, without adding significant amounts of redundancy. The main challenge however is that it is difficult for programmers to reason about hardware faults at the software level, and to protect their applications without expending a huge amount of effort. Our goal is to completely automate the process of making applications error resilient and reasoning about them, without burdening the programmer. This requires novel techniques for program (static) analysis, formal methods, and empirical evaluations, which this proposal will focus on. </span></p><p>By leveraging the research in this proposal, next generation computer systems will be able to deliver higher performance and energy efficiency, even while operating reliably. This in turn can lead to higher adoption of mobile computing and smart phones, as well as more energy-efficient internet data centers. These are the drivers of economic growth, and as such, the research in this proposal has significant potential impact on these domains. Further, the HQP trained in this project will acquire important skills to design and validate reliable computer systems, which is becoming more important as computer systems are becoming more complex. Finally, the research proposed can lead to significant energy savings for next generation computer systems, which are predicted to occupy a substantial fraction of the total energy consumption costs by 2020, thus benefiting society as a whole.</p>""586439,""Pattabiraman, Karthik"
"589286"	"Pilarski, Patrick"	"Real-time Machine Intelligence for Assistive Technology"	"<p>During every-day life, humans leverage a detailed stream of sensation and actuation to interpret and manipulate the world around them. However, when a body part is lost or damaged-for example, through accident, illness, or birth complications-an individual can suffer a significant reduction in their ability to interact with the world and, as a consequence, reduced quality of life. New computing methods and assistive technologies are essential to augmenting and restoring an injured individual's sensory and motor abilities. My proposed research program focuses on a concrete example of this theme: using machine intelligence to create richer and more powerful interfaces to bridge the gap between a human and a robotic artificial limb that is affixed to their body. Designers of assistive robots of this kind face a number of challenges, perhaps the most significant of which is the development of natural, flexible control and communication methods that connect users to their assistive devices. It is apparent that traditional approaches to control and communication will become insufficient when faced with an increasing repertoire of diverse, data-dense sensor and actuation technologies. The literature also indicates that artificial limbs are in need of adaptive control systems-interfaces that adapt to both the unique factors in each individual and the day-to-day changes that occur in the user and their environment.</p><p><span style=""line-height: 1.5em;"">With this in mind, my proposed program of research will study, develop, and evaluate machine intelligence methods that accelerate progress on next-generation artificial limb technologies. In addition to studying core principles of machine intelligence and human-machine interaction, my proposed work will translate new algorithmic advances to practical use and assess their strengths and weaknesses during real-world deployment. I will focus specifically on computationally efficient approaches to prediction learning, control learning, and meta-learning that are suitable for ongoing use within a wearable robot.  As such, I will employ techniques from the area of reinforcement learning; my work will have its foundation in temporally extended predictions made via generalized value functions, and in actor-critic policy gradient methods. The ultimate goal of this program of research is to create a complete system of representation, prediction, control, and meta-learning that enables seamless collaboration between a human and their assistive technology. Being an active part of my proposed program of research will significantly enhance the academic development and career potential for my current students and those that join over the course of the program. Furthermore, platform technologies generated by this program will contribute directly to progress in the theory and practice of reinforcement learning, and will be readily transferrable to medical and commercial domains.</span></p>""569060,""Pilgrim, David"
"590292"	"Pisana, Simone"	"Enhancing nanoscale heat transport in novel materials and electronic devices"	"Electronic equipment accounts for a significant fraction of our rapidly increasing global demand for energy and it is projected to consume 30% of our energy budget in 10 years. Improving the energy efficiency in electronic devices such as the transistor at the computing core of data centers, or the light-emitting diode that will become pervasive in lighting applications, can have a remarkable influence on a country's future economy and environmental impact.<br /><br />Heat transport has profound effects on the energy efficiency, performance and reliability of electronic devices. Particularly at the nanoscale, heat flow is greatly affected by low-dimensionality, device geometry and the presence of interfaces across materials with dissimilar phonon spectra.  As device size is reduced or more functionality is added, the quality and number of interfaces that pose as barriers to heat flow have greater impact, and understanding their role can lead to ways to engineer energy-efficient devices. On a fundamental level, knowledge of how heat flow affects electronic transport phenomena can drive the discovery of new functionality. The proposed research aims to answer the following questions: how is heat transported in novel materials that may be used in future electronic devices? How can interfaces be tailored to improve cooling efficiency? How can one probe heat transport at the nanoscale? Answering these questions can in the long term shape the functionality and energy use of the evolving electronics landscape.<br /><br />Versatile optical pump-probe techniques have emerged to investigate heat transport in materials, interfaces and composites. In the short term we will adopt and extend these techniques to make them sensitive to heat transport over a large range of heat transport length scales, in order to span the diffusive to the quasi-ballistic limits. Additionally, electrical thermometry will allow us to probe devices locally. These tools will be applied to the study of heat flow in: (i) emerging 2-dimensional electronic materials, (ii) nanoscale composites with high thermal anisotropy, (iii) magnetic junctions used in magnetic field sensors and magnetic random access memories.<br /><br />The impact of this work will benefit Canada's economy in the short term through the training of highly qualified personnel and raising the profile of the country's university research work. Furthermore, research on thermal anisotropy in composites has immediate applications in the data storage industry, and can contribute directly to progress in hard disk drive products. In the long run this research program will improve the performance, energy efficiency and environmental impact of the world's rapidly growing electronic, computing and telecommunications infrastructure.""582037,""Pisano, Olivia"
"591043"	"Plamondon, Réjean"	"Modeling and Comprehension of Human Movements: Personal Digital Bodyguards for e-Security, e-Health and e-Learning"	"<strong>VISION:</strong> In five years, the ubiquity of hand-held tablets and cell phones, along with their increased computing power and ergonomic data capture performances, will make it possible to convert these devices into <strong>Personalized Digital Bodyguards (PDB</strong>s<strong>)</strong>. PDBs will protect people's sensitive data with signature verification, provide equipment use security with writer authentication <strong>(e-security)</strong> and perform word spotting to monitor user fine motor control, which can detect stress, aging and health problems <strong>(e-health</strong><strong>)</strong>. In the hands of children, these tools will help them to learn and master their fine motricity and become better writers and students <strong>(e-learning</strong><strong>)</strong>.<br /><br /><strong>BACKGROUND:</strong> Our team developed the kinematic theory of rapid human movements and its lognormal models ,,   to explain many basic phenomena reported in classical studies on human motor control and to study several factors involved in fine motricity. We have created several software packages over the years to extract the lognormal parameters from various pen tip velocity curves under a range of experimental conditions. These tools allow a researcher to study the state of a writer's neuromuscular system.<br /><br /><strong>BASIC HYPOTHESIS:</strong> <strong>the lognormality of the neuromuscular impulse response is a basic feature reflecting the behaviour of people who are in perfect control of their movements.</strong> As a corollary, motor control learning can be interpreted as a migration toward an ideal representation of perfectly mastered movements. In turn, aging and health problems should reveal a progressive departure from lognormality. In between these two progressive processes, for most of their lives, humans take advantage of lognormality to control their movements.<br /><br /><strong>GOALS:</strong> This research program focuses on <strong>three specific goals:</strong><br /><strong>1. Theory: Provide new experimental support to the kinematic theory using quantitative Magnetic Resonance Imaging (qMRI), and propose original ideas for ways to use the lognormality principle.<br />2. Technique: Define, implement and test an ergonomic research platform that takes advantage of new input devices for handwriting acquisition, powerful algorithms working on a server in a cloud environment, and Graphic Processing Units to investigate lognormality.<br />3. E-application: Exploit, expand and disseminate our know-how about lognormality for innovative e-applications in biometrical (e-security), biomedical (e-health) and neurocognitive (e-learning) engineering by defining new evaluation metrics in collaboration with leading experts all over the world.</strong><br /><br /><strong>EXPECTED IMPACT</strong>: As well as training many post-doc fellows and Ph.D students, the whole program will lead to major theoretical breakthroughs in neuromotor modeling, setting the stage for us to design efficient <strong>PDB</strong>s that will put individual security and protection choices in people's hands for a better e-society.<br /><br /><br />""592451,""Plan, Yaniv"
"590617"	"Polushin, Ilia"	"Generalized scattering-based design and cooperative control in teleoperator systems"	"The research program proposed in this application aims at development of new methods for analysis and design of force reflecting teleoperator systems that, in particular, include cooperation between different parts of a teleoperator system.  The research program will address important theoretical and practical issues related to the analysis and design of complex  teleoperator systems. The major directions of this new research program are related to the development of new powerful design framework for complex teleoperator systems that will be based on behavioural approach to modeling and extensive use of  generalized scattering transformations, as well as detailed investigation of methods and algorithms for motions/forces synchronization, consensus, and cooperation between parts of a teleoperator system. As an outcome of the proposed research program, new methods for analysis and design of complex teleoperator systems will be developed that are substantially more powerful in comparison with the currently existing methods. In particular, the approach based on behavioral modeling of teleoperator systems and nonlinear conic systems formalism as well as extensive use of generalized scattering transformations will extend the existing limited passivity-based and small-gain-based design approaches and will result in more general and less conservative design methods and procedures, particularly in the presence of communication delays. These new methods will also allow for an extension of many existing linear frequency-domain methods to the practically important case of nonlinear teleoperator systems. The new algorithms for consensus and synchronization in cooperative teleoperator systems will result in improved dexterity and precision of cooperative telemanipulation and allow for execution of complex and difficult tasks which require precise synchronization of motion and forces between slaves. These results will lead to design of new high-performance haptic teleoperation systems for wide range of practical applications, including minimally-invasive surgical systems, cooperative dextrous handling of materials, as well as haptic teleoperation of groups of  unmanned aerial vehicles cooperatively executing a task such as handling a common load.  The training of highly qualified personnel constitutes an important part of the proposed research program.  The training received over the course of the proposed research program will provide a solid background for the trainees' future professional success and will give them competitive advantage in today's high-tech job market. The results achieved during the course of the proposed research program are expected to make substantial impact on the theory and practice of advanced teleoperator systems.""576166,""Pomaranski, David"
"589760"	"Poulin, Pierre"	"Adaptive Multi-level Multi-phenomena Appearance Models"	"<p>Rendering realistic synthetic scenes requires an accurate representation of <span style=""line-height: 1.5em;"">3D objects and light transport.  The result of this simulation is filtered </span><span style=""line-height: 1.5em;"">in an image as surface appearance, which varies with distance, orientation, </span><span style=""line-height: 1.5em;"">incident illumination, reflection properties, interreflections, subsurface </span><span style=""line-height: 1.5em;"">scattering, microgeometry, and textures. </span><span style=""line-height: 1.5em;"">Light transport and visibility are often resolved using a combination of Monte Carlo (MC) sampling and ray tracing, well adapted through multiple </span><span style=""line-height: 1.5em;"">sampling to most integration problems in rendering.  However, they are </span><span style=""line-height: 1.5em;"">inefficient in complex scenes because they operate on the finest-scale of </span><span style=""line-height: 1.5em;"">the scene representation, and thus, this approach does not scale for </span><span style=""line-height: 1.5em;"">real-time rendering of complex scenes nor offline high-quality rendering. </span><span style=""line-height: 1.5em;"">Instead, we propose to adapt pre-filtered representations of geometries, </span><span style=""line-height: 1.5em;"">visibility, and appearances to the pixel size, i.e., to find an appropriate </span><span style=""line-height: 1.5em;"">level-of-detail computation for light-object interactions.  Our recent </span><span style=""line-height: 1.5em;"">LEADR method computes the color over an entire pixel footprint, </span><span style=""line-height: 1.5em;"">interactively and independent of the footprint's size.  It is filterable, </span><span style=""line-height: 1.5em;"">efficient, suitable for GPUs, scalable with complexity, and high-quality, </span><span style=""line-height: 1.5em;"">but it is limited to textured height maps under direct illumination.</span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">This research program will design several novel hierarchical </span><span style=""line-height: 1.5em;"">representations of surfaces, volumes, visibility/transparency, textures, </span><span style=""line-height: 1.5em;"">and light transport, to cast the phenomena that affect appearance into our </span><span style=""line-height: 1.5em;"">prefiltered approach.  We propose to generalize the concept of light </span><span style=""line-height: 1.5em;"">transport within a black box to all combined factors affecting appearance, </span><span style=""line-height: 1.5em;"">thus efficiently determining the appropriate representation under viewing </span><span style=""line-height: 1.5em;"">and illumination conditions, leading to real-time rendering of complex </span><span style=""line-height: 1.5em;"">scenes, as well as an adapted scheme to be integrated in </span><span style=""line-height: 1.5em;"">path-tracing methods.</span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">We will build knowledge from a number of specific case studies, such as </span><span style=""line-height: 1.5em;"">grass-to-lawn, tree-to-canopy, sand-to-dune, fabrics-to-drapes, </span><span style=""line-height: 1.5em;"">building-to-city, etc.  We will develop interpolation schemes between two </span><span style=""line-height: 1.5em;"">adjacent levels in a hierarchy.  Then we will work our way up by </span><span style=""line-height: 1.5em;"">introducing representations at coarser hierarchy levels, and adapting </span><span style=""line-height: 1.5em;"">interpolation schemes.  Next we will add the other effects affecting </span><span style=""line-height: 1.5em;"">appearance, integrating them into a more general model.  Our long-term goal </span><span style=""line-height: 1.5em;"">is to develop a unified multi-level multi-phenomena appearance scheme </span><span style=""line-height: 1.5em;"">suitable for high-quality real-time and offline rendering.</span></p>""568232,""PoulinDubois, Diane"
"590159"	"Pratte, JeanFrancois"	"3D Single Photon Counting Modules for Radiation Instrumentation and Medical Imaging"	"Optoelectronics systems have become the ubiquitous backbone of a multitude of applications, from smartphones and tablets to medical imaging instruments and telecommunication systems. They are in constant evolution, with performance improvement targets such as increasing the number of pixels, photodetection efficiency and timing measurement accuracy. Continuing impetus for growth is also provided by the integration of smart microsystems capable of total or partial signal processing from photodetectors. In that line of thought, optoelectronics microsystems integration and signal processing capability are starting to take advantage of the technological breakthroughs required to enable the ""more than Moore's law"" in the field of microelectronics. One of those technological milestones is 3D vertical integration, where a minimum of 2 integrated circuits are stacked and interconnected using Through Silicon Vias (TSV). This proposal is about the realisation of 3D Single Photon Counting Modules, where an array of photodetectors is integrated on top of the required front-end microelectronic circuits. The targeted optoelectronics microsystem features single photon detection capability with single photon resolution, the ability to timestamp the arrival of the photons with very high accuracy and advanced digital signal processing. The challenging objective is to achieve a timing resolution in the range of 100 fs to 10 ps. For this, novel photodetector arrays and readout integrated circuits in deep-submicron CMOS technologies are required and are the core of the short term objectives of this research program. This work will lead to the next generation of detection modules that will be used in Positron Emission Tomography scanner, the main tool used in cancer imaging and diagnosis, with unprecedented time-of-flight coincidence timing resolution and leading to considerable improvement in the image contrast. This has a direct impact on the reduction of the radiotracer radiation dose injected to the patient, on scan time (and consequently patient throughput) and new diagnostic opportunities. The impact of this program also extends beyond PET detectors, since many scientific apparatus and industrial instrumentations will benefit from single photon sensitivity and precise timing measurements, such as Diffuse Optical Tomography, fluorescence imaging, 3D imaging and in apparatus using Light Detection And Ranging (LIDAR) systems. The originality and innovation of the resulting 3D Single Photon Counting Modules represents a timely opportunity for Canada to participate in a high-impact international collaboration with CERN.""582099,""Pratte, Sydney"
"591064"	"Quine, Brendan"	"Micro instrumentation for space"	"Small satellites have become proven entry-level platforms for the development of successful academic, commercial and governmental space programs. The objective of this research program is to expand the capabilities of small space and near-space platforms through the development of miniaturized instrumentation that leverages the advantages of remote sensing from space. Our program will build on the success of our existing space hardware development programs. <br />The NIR region offers a window to remotely sense climatology, geology and geography on a global scale. The laboratory designed and now operates a miniaturized instrument that collects Earth spectra at 3 Hz and with a footprint equating to approximately 1.5 km from a Low Earth Orbit of 750 km. The spectra contain radiometric measurements of Earth reflection in this important energy-exchange region that may be analyzed to detect and quantify aerosols, clouds, coastlines, soil moisture content and atmospheric composition. This research will extend our Shortwave Infra Red (SWIR) sensor work to the development of instrumentation to measure the Earth's radiation budget as an indicator for climate change. We will enhance spectral throughput by developing instrumentation targeted at specific species and by extending the sensitivity range of the spectrometer. In tandem, the laboratory will also develop a hardware program to miniaturize new electromagnetic sensing instrumentation to further objectives including power conservation and power grid integrity. Our research will focus on the development of micro-instrumentation that, placed in an orbit about the Lagrangian point between the Earth and Sun, can provide direct detection of electromagnetic disturbances giving minutes of warning time to isolate distribution systems. We will develop and characterize the performance of a space sensor suitable for the reliable identification of solar events that cause power disruption and that may also be applied to localize faulty transmission components though UAV inspection. We will also investigate the application of SWIR technology to the detection of faulty power-line components. <br />The development of miniaturized space sensors can enhance significantly our ability to monitor weather and climate. Miniaturized instrumentation may be deployed in space more numerously than larger instrumentation increasing temporal and spatial coverage. Space and near-space monitoring of power grids and space weather provides a means both to improve the stability of supply and to reap significant economic efficiencies in power distribution. The research will employ HQP including three masters and three doctoral candidates who will each lead a major research objective.<br />""571681,""Quinlan, Roberto"
"589333"	"Quitoriano, Nathaniel"	"Lattice-Mismatched Liquid Phase Epitaxy"	"For decades researchers were limited to designing devices that were lattice-matched to a commercially available substrate; more recently, however, researchers have learned how to design around some lattice mismatch.  This work has resulted in state-of-the-art, strained-Si transistors, efficient (world-record breaking) solar cells, and nobel-prize-winning LEDs.  In this research program, the PI will develop advanced Liquid Phase Epitaxy (LPE is the least expensive way to grow high-quality semiconductors.) techniques to support and enable advanced semiconductor devices. Having worked in high-quality materials integration for 17 years, the PI has demonstrated the fundamental aspects of this proposal to grow high-quality Ge on Si, which will build a sturdy foundation for third-generation photovoltaics and other state-of-the-art devices. He now wants to demonstrate these techniques using LPE. The 4 Ph.D. students and 10 undergraduate students assigned to this program will gain valuable and broad experience in semiconductor materials growth, device fabrcation as well as materials and device characterization. The technology described in this proposal can be applied to a myriad of devices including large area electronics (like solar cells or displays), computer processors, and telecommunications. This work is expected to revitalize the field of LPE by bringing advanced techniques and reduce the cost for state-of-the-art devices.""585489,""Qureshi, Faisal"
"589857"	"Qureshi, Faisal"	"Next Generation Smart Camera Networks"	"<p>Camera networks have received much attention in recent years, in part due to their suitability in a wide range of applications. We also bear witness to a sudden increase in the availability of camera-equipped mobile devices. The immediate access to cameras creates new opportunities for designing the next generation of Smart Camera Networks (SCNs) capable of unattended operation and supporting new forms of collaborative sensing over expansive areas. We propose to develop scientific & theoretical foundations, investigate new methods, and research design principles needed to develop such camera networks.<br /><strong>Control and coordination in SCNs:</strong> We will develop scientific theory and mathematical machinery to study and characterize distributed sensing and control in ad hoc networks of non-stationary smart cameras, focusing in particular upon new models for collaborative sensing in such networks.<br /><strong>Networks of camera-equipped mobile devices:</strong> We will explore design principles and develop mathematical underpinnings for setting up short-duration, tasks-specific, ad hoc camera networks of transient and highly dynamic camera-equipped mobile devices in a manner that does not adversely affect the usability, battery-life, security, and privacy of these devices. <br /><strong>Crowd tracking and scene understanding in SCNs:</strong> We will develop scientific and theoretical foundations for dense crowd tracking using ad hoc networks of non-stationary, smart cameras. In particular, we will explore ""analysis via synthesis"" approaches for dense crowd tracking in camera networks, where virtual crowd simulations exhibiting the statistical properties of the observed crowd will be used to refine crowd analysis and facilitate scene understanding.<br />I envision significant benefits and numerous applications of this research program. The proposed research program provides many opportunities for unique and interdisciplinary HQP training. Furthermore, the outcomes of this program will directly benefit both local & international camera technology, video analytics, and digital entertainment industries. E.g., the theory and algorithms developed here will lead to more capable camera-equipped consumer devices that will support new forms of participatory sensing and collaborative scene analysis. Scientific, algorithmic, and technical contributions of this program will increase our understanding of distributed, collaborative scene analysis in large-scale ad hoc networks of non-stationary cameras, benefitting the wider camera networks and computer vision communities.<br />This proposal aims to carryout highly original, fundamental scientific and technical research needed to realize the next generation of smart camera networks, and my existing work on camera networks and image analysis makes me an ideal candidate for pursuing the goals identified herein.</p>""586891,""Ra, YongHo"
"589385"	"Ray, Nilanjan"	"Compressed Sensing for Computer Vision"	"The goal of computer vision (CV) is automated visual recognition by processing images and videos. CV significantly lags behind human visual capability. However, a few recent success stories in commercial applications are noteworthy:  face detection and recognition, optical character recognition, Kinect gaming, fingerprint verification and so on. Thus, ample research opportunity exists in the progression of CV in its theory and practice. The principal roadblock to the success of computer vision research seem to be the lack of a suitable numerical representation of images that the computer algorithms (AI / machine learning) can easily process. In particular, an image could be a collection of a million or more pixels, which is overwhelming for an algorithm to process meaningfully. The art will be representing these pixels in a compressed form, which would be useful for the algorithms. In this research program, I propose to look at compressed sensing (CS)-based representation of images for CV. CS is a rapidly emerging area in signal processing research that has acted as a paradigm shift in signal processing applications, because using CS, some signals can be reconstructed using far fewer samples than what the classical theory predicts. A prominent application example of CS is rapid magnetic resonance imaging, where the time of signal acquisition has been significantly reduced. In this research proposal, I explain the rational behind using CS within CV applications. Potential impact of the proposed research is the proliferation of CV methods to limited memory devices, such as wearable devices and understanding of the image representation for CV. The program also plans to train 4 PhD and 2 MSc students in the state-of-the-art CV research.""584599,""Ray, Nilanjan"
"590906"	"Rayside, Derek"	"Synthesis of Microfluidic Chip Designs using SMT Solvers"	"Microfluidic chips, lab-on-a-chip devices that have channels transporting liquids instead of wires carrying electrons, have attracted considerable attention recently from the bio-medical industry because of their application in testing assay and large-scale chemical reaction automation. These chips promise dramatic reduction in the cost of large-scale reactions and bio-chemical sensors. The market for microfluidics is estimated to grow to over $10B in the duration of this grant.<br /><br />As in computer chip design, there is an acute need for automation tools that can assist with design, testing and verification of microfluidic chips.  The current state of practice is that microfluidics chips are designed manually using a guess-and-check approach.  Typically a designer will select design parameters based on experience, and then code a simulation in Matlab, COMSOL, or a similar tool. If the simulation succeeds, then a prototype chip might be built for physical testing.<br /><br />This state of affairs is exacerbated by the reality that a microfluidic chip design must consider a wide variety of physical properties, potentially including fluid, pressure, spatial, electrical, thermal, and optical.  If the designer makes an error or omission on any one of these dimensions then the design might not work as intended.<br /><br />We propose a design methodology for microfluidic chips based on SMT solvers, with the goal of producing designs that are correct by construction. (At least correct with respect to the mathematical models that are used to describe the design; as in other areas of engineering, these models are sometimes imperfect representations of physical reality.) The chip designer will specify known values, such as the location of wells or certain activation voltages, and then the system will solve for the other design parameters (such as the pull-back voltages).<br /><br />Recently, CMU has developed dReal: an SMT solver for nonlinear multi-variate inequalities over the reals.  We have constructed proof-of-concept prototypes with dReal to demonstrate that it can be used for microfluidic chip design. <br /><br />We intend to develop high-level hardware description languages for microfluidic circuits, and a computational framework based on CEGAR (Counter-Example Guided Abstraction Refinement), using Reduced Order Models as the starting point.<br /><br />This proposal comprises 7 graduate students: 2 PhD + 5 MASc<br />1 PhD co-supervised with Abukhdeir (computational fluid dynamics)<br />1 MASc co-supervised with Ren & Backhouse (microfluidics)<br />1 MASc co-supervised with Kennings (electronic design automation)<br /><br /><br />Microfluidic chip design is exceedingly complex. There is a great need for design automation tools. Recent advances in SMT solvers make them applicable to these problems. We propose to use (and enhance) these solvers to create microfluidic design tools that help chip designers produce designs that are correct by construction. <br />""574926,""Razani, Marjan"
"590096"	"ReyhaniMasoleh, Arash"	"Efficient and reliable computations for lightweight and/or  high-performance cryptosystems: algorithms, architectures, designs and implementations"	"<p>With rapid advances in information, computer and communication technologies and systems, huge amount of data has to be processed, stored and exchanged among users, which makes data integrity and information security very important issues. To protect the data and make it secure, various cryptographic primitives are being incorporated in many standards and protocols and implemented in a wide range of platforms; from powerful computers used in servers and base stations to lightweight cryptosystems used in wireless devices. The performance of cryptosystems rely heavily on efficient cryptographic computations, specially in resource constrained embedded systems, such as smart cards, RFID tags, Internet of Things, and nodes in wireless sensor networks, where the power consumption, memory, and bandwidth are very limited. Reliable and efficient design and implementation of cryptographic computations are challenging due to their complex nature. The main objectives of the research are summarized as follows:</p><p><span style=""line-height: 1.5em;"">(i) We will investigate the development of new efficient algorithms and architectures for finite field arithmetic operations, such as multiplication, squaring, inversion, and exponentiation in binary extension fields using polynomial, shifted polynomial and Gaussian normal basis representations. We will consider their designs and implementations in ASICs and FPGAs. For each and every working platform, we plan to further optimize these operations for both lightweight and high-performance applications to have low-resource and/or high-speed computations.</span><br />(ii) We will study the designs and implementations of a number of public and private key cryptosystems based on our newly proposed multipliers. Finite field multiplication is the main arithmetic operation used in elliptic curve and exponentiation-based cryptosystems, the AES-GCM authenticated encryption and the WG stream cipher. The effects of using different digit-level multipliers will be investigated to further optimize these cryptosystems for two-end applications namely, lightweight and high-performance cryptosystems. <br />(iii) We will explore new reliable and robust schemes for private and public key cryptosystems to counteract natural faults and fault attacks. This research is very important due to the fact that fault attacks have become a serious concern in cryptographic applications. The research will be based on adopting efficient concurrent error control coding approaches with low overhead and acceptable error detection capability. </p><p >This research will lead to more secure and reliable cryptosystems with lower cost and higher performance. It will also significantly contribute to training highly qualified personnel for academia and a large number of companies in the Canadian Information and Communications Technologies (ICT) sector.</p>""579200,""Reynolds, Craig"
"590098"	"Richards, Gregor"	"Gradual Typing by Annotating Flow"	"Web programming is a vital and thriving field, and web software is becoming dominant in many software domains. However, web programming languages are almost exclusively dynamically typed, and dynamically typed languages do not offer the speed, assurance or tool support of statically typed languages.<br /><br />Dynamic types allow a lot of flexibility, but with flexibility comes imprecision and unpredictability. Static types are more rigid, and this aids software tools, but their rigidity can make initial development slower and sometimes more difficult, due to narrowing expressibility. Gradual typing aims to fill the gap between dynamic and static types: By making type annotations optional, but validated where present, it is possible to gradually move a software system from dynamic and unchecked to static and checked. However, gradual types have significant drawbacks. They introduce a new range of possible errors. To avoid these errors, it is necessary to specify types very precisely, which is both cumbersome and hard to understand. This research aims to rectify the known problems at the interface of static and dynamic types, and discover and rectify the as-yet-unknown problems as well.<br /><br />Gradually-typed languages allow sound typing, assurance and analysis tools, but create a surface between the ""dynamic"" and ""static"" worlds, introducing new errors and a new source for confusion. Partially-gradually-typed languages for the web have been created, but opting for an unsound ""optional"" type system, as in TypeScript and Dart. Although not generating any new runtime errors, these languages are unsound, and so not suitable for many forms of analysis. My previous work extends these to be fully gradually typed, but does not solve the fundamental problem of gradual typing's expressibility at the boundary between static and dynamic types.<br /><br />This research program aims to extend the JavaScript programming language. This work involves both investigative and developmental sub-projects. New constructs must be added to restrict the incompleteness caused by dynamic flows and specify types in a dynamic way. It is also necessary to study existing programs to determine the applicability of language changes.<br /><br />Upon the successful completion of this project, a set of extensions to the web programming language JavaScript will be available, with gradual typing which will greatly improve the maintainability of web software, while also avoiding many of the pitfalls that have hindered previous gradually-typed languages from being adopted. This enables more efficient code maintenance, better security and faster code extension for web programs.""590370,""Richards, Jeffrey"
"589954"	"Rilling, Juergen"	"Ambient Software Engineering - Moving Towards A Software Engineering Web of Things"	"<p>The Internet has evolved over the last two decades from a global (read only) information hub for end-users, to its current form (Web 2.0) with its end-user support for content creation (e.g., blogging, social networking). However, even in its current development stage, Web 2.0 is only a step towards the original vision of what the Internet might become - a Web of Things also known as Web 3.0. Web 3.0 will be built upon several key technologies (e.g., Semantic Web, automated tagging, reasoning and semantic linking), with machines talking to other machines to deal with much of the daily routine work, leaving humans to provide the inspiration and intuition all in a global context. While many domains (e.g., automotive, manufacturing, home automation) have started to make the Web 3.0 an important and integrated part of their future solution space, the software engineering community has yet to embrace this move. The objective of this research program is to introduce  Ambient Software Engineering as a cross disciplinary approach, integrating traditional software engineering (evolution) research, with state-of-the art research from domains such as Semantic Web, Natural Language Processing, knowledge modeling and Mining Software Repositories. The approach takes advantage of matured Semantic Web technologies, which support a standardized unified and formal knowledge representation using ontologies and Linked Data of existing heterogeneous knowledge resources.</p><p>The proposed research will focus on the introduction of information hubs, with facts and knowledge extracted from existing software resources being formally modeled using ontologies and Linked Data representations. In addition the use of automated tagging, machine learning, source code analysis and Semantic Web reasoning will be investigate to further enrich these information hubs with semantic information and semantic traceability links - both being an essential prerequisite for establishing a Software Engineering Web of Things. For the evaluation an initial set of smart workflows and services for the software evolution domain will be introduced that allow for the support of composite applications or task contexts across global heterogeneous knowledge extracted from the software engineering domain. </p><p>Both, the datasets and smart services being developed, will be publicly available through an online portal, to create an initial version of a Web of Things for the software engineering domain, where information hubs replace traditional information silos found in the software domain. These information hubs (things) will promote knowledge sharing, reuse and integration, by facilitating a crowd-sourcing effort involving the software community at large to share and link their own data (facts and analysis results) and smart services to continue evolving this Web of Things for the software engineering domain.<br /><br /></p>""578523,""Rilling, Madison"
"589613"	"Rivaz, Hassan"	"Estimation of tissue deformation in medical images"	"We have seen a significant increase in the use of computers in medicine in the past decade. Extensive use of medical image analysis techniques has helped clinicians transcend many limitations of conventional medicine. The focus of my research lies in the same paradigm: to develop novel medical image analysis techniques that are reliable for clinical use. More specifically, my research program involves the development of novel computing techniques for ultrasound (US) elastography and fusion/registration of US images to magnetic resonance (MR) and computed tomography (CT).<br /><br />US is an easy-to-use, inexpensive, real-time and safe imaging modality. This relatively young medical imaging modality is also one of the most frequently used, and is rapidly growing. In fact, numerous new research projects have recently been translated into commercial products. This research proposal has two thrusts that further improve the capabilities of US. First, we will develop novel image processing techniques that accurately track tissue deformation with US to create new images that show tissue mechanical properties. This is of significant clinical impact since pathologic changes in tissue, such as cancer tumours, are highly correlated with tissue mechanical properties. Second, we will develop robust and accurate techniques for automatic deformable registration of US to MR and CT. Image registration involve aligning two images that are obtained from the same tissue, so that they can be compared or combined. Registration of US with MR and CT has numerous applications in both diagnosis and therapy/interventions, and is an active field of research. Our approach in both thrusts is to cast the problem as optimization of a cost function that incorporates image similarity and prior information. To achieve widespread clinical use, a critical requirement in both elastography and registration is real-time performance. The proposed research program will achieve this goal by developing novel computationally efficient image analysis techniques.<br /><br />The proposed interdisciplinary research program will train two PhD and four MASc students in an exciting environment at the Department of Electrical and Computer Engineering and the PERFORM Centre at Concordia University.""586137,""Rivaz, Hassan"
"590317"	"Rodrigues, Luis"	"Optimal Networked Piecewise Affine Control Systems"	"<p>The overall goal of the applicant's research is to develop a general-purpose methodology for analyzing and performing optimal energy efficient design of networked systems with distributed sensing and control. Applications will focus on networks of uninhabited air vehicles (UAVs). Networks of flying vehicles hold a strong potential to solve many surveillance problems such as pipelines, power lines, and street traffic. They are also important for search and rescue or inspection inside buildings with dangerous gas and chemical leaks. The proposed research will address an important open problem adding a new perspective by bringing together two promising areas: optimal piecewise-affine control and networked control. The proposed strategies are expected to be energy efficient and thus contribute to larger endurance for each agent by saving battery power. This research will benefit Canada by providing safer and economically more viable solutions for networks of autonomous vehicles.</p>""584682,""Rodrigues, Luis"
"589521"	"Rohling, Robert"	"Ultrasound Guidance and Display"	"<p>Ultrasound is the most common medical image, recently surpassing x-rays. Point-of-care is the fastest growing sector in ultrasound, especially anesthesiology. This growth is driven by several trends in ultrasound: miniaturization, cost-reduction, and advanced digital image processing on generic hardware.  A typical use-case is to target a peripheral nerve with ultrasound, then guide a needle to that target and inject anesthesia. The problems limiting further growth are that the image quality is still often inadequate, and the point-of-care operators are novices/untrained in ultrasound. Operators still struggle with the hand-eye coordination needed to guide a needle to the target in the patient based on an often-noisy image on a bedside monitor. What is needed is a significant leap in image quality and ease-of-use for specific clinical applications. Such a leap would not only benefit point-of-care applications, but in the long term could finally open the ultrasound market to primary care physicians in neighbourhood offices. Primary care physicians comprise one third of all physicians and see 51% of all office visits, but do not widely use ultrasound due to these problems. This grant will develop methods to improve image quality in both 2D and 3D, and improve ease-of-use by projecting key information directly on the patient.<br />Better 2D image quality will be addressed by improving the imaging speed and then capturing more information before forming a final image. Previously, increasing speed compromised spatial detail or depth penetration. We propose to leverage recent advances in telecommunications, namely digital codes, to develop new fast ultrasound imaging techniques without this compromise. As the first applications, we will aim specifically to improve the image quality of bone surfaces and nerves because they are key for many procedures in anesthesiology. <br />Better 3D image quality will be addressed by using fast imaging to quickly sweep the ultrasound transducer over a region of interest to capture a 3D panorama, without blurring or encountering inadvertent patient motion.<br />Better ease-of-use will be addressed by improving the way the anatomical and guidance information is presented to the physician. We will develop a simple, low cost, augmented reality display directly over the region of interest on the patient, in addition to the usual monitor at the patient bedside. This will include the depictions of the target location, identified in ultrasound, and appropriate needle trajectory. <br />All of these techniques can be implemented individually or together on commercial ultrasound machines starting with the Canadian-made Ultrasonix. These techniques will make several anesthesia procedures easier and more accurate than current ""blind"" injections, which should translate into better and greater access to anesthesia. Fewer complications from misplaced needles will also reduce hospital costs.</p>""589493,""Rohrbach, Petra"
"589773"	"Roy, Chanchal"	"Change, Similarity and Redundancy in Software"	"<p>Software clones are defined to be duplicated or similar fragments of code in a software system. Copying a code fragment and reusing it by pasting with editing changes is a common practice in software development. Developers in fact often intentionally practice cloning because of the underlying benefits such as faster development, reuse of well-tested code by avoiding risks, or time limits assigned to them. On the other hand, clones are the #1 code ""bad smell"" in Flower's refactoring list. Recent studies both with industrial and open source software show that while clones are not always harmful and clones can be useful in many ways, many of them can also be detrimental to software maintenance. Of course, reusing a fragment containing unknown bugs may result in fault propagation, or any changes in a cloned fragment may lead to changes to all the similar fragments to it, which is challenging, time consuming, and error-prone. Clones are thus considered to be one of the primary contributors to the high maintenance cost of software, which is up to 80% of the total development cost. </p><p>Despite advances in IDEs that assist with detection and analysis of clones, there is a marked lack of support for developers in the ""safe and efficient practice of cloning"" during development or in assisting them in managing existing clones during software maintenance. The lack of support is particularly evident for ""near-miss"" clones (clones where minor to significant editing changes occur in the copy/pasted fragments). While there has been some recent progress with near-miss clones as well, this is mainly based on past change histories, and/or contexts of the clones. Given the dual role of cloning, it is however important to consider quality (e.g., how safe the cloning result is) and flexibility (e.g., how to efficiently make clones based on the current context) both during cloning practice in software development as well as managing existing clones during software maintenance. These aspects are poorly supported. </p><p>The long-term objective of this research is to provide better support for developers in ""safe and efficient cloning practices"". This will involve making research contributions in two areas: (1) building tools to assist developers in ""safe cloning"" practices within IDEs and version control systems, and (2) building tools to help developers manage existing clones during maintenance and evolution. All of these activities will be applied to both exact clones and the more difficult problem of near-miss clones with due consideration of quality and flexibility aspects. The products of this research will allow software engineering researchers to better understand clone evolution and developer cloning practices, and will substantially improve support for developers in dealing with clones effectively and efficiently. Better support for safe cloning and management will lead to a substantial improvement in overall software quality.</p>""586927,""Roy, ChanchalKumar"
"591301"	"Roy, Sébastien"	"Full 3D Immersion: Capture and Visualization"	"This grant proposal is in the field of computer vision. Its primary focus is on developing new 3D immersion technologies, from the perspective of capturing reality accurately as well as visualization (large immersive projections, single virtual headset). The goals are simple. First, we intend to provide highly accurate, fast, and robust methods for capturing reality immersively (large field of view) and in 3D, without any constraint on the motion of camera or the scene observed. Second, we intend to provide highly efficient methods for large scale visualization of immersive 3D content (i.e. dome multi-projection, or other large surfaces), and for small scale visualization, such as virtual headsets (i.e. oculus rift).<br /><br />3D Immersion is becoming more prevalent everyday, especially in the arts, entertainment, and video games. This is due mostly to recent advances in virtual headset technology such as the Oculus rift and to new interest toward 3D dome projection, where the screen is not only filling the observers fields of view, but also provides stereo images, so depth can be perceived accurately in all directions. Content creation for 3D immersion is mostly performed with 3D computer graphics, where it is easy to make stereo-immersive rendering. However, the accurate depiction of reality requires real world filming and has remained a challenge. Traditional ""small field of view"" stereo capture can not be used directly for immersion because it provides a single point of view while immersive capture requires capturing from all possible point of views simultaneously. The single person immersive 3D experience is also evolving rapidly in the form of new high performance virtual reality headsets. Software for viewing content has matured, especially with regards to 3D real-time graphics and video games. However, for immersive video, the large resolutions involved and geometry corrections (including parallax) still constitute a challenge. Our research program intends to explore 3D immersion, from the perspective of content creation and content presentation.<br /><br />We will develop new cameras for 3D spherical capture of reality, so it can be observed somewhere else with accurate depth perception in any direction, and in real-time.<br /><br />We will also develop better multi-projection, to simplify and automate its deployment while making it more perceptually accurate immersion with depth.<br />Our long term goal is the ""ultimate immersive 3D"" experience, where the observer not only controls his gaze, but also his location. This new freedom for the observer will have a profound impact on the creation of 3D immersive content and make it much more challenging, thereby opening new avenues of research. Overall, if this research program is successful, we will have contributed to achieving the ultimate goal of 3D immersion: giving full freedom of movement to the observer, giving him a true felling of omnipresence.""571205,""Roy, Sébastien"
"590585"	"Rudie, Karen"	"A Framework for the Integration of Dynamic Observation, Control and Knowledge in Discrete-Event Systems"	"<p>My research focuses on developing mathematical models to represent systems whose behaviour can be thought of as sequences of events or actions in order to automate problem-solving and test potential solutions in such discrete-event systems. The goal of the modeling is to generate controllers to meet given specifications.  Controllers, which may be software, hardware or human strategies, must make decisions about which events to disable at various points in time and they must make their decisions based only on partial observations of the system. Prior work in the field uses models where controllability and observability of an event are static properties. In contrast, my proposed research aims to model dynamic control and dynamic observation.</p><p>Furthermore, just as humans make decisions based not only on their direct observations of the physical world, but also based on inferences they make about what other people know, so too should automated agents make decisions based on what they know about what other agents know. Consequently, my research will focus on the interaction between observation, control, and ""knowledge"".  I will incorporate formal models of agent knowledge into my framework. <br /><br />Fundamental shifts in the way we view observation and control are anticipated in the proposed research program. By incorporating dynamic observation and control, a supervisory control theory model of service composition will be developed for service-oriented computing applications. My research will also have an impact in decentralized distribution systems, for example in electricity markets where the status quo will no longer be viable in the future, when autonomous agents (homes, electric vehicles, private generators) will produce and consume electricity. My research into knowledge-based models for control can be applied to strategies where individual agents are guided by incentives to act in a way that produces some overall desired results.  <br /><br />Most ambitiously, my research will be applied to solve computer security problems in which adversarial agents have only partial observation of a system and yet try to exert control over the system to glean secret information.  With more accurate models of discrete-event systems-based on dynamic observation and dynamic control and using knowledge-based reasoning-we can develop controllers that prevent adversaries from being able to discern information that is intended to be kept private. I will investigate scenarios where the attacker can control and change the system's behaviour. I will consider possible responses a controller can take. I hope to determine whether it is more advantageous to restrict the behaviour of the system or to prevent the attacker from controlling some events. This will have an impact, for example, on Canadian banking and credit card companies, where protecting the privacy of information is of paramount concern.</p><p>The research will train 10 HQP.</p>""588807,""Rudner, Adam"
"589836"	"Sabourin, Robert"	"Étude des systèmes de reconnaissance adaptatifs et évolutifs dans les environnements dynamiques"	"    La conception des systèmes de surveillance biométriques repose sur les techniques de reconnaissance de formes (SR) et d'apprentissage machine robustes. En fait, pour être robustes, les SR doivent être en mesure de s'adapter aux conditions changeantes des environnements pour lesquels ils sont déployés. Par exemple, les variations d'illumination et de l'orientation du visage pour la reconnaissance faciale dans un contexte de vidéosurveillance; la variation temporelle de la forme de la signature manuscrite pour la vérification de l'identité des scripteurs, etc.<br />    Malgré tous les efforts déployés par les chercheurs dans les domaines de l'apprentissage-machine et de la reconnaissance de formes, plusieurs problèmes spécifiques à la conception de SR adaptatifs demeurent non résolus. En particulier, la création de classifieurs fiables passe nécessairement par l'apprentissage des ensembles de classifieurs (EoC). Cependant, il n'y a pas consensus chez les chercheurs concernant la meilleure approche à suivre pour la création des comités d'experts.<br />    Un <strong>problème fondamental</strong> et souvent négligé est de doter les systèmes de reconnaissance de la capacité de s'adapter en présence de nouvelles données, de nouvelles caractéristiques, de nouvelles classes, etc. De plus, les solutions proposées dans la littérature spécialisée sur les problèmes reliés à la vidéosurveillance, la détection d'intrusions et la vérification des signatures manuscrites sont pour la plupart conçues pour les environnements statiques. L'hypothèse forte est alors que les paramètres du problème de reconnaissance sont parfaitement connus lors de la conception du système de classification et que ceux-ci ne changeront pas dans le temps. Conséquemment, ces systèmes ne sont pas aussi performants lorsqu'ils sont déployés dans le monde réel puisque très souvent ces environnements sont dynamiques.<br />    <strong>L'objectif principal</strong> de notre programme de recherche réside dans la mise en oeuvre d'une méthodologie efficace pour la conception de systèmes de reconnaissance adaptatifs et évolutifs, capables d'apprendre de manière incrémentale sur des nouvelles données dans les environnements statiques et dynamiques. Cet objectif se traduit par plusieurs <strong>objectifs spécifiques à court terme </strong>reliés à la définition de nouvelles méthodes robustes pour : (1) la sélection des exemples et des caractéristiques dans les espaces de différences, (2) l'apprentissage adaptatif et évolutif des EoC dans l'espace ROC et (3) la sélection dynamique de classifieurs adaptés aux environnements statiques et dynamiques. Les  algorithmes évolutionnaires et l'apprentissage machine seront au centre de nos travaux. <br />    Les méthodes développées dans le présent programme de recherche seront suffisamment générales pour être utilisées pour la conception de systèmes de reconnaissance de formes indépendamment de l'application visée.""583931,""Sabovich, Rebecca"
"590397"	"Salvail, Louis"	"On the Foundations and Methods of Quantum Cryptography"	"This project aims at improving our understanding of the foundations of quantum cryptography, to develop quantum tools allowing for new cryptographic primitives and a better understanding of its limits. The fact that two parties can agree upon a secret key using quantum communication (i.e. quantum key distribution) has first given hopes that the main goals of cryptography: data privacy, data authenticity, and data integrity could be provided information theoretically by quantum mechanics while, as we know, being impossible by classical means alone. Even before the security of quantum key distribution was fully established, researchers have shown that other cryptographic primitives would remain impossible even by all powerful quantum experimentalists. The picture of cryptography in a quantum world is therefore not as unconstrained as one could have first thought. In addition to the realization of classical cryptographic tasks by quantum information processing, quantum cryptography is also interested in providing means by which cryptographic techniques can be applied to quantum systems. Very often, classical primitives have a natural quantum interpretation. For instance, signing documents in order to publicly guarantee their integrity and authenticity has a natural quantum counterpart: signing quantum states. Quantum cryptography is interested in determining under what conditions, if any, these quantum primitives are possible. As an example, although signing documents can be performed securely based upon reasonable computational assumptions, signing quantum states remains impossible in this setting. On the other hand, while electronic money cannot be easily protected against double-spending, quantum money does not have this vulnerability, thanks to the uncertainty principle.   <br /><br /><p>During the last fifteen years or so, research in quantum information theory has improved a great deal our understanding of the cryptographic power and limits of quantum mechanics. One thing is clear, the quantum cryptographic landscape is very different from its classical counterpart and its outlines are still not very well understood. The anticipated outcomes of this program include a better understanding of what is possible and at what cost, new primitives that can be implemented in practice and that provide satisfactory security guarantees, and information theoretical tools allowing to shed new lights on the foundations of quantum cryptography. The program will contribute in a better understanding of how classical and quantum methods can be used together to improve cryptographic constructs for the digital age, at the outset of the quantum age.</p><br />""579137,""Salvail, Louis"
"577811"	"Samadani, AliAkbar"		"NSERC"
"590116"	"Schlegel, Christian"	"High-Density Random Access for High-Speed Satellite Data Services"	"<p>In recent years there has been a strong resurgence in interest in satellite communications. The European Space Agency <span style=""line-height: 1.5em;"">(ESA), for example is investigating satellite-based wireless data services using multi-spot, high-capacity geo-stationary satellites. Such </span><span style=""line-height: 1.5em;"">a system will require a satellite throughput capacity on the order of a terabyte/s in order to be economically competitive </span><span style=""line-height: 1.5em;"">with ground-based wireless services. </span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">A modern satellite, operating with multiple spot beams, is capable of covering a large geographic area and servicing a very </span><span style=""line-height: 1.5em;"">large number of users. Difficulties arise when the traffic through the satellite increases to the point where basic access and </span><span style=""line-height: 1.5em;"">communications methods no longer suffice to service the data demand. This difficulty is compounded by the long </span><span style=""line-height: 1.5em;"">round-trip delays between transmissions on the up-link and reception of acknowledgements on the down-link, which </span><span style=""line-height: 1.5em;"">prevents rapid coordination between transmitters. The receiver also needs to be able to resolve signal collisions and be </span><span style=""line-height: 1.5em;"">able to estimate and track channel conditions for a multitude of user terminals. These challenges represent a are</span><span style=""line-height: 1.5em;""> the major focus of the proposed research project.</span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">The most recent variations of random access contention resolution transmit multiple copies of a data packet in </span><span style=""line-height: 1.5em;"">randomly distributed time slots throughout a transmission frame. This approach statistically increases the rate of packet </span><span style=""line-height: 1.5em;"">collisions, but the probability that at least one copy of a packet is received collision-free is increased. This packet can then </span><span style=""line-height: 1.5em;"">be cancelled from the other time slots where it was also transmitted, allowing for iterative peel-off decoding of the entire </span><span style=""line-height: 1.5em;"">frame. Theoretical studies have shown that this method can achieve 80% of the single user capacity for practical scenarios, which</span><span style=""line-height: 1.5em;""> represents a vast improvement over the 37% achievable with a basic slotted ALOHA protocol. </span></p><p><span style=""line-height: 1.5em;"">However, the channel itself, is a multiple access channel, and as such it has a theoretical capacity which is superior to that of any multiplexed channel. Exploitation of this advantage requires the incorporation of joint detection principles. Multi-packet reception(MPR) will be addressed with iterative interference cancellation, which has recently been shown to be able to approach the multiple access channel capacity in theory. However, substantial problems remain before such MPR receivers can be deployed. This project focuses on the challenges that arise when realistic channel propagation models are used, in particular channels with highly dynamic behavior. It is anticipated that the fundamental results from this work will lead to increased utilization of spectral resources and robustness of networks. We have been collaborating with the German Aerospace Institute (DLR), and this grant will accelerate this collaboration, and lead to a strong research program in resource-efficient data communications at Dalhousie University.</span></p>""589874,""Schlomiuk, Dana"
"591144"	"Shafai, Lotfollah"	"Applied Electromagnetics and Advanced Antennas"	"<p>The objective of my proposed research is to use electromagnetic principles for developing mathematical tools for investigating the wave interactions with surrounding media, and to enable the design of electromagnetic devices, such as antennas, sensors, and other components for applications in emerging areas in science and engineering. It will have several groundbreaking advances in communication and remote sensing.  <br />I will conduct research on the development of low loss engineered materials. The conductor losses have been <em>""the source of all evil""</em> in radio science and anything dealing with electromagnetic waves, and created the dream of superconductivity. Recently, in a modest way, I have demonstrated loss reduction in layered structures, and this research will investigate it in detail, using physical principles, and develop mathematical models for its minimization. Conductive loss reduction will launch a new wave of innovation in many areas of science and technology.<br />I propose to advance my research on virtual antenna arrays, which replaces multiple antennas with only one, with the aid of electromagnetic modeling and signal processing. With further development, I intend to investigate its applications in synthetic aperture radars and satellite observations, using high gain antennas. I will also study applications where device miniaturization has a transformative impact, by hardware minimization, or replacement with software.<br />I plan to further study the electromagnetic response of Arctic sea ice, and develop dedicated algorithms for scattering by its special features, such as frost flowers and brine inclusions. The results of this research will be invaluable for improving our understanding of the sea ice, and predictions by large-scale satellite observations, and sustainable development of the North.<br />I propose investigating the symmetric properties of electromagnetic waves scattered from objects and tissue anomalies. This research will relate the received response to the object's location, rather than the source contrast, used currently in microwave imaging. Its features will be similar to MRI imaging and the latter's signal processing can be used to improve the detection and identification of the anomalies.</p>""591746,""Shafai, Lotfollah"
"591258"	"Shepherd, Bruce"	"Discrete Optimization: From Applications to Relaxations"	"<p>This is  a research proposal on optimization, an area which crosses several disciplines: pure and applied mathematics, computer science, operations research and engineering. Mathematical optimization is thriving in part due to a wealth of applications. In addition to traditional logistic outlets such as inventory management and scheduling,  new applications put demands on optimization  methodology to cope with very large scale data sets and the desire for real-time answers. Additional applications seek new models which incorporate game-theoretic strategy into the optimizer's objective.   Virtually all online retailers, for instance, are leveraging optimization tools in various aspects of their operations (fulfilment, shipping, revenue management). There is a corresponding demand for new personnel in the area to meet a seemingly insatiable appetite for expertise in  machine learning, analytics and data science, areas which are strong users of discrete and continuous optimization.</p><p>Research directions proposed range from purely theoretical and structural topics in integer programming to modelling and empirical questions well-motivated by practical considerations in data networks. Considerable emphasis is placed on further development of robust optimization, one approach for  coping with optimization under uncertainty. The PI has also recently worked in online algorithms, which can be viewed as another model dealing with uncertainty. A recurring argument in the proposal is that harnessing the power of optimization for real-world problems is improved when it is informed by the underlying theoretical principles.  Thus, apart from presenting several computational activities, a number of  challenging medium/long-term  theoretical objectives are identified. These include mathematical questions about flow-cut gaps and sparsest cut, such as when is a finite metric embeddable into L_1 with constant distortion?, to when do all lattice points in a parallelepiped lie on a hyperplane?</p><p> </p>""568430,""Shepherd, Gordon"
"590402"	"Shi, Wei"	"Improving Big Data Infrastructure to Effectively Utilize Resources for Data Processing and Services"	"<p>Big data analytics is hailed as an emerging driving force of the next innovation wave. It has formed the basis for many novel applications across a wide range of fields from business workflows to engineering activities and scientific computing. One such typical application is cloud-based network services for business and individual mobile or vehicular users, where data is collected through daily log files, from stationary or mobile entities such as smartphones equipped with a variety of built-in sensors (e.g., GPS, accelerometers and cameras) and then processed by a big-data infrastructure in order to provision data as a service. For example, vehicle trajectories can be generated in real time from video data to facilitate the analysis of traffic flow parameters for transportation management. Clearly, in such applications, the response time of the underlying infrastructure for data processing has great impact on Quality of Service (QoS). </p><br /><p>My ongoing overall research goal is the amelioration of QoS through better resource utilization in big data infrastructure. I approach this research goal from two directions: First, I aim at better balancing network traffic and more efficiently sharing cloud resources for applications by exploiting network topology, job characteristics and execution processes. These proposed improvements are particularly important for service providers deploying big data infrastructure in clouds, as the provisioned resources are typically based on pay-as-you-go billing strategies, and effectively utilizing the resources means maximizing the monetary profits for cloud service providers and minimizing cost for users of big data analytics. Second, I search for an effective way to disseminate data to the outside world as a service. Ultimately, I want to increase QoS while also improving the tolerance of big data infrastructure for a variety of service-disrupting faults. </p><br />According to a recent whitepaper on the Big Data market written by International Data Corporation and sponsored by SAS, a leading developer of analytics software, ""Canadian businesses are at risk of falling behind international competitors in the era of Big Data"". In particular, this survey and analysis found that Canadian companies are late and slow in adoption of technology capable of processing Big Data in comparison to international companies that are more willing to invest in and have more defined plans for adoption of these technologies in the near future.<br /><br />My proposed research will provide a more cost effective and more reliable Big Data infrastructure that offers faster processing of Big Data. This will attract more Canadian companies to adopt this new technology in order to become more innovative and productive. My results will bring significant economic benefits to Canada's telecommunication and automobile industries, in particular, and significantly enhance Canada's profile in this field.""579047,""Shi, Xiaoming"
"591127"	"Shihab, Emad"	"Detecting and Recommending Mitigations for Impactful Risky Software Changes"	"Software systems play an important role in our daily lives, making the quality of software systems of paramount importance. To ensure the quality of software systems, a large amount of research focused on the detection of source code packages, files and changes that contain defects. However, the adoption and usefulness of such approaches in practice remains limited. Some of the most commonly cited reasons for the lack of adoption are 1) metrics used in the detection are not actionable, 2) the impact of defects is not taken into consideration and 3) no guidance is given on how to mitigate the detected defects.<br /><br />The aim of the proposed research is to address the aforementioned limitations, making defect detection techniques more pragmatic. To do so, we plan to propose models that use actionable metrics to detect impactful risky changes. Furthermore, we will use historical data and employ qualitative and quantitative approaches on the fixes of the impactful risky changes in order to propose mitigations for the detected impactful risky changes. <br /><br />The successful completion of this proposed project will enable software researchers and practitioners to better understand the key development factors that enable pragmatic defect detection. The novel contributions of the proposed research are: 1) the proposal of models that use actionable metrics to effectively detect impactful risky changes and 2) a catalog of mitigations that can be applied to address these impactful risky changes. Large-scale empirical studies will be performed using large open source and industrial projects, to determine the effectiveness of the proposed models, actionable metrics and proposed mitigations.<br /><br /><p>The proposed research will have a direct impact on the software engineering research and practice since it will advance the field by providing pragmatic detection techniques, proposing novel mitigation strategies and providing an extensible tooling framework. Furthermore, the proposed research will expose, train and enable highly qualified personnel (HQP) to contribute to the state-of-the-art in software quality research.</p>""572925,""Shiller, Douglas"
"591188"	"Shirani, Shahram"	"Multi-Modality-Enriched Video: Potential, Strategies and Applications"	"Visual sensors are ubiquitous and have found their ways into many different applications. They capturing data at an incredible pace and are being integrated with other sensors into single devices. For example a typical smartphone has a camera (visual sensor), a GPS sensor, an accelerometer, a gyroscope, and a magnetometer. These consolidated sensors generate multi-modality-enriched video signal: video signal as well as one or multiple 1-D or 2-D non-video signals. The non-video signals can provide additional context for video signal processing or directly provide side information to facilitate video signal processing and communication. For example a motion sensor can provide motion data useful for motion estimation and thus video compression. Moreover, multi-modality enriched video enables us to develop new functionalities and create new user experiences. The enriching non-video signals have a tremendous potential for improving the way we communicate and make decision based on video. However, realization of this potential hinges upon advancement in techniques for managing, analyzing, and extracting useful information from these multi-modality data sets. Our utilization of interdependency between different signal modality acquired by enriched visual sensors is rather intuitive. Currently, there is no established rationale for determining if there are any benefits to use more than one signal modality in a particular video processing task (e.g. video compression, detection, tracking) and which one to use and how to use it. In this Discovery Grant research, a new line of inquiry will be pursued in order to systematically utilize the inter dependencies between different signal modalities for video compression, post processing, detection and classification. A tensor based framework will be adopted that will characterize the interaction between sensing modalities. The proposed research is expected to yield new approaches for systematic analysis and processing of multi-modality enriched video. The research will lead to superior video communication and better video-based tracking, recognition and classifications. The new knowledge arising from this research will find its way into new technologies and products in areas of great importance to Canada such as education, healthcare, manufacturing and auto industry. Canadian companies in this area will benefit from the new knowledge and also the highly qualified personnel that will participate in the research.""568409,""ShiraziAdl, Aboulfazl"
"589619"	"ShiriVarnaamkhaasti, Nematollaah"	"Uncertain Relational Database Management and Integration Systems: Concepts and Techniques"	"<p>Many applications in diverse domains require the ability to store and process voluminous uncertain data. While traditional database management systems (DBMS) have been quite successful in handling precise data and missing values, they offer little or no support for uncertain data. What is also needed is the ability to ""quantify"" the integrity of the data. Informally, a piece of data is uncertain if its truth is not established definitely.  Uncertainty has been studied extensively for over two decades in database and AI communities, resulting in a substantial body of principles, concepts, representations, and algorithms. However, more work is required to develop better solutions for a full realization of uncertain data management. This explains recent increased attention to the subject in database research, and the motivation of this proposal. The major problems include (1) semantics (representations), (2) evaluation (correct and efficient processing),<span style=""line-height: 1.5em;""> and (3) usability (ease of use). These problems which are related stem from the semantics of uncertain databases, defined based on the notion of <em>possible worlds</em>: essentially a set of standard relational databases only one of which represents the true state of the database, but we don't know exactly which one. The problem with the semantics is the exponential size of the possible worlds in the number of tuples in the database. To illustrate this, consider two scenarios of modeling uncertainty at attribute level and at tuple level. If a tuple in a relation has <em>n</em> alternative values for an attribute, then there are <em>n</em> relations in the possible worlds w.r.t. to this tuple. Analogously, if presence of a tuple <em>t</em> in a relation <em>R</em> is not sure, then <em>R</em> has two possible worlds, one which includes <em>t</em> and the other which doesn't. Probabilistic databases are uncertain databases with probabilities defined over the corresponding possible worlds. The main approach in related literature has been extending the concepts and techniques from traditional databases. A major difficulty is ensuring the ""correctness"" of query results, as different correlation assumptions among tuples require different run-time control and tracking algorithms. This turns query processing into <em>reasoning </em>which is more complex, if not infeasible. The expected outcomes include (1) novel query processing and rewriting techniques for extended relational algebra operations with desired semantics,  (2) development of extended algebra techniques into an <em>integrated calculator</em> , and (3) development of prototype subsystems for uncertain data management and integration. All the practical work in this research will be evaluated experimentally. The research is anticipated to provide a more comprehensive understanding of uncertain data and the needs. I believe</span><span style=""line-height: 1.5em;""> that uncertain data research will influence, if not lead to, the  </span><span style=""line-height: 1.5em;"">development of next generation DBMSs.</span><span style=""line-height: 1.5em;""> This research will make a leap in this direction.</span></p>""589013,""Shirmohammadi, Shervin"
"589938"	"SolisOba, Roberto"	"Approximation algorithms for optimization problems"	"Combinatorial optimization deals with finding optimum solutions for problems of a discrete and finite nature. Such problems arise in areas as diverse as Computer Science, Mathematics, Finances, Biology, Medicine, and Operations Research, among others. Many of these problems are too complex to solve without the use of computers and, therefore, efficient algorithms for solving them are needed. <br /><br />There is strong theoretical evidence suggesting that for many combinatorial optimization problems no polynomial time algorithm can compute exact solutions for them. Such problems get the technical name of NP-hard. One fundamental tool for dealing with these problems is approximation algorithms; these are efficient algorithms that yield solutions whose values can be proven to be no more than some factor c away from the optimum.<br /><br />My research interests center on the design of approximation algorithms. I am particularly interested in the study of approximation algorithms for two kinds of problems: Network and packing problems. Networks are one of the most fundamental modelling tools in optimization, with application to a large number of fields. Packing problems are of great importance in transportation, VLSI design, scheduling, and any other application domains requiring the arrangement of objects in bounded spaces.<br /><br />Within this research proposal we will pursue the following short term goals:<br /><br />- Investigate the combined use of local search and other optimization techniques in the design of approximation algorithms for facility location and clustering problems.<br /><br />- Design fixed parameter tractable approximation algorithms for packing and scheduling problems with restricted inputs.  <br /><br />- Design efficient approximation algorithms for clustering problems in very large networks, like social networks.  <br /><br />- Design effective experimental techniques for analyzing and improving the performance of approximation algorithms.""581314,""SolisReyes, Stephen"
"589593"	"Stakhanova, Natalia"	"Towards Malware Author Attribution"	"<p>In 2008 Symantec added 1.6 million new malware signatures to its malware database. This number increased to 2.9 million in 2009. In 2010, the new addition <span style=""line-height: 1.5em;"">to the database constituted 4.4 million new signatures. This trend continued, and in 2011 an increase in malware has reached 41%. Traditional approaches </span><span style=""line-height: 1.5em;"">predominantly based on recognizing well-documented threats (signatures) are struggling to cope with this rate of growth in malware numbers each year. </span><span style=""line-height: 1.5em;"">Consequently, the majority of malware strains (many of which are short lived, i.e., less than 24 hours) go undetected. This inadequacy of traditional </span><span style=""line-height: 1.5em;"">approaches exposes a dire need for alternative defences. We propose to turn our attention to malware source, i.e, malware authors. The benefit of such a </span><span style=""line-height: 1.5em;"">strategy is clear: instead of detecting every malware strain, we could effectively characterize all malware variants generated by its author. We propose to study </span><span style=""line-height: 1.5em;"">the theory and practice behind malware author attribution. Although authorship attribution, a technique aiming to determine an author of a document given </span><span style=""line-height: 1.5em;"">some textual characteristics of the author's writing style extracted from his previous works, is well established in social science, it raises many research </span><span style=""line-height: 1.5em;"">questions in malware detection domain. For example, how can we define an author (e.g., malware generation engine, a malware campaign, an individual) so that </span><span style=""line-height: 1.5em;"">it can be leveraged for automatic and accurate detection, and to what extend source and binary code embed distinct features traceable to its author.</span></p>""577998,""StAmand, Julien"
"590781"	"Stéphenne, Alex"	"Advanced Channel Parameter Estimation Techniques and Bounds for Next-Generation Wireless Networks"	"Channel parameter estimation is a crucial task that lies at the heart of any wireless communication system. In fact, it allows the system to properly operate in complete awareness of its environment through the knowledge of its propagation channel. Within the framework of this discovery grant research program, we will consider the problem of wireless channel parameter estimation for various system configurations and signalling schemes that are geared toward current and future communication systems such as the new 4th-generation (4G) LTE-Advanced (LTE-A) wireless networks and beyond (LTE-B) as well as 5G. The key goal is to develop advanced channel identification and parameter estimation techniques and Cramer Rao lower bounds (CRLBs), preferably in closed-form, in the presence of various types of diversities; namely multi-path propagation, multiple receiving antennas, and multi-carrier communications combined with higher-order modulations (M-QAM). We will adopt a maximum likelihood (ML) approach to rip the full benefits of jointly exploiting these types of diversity. To significantly reduce complexity, we will recur to the new powerful importance sampling technique and global optimization principle to implement the ML approach. To increase spectrum efficiency, we will preferably develop non-data-aided (NDA) estimators or alternative data-aided (DA) versions that require the least pilot signal overhead. We will also investigate novel code-aided (CA) designs for turbo parameter estimation. To cope with short observation records in non-continuous packet-switched transmissions in non-persistent scheduling such as on the uplink of LTE-A, we will recur to the new powerful compressive sensing (CS) techniques.""575205,""Stephens, David"
"589352"	"Syriani, Eugene"	"Collaborative Modeling Among Cross-Domain Teams"	"Complex systems engineering typically involves many stakeholder groups working in a coordinated manner on different aspects of a system. Each aspect addresses a specific set of system concerns and is associated with a domain space consisting of problem or solution concepts described using specialized terminology. A domain-specific modeling language (DSL) allows engineers in a stakeholder group to address their specific system development concerns by building and using models that express abstractions in domain-specific terms. Complex heterogeneous systems engineered by heterogeneous teams thus require that multiple DSLs are used cooperatively to support design, execution, and validation of the modeled system.<br /><br />The field of model-driven engineering (MDE) provides a promising foundation to support modeling, executing, and validating domain-specific models. However, the lack of focus on the collaboration and integration of models and languages among teams and users has been a major impeachment for true industrial adoption, despite some recent success stories reported.<br /><br />There are many fundamental challenges to solve in order to enable various stakeholders to work collaboratively on a common modeled system. In this program, we will concentrate our efforts to address scalability and usability issues in such distributed environments. The long-term goal of this research is to produce a framework where engineers and domain-experts can collaboratively model and synthesize complex heterogeneous systems.<br /><br />We will investigate the appropriate storage representation and medium for a model repository able to handle a large amount of large-scale models (in the order of millions models containing millions of elements). Executing such models requires an imperative improvement in the performance of model transformations, by taking advantage of concurrency and the distributed resources available in the framework. Because multiple users will be managing models by means of multiple views on a same model, the framework must ensure a synchronization to keep models and views consistent with each other. Furthermore, each model or view will evolve over time during the development process and thus we will investigate for a domain-specific version control system that allows differencing and merging mechanisms adapted to the domain of the model, by taking into account the semantics and concrete syntax of models. Finally, we believe that the workspace of the domain-specific user must give him a sensation of complete immersion in his domain. Therefore, the framework must support the automatic generation of domain-specific IDEs that are customizable per domain and per user preference.""586431,""Syriani, Eugene"
"590291"	"Taboada, Maite"	"A computational treatment of negation and speculation in natural language"	"Sentiment analysis (also known as opinion mining) makes use of methods in computational linguistics and text analysis to determine whether a text (headline, news article, movie review, e-mail message, blog or Twitter post) is mostly objective or subjective and, if the latter, whether positive or negative towards its subject matter. Much progress has been made in the last few years in how to represent and analyze sentiment and opinion, but the interaction of negation, speculation and opinion remains an elusive task. For instance, in the sentence <em>'Some may say that it's not a good movie'</em>, the apparent positive opinion in <em>'good'</em> is negated by <em>'not'</em> in the same phrase, but tempered again by the speculation conveyed by <em>'some may say'</em>. This sentence, in fact, seems to imply that the author actually has a favourable opinion of the film. The implication is not typically detected in current sentiment analysis systems, because speculation is rarely addressed.<br />The long-term objectives of my research program are to develop computational models of natural language, evaluating and implementing such models so that they can be applied to different types of texts. The short-term objectives are to find methods to extract negation and speculation, and to discover their role in the expression of sentiment and opinion.<br />My research will address the problem of negation and speculation by using machine learning methods to automatically find these phenomena in natural language. Machine learning methods involve classifiers which learn which features and cues are reliable indicators of a particular phenomenon, in this case negation and speculation. The result will be a software module which takes a text as input, and accurately identifies where negation and speculation lie. This module will be integrated into the sentiment analysis system developed within my lab, the Semantic Orientation Calculator (SO-CAL). <br />The anticipated outcomes of this research are a set of methods for identifying negation and speculation, a module that implements those methods, and an improved SO-CAL, which will be able to determine with high accuracy the sentiment expressed in language. The software will be made available to the scientific community. Potential applications include extracting opinion about products, politicians and current events, which will be of interest to companies in social media analysis, decision-makers, journalists, moderators, and the general public, in particular those who use social media regularly. In addition to the contribution to the field of sentiment analysis, detection of negation and speculation is important in many computational linguistics tasks, including text summarization and information retrieval.""587616,""Tabrizchi, Reza"
"591298"	"Tahar, Sofiene"	"Formal Analysis of Physical Systems"	"<p>With the advent of smart technologies, most of the engineered physical systems can now be characterized as computational systems composed of software and digital and/or analog hardware components, which closely interact with their continuously changing physical surroundings. The continuous and randomized nature of the analog components and physical surroundings of these (cyber) physical systems makes their computer-based analysis very challenging due to the inherent imprecision of computer arithmetic while dealing, for example, with real numbers. Thus, the accuracy of the analysis is compromised, which is extremely undesirable when systems are used in safety-critical applications, such as medicine or transportation, where an undetected bug in the system design phase may even lead to the loss of human life. The main focus of the proposed program is to overcome the above-mentioned limitations by using formal methods for analyzing physical systems. Formal methods are primarily based on using deductive reasoning or rigorous state-space evaluation for verifying system characteristics and thus can be used to analyze systems without compromising the precision and soundness of the analysis process. There is a plethora of research available for formal modeling and verification of digital and software systems. However, very few physical systems have been formally verified mainly because of lack of formalized mathematics to model and then analyse their behavior. What distinguishes the proposed research from existing work is the focus on formalizing foundational theories of Physics like Electromagnetics, Quantum or Stochastic Processes and use them to model and reason about the correctness of continuous and randomized physical systems. Moreover, besides functional analysis, we also want to focus upon performance aspects of physical systems. The ultimate goal of this research program is to provide a platform for the formal analysis of a variety of physical systems using a combination of formal technologies. The objective of the current proposal, however, is the formalization in higher-order logic of the concepts of queueing, control, electromagnetics and quantum theories to facilitate the formal reasoning about the corresponding systems within the core of a theorem prover. In order to illustrate the practical effectiveness of our results, we plan to use our formalizations for the analysis of various engineering systems in the domains of optics, photonics, electronics and avionics. The direct beneficiary of this multidisciplinary research will be the Canadian microelectronics, telecommunications and transportation industry as already shown in past and on-going collaborative projects. Furthermore, this proposal will contribute towards the training of a number of highly skilled personnel available to Canadian industry and academia in these emerging fields of science and engineering.</p>""576418,""Taher, Jennifer"
"590261"	"Taheri, Shamsodin"	"Photovoltaic (PV) system performance under cold and polluted conditions"	"<p>Solar energy is now considered to be one of the most reliable and promising sources of renewable energy. Efficiency of conversion of solar energy into electricity by means of photovoltaic (PV) systems varies considerably depending on atmospheric conditions such as ice, snow and pollution. In fact, ice, snow and pollution deposits on solar panels can reduce their performance or even stop them from absorbing solar radiation. The general objective of this research program is to investigate fundamentally the effect of such conditions on the energy efficiency of the PV systems in Canada. The energy reduction of the PV systems due to the partial shading phenomenon when ice, snow and pollution layers cover the PV panel surface non-uniformly will be investigated experimentally. The lack of mathematical models for forecasting the electrical performance of PV systems under such conditions has motivated the present study, which aims at developing reliable models to analyze behavior of the PV system integrated into the power system. The PV system will be modeled through extracting mathematical equations from associated physical phenomena. Due to the partial shading phenomenon, the PV system experiences several local Maximum Power Points (MPP). Hence, a Maximum Power Point Tracker (MPPT) to extract maximum power from the PV array through tracing the PV operating voltage corresponding to the MPP will be proposed. Moreover, a control system will be presented to regulate the voltage and frequency when the PV system energizes the power grid.<br /><br />Based on the results obtained from laboratory work and simulation using the 3D Finite Element Method (FEM), several possible solutions to optimize the energy generated by the PV will be offered. In particular, this work will provide valuable information for the selection of the optimized dimension and tilt angle of the PV arrays with respect to the severity of ice, snow and pollution. The generated MPPT model will offer effective methods to extract the maximum power from the PV systems in cold and polluted regions. The proposed control system will provide smooth integration of the PV system into the power grid, efficient energy extraction from the PV system as well as frequency/voltage regulation of the power grid in cold and polluted environments. Due to particular climate conditions in Canada as well as the rapid growth of photovoltaic systems, this work will attract considerable interest of the PV community, both researchers and practitioners.</p>""579223,""Tahir, Hamid"
"590346"	"Talbi, Larbi"	"Caractérisation du canal de propagation à 60GHz pour la 5G"	"L'intérêt croissant pour des services sans fil à très haute capacité, ainsi que l'avènement de la 5e génération (5G) des réseaux cellulaires ont stimulé la recherche pour le développement de systèmes de communication sans fil à courte portée opérant dans la bande de fréquences non-licenciée des ondes millimétriques. Ceci résulte du fait que cette bande de fréquence peut, d'une part, procurer une largeur de bande importante, environ 7 GHz, capable de transporter des signaux numériques haute définition (HD) avec une possibilité d'une gestion économe du spectre et, d'autre part, vu la petitesse de la longueur d'onde, être réalisée physiquement de façon compacte grâce aux développements technologiques dans les domaines des circuits intégrés à grande échelle (VLSI) et des circuits micro-ondes millimétriques intégrés (MMIC). C'est dans ce contexte que notre programme de recherche s'inscrit.  En effet, l'objectif ultime de notre programme de recherche est de réaliser des percées majeures dans le domaine des systèmes de communication sans fil opérant sur la bande des ondes millimétriques. La réalisation de cet objectif ambitieux nécessite une bonne connaissance des phénomènes de propagation et une classification réaliste des scénarios de déploiement. Ayant acquis de l'expérience sur ce domaine, à savoir, la conception de circuits MMIC et la caractérisation du canal de communication sur cette bande de fréquence, il est clair que le prochain défi consiste à approfondir certains concepts, à concevoir et à implémenter un système de communication millimétrique. En effet, les réseaux cellulaires 4G et les réseaux d'accès de technologies mixtes, optiques et cuivre (ASDL - Asymetric Digital Subscriber Line), ou coaxiaux (HFC - Hybrid Fiber Coax), permettent l'introduction des services multimédias jusque chez l'abonné, préparant ainsi l'arrivée dans un avenir proche de services très hauts débits plus performants et qui seront certainement transportés de façon sans fil sur la bande de 60 GHz.<br />Dans un cadre concurrentiel, l'opérateur devra être réactif et disposer d'un réseau rapidement configurable et déployable. Il est clair que, compte tenu de la diversité géographique et de la densité de population, le réseau de la 5G se situe essentiellement dans la possibilité d'assurer un prolongement sans fil et de manière transparente du réseau déjà existant de la 4G. Il est donc indispensable de relever ce défi et de prendre en considération l'ensemble des contraintes pour définir et spécifier un réseau courte portée sans fil multimédia grand public offrant un débit supérieur à 10Gb/s par utilisateur. Nous proposons le développement d'antennes et de réseaux d'antennes, la caractérisation expérimentale du canal de propagation et l'étude des différentes approches de déploiement à moindre coûts permettant l'optimisation des services offerts sur la bande des ondes millimétriques.<br />""574670,""Talbot, Benoit"
"589815"	"Tawbi, Nadia"	"Hybrid approaches for enforcing security policies."	"<p>In our highly connected and highly computerized world, security issues are of major importance.  <br />Trusted systems need to communicate with untrustworthy parties exposing the users to a risk of information leakage or data corruption. It is urgent to find effective solutions to protect users. Despite major research efforts that have been devised, there still are many challenging problems to be explored.<br />My broad research aim is to contribute to the design of provably sound techniques, methods and enforcement mechanisms that enforce security policies at the level of applications. <br />Mechanisms such as access control, encryption, firewalls, digital signatures, and antivirus scanning are either too restrictive, causing loss of flexibility, or unable to protect from newly introduced threats or malicious attacks. One of the underlying reasons is that they do not address the fundamental problem, which is tracking information flow in a fine-grained application-specific way. To do this one needs to analyze the code of an application; this  research trend is known as ""language-based security"". In this research program, I will address information flow enforcement mechanisms that prevent information from flowing from a higher security level to a lower one or in a dual way to prevent lower security level data from corrupting higher ones. The main idea is to inspect the code of an application and control its execution in order to apply the policy. <br />I will adopt the following objectives and methodological lines. <br /><br />1. Introducing the least overhead possible while reducing false positives.  A hybrid approach combining static and dynamic analysis in an effective way is to be adopted. <br />2. Dealing with declassification. Declassification means an intentional leak from higher security levels to lower ones. <br />Information flow policy is very restrictive; real-world applications release information as part of their intended function. For instance, login procedures, communication of encrypted data, or voting systems do not comply with information-flow policy. Release of some information must be allowed but controlled to prevent non-intended leaks. <br /><br />3. Designing mechanisms that target more expressive languages such as concurrent languages. More challenging issues may appear such as timing channels, which cause leaks due to attackers' capacity to observe time. Extensive data-flow analysis and semantic models suitable to concurrency will be adopted. <br />4. Quantifying the amount of leakage in a probabilistic framework. The aim is to relax non-interference and tolerate some quantified leaks. <br />5. Leveraging all these techniques to a real world language. <br /><br /><br />The results will provide practical and theoretical foundations for the development of effective and robust mechanisms to preserve the confidentiality and the integrity of their data. <br /> </p>""590482,""Tawhid, Mohamed"
"590796"	"Thomson, Douglas"	"Models and ultra-sensitive apparatus for electronic single cell analysis"	"Cells are dynamic electronic materials that change electronic state in response to environmental pressures. There have been many demonstrations that cells exhibit dielectric changes that coincide with important physiological changes such as the differentiation of stem cells, the development of multi drug resistance (MDR) in cancer cells, and programmed cell death (apoptosis). Single cell measurements of the dielectric response can identify dielectric sub-populations and temporal changes to these sub-populations that are not possible with common bulk probes. The proposed research program will derive quantitative models of the relationship between electronic and physiological changes in cells and also to produce instrumentation with superior ability to detect these dielectric changes in single cells. The work will focus on the use of dielectrophoresis (DEP) in a flow system that bring cells from a reservoir to the analysis region, making the analysis of a large number of cells practical in a reasonable time frame. In collaboration with G. Bridges I have developed a flow based single cell DEP analysis instrument. It can detect apoptosis by sensing the modified polarization that results from a drop in intracellular potassium, concentrations. Using a microwave interferometer in a differential detection approach, this DEP cytometer detected apoptosis in Chinese Hamster Ovary (CHO) cells.<br />I propose to investigate some of the basic mechanisms of dielectric changes in cells using compounds that are well known to inhibit or activate specific biochemical pathways that will impact the dielectric properties of the cell. For example, Ouabain inhibits the operation of the K+Na+ pumps in the cell causing changes to the ion concentrations within the cell and hence its dielectric properties. Other compounds control the pores that allow ions in and out of the cells. Some of these compounds may also prove useful as ""dyes"" that provide a specific dielectric read out. In some of the physiological events outlined above these pumps and pores have been hypothesized to play a major role. The proposed research will measure the impact of these compounds and derive mathematical models that relate the inhibition of specific processes to dielectric changes. The small volume of cells makes single cell dielectric measurements very challenging. New microwave interferometer designs and DEP actuation methods will be used to lower the dielectric detection limits. The goal of the work is to detect dielectric changes (Clausius Mossotti Factor) of less than 5% in a single cell.<br />The HQP from this interdisciplinary research program will gain skills for many career paths. This group is one of the few with the collaborations and tools to undertake this research. This work will impact users and researchers of bulk dielectric and single cell dielectric measurements and provide desperately needed new tools for single cell biophysiology.""580634,""Thomson, Ella"
"591193"	"Umapathy, Karthikeyan"	"Adaptive Signal Modeling and Feature Extraction Methods for Analyzing Cardiac Fibrillation"	"The heart is a fascinating and complexly designed vital organ with electro mechanical functionalities that beats (i.e. expands and contracts) rhythmically to maintain blood circulation throughout the lifetime of a living being. When this rhythm gets disturbed or the heart goes into arrhythmic contractions and expansions for a multitude of pathophysiological reasons, it may result in life threatening medical conditions. These rhythmic disorders can result in cardiac arrhythmias, which can seriously affect cardiac output (or blood flow). Ventricular fibrillation (VF) is an arrhythmia that originates from the lower chambers of the heart and can lead to sudden cardiac death if medical attention is not provided within minutes of onset. Most of the approximately 300,000 sudden cardiac deaths (SCDs) reported every year in North America (45,000 of them in Canada) is related to VF.  Atrial Fibrillation (AF), in comparison, originates from the upper chambers of the heart, and although not as lethal as VF, can seriously affect quality of life and increase the risk of stroke in patients. There is a great need to develop new engineering methodologies to improve understanding and assist in reducing the mortality rates associated with these cardiac arrhythmias. The mechanisms behind VF and AF are elusive due to the nonstationary nature of the processes and the ethical/practical limitations in studying human arrhythmias. Over the years, signal processing approaches have aided the medical community in extracting information from electrograms (electrical signals from the heart's surface) and electrocardiograms (cardiac electrical signals from the body surface), optimizing treatment options, and developing intelligent medical devices. The proposed research program will aim to identify novel ways to quantify these cardiac arrhythmias, so as to arrive at short-term and long-term treatment options. Specifically, the research, in collaboration with Toronto General Hospital and St. Michael's Hospital, will develop advanced electrogram and electrocardiogram signal and image processing techniques to improve the efficiency of long-term focused medical therapies in identifying and eliminating the sources responsible for these arrhythmias, increase the intelligence of implantable devices, and provide vital information to the emergency medical services personnel to improve survival rates in cardiac resuscitation efforts.""589817,""Ung, Bora"
"589433"	"Valizadeh, Pouya"	"Investigation of the role of isolation-feature geometry in improving the threshold-voltage adjustability and power-handling of polar III-Nitride HFETs, and physics-based modeling of the gate-current"	"Over the past two decades research on wide-bandgap polar AlGaN/GaN heterostructure field effect transistors (HFETs) has resulted in a rapidly growing market in the area of high-frequency/high-power transistors. As the share of this market is inching towards the billion-dollar mark, innovative solutions for improving the transistors' power-handling/heat-management, and for reliable realization of both enhancement- and depletion-modes of operation (i.e. having a positive and negative threshold-voltage, respectively) are gaining a more prominent status. <br />The novel approach of the threshold-voltage engineering via modifying the isolation-feature geometry, which was proposed by the PI in his previous Discovery grant application, has found a strong foothold as the most viable technique for realization of enhancement- and depletion-mode AlGaN/GaN HFETs on the same chip.   Besides the theoretical and experimentally-based contributions of the PI to understanding this phenomenon, a few other groups are also currently involved in a very lively technological and scientific endeavor in this area. However, understanding of this phenomenon and its capacity are far from being complete. <br />Under the support of the present Discovery grant application, PI intends to continue on his experimental/theoretical involvement in assessing this problem via looking into submicron-scale isolation-features, and to expand the scope of coverage by examining other alternative III-Nitride heterostructures. Another novel dimension that will be explored for the first time is the role of isolation-feature geometry in limiting the extent of self-heating in AlGaN/GaN HFETs realized on submicron-scale isolation-features. In addition, PI will continue to work on physics-based modeling of the gate-leakage current of these HFETs. Each of the proposed research directions of this program presents major strides beyond the coverage of PI's current Discovery grant, dealing with fundamentally new challenges. The microfabrication recipe developed by the PI's team, with the support of his current NSERC Discovery grant, at the microfabrication facilities of McGill University, will be used in realizing the transistors envisioned in this application. <br />Without a viable technique, such as the one proposed here, for reliable realization of enhancement-/depletion-mode pairs of III-Nitride HFET, full commercialization of this technology is not possible. Since a major market for these HFETs is defined in the area of developing high-efficiency electronics for high-voltage/high-temperature applications in hybrid and electric vehicles, improving their operation and reliability can lead to sizable reduction of the society's carbon footprint. This research program can also provide an edge for the Canadian auto-industry.<br />""575099,""Vallance, Bruce"
"590408"	"vanKaick, Oliver"	"High-level Shape Representations for Content Creation"	"We propose to investigate high-level representations to facilitate content creation. The creation of geometric content is an important task in areas such as computer graphics and animation, visual effects, game development, and fabrication-oriented design. The most common approach for creating 3D content is by directly modeling the geometry of 3D shapes. However, modeling is a laborious task where artists need to be skilled to use the conventional software tools, which typically involve working on low-level shape representations, e.g., triangle meshes. <br />Thus, during the last decade, efforts in computer graphics have focused on the task of facilitating the creation of 3D models. A recent trend towards this direction is to represent shapes as a collection of primitives defined at a more semantic level, e.g., shape parts (such as the legs, seat and back of a chair). These representations allow manipulating shapes at a more abstract level, independently of the underlying low-level representation. High-level representations can also be used to synthesize new models or to guide the modeling of shapes or exploration of a shape space by capturing additional aspects of the structure and appearance of shapes.<br />In this context, the goal of the proposed research program is to develop high-level representations for shape manipulation and synthesis to facilitate the creation of geometric content, especially 3D shapes. The successful use of such representations requires three components: 1) A shape representation that is suitable for the target applications; 2) Algorithms that automatically or semi-automatically build the representations for existing geometric data; and 3) Guiding mechanisms that allow a user to employ the representations for content creation. Thus, these three components comprise the research streams of the program. <br />In the first stream, we will investigate the design of shape representations that are more suitable for manipulation and synthesis. In the second stream, our goal is to study how big data analytics can lead to significant advancements in shape analysis, and facilitate the automatic construction of the high-level representations. In the third stream, we will investigate the development of guiding mechanisms that can support creative modeling. We expect that, in the long term, this program can contribute to facilitating the creation of content and thus be of interest to the design and entertainment industries.""571181,""vanKerkwijk, Marten"
"578138"	"Vertolli, Michael"		"NSERC"
"590119"	"Wagner, Alan"	"Parallel and Distributed Computing for Real-time Analytics"	"<p>The goal of my research is to create a runtime system that can scale-out to large size clusters of multicore machines for in-memory computation of analytics on big data.  Many of the existing distributed systems do not take advantage of the tools and techniques developed in high performance computing for addressing the performance issues that ultimately limits scalability. MPI (Message-Passing Interface) is a message-passing library that is the de-facto standard in parallel programming for high performance computing. I intend to develop new MPI middleware along with associated tools as the basis for a scalable runtime system for computing analytics.</p><p>Scaling-out requires the ability to express massive amounts of concurrency and ultimately relies on message-passing.  As such, the starting point for this work will be our FG-MPI (Fine-Grain MPI (Message-Passing Interface)) system, which we have shown can scale to thousands and millions of processes.  FG-MPI extends MPICH, an MPI runtime system that is used to derive most of the commercial versions of MPI.  This work will require extending the existing FG-MPI middleware as well as introducing new tools and techniques for programming at large scale.</p><p>The research involves (a) investigating further improvements to the MPI middleware to support multicore machines, (b) developing tools to support an ""actor-like"" programming environment for specifying the composition of processes and the development of service-oriented data structures for MPI, (c) following a million process methodology to develop techniques for developing and deploying programs at scale, (d) developing innovative message-based data structures and algorithms to support knowledge-based analysis and reasoning on big data. </p><p>This research directly addresses the two main trends in computer technology: the appearance of multicore and cluster/cloud computing.  In both cases --- the future is parallel!  The central problem is how to use multicore to scale-up or how to use more machines to scale-out to provide the performance needed to process and compute analytics on the vast amount of data that is available today.<br /><br /></p>""569041,""Wagner, David"
"590907"	"Wainer, Gabriel"	"Advanced Methodologies for Real-Time Discrete Event Modelling and Simulation"	"Embedded real-time systems are built as sets of components interacting with their surrounding environment. These are highly reactive systems, where the decisions can lead to catastrophic consequences for goods or lives (industrial automation, transport, robotics, etc.). In these systems, not only correctness is critical, but also the timing for executing the system tasks. Modeling and Simulation has proven to be a practical approach to verification of these systems with reduced costs and risks. Formal modeling and simulation provides even better results as the software artifacts can be built faster and safer. In particular, the Discrete-EVent Systems specification formalism (DEVS), a formal modeling and simulation framework based on generic dynamic systems concepts, is suitable to deal with these issues. <br />Recent research has been oriented in using DEVS as a framework for Real-time systems development method. In addition, DEVS theory has been extended by formulating a rigorous theory of quantized systems in DEVS. Nevertheless, none of the existing DEVS environments and formalisms used for real-time systems has considered well-known problems in RT applications, for instance, fault tolerance, sensor replication or transient system overloading. The occurrences of system failures usually generate new real-time alarm tasks dynamically, generating a task load that cannot be supported by the system. Several techniques have been proposed to solve this problem. For instance, the techniques of imprecise computation allow to provide controlled degradation under transient overloads (by dividing the tasks in a mandatory part to provide basic results, and an optional part to improve the results obtained; also multiple versions of the same task can be provided, using a different version depending on the present load). <br />The goal of our research program is to combine these theoretical frameworks, providing a technique for imprecise real-time computing based on DEVS. From the methodological point of view, we are interested in developing a theory of dynamic real-time DEVS using imprecise computing. The goal is to provide security, reduction in the development times, and the advantages of a formal framework to develop simulations. The results of the simulated system can be applied in the development in the real-time system. <br />From the practical point of view, we expect to provide a set of tools that can be applied to develop real-time software and simulations with hardware-in-the-loop (requiring real-time response). By invoking appropriate execution engines, we will provide the ability to execute models in both logical time (the usual simulation clock) and real time (constrained to wall-clock time). These tools can be applied to a wide range of applications, ranging from mobile communication, emergency planning or traffic control systems.""590163,""Waite, Michael"
"589380"	"Waldispuhl, Jerome"	"Computational methods and integrative approaches to predict 3D structures of large RNA molecules"	"Ribonucleic acids (RNAs) are versatile biomolecules that are involved a diverse number of biological functions. For example, as messenger RNA it encodes genes, as microRNA it regulates genes and as ribosomal RNA it translates genes. To achieve their non-coding functions, RNAs often use sophisticated structures that can be described at two levels. First, the secondary structure encompasses the maximal set of stem and stem-loops formed by canonical base-pairing interactions (Watson-Crick and Wobble). Then, these secondary structure elements are assembled together via numerous van der Waals contacts and specific hydrogen bonds into the tertiary (3D) structure. The description of the complete 3D structure is a milestone into the understanding the biological function of a RNA molecule. We propose to develop a robust computational framework to predict the 3D structure of large RNA molecules. Our proposal is articulated around 3 distinct modules, which will be conducted in collaboration with Canadian and international theoreticians and experimentalists.<br />The first module aims to develop algorithms to mine experimentally determined RNA 3D structures stored in databases and identify conserved RNA 3D motifs. In particular, we propose to search for a new class of motifs involving long-range interaction and non-Watson-Crick interactions. Then, we will expand our RNA-MoIP framework to use this information in order to predict 3D structures of large RNA molecules.<br />In the second module, we will develop integrative approaches to determine RNA structures from collections of SHAPE experiments on wild type and mutant RNAs. Here, we propose to expand the dynamic programming techniques introduced with our previous algorithm RNAmutants and add new features allowing to use RNA motifs data sets.<br />Finally, in the third module, we will apply our techniques to perform 3D structural annotation of regulatory elements in drosophila and human genes. We will assess the significance of RNA structures in coding and non-coding regions of mRNAs and investigate contribution to gene regulation mechanisms.<br />The technology developed in this proposal aims to develop robust and rigorous computational tools to decipher the role of RNA structures in gene regulation processes. These results may help us to understand the molecular basis of several diseases and eventually provide us key information to inhibit or restore gene expression pathways.?The source code of all the methods developed within this proposal will be publicly released and accessible to the community through Web servers.<br />""579148,""Waldispühl, Jérôme"
"591154"	"Wallace, James"	"Exploring Human Perception and Interaction in Multi-Device Environments"	"<p>Today's computing devices have not fully realized the potential imagined by early computer scientists and engineers when they imagined a world in which computing infrastructure seamlessly supported our everyday computing needs; mobile, portable, and large, shared devices pervade our lives yet are rarely used together. <span style=""line-height: 1.5em;"">For example, a smartphone in your pocket and a nearby tablet might contain important data, but cannot easily interface with a nearby interactive TV for sharing with co-workers, even though wireless networks are </span><span style=""line-height: 1.5em;"">readably</span><span style=""line-height: 1.5em;""> available. These interactions are useful in unlocking potential from data stored on small devices with nearby large displays. Even if such interactions were allowed, it is unclear how software should be designed to enable cross-device sharing of data and whether doing so may impact a user's ability to accurately and effectively utilize data. </span></p><p><span style=""line-height: 1.5em;""></span><span style=""line-height: 1.5em;"">This research explores human ability to interact and perceive with data that is spread across multiple devices, with the goal of ultimately enabling cross-device interaction. We will develop three sets of test-beds that will be used to conduct controlled, empirical studies that investigate human perception and interaction across personal and shared devices in three contexts. </span><span style=""line-height: 1.5em;"">The first series of experiments will explore interaction in laboratory setting, with carefully focused studies on aspects of human perception such as magnitude judgement, visual search, reading comprehension, and navigation. </span><span style=""line-height: 1.5em;"">The second and third series of experiments will explore interaction in an applied setting, in collaboration with industrial partners, where simulated tasks will be used to determine negative effects of cross-device information. Finally, results will be ecologically validated by conducting </span><span style=""line-height: 1.5em;"">By developing such an understanding, we will inform the design of multi-device software systems that support collaborative work, such as data analytics, command and control, and emergency response, where </span><span style=""line-height: 1.5em;"">information </span><span style=""line-height: 1.5em;"">is divided</span><span style=""line-height: 1.5em;""> across any number of individuals, devices, or displays. </span></p>""587380,""Wallace, James"
"589557"	"Wan, Justin"	"Numerical Algorithms for Nonlinear Models with Applications in Economics and Medical Image Processing"	"<p>Scientific simulation and visualization have been playing an increasingly central role in scientific discoveries and engineering designs, as well as in business and social sciences. In the heath care industry, common diseases such as diabetes, stroke, and cancer results in substantial expenditure in Canada every year. Understanding how the regulatory gene networks affect cell function will facilitate treatment of common disorders. Such studies record thousands of images in a time series, with tens to hundreds of cells in each frame. Computational models provide an efficient, robust, and reliable tool to analyze cell images and the tracking of cellular behaviors.</p><p>Another example is in economics research. The ever increasing price of gasoline has raised the question of how oil reserves impact the energy supply and market prices. The oil industry can be viewed as an example of oligopoly in which the competitive market consists of a small number of firms. The firms compete with each other by strategically setting the optimal quantity in order to maximize their expected profit. A similar circumstance exists in the case of the smart phone market where the firms strategically control the prize (rather than the quantity). The study  of the dynamics of different market structures is of longstanding importance in economics. The problem becomes more interesting and challenging when the resources are exhaustible (natural resources). Effective models and numerical methods are crucial to provide a powerful means to compute accurately the (Nash) equilibriums that set the market prices and quantities.</p><p>Whether it is a medical or economics application, they can be both studied and modelled by nonlinear mathematical equations. Numerical modelling and algorithms are fundamental building blocks in computational science. However, as applications become more complex, solving these equations become more challenging and time consuming.</p><p>The proposed research will contribute to the development of fast and robust numerical solvers as well as accurate and effective models. A major goal of this research program will be to develop sound and predictive scientific simulations for applications in economics and medical imaging. This numerical algorithm research expands the applicability of the current computational tools. This research will advance the development in the field and also offer fast algorithms for practitioners in financial institutions. For the medical imaging research, the proposed research will help develop an image processing software that is capable of processing cell images automatically. The computational framework will analyze the cell information quickly, thus facilitating better understanding of biological processes which will lead to improvements in drug discovery and new therapies.</p>""578457,""Wan, KaiYang"
"578479"	"Wehbe, Rina"		"NSERC"
"591082"	"Westwick, David"	"Identification of Nonlinear Systems"	"Automatic control systems are ubiquitous in the modern world. They can range in scale from the position controller that regulates the position of the read/write head in a hard disk, to the autopilots that control the movement of aircraft, to the advanced traffic control systems that regulate the flow of traffic in major cities. Since the controller is designed based on a mathematical model of the system that will be controlled, the system's performance will be limited by the accuracy of the model used in its design. Thus, having an accurate mathematical model of the target system is a necessary pre-requisite to the design of a high-performance control system.<br /><br />While models of comparatively simple systems can be derived based on first principles, this approach quickly becomes intractable as the system's scale and/or complexity increases. System identification, on the other hand, is a data-driven process that creates mathematical models of dynamic systems from measurements of their inputs and outputs.   This research programme will develop system identification techniques that can create accurate mathematical models of nonlinear systems from data that have been gathered under normal operating conditions and when some form of control system is operating.    <br /><br />The system identification process is greatly simplified if the input to the system is independent of any disturbances that affect the system, or of any noise that may be contained in the measurements.  Unfortunately, many systems can only be operated in a ""closed-loop"" configuration, where the system output is measured, compared to a desired trajectory, and this error is then used to adjust the input signal.  This type of feedback control may be necessary for safety, performance or economic reasons.  The nonlinear system identification algorithms developed in this research programme will be designed to operate on data that have been gathered under closed-loop conditions, as this will allow them to be applied without the need for a separate, often impossible to perform, open-loop experiment.  Instead, we will concentrate on constructing models from normal operating data, as this will minimize the economic impact of generating the model.""575499,""Westwood, Jocelyn"
"581256"	"Yancheva, Maria"	"Ontario"	"CANADA"
"590747"	"Yang, Simon"	"New approaches to intelligent navigation of autonomous surface and underwater robotic systems"	"Unmanned surface robots (USR) and unmanned underwater robots (UUR) have many important applications such as security surveillance, environmental monitoring and exploration in rivers, lakes and seas, particularly in unstructured and hazardous environments. Real-time sensing, localization, and navigation are fundamentally important issues for autonomous surface robots (ASR) and autonomous underwater robots (AUR). The key challenge is to infuse these robots with ""human-like"" intelligence that will enable safe autonomous navigation and successful completion of designated tasks.<br /><br />The proposed 5-year research program is to investigate the real-time sensing, localization, path planning, tracking, and control of autonomous surface and underwater robotic systems, using biologically inspired intelligent approaches. In particular, the proposed research will focus on: (1) real-time collision-free navigation and control of water surface robots, including real-time sensing and multi-sensor fusion, integration of acquired information from its onboard sensors and other available sensed information, simultaneous localization and mapping (SLAM), and real-time collision-free path generation and control of the water surface robots; and (2) real-time collision-free navigation and control of autonomous underwater robots in dynamic 3-dimensional (3D) underwater environments, with the presence of unknown ocean currents and other uncertainties. Experimental surface and underwater robotic platforms will be developed as a test bed for this research. Novel biologically inspired intelligent approaches to multi-sensor fusion and autonomous navigation of surface and underwater robotic systems will be developed and implemented on the experimental platforms. Rigorous theoretical investigation and experimental verification of the novel approaches will be conducted, with reference to established leading approaches. <br /><br />The proposed research has been motivated by the fact that biological systems, such as fishes and other animals, can efficiently sense the dynamic environments and take effective actions based on those inputs. The novelty and anticipated significance of the proposed research will be the provision of new knowledge to the respective research communities and the development of new biologically inspired intelligent techniques for real-time sensing and navigation of autonomous surface and underwater robots, which will have many civilian and military applications. In addition, the proposed research will offer some insights into biologically inspired methodologies for other areas of research. Furthermore, this research program will provide good opportunities for the training of highly qualified personnel in the areas of robotics and intelligent systems, and also offer great commercial and social impacts.""572977,""Yang, Tony"
"590553"	"You, JiaHuai"	"Extending Answer Set Programming"	"<p>A computational problem can be solved by designing and implementing an algorithm for it, or by expressing it in an intuitive modelling language that comes with software for evaluating expressions written in the language. The latter is an example of what is called declarative problem solving. The goal of this research is to extend Answer Set Programming (ASP for short), which has emerged as a promising declarative paradigm for solving computational problems, by addressing two important problems: (a) on integrating open-world with closed-world reasoning, and (b) on using ASP as a uniform language for modeling and reasoning with domain as well as defeasible ontological knowledge. </p><p>The main idea of ASP is that a given problem is stated in terms of constraints, which are expressed in a rule-based language under the stable model (also called answer set) semantics. Intuitively, an answer set corresponds to a solution to the problem being solved. When an answer set is computed, the user can extract from it the corresponding solution. The formulation of ASP has been crystallized from years of research in knowledge representation, logic programming, and constraint satisfaction. The goal is to provide a declarative language for modelling domain knowledge and computational tools for processing programs written in such a language. With highly competitive ASP solvers already built, several applications have been developed, for example, in molecular biology, decision support systems, planning and scheduling, solving puzzles and games, and more recently, in reasoning with the Web where ontological knowledge plays a critical role. </p><p>In this research, we propose to study two directions of extending ASP, each with distinguished features and merits. One is on a tight integration of ASP with decidable fragments of class logic, where we will address the problem of combining open and closed world reasoning. These kind of reasoning tasks often arise in the context of reasoning with complex heterogeneous systems. One advantage of this approach is that it allows the adoption of efficient inference engines, e.g., those developed for reasoning with ontology. The other direction concerns ASP with existential rules for representing and reasoning with domain and defeasible ontological knowledge in a uniform language. This will result in a seamless integration of different kinds of reasoning and establish foundations for building the next generation ASP solvers. </p><p>It is well anticipated that many further applications would require processing domain knowledge in conjunction with ontological knowledge, such as planning and scheduling, clinical trial and complex management systems, and reasoning with the Web. The success of this research will provide needed insights, and computational mechanisms, for advancing the ASP adoption in solving the next wave of new applications.</p><p></p>""579190,""You, Jonathan"
"590569"	"Yousefi, Shahram"	"Sustainable data storage solutions: embracing the cloud and big data"	"<p>Data storage (DS) is one of the basic foundations on which digital information systems thrive. With the ever-increasing popularity of <strong>cloud-based</strong> systems and high-capacity mobile units, there is critical need for sustainable storage solutions. Current DS systems will not be able to handle the rate at which data is being created. Every day, we create in the order of 2.5 quintillion (1 followed by 18 zeros) bytes of data. Astonishingly, almost 90% of the data in the world today has been created just in the last two years alone. This indicates an exponential growth and will be unsustainable unless disruptive data storage solutions are found. This data comes from everywhere: social media posts, pictures and videos, ebooks, digital music, sensors for climate data, financial transaction records, and cell phone GPS and connection information to name a few. This is what we refer to as <strong>big data</strong>.</p><p>This research program takes on two distinct challenges: 1) designing efficient DS solutions, and, 2) inventing capacity-achieving codes to write on flash memories. Together, these two areas provide a complete picture for the next generation of cloud storage where users/nodes store information locally on flash drives with improved capacity and longevity. Globally, a highly sophisticated network storage protocol provides a distributed information repository which is expected to be reliable, efficient, scalable, low-latency, cost-effective, and secure. </p><p>Prof. Yousefi's group at the Signal Design and Analysis Laboratory (SDAL) at Queen's University is a world leader in the design of graph codes and network protocols. Previous findings in the areas of distributed network and rateless coding will be leveraged to achieve our goals in the distributed storage studies of this proposal. Code design for flash memory will also benefit largely from our previous work and inventions in algebraic and random coding. </p><p>The outcome of this research will have far-reaching implications for millions of users in Canada and around the globe as we transform the way today's big data is stored and shared. Our research will conserve natural resources and energy by reducing the number of storage units needed and prolonging the lifespan of many electronic devices requiring data storage. This will have a profound effect on our environment by <strong>reducing pollution</strong>. The results provide technological advantages and highly qualified personnel (HQP) for a sector of the Canadian economy with $155 billion in revenues. This sector is one of the largest performers of private sector R&D with a Canadian GDP contribution of $67.2 billion and employs a knowledge-intensive workforce. The HQP will be highly trained in technical entrepreneurship, research and development, presentation and technical writing. </p>""590716,""Youssef, Amr"
"582061"	"Zarifi, Omar"		"NSERC"
"589869"	"Zhu, WeiPing"	"Robust and Energy Efficient Signal Processing for Massive MIMO Communication"	"Recently, telecommunication industries have paid more attention to the fifth-generation (5G) communication networks. The 5G technology will likely feature network intelligence to provide users with very fast yet seamless access to different radio air interfaces in various complicated environments. Meanwhile, green communication is drawing a great deal of attention worldwide, in which energy efficiency becomes one of the major design objectives in future 5G networks. Motivated by the increasing demands for better services in cellular networks, such as higher transmission rate, link reliability, ubiquitous access to internet etc, massive multiple-input multiple-output (MIMO) or large-scale MIMO is considered as one of the enabling and promising technologies for 5G wireless communication. Although MIMO has been developed as a mature technology over the last decade, massive MIMO is relatively new. Despite the merits of massive MIMO, its use in practical mobile environments faces some difficulties. One of its major limitations is that its performance depends largely on the channel state information (CSI), which is very difficult to obtain either through measurement or by channel estimation, especially when the number of antennas deployed is very large. Another restriction is the limited battery power in mobile terminals that affects severely the wireless transmission performance and link capacity. To realize broad-band and energy efficient green communications, more effort should be devoted to the development of intelligent wireless networks that can optimally and adaptively allocate the available resources based on channel condition, link reliability and total system power as a whole.<br />The proposed research addresses complex signal processing issues in the next-generation massive MIMO communication. The project focuses on the development of robust and energy efficient signal processing techniques for multi-user massive MIMO systems. The main objective of this research is to maximize the transmission reliability, spectral usage and energy efficiency of massive MIMO networks under realistic channel conditions. The new techniques to be developed include (1) robust channel estimation of massive MIMO systems with an objective to acquire imperfect CSI; (2) energy efficient beamforming design for large-scale antenna base stations using practical CSI estimate; (3) design of low-complexity near-optimal receivers for large-scale antenna systems; (4) development of analytical methods for performance study of massive MIMO systems; and (5) design of software platform for validation of the massive MIMO system integrating the proposed signal processing algorithms. The proposed project also contains a significant training component for graduate students and post-doctoral research fellows that will take place at Concordia University. <br />""573573,""Zhu, WenHong"
"590698"	"Ziou, Djemel"	"Photographie à champ de lumière et optique numérique"	"<p>Ce programme de recherche concerne la photographie plénoptique. Selon le modèle établi de formation de l'image, une caméra ne peut produire qu'une seule photo de l'instant prise avec une seule configuration de paramètres (focal, position.). Cette limitation a conduit au développement d'un nouveau paradigme de la photographie et plus généralement de l'imagerie numérique. Il s'agit plutôt d'acquérir un champ de lumière et de le réutiliser pour produire et afficher ce champ. On peut aussi générer  à partir du champ des photos 2D couleur de l'instant avec plusieurs configurations de paramètres. <span style=""line-height: 1.5em;"">L'implantation nécessite de revoir la caméra,  l'affichage et la convergence technologique de ceux-ci et de l'ordinateur. Par exemple, on ajoute des microlentilles dans la caméra et dans l'écran. La stratégie que nous proposons est différente de l'existant puisqu'il s'agit d'une vision holistique dans laquelle nous considérons les champs de lumières lors de l'acquisition par une caméra (visible, infrarouge, ultraviolet.), lors de l'affichage et lors de l'acquisition par la rétine humaine. Elle s'appuie à la fois sur le modèle mathématique/physique des formations de champ et sur une démarche écologique qui consiste à numériser des composants optiques. L'implantation de cette stratégie permet ainsi de pratiquer la photographie plénoptique en utilisant des équipements usuels (appareil photo numérique, caméra de téléphone, PC, écran.), sans être obligé de les renouveler ou moderniser. Sur le plan méthodologique, nous revisiterons, avec une démarche unificatrice, les modèles de formation du champ de lumière pour l'acquisition et pour l'affichage à l'aide de concepts avancés de mathématiques/physique; nous proposerons des estimateurs du champ à partir d'une seule image prise par une caméra numérique usuelle; nous développerons des méthodes de fabrication et d'affichage de photos 3D à partir du champ; nous nous focaliserons sur les champs de lumière pris dans les spectres visible, infrarouge et ultraviolet; nous intégrerons les modèles résultants pour produire une plateforme avant-gardiste destinée à la photographie de demain. Le grand besoin en calcul des estimateurs de champ de lumière et des photos 3D conduira à mener aussi des travaux en algorithmique et en programmation parallèle à l'aide de processeur graphique (GPU). Enfin, de nombreuses expérimentations dans des conditions réelles seront conduites pour valider les modèles qui seront développés.</span></p>""579341,""Ziraldo, Erika"
