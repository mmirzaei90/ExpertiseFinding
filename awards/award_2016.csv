"id"	"researcher_name"	"application_title"	"application_summary"
"615919"	"Aamodt, Tor"	"Energy-Efficient Programmable Accelerators"	"For 50 years computing systems have benefited from exponentially increasing performance per dollar. The first microprocessor was fabricated using 10 micrometer transistors in 1971 while recent designs have been manufactured using 14 nanometer transistors. The resulting five-orders of magnitude increase in transistor density combined with three orders of magnitude increase in clock frequency has enabled computing to change from handling only simple business calculations to supporting machine learning algorithms rivaling humans in accuracy and led to the emerging ""internet of things"" that promises to broadly increase convenience and productivity. However, the rate of transistor scaling will likely slow significantly after the 5nm process node threatening such visions for the future. Indeed, transistor threshold voltage scaling essentially stopped a decade ago resulting in stagnant clock frequencies, a shift to multicore and more recent concerns about ""dark silicon"". While alternative technologies (e.g., quantum computing) may provide excellent long term solutions these will take decades to fully develop resulting in a ""fallow period"" during which computing system capability will stagnate unless alternatives are found. In principle, computing system capability can be improved by orders of magnitude by exploiting specialized hardware. This promise comes at the expense of reduced flexibility and/or difficult programming models. The long-term goal of this research program is to enable order-of-magnitude improvements in computing capability per dollar versus today's computing hardware for diverse software applications. Such an improvement will almost certainly benefit business and society in general. For example, such increases could benefit society enabling more sophisticated machine learning to be applied embedded internet-of-things devices. An order of magnitude gain in computing capability could translate into the difference between a IoT enabled fridge that emails you if the milk is about to reach its use before date, and one that recognizes you are getting bored with your food choices and suggests a shopping list based upon past preferences and your family's health goals.This research program will tackle these goals by exploring both software and hardware approaches to improve the tradeoff between ease of software development and increasing computing capability by exploiting specialized hardware. The expected outcome is insights into how best to structure future computing systems so that the currently robust software industry remains viable past the end of Moore's Law. Canada will benefit because there is a growing number of computer hardware (Intel, AMD, Qualcomm) and software (Electronic Arts, IBM, Microsoft) companies with an RD presence here that can readily leverage these insights.""604464,""Aarab, Azzouz"
"614596"	"Abran, Alain"	"Software Estimation & Measurement Issues"	"The research hypothesis is the following: the software development process shares some characteristics with traditional engineering production processes implemented in the manufacturing of high technology products. For example, performance can vary across ranges of outputs: it may display economies of scale within some range, and it may display diseconomies of scale across other ranges of outputs. To overcome such diseconomies of scale within some range of a specific manufacturing design, production engineers design another production process with a larger capability to handle larger volumes of outputs. To investigate empirically the proposed theory and related hypotheses for software estimation purposes, requires an access to a large set of software projects for which not only specific variables must be available, but as well information about the projects size classification specific to each organization. Statistical studies will be carried on with data from two large repositories of software projects.Once research insights obtained on the presence-absence of economies-diseconomies of scale, we will revisit causal factors and related productivity impacts across size ranges, project scope and across organizations. Research will be conducted for estimation for pre-project, in-project and post-project contexts.""605406,""Abtahi, Shahen"
"616115"	"Achar, Ramachandra"	"Managing the Complexity of Multi-Physics based High-Speed Electronic Designs"	"The rapid advances in the global electronics industry have made the communications and computing products to be part of day-to-day life and highly pervasive. The massive user demand for higher bandwidth and advanced communication as well as better information management are necessitating newer generations of multi-function electronic circuits and systems that operate with lower-power and sharper excitations. However, due to such high data rates and the desire for nanoscale designs, high-frequency and high-density related issues are fast becoming the dominant factors limiting the performance of electronic designs. Consequently, it is imperative to note that, the traditional/artificial boundaries between different design disciplines such as circuits, electromagnetics, thermal and mechanical, etc. are fast vanishing and mixed-domain design challenges are emerging. Designers, while being challenged with the increasing need for multiphysics based co-design and co-analysis involving diverse and heterogeneous design modules, are not adequately supported by the currently available design framework and methodologies. To address the above difficulties, following research initiatives are proposed to develop a new-frame work and design methodologies focused on high-speed electronic circuits and systems:(a) Development of efficient and accurate multiphysics based frameworks for mixed-domain analysis of analog, digital, radio frequency (RF), electromagnetic (EM) and microelectromechanical (MEMS) modules.(b) Development of signal, power and EMI (electromagnetic interference) co-design and co-analysis methodologies and tools, focusing on advanced interconnect models, power distribution networks and tabulated data based subnetworks.(c) Development of novel circuit simulation and optimization methodologies with emphasis on exploiting the emerging multicore computational platforms.The proposed research initiatives are focused on strategic issues, to advance the state-of-the art in various aspects of high-speed and low-power applications in a wide range of electronic design levels. The impact of the proposed research would be to facilitate the development of new generation multidisciplinary framework for concurrent analysis and validation of high-speed modules encompassing all levels of VLSI design hierarchy, such as on-chip, multi-chip modules, interconnects, packages and printed circuit boards. The outcomes of the proposed research initiatives are expected to expand the horizons of existing design platforms while reducing the design cycle time of new products, leading to innovative multi-function electronic and communication products entering our markets, resulting in a healthier Canadian and global economy.""599928,""Achari, Gopal"
"615661"	"Afsahi, Ahmad"	"Efficient and Scalable Communication and System Software for Exascale Computing"	"High-Performance Computing (HPC) is used to tackle computationally-intensive problems in fields as diverse as green energy and biofuels, cancer research and drug discovery, weather forecasting and climate change, seismic processing for oil and gas, genomics and bioinformatics, astrophysics, material science, automotive, defense, data mining and analytics, and financial computing. It is the key to many scientific discoveries and engineering innovations. However, the demand for more computational power is never ending. Far more computational power on much larger-scale machines is required to unravel the scientific mysteries.The Message-Passing Interface (MPI) is the de facto standard for communication in HPC systems, and by far the dominant parallel programming model used by large-scale scientific and engineering applications. Processes in such applications compute on their local data while extensively communicating with each other through the interconnection networks. MPI has proved to be scalable and is smoothly transitioning in the current systems. However, on extreme-scale systems that are characterized by massive parallelism, highly hierarchical architectures and communication channels, smaller memory per core and heterogeneity, there will be immense pressure on the interconnection networks and communication system software to deliver the required performance and scalability. This research seeks to address the challenges for high-performance and scalable communication subsystems on extreme-scale systems. The proposed research is highly original and innovative in the sense that it addresses key questions in high-performance communications and system software in MPI and hybrid MPI+X programming models. Such research will pave the way for adoption by industry. The outcome of this research will be relevant to various sectors in Canada, including Environment Canada, Compute Canada, Canada Genome Sciences Centre, automotive and oil/gas industries, and ultimately the Canadian public at large. It is expected that the findings from this research will have significant impact on the target community, and that it will lead to new directions for future research. In such a key and fast-paced field, the results of this research will keep Canada at the forefront of science and technology. The proposed research is ideal for training HQP in that it has a strong foundation that translates immediately into practical applications and implementations. There is a high demand for graduates in HPC and networking, and the HQP trained will be well positioned to compete for jobs in academia and industry.""613474,""Afshordi, Niayesh"
"616419"	"Ahmad, MOmair"	"Design and Implementation of Digital Signal Processing Algorithms for Communication, Biomedical and other Applications"	"Design and Implementation of Digital Signal Processing Algorithms for Communication, Biomedical and other ApplicationsThe field of digital signal processing (DSP) has experienced explosive growth during the past couple of decades. The DSP techniques have become integral parts of the products and services that we need or encounter in our daily lives. The research efforts of the applicant in this area during the past five years have led to some very concrete results that have been shared with the international scientific community, both from academia and industry, and have given rise to new ideas and directions that need to be further investigated. The overall objective of the proposed research program is to develop efficient algorithms and architectures and to lay sound mathematical foundations for reliable processing of image, video, biomedical and genomic signals, and cost-effective implementations for communication, biomedical and other applications. Discrete transforms are essential to signal processing tasks. To this end, new parameterized transforms capable of optimizing the performance of given applications will be investigated. Presence of noise makes the automatic interpolation of image data very difficult. Another concern is the security of the image data. This research will investigate the suitability of the various signal transforms and the statistical models of the transformed image data in finding effective solutions to the problems of denoising and watermarking. Object detection and tracking in images and videos have important applications in surveillance, traffic management and in cell biology. A study to develop a suitable collaborative model for detection based object tracking will be undertaken. Automatic cell cycle analysis is important for discovery of new drugs and in life science research. This research will aim at developing in a signal-processing framework the detection, tracking and identification of the different phases of cells in time-lapse microscopic images to automate cell cycle analysis while meeting challenges of clustering, deformation and uneven motion of cells. Proteins are the most important molecules in living organisms as they are involved in every function of the cells such as signal transmission, metabolic regulation, transportation of molecules and defense mechanism. Insertion and deletion of amino acids are common events that lead to the evolution and variations in protein sequences. This research will focus on developing signal processing techniques for alignment of multiple protein sequences by employing the information on insertion/deletion regions in the protein sequences. Use of a reliable personal identifier is a necessary to fight identity fraud. This research will investigate the score-level and feature-level fusions of multiple biometric traits in order to improve the recognition rate of multimodal biometric systems.""616688,""Ahmad, Rafiq"
"616387"	"Aitchison, JStewart"	"Photonic Biosensors for Point of Care Diagnostics, Monitoring and Screening"	"The combination of photonics and microfluidics provides an opportunity to develop portable medical diagnostics, screening and monitoring systems. These high quality, point of care testing systems enable health care technologies to be taken directly to the patient. Such systems have particularly important applications in developing countries where there is often a lack of access to state of the art medical diagnostic facilities. However there is also potential for their use in developed countries, which are struggling with spiraling health costs and where low cost portable diagnostics can provide more rapid results which leads to more effective care. In addition point of care systems open up the possibility of personalized treatment whereby the effect of medications can be monitored in real time and effective dosages optimized on an individualized patient basis.To make point of care testing a reality, there is a need to develop improved, multi-modal systems which are capable of measuring simultaneously more than a single parameter. This discovery grant application outlines a program of work that will enable a) the development of multi-modal sensing methodologies and b) the development of on-chip lasers and spectrometers. It is the combination of these research themes which will enable the development of rugged and robust point of care systems.Trainees engaged on the project will have the opportunity to work on a number of multi-disciplinary projects aimed at developing tests such as optical enzyme-linked immunosorbent assays (ELISA) and surface plasmon molecular sensors. The research program will also focus on integrating sample preparation steps with the photonic sensors to produce a cartridge-based approach to integrating a particular test. In parallel this discovery grant will support work on the development of a vertical approach to monolithically integrating III-V semiconductor components on a single chip to produce the spectrometers and lasers required to enable the portable systems described. Trainees will have the opportunity to interact with experts in nanotechnology, biomedical engineering and medical diagnostics.The combination of integrated optics and micro-fluidics is set to enable portable medical diagnostics, screening and monitoring systems. In the longer term, advance laser sources and integrated spectrometers will deliver a completely non-invasive approach to blood testing which does not require a finger prick sized sample. This will make systems more user friendly and compatible with the burgeoning wearable health-related technology sector.""593286,""AitKadi, Daoud"
"615497"	"Anis, Hanan"	"Ultrafast fibre­-based devices for biomedical applications"	"The overarching research objective is to bring nonlinear imaging and sensing closer to the bedside by conceiving all-fiber, ultrafast, high-sensitivity platforms targeted at biomedical applications. This proposal seeks to further our understanding of light propagation in fiber media at high intensities, and translate that understanding into tangible outcomes in the form of novel all-fiber platforms. The proposal comprises of three inter-related themes: ultrafast sources, nonlinear microscopy/endoscopy and biosensors.Theme 1: Novel ultrafast fiber lasers: The objective of Theme 1 is to investigate methods to increase energy, reduce noise and enhance stability of an all-fiber femtosecond laser, such that it may serve as a suitable source within bioimaging (Theme 2) and biosensing (Theme 3) platforms. A successful outcome of this theme is the realization of a self-starting all-fiber femtosecond fiber laser that delivers high power, generates low noise, and is stable enough for multimodal microscopy and endoscopy. Such an outcome would be a significant step towards enabling bedside imaging and diagnostic applications.Theme 2: Fiber-based Fast Broadband Coherent Raman Scattering (CRS) bioimaging: The ultimate objective of Theme 2 is to enable fast in-vivo spectroscopic imaging of thick tissue in the fingerprint region by using an all-fiber configuration to maximize the signal of the stimulating sources and minimize system noise. This can open the door for spectral imaging of fast events such as flowing tumor cells and in-vivo monitoring of drug diffusion through tissue.Theme 3: Hollow Core Photonic Crystal fiber for Sensing: The objective of Theme 3 is to a) further investigate the use of our Hollow core photonic crystal fiber biosensing platform for fast and early detection of polycystic ovary syndrome (PCOS). A better understanding of the ovarian function and PCOS would facilitate the development of new treatment strategies for this complex syndrome. Our SERS-HCPCF platform can be extended to detect a number of other diseases including early bacterial infections, which is critical to fragile patients, e.g. the elderly, burn victims, and chemotherapy patients, when timely intervention is of the utmost essence and b) explore a novel biosensing platform based on nonlinear spectroscopy in HC-PCF. This would enable us to study two-photon induced photochemistry processes, which until now was limited due to difficulty in detecting photochemical events at a small excitation volume. This can open the door to study photochemical mechanism and products for a number of photoactive drugs, e.g. 5-aminolevulinic acid (ALA). .""616122,""Anisman, Hymie"
"615969"	"Annakkage, Udaya"	"Mitigation of Power System Oscillations"	"Oscillations occur in power systems due to disturbances such as sudden changes in generation, the sudden addition or removal of large loads, transmission lines going out of service, and faults in the system. These oscillations, when not eliminated within an acceptable time, have led to major blackouts in the past, and damaging expensive equipment in the power systems. The presence of oscillations also restricts the amount of energy exported or imported between power utilities, resulting in loss of revenue. Oscillations can be of different types. The most common is electromechanical oscillations, which involves large rotating machines oscillating against each other across the transmission network. These oscillations are of low frequency in the range of 0-5 Hz. The other type is known as sub synchronous oscillations of which the frequency can be up to the nominal system frequency (50 or 60 Hz). These oscillations have been less common in the past, but due to the integration of renewable sources through power electronic interfaces they are becoming more common. Currently there is no procedure or tools available to directly design oscillation mitigating controllers to cater for the modern power grid. I will use state space control design techniques and optimization techniques to propose well tested new procedures to enable the utilities, manufacturers, and consultants to develop reliable and cost effective solutions. The HQPs trained under this program will gain expertise in power system simulation techniques and tools. They will also apply the new techniques to practical situations through collaborations with the industry.""596132,""Annan, CharlesDarwin"
"615189"	"Antle, Alissa"	"Design and Evaluation of a Children's Brain-Computer Application for Learning Self-Regulation"	"Recently, best clinical practices in attention deficit and hyperactivity disorder (ADHD) and anxiety have involved mindfulness, yoga, and other body-based approaches in conjunction with cognitive approaches. Neurofeedback has been proposed as an effective aid to learning and practicing these techniques. Advances in neurobiology and computer science make possible these kinds of interventions designed to strengthen basic processes behind attention and anxiety regulation and control. In previous work, our team developed a preliminary series of electroencephalogram (EEG) neurofeedback training games on an interactive tablet for Nepalese children aged 6 to 10 years old. Although there is no clear and consistent empirical evidence demonstrating the efficacy of this type of new approach, this type of technology may be useful for work with children since it helps them visualize internal brain processes. In addition, computer games are familiar and motivate the type of repetitive practice necessary for learning self-regulation.This research program will focus on the design and implementation of a series of Android tablet neurofeedback applications that can be used with a consumer grade EEG headset, and a second application that can be used to calibrate children's brainwave data as they use the system rather than with extensive pre-use training. The applications will be evaluated with different groups of young children with ADHD and anxiety in order to determine if they can effectively help them learn to self-regulate attention and anxiety. The proposed research will uncover new theoretical and design knowledge that illustrates how researchers should think about and develop brain-computer technologies to support children's social-emotional development, generally, and ADHD and anxiety, specifically. This research will be valuable to companies and institutions within Canada and abroad, as it will provide them with guidelines to help them develop the next versions of such technologies and applications. It will provide institutions and individuals who work with children with a broader understanding of the impact and use of such technologies to support children's development. This research can benefit millions of children, since ADHD and anxiety negatively affect children's education and career outcomes, feelings of self-esteem and self-efficacy, social competence, and physical and mental well-being.""614813,""Antle, Michael"
"616271"	"Antoniol, Giuliano"	"A framework to assess Android mobile applications energy consumption and privacy policies."	"Why is my cell phone battery already low? How did it happen that almost all the data of my monthly phone plan is already used up? Why is the new app tracking my location? These are non-obvious questions. Choosing apps requires effort, patience to examine pertinent information, and technical knowledge. In fact, the best app combination depends on user preference, frequency of use of different features, single feature performance, user privacy concerns, and app (hidden) costs. If we consider the Android ecosystem, for any given app category (e.g., e-mail clients, weather forecast, business, communication, tools, games), the user has plenty of choices for different apps (often implementing the same, or very similar, features). However, quite often, while an app category lists dozens of apps, the information supplied is limited to the advertisement for a specific app, the Android market meta-data, and the app ranking. When setting up an Android phone environment, the user has to try different apps, verify implemented features, evaluate intrusiveness of advertisements, ponder pros and cons of different apps, hoping that, in doing so, no personal or confidential information is leaked. This research program is directed to the development of a framework that will support users and developers. We aim at supporting users in selecting the best set of apps, given the user needs, and supporting developers in designing and implementing efficient and energy aware apps. Specifically, we will support the analysis, assessment and verification of mobile apps' 1) energy consumption per specified scenario; 2) app implemented features (and feature specific energy consumption); and 3) app privacy implemented policies.In the short term, we will make available a WEB portal to users and selected developers. Users will be able to select some app categories, set their energy, network, and confidentiality concerns (high, medium, low or don't care), specify the list of grants they are willing to give to apps, and, automatically, obtain a list of optimal, or close to optimal, apps to be installed in their phones. We will generalize this with a scenario definition language. Also, we will offer selected developers the possibility to upload an app, select a scenario (define an app specific scenario if needed), and produce an app energy consumption profile.In the long term, we aim at extending the tool chain supporting 1) mixed language apps (native C and Java or pure C/C++ apps); 2) app and app execution scenarios co-evolution; and 3) different hardware and Android versions. These are all non-trivial and challenging goals. Different hardware implies different screen geometry, device drivers and Android API. App and Android evolution may substantially modify the way in which the user and the app interact: scenarios, privacy policies, and setup to gather information leading to the need of app and scenarios co-evolution.""605037,""Antonyshyn, Jeremy"
"615749"	"Bacchus, Fahiem"	"Advancing SAT solving algorithms with Applications to problems in Verification and AI"	"With increasing computing power comes the desire to apply computers to solve ever more complex tasks. Many of these tasks require dealing with problems that are known to be computationally intractable. This means that in the worst case no matter how much computing power we have available to us now or in the future we will not be able to solve such problems. Formally, such problems are characterized as being NP-Hard. Nevertheless, recent developments have in practical computing have not been deterred by this worst case analysis. In fact, there are many modern computing applications addressing important practical problems that routinely solve NP-Hard problems. The key this seeming contradiction is that often these practical problems possess extra structure that make them solvable. However, except for a few special cases, as yet no one has been able provide an adequate formal characterization of the features that make many practical problems solvable. As a result there are no algorithms for solving such problems with guaranteed reasonable computation times: in order to achieve such guarantees one would need to have a formal characterization of the features that ensure tractability. Instead what has proved to be successful is the development of algorithms for solving the general problem; despite the fact that the general problem is intractable. Research over the past 15 years has shown that it is often the case that techniques designed to make an algorithm more effective on the general problem end up being most effective on problems arising from practice; i.e., such techniques tend to be more effective on practical problems. An example of this is the use of clause learning in SAT solvers. This observation has motivated much of my previous research and will continue to motivate the research proposed here. In particular, my work has focused on finding techniques and algorithms for solving intractable problems by exploiting ideas designed to make the solver more effective on the general problem. That is, my research has found new algorithms and techniques for building better general purpose solvers and the effect has been that such solvers have been increasingly more effective on a range of important practical problems. The proposed research will be to find more effective techniques for solving general purpose problems, specifically MaxSat, #SAT and QBF. Many practical problems can be cast as a MaxSat, #SAT or QBF problem. Hence, once we have an effective solver for these problems we have a method for solving a range of practical problems. In addition to this fundamental research into improving solvers for these general problems the proposal is also to apply such solvers to a wider range of practical problems arising in the areas of AI and formal verification. Some work has already been accomplished on this aspect of the research and the proposal involves accomplishing more.""595311,""Bachand, Francois"
"615164"	"Bajic, Ivan"	"Multimedia Ergonomics in the World of Big Data"	"Big Data is redefining our world. Among the various types of data being generated at ever increasing rates, multimedia has been termed ""the biggest Big Data"" due to its sheer volume, and the ease and pace with which it is being created. From surveillance video, both terrestrial and satellite, through scientific and medical imaging, entertainment, gaming, advertising, and billions of users creating their own content for private and social consumption, multimedia keeps pushing the limits of our technology. For us to make sense of and effectively utilize the information present in this vast universe of multimedia data, new science and engineering will be required. The Big Data problem, however, is not new - it has existed in nature for a long time. Our own brains, as well as those of most other animals, had to evolve various strategies to keep up with the large amount of sensory data in order to survive. Since the brain cannot consciously process all the sensory data in real time, selection has to be made as to what is important and what to focus on. And these selections need to be made rather quickly in order to be effective. This is akin to the technological Big Data problem we have today, where the available processing power is miniscule compared to the amount of data generated, and the gap keeps increasing. Questions about what is relevant, important, and deserves our focus are also at the core of the technological Big Data problem. One should therefore expect that useful insights and strategies can be obtained by studying brain's own ways of dealing with this challenge. The proposed research program aims to address some of the pressing multimedia Big Data challenges by leveraging on multimedia ergonomics, inspired by the strategies humans have evolved to cope with the streams of their sensory data. Computational attention plays a crucial part in this regard, as it allows one to select a small amount of the most relevant data from a large pool of available data. In recent years, we have applied these principles to the traditional problems of video compression and communications, achieving state of the art results. In the proposed program, we will address the multimedia Big Data challenge on two fronts. On the theory side, we will seek a solid scientific underpinning and tractable mathematical models for the computational attention approaches from multiple multimedia sources. These will enable the design of multimedia systems on the Big Data scale. On the application side, we will work to apply the principles of multimedia ergonomics and utilize compressed data representation to develop sophisticated, yet low-cost algorithms for tracking, activity detection and discovery, annotation and retrieval - necessary ingredients for making sense of multimedia Big Data.""610349,""Bajic, Ivan"
"616363"	"Bakhshai, Alireza"	"Highly-efficient and smart Power Electronics Systems for long-lasting energy storage Systems in smart grid applications"	"Electricity and transportation sectors are accountable for producing the largest amount of greenhouse gas (GHG) emissions. Although Canada generates the cleanest worldwide electricity, emission from transportation is the largest contributor to Canada's GHG emissions, representing 24% of overall GHGs. Current worldwide incentives have spurred the development of Electric Vehicles (EVs). It is expected that the number of such vehicles will grow exponentially, to 30 million annually by 2040. Electric Vehicles are more efficient and have few direct emissions. However, they all rely on energy generated by the electricity sector. This implies that the GHG emission from transportation is moved to the electricity sector. To truly reduce this emission, more electricity should be produced from renewable energy sources such as wind and solar. A major problem in integrating more renewables into the power grid is the intermittent nature of solar and wind energies that reduces the performance of the power grid. Energy storage will provide flexibility to a relatively inflexible grid and will enable a larger scale deployment of renewable energies, without threatening the grid stability or power quality. Due to a push in producing EVs and a premeditated grow in the number of these vehicles, on-board traction batteries of EVs are the most promising means for energy storage. This research proposal aims to introduce and design innovative highly-efficient power electronics systems and the invention of next-generation devices for distributed energy storage and energy conversion based on EV technology for both utility scale and Vehicle to Grid (V2G) energy storage systems. These power electronics and control systems will mostly rely on local parameters and need no communication link. This will be a very important achievement since it has been a challenging issue in microgids and smart grid applications. The proposed research will be advantageous for multiple research streams in the rapidly growing areas of energy integration, renewable generation, and converter enabled microgrids It is expected that industry and scientists, specifically electricity and transportation sectors will benefit from the outcome of this project. People and policymakers will benefit economically due to an improvement in the efficient use of energy. Because of significant GHE reductions, all Canadians can benefit from the improved air quality, and as a result, a better quality of life.The proposed research will directly support the training of 5 graduate students. In addition, 2-3 senior undergraduate students (4th year) will be trained annually on various aspect of this project. The participating students will develop research skills in advanced design of power electronics circuits, control systems, power electronics simulation tools, and FPGA based digital control techniques.""598230,""Baki, AbulBasarMohammad"
"615057"	"Beauchemin, Steven"	"Predicting Driver Intent and Maneuvers for Road Safety"	"The focus of this research proposal rests on the identification of cognitive factors involved in driving that impact traffic safety, the definition of sound principles for the design of automated vehicular safety technologies, and the development of intelligent, Advanced Driving Assistance Systems (i-ADAS), with driver behaviour prediction and correction as the central tenet of safety improvement. More precisely, the objective of this research is to develop an operational understanding of driving maneuvers that may be used in the conception of prediction engines incorporated into future implementations of i-ADAS. Toward this end, we instrumented an experimental vehicle capable of recording its immediate frontal environment in 3D, the vehicle odometry, driver maneuvers and operations of vehicular functions, driver cephalo-ocular behaviour (head pose and 3D gaze), and the 3D Point of Gaze (PoG) in absolute coordinates within the perceived 3D frontal environment of the vehicle. We proceeded to obtain more than 3TB of data from 16 drivers, on a predefined path around the city of London, Ontario. Algorithms to automatically annotate this data set were devised. For instance, novel techniques for multi-lane detection, vehicle detection, ground-plane detection, and GPS-correcting techniques were employed in the labeling process. We are now devising techniques to semantically segment the 3D stereo data stream using constraints related to spatiotemporal coherence, in order to obtain object descriptors along with their 3D bounding volumes. In turn, the 3D gaze of drivers may be intersected with these bounding volumes, allowing for the identification of what the driver is looking at, in addition to where. Our interest is in predicting the most probable driver maneuver in a time window from 0.25 to about 2 seconds: this amount of time is sufficient for an i-ADAS to perform mitigating actions in cases when the predicted maneuver is inconsistent with the current traffic situation the vehicle and driver find themselves in, and avoids the prediction reliability problem posed by larger time windows. Experiments recently conducted with our data and a 3-layer neural network using logistic regression show that for a data sequence representing approximately one hour of driving the canonical maneuvers accelerate and decelerate can be predicted with an accuracy of 99.6 percent for the next 0.5 to 1 second. Other canonical maneuvers have been predicted with this level of accuracy. We are currently developing and testing concurrent approaches to the problem of driver maneuver prediction. We are very interested in determining what data streams in our driving sequences have the most impact on maneuver prediction (there is evidence that cephalo-ocular behaviour is important in this regard).""604379,""BeaucheminCôté, Francis"
"616430"	"Becker, Christoph"	"Sustainability Design in software and information systems engineering"	"Modern society's heavy reliance on software results in increasing vulnerability to the obsolescence and depreciation of critical software-dependent assets. This is exacerbated by the prominence of ultra-large-scale systems underpinning critical technical, social, scientific and economic infrastructure. As software is embedded in the social and technical fabric of our society, the rising importance of long-term concerns requires new perspectives and methods for identifying, measuring and assessing systemic effects of engineering decisions across the systems lifecycle. This requires a consideration of the interactions of the systems under design with the context in which they are embedded, since crucial effects emerge only in these interactions over time. The challenges of sustainability bring this to the forefront. The sustainability of a software system- its ""capacity to endure"" - is very different from the capacity of a socio-economic or natural system to endure. In systems design however, the connections between these types of systems require a simultaneous consideration of ecological, technical, social, and economic concerns on several levels, and an understanding of software systems integrated in their context. Sustainability has emerged as ""the non-functional requirement of the 21st century."" Yet, software engineers are ill-equipped to address it: There is a profound lack of robust techniques, models and metrics for discovering, assessing, and managing the effects of key decisions on sustainability. It is not clear how incorporating sustainability changes established processes, which new activities it requires, and how to measure their effectiveness. Hidden effects of engineering decisions that negatively affect future system value remain undiscovered.The proposed research program builds on my work on systematic evaluation of software systems for digital preservation that mitigate long-term risks to the sustainability of information assets, such as obsolescence. We will develop risk indicators and flexible tools that enable software engineers to identify, model and visualize sustainability requirements and integrate this into software architecture assessment throughout the systems lifecycle. We will enable evidence sharing and assess the effectiveness of digital preservation techniques in reducing the technical debt of exemplary systems through case studies. Our process models will enable teams to assess and improve their capabilities for sustainable systems development. Empirical studies will be used to evaluate our results and build a theoretical account of sustainability in software engineering.By developing innovative ways to sustainable systems development, we will reduce the risk exposure of software and information systems and enable Canadian companies to position themselves at the forefront of sustainability in software engineering.""601713,""Becker, Michael"
"614812"	"Behjat, Laleh"	"Development of Efficient and Interdisciplinary Techniques for Physical Design of Integrated Circuits and and Beyond"	"Integrated circuits (IC) are used in every aspect of our modern lives, and their performance and cost have significant impact on our society. ICs are made of billions of transistors ranging between 45 to 14 nm in size, making them smaller than most viruses. To be able to deal with the sheer numbers and extreme sizes of IC, every aspect of their production needs to be automatically designed and optimized using Electronic Design Automation (EDA) tools. The overall goal of the proposed research program is use optimization, data mining, parallel programming and estimation techniques to design better EDA tools that address the above challenges. In addition, the developed design automation tools will be modified to optimize the operation of other research areas such as communication and oil and gas extraction.The pace by which EDA researchers have to come up with new technologies has been unparalleled in history. For the past 50 years, the number of transistors in IC has been doubling every 18 months, as predicted by the Moore's law. This means that new techniques, algorithms and tools need to be developed to be able to deal with the added complexities. The first theme of the research program deals with development of EDA tools for computer-aided design (CAD) of IC. Major challenges faced by today's IC designers are related to the huge number of the transistors that make an IC, the extremely small sizes of them and the high energy demand required for high-speed computation. The goal of the first theme is to develop innovative and creative EDA methodologies that can solve the aforementioned problems and significantly improve the performance, cost and reliability of IC. In the proposed program, multidisciplinary techniques such as optimization, mathematical modeling, estimation, computational geometry, data mining, parallel programming, and artificial intelligence are used along with the traditional EDA methodologies to develop creative, interdisciplinary and transformative solutions. The proposed research activities build on my group's award-winning work in EDA to push the boundaries of what is possible in circuit placement and physical design. The second theme of the proposed research program is to apply the methods developed for EDA to other fields in engineering. For example, the algorithms used for optimizing the power in circuits can also be used for optimizing the power used in communication systems. This part of the proposed research program is aimed at diversifying the research and fostering research innovation and excellence through collaborations.The proposed program will have a direct impact on the multi-billion-dollar EDA industry, and aligns with Canada's high priority research direction of Information and communication Technologies (ICT). The second theme will also have a positive impact research in the area of Natural Resources and Energy.""610313,""Behm, David"
"614908"	"Berini, Pierre"	"Surface plasmon photonics: from fundamentals to applications"	"This proposal is at the frontiers of photonics and devoted to the investigation of devices operating with surface plasmons, particularly lasers and parity time symmetric structures, high-speed photodetectors and modulators, and biosensors for disease detection and point-of-care applications. Conducting the work described herein will result in significant and far-reaching advances, of interest to the plasmonics and broad photonics communities, and to telecom, sensor and healthcare companies. The surface plasmon lasers, photodetectors and modulators proposed hold the promise of high-performance at very low cost, attributes that are of significant interest for telecom applications, particularly next-generation optical networks and short-reach optical communications. Our work on surface plasmon biosensors will lead to point-of-care devices for the detection of cardiac troponin I (a protein marker for myocardial injury or infarction), for human thyroid stimulating hormone (for which many Canadians suffer from a deficiency), and for the Dengue virus which afflicts about 500M people globally each year including Canadian military personnel deployed in Dengue prevalent regions.Over the duration of this program, 5 Ph.D. students will complete their studies and 5 new students will begin. The students will be integrated into a productive, collaborative and multidisciplinary team. The applicant's group is attractive to students from many disciplines - engineering (EE, Phys, Chem, Biomed) and science (Phys, Biochem). The students will receive training on the modelling, design, fabrication in cleanrooms, and experimental characterisation of photonic devices and biosensors. The students will interact directly with our industrial, international and medical collaborators. They will obtain their graduate degrees having acquired theoretical and experimental skills which are immediately useful and valued by industry and academia.New knowledge will be disseminated in top journals, conferences, and in patents, new industrial partnerships will be formed, and potentially a new company created to ensure the transfer of technologies to the private sector. The research outcomes and the highly qualified personnel produced will be of high interest to Canadian high-technology industry, particularly to the electronics and photonics industries, which are of strategic importance to Canada's innovation agenda. The proposed research aligns very well with the Government of Canada's Strategy on RD, specifically with the ""Nanotechnology"" focus area of the ""Advanced Manufacturing"" research priority, the ""Communications networks and services"" focus area of the ""Information and Communications Technologies"" research priority, and the ""Biomedical engineering and medical technologies"" focus area of the ""Health and Life Sciences"" research priority.""611055,""Beristain, Alexander"
"600945"	"Berseth, Glen"	"Québec"	"CANADA"
"616127"	"Bertossi, Leopoldo"	"Causality in Data Management: Foundations and Applications"	"Trying to understand why and how the occurrence of an event or the execution of an action affect other events or properties of objects belongs to the essence of human existence. If the search for causal explanations is not the oldest activity performed by humans, it is certainly one of them, yet it still has been an elusive concept that has been studied in many fields of knowledge, ranging from philosophy to computer science.In these times of big data and data science, analyzing large volumes of data has become an increasingly common and complex problem. Beyond the traditional problem of extracting explicit data from a data source, it has become crucial to understand and make sense of the data, and extract implicit knowledge from such a source. In particular, finding explanations for data phenomena as shown, for example, in query answers, satisfaction or violation of semantic constraints, and view contents, has been identified as an important task, for which explanation models, algorithmic results, and practical computational implementations still need to be developed. Causality is used to address the problem of providing explanations for different forms of manifestations of data.Although causality has been investigated in several areas, e.g. statistics, artificial intelligence, economics, its emergence in data management, and databases in particular, is quite recent. Addressing causality in databases requires a mathematical characterization of the notion of cause, the investigation of the mathematical model, and the algorithmic and complexity analysis of computational problems. Among the latter, we find development of efficient algorithms for computing causes for query answers, whenever possible, or of efficient approximation algorithms when the intrinsic complexity of the problem is high. It also becomes necessary to rank causes, identifying those most relevant, for which the notion of responsibility has been introduced. Several computational problems emerge around responsibility computation. The PI has already obtained interesting results in this area. However, many aspects and problems of causality in databases, and more generally in data management, are still open.In this proposal we address causality in the context of semantically enriched data sources, with the aim to propose and investigate a model of causality in scenarios such as ontology-based data access, virtual data integration, and multidimensional databases, among others. We also go into more fundamental problems, such as the logical characterization of causes, which should tell us what theory embedding the data source can be used to infer causes and how. We investigate the problem of learning causal relations from (possibly probabilistic) data sources, by developing machine learning methods specifically tailored for databases, which normally lack the additional semantic information that is useful for learning.""594367,""Bertram, Allan"
"614794"	"Bhattacharya, Binay"	"Resource allocation problems under uncertainty"	"Efficiency in modern industrial operations requires that resources be deployed in an optimal manner. This has resulted in extensive work in operations research on resource allocation with a focus on optimal or near-optimal planning.Resource allocation decisions are very costly and difficult to reverse. Their impact is felt over a long period of time. The parameters, such as cost, demand, and distance, used during the design phase may change over the time period. Researchers have been developing models for resource allocation under uncertainty for decades. The goal of robust optimization is to look for a solution that will work well in the worst case under various possible realizations of the parameters. The random parameters can be either continuous or described by discrete scenarios.The main objective of our project is to study the problem locating facilities in weighted networks under uncertainty. Our proposal has two parts: (a) min-max regret facility location problems in a tree-like networks, (b) pickup and delivery problem with time window (PDPTW) in road networks.Facility location problems in tree-like networks have been studied extensively in the past. There are many open problems still to be solved. This proposal suggests a few directions for further studies. PDPTW in general networks is an NP-hard problem. Still this problem is very important in real world. We have built a prototype software for this class of problems. This proposal also identifies a few areas where the enhancing of the existing approaches are needed.For general networks, very little is known about how to approximate the optimal solution. Many interesting, important results on approximating location problems in general networks are known when the parameters are assumed to be certain. We will explore when the parameters have interval uncertainty.The research on exactly solving NP-hard optimization problems is intense. The advent of integer programming solver has hastened this progress. We are currently working on a model for general assignment problems. One of our objectives would be to provide structure to the solution approach.""611839,""Bhattacharya, Janok"
"615677"	"Blouin, Christian"	"Application of graph modularity inference to represent protein structures."	"Proteins are large molecules that are implicated in the majority of the work done in living cells. We have the ability to solve the shape of the protein encoded by a particular gene. However, their three-dimensional structures are large and complex. This research program aims to provide new ways to conceptualize the inner mechanics of these large molecules. We are first interested in understanding how these long chains arrange themselves into a smaller number of rigid components, or modules. To do this, we are using cutting edge analysis of molecular simulations, and also will develop new analyses. We ascribe that attempts to engineer proteins to achieve a particular function should consider this modular structure. We are also interested in inferring modules that are stable over evolutionary time. This is done by comparing the shape of many proteins sharing the same ancestor. Modules that are stable over evolutionary time are likely to be the result of networks of complementary mutations. Since protein engineering is commonly using mutations to alter the function of a protein, understanding better the relation of one position in the protein with respect to its neighbors increases the chance of anticipating the indirect consequences of mutations. We achieve this by applying and extending work in computer science on network analyses. Going forward, we are interested to explore how evolutionary and stability modules are related. This kind of knowledge is important to further our understanding of protein stability, evolution at the molecular level, and to assist in the rational engineering of protein to design proteins capable of novel functions. What makes proteins semi-rigid and capable of performing important functions is their ability to adopt and maintain a stable 3D structure. Let's conceptualise stable conformations as low energy structures. To transition from one stable conformation to another, the structure must cross a peak of high energy. We coin the term ""threshold event"" when a structure crosses a high energy peak. To study threshold events, we use the simulation of a small protein called Amyloid-beta. The transition between two stable structures for this protein is believed to be one of the factors in the pathology of Alzheimer's disease. We use computational and numerical techniques to discover the key components of the threshold events between the normal and pathological form of this protein. Understanding threshold events in general is important in protein science, and in the design of new protein structural elements. Particularly, studying the causative agent of Alzheimer's disease is important because the mechanism is common to many neurodegenerative diseases: discovering a mean to disrupt protein aggregation could be the basic science seed of new treatments for these terrible conditions.""616545,""Blouin, JeanSébastien"
"614989"	"Booth, Kellogg"	"Collaboration technology and multi-user interfaces"	"The proposed research is part of a multi-year program on shared display collaboration technology with two closely linked projects (a) interaction techniques for pointing and selection on large wall displays, and (b) pedagogically-motivated technology for student engagement in classrooms.Pointing and selection on large screens. Traditional GUIs use a WIMP (windows, icons, menus, and pointers) paradigm. Pointing and selection are fundamental building blocks. Despite multi-touch, haptics, and voice input, pointing remains a significant interaction technique, especially for large displays (3m wide or more) in collaborative settings (board rooms, classrooms, and meeting/project rooms). In ""mid-air"" pointing a user stands in front of a display and has no surface on which to rest the pointing hand. Mid-air pointing uses laser pointers, specialized wands, and gaming devices (Nintendo Wii or Microsoft Kinect) that rely on computer vision; hybrid solutions track body movement to interpret gestures for ""natural"" pointing. My prior studies showed that gain (the ratio of cursor or tracker movement on the screen to hand movement) or distance from the screen (related to but not the same as gain) affect the accuracy of the standard Fitts's Law model for assessing pointing performance; my most recent paper in ACM Trans. on Computer-Human Interaction (Shoemaker et al., 2012) argued that Welford's two-part formulation that decouples movement amplitude from target width when predicting movement time is better than a one-part Fitts's Law formulation for some gain values. Our subsequent studies varied distance-to-the screen for 3D stereo targets and found similar effects when binocular depth varies. The next steps will systematically examine relationships between gain and depth in both virtual and physical environments.In-classroom technology to increase student engagement. MOOCs are argued to be the death of the traditional classroom lecture; ""flipped classrooms"" re-target classroom time away from lectures toward activities usually done in tutorials or labs. I believe there are still opportunities to deploy technology to make traditional lectures ""scalable"" and achieve the advantages promised by MOOCs and flipped classrooms while preserving the benefits of classroom lectures. Classroom displays equivalent to the 6 to 10 full-size blackboards in a traditional lecture hall provide rich persistent information (rather than PowerPoint's ephemeral linear trickle). Imaginative interaction using simple student response systems, smart phones, or tablets can bring students ""into the loop."" The next steps in my research will design and evaluate class-based collaborative interaction and multi-team ""competitions"" to take advantage of the captivating nature of computer games (the so-called ""gamification"" of lectures) to provide students active learning opportunities.""607170,""Booth, Samuel"
"614950"	"BouchardCôté, Alexandre"	"Efficient probabilistic inference and Bayesian non-parametrics with applications in phylogenetics and cancer genomics"	"The fields of computational and statistical phylogenetics are concerned with the modelling and inference of evolutionary relationships. These fields have grown rapidly in recent years due to important advances in sequencing technologies. Many challenges, however, remain. One such example arises in the area of cancer phylogenetics, and relates to the characterization of evolutionary dynamics within cancer tumours. More specifically, the challenge arises from the need to characterize the evolution of individual cancer cells, where researchers are presented with mixtures of multiple sub-populations of cancer cells that have acquired different sets of mutations.At present, the core challenges in phylogenetics are computational and statistical in nature. This is not only true in the cancer phylogenetics example just referred to, but also in many other cases where phylogenetic models are based on complex datatypes such as sequence alignments or gene trees. A unifying feature of the computational and statistical challenges presently facing phylogenetics are that they require complex and nuanced approaches that incorporate the building of models and the task of performing inference over combinatorial structures.My research aims to address this important challenge in phylogenetics. More specifically, my research aims to create efficient methods for statistical inference over combinatorial structures, with a focus on models that arise in phylogenetic analysis. My research proposal emphasizes models from the field of cancer phylogenetics, but also considers applications to other phylogenetic contexts, such as joint tree and alignment inference. Many of the methods developed from this research proposal will also be applicable to data analysis situations encountered in several other branches of machine learning (such as natural language processing, computer vision, and computational biology).My research proposal is composed of three inter-related goals:1. To develop practical Bayesian Non-Parametrics (BNP): BNP provides an effective framework to approach latent variables over combinatorial spaces, however, significant limitations remain with BNP, in particular, computational scalability, and a steep learning curve for users. I will develop methods that address these issues.2. To develop scalable inference methods for intractable evolutionary models: Traditional phylogenetic models typically assume that given a hypothesized tree, the likelihood of the data can be computed in polynomial time. I will develop scalable phylogenetic methods that relax this assumption.3. Probabilistic inference over partially observed stochastic differential equations (SDEs): My goal is to develop new methodologies that make it easier for practitioners to develop models declaratively while keeping computational costs low.""616278,""Boucheneb, Hanifa"
"616263"	"Brassard, Gilles"	"Quantum Information Science, Cryptography and Privacy"	"Throughout my career, I have pursued a multifaceted exploration of how information theory and computer science can herald the Rise of the Quantum Age, thus unleashing the full power of our quantum world in ways never before thought possible. Indeed, we are witnessing the prospect of transmitting and processing information with unconditionally secure communication and computers powerful beyond imagination. My main objective is to further our understanding of how quantum physics can be harnessed for information processing purposes and, conversely, how a fresh computer science perspective can help us unravel the deep mysteries of Nature. I highlight below the cases of cryptography and quantum foundations, which are dear to my heart among several other topics of interest such as quantum computing.For ages, cryptography has been a battle between codemakers and codebreakers, set on strong mathematical foundations by Shannon in the mid-20th century. However, his theory was rooted in the classical physics of Newton. When quantum physics is taken into account, new vistas open up both for codemakers and codebreakers. Is this a blessing or a curse for the protection of privacy? This is the exciting tale of quantum cryptography, in which codemakers appear to have won, and then of quantum hacking, which exploits the imperfections in the implementation of theoretically perfect protocols, followed by device-independent quantum cryptography, which would restore the codemakers supremacy if only it could be implemented at all. In the mean time, post-quantum cryptography attempts to find solutions that would protect classical codemakers against quantum codebreakers. A major ongoing research thread has been to probe these questions from the perspective of both sides in the battle. As a passionate advocate of privacy, I am also involved in all manners, quantum or classical, by which this inalienable human right can be protected for all citizens.On a more fundamental level, I shall continue to investigate the foundations of quantum physics in the light of quantum information. In particular, most physicists believe that there is experimental evidence that our universe is nonlocal, but could this only attest to a lack of imagination? Indeed, my computer science perspective provides tools to debunk this belief, and I have made great strides with my students in this direction. Our wish is to rehabilitate Einstein's belief in a local realistic universe. These are exciting times indeed!In summary, my research program strives to push relentlessly the work initiated 35 years ago, when I was one of a handful of researchers worldwide-and the only Canadian-to dream about quantum information. My ambition is that Canada will continue to be at the forefront of the Quantum Revolution that has since spread to the international research community and whose benefits may soon be reaped throughout society.""617428,""Brassard, Gilles"
"615686"	"Brooks, Stephen"	"Graphics-intensive methods for data visualization"	"There continues to be an explosive growth of data emanating from all areas of human endeavor, the analysis and understanding of which has become paramount in industry, government and academia. But many of the more advanced approaches to computer graphics and visualization often remain apart and in the domain of the specialist since they often require an uncommon set of skills, both artistic and technical, to design effective and engaging visual communication. The visual designer must possess a natural aptitude for artistic aspects such as composition, colour, and visual flow, and also engage in esoteric technical tasks such as data transformations and often programming.The proposed research will develop visual data representations intended to offer visually engaging, physical metaphors that are understandable without onerous levels of training and that are suggestive of underlying uncertainties in datasets. The proposed methods will delve into technical, photo-realistic and stylistic elements, and these methods will be evaluated in the context of datasets with a significant degree of uncertainty deemed to be of public interest. Visualizing data in ways that are simultaneously engaging and informative to a broad set of users is challenging and is the subject of this proposed work, specifically with regards to the development of novel graphics-intensive approaches. This theme will be manifested in several closely related initiatives that will develop and evaluate new graphics-intensive methods with distinctive incarnations and combinations of surfaces, color, spatial arrangement and lighting styles for data visualization. These initiatives feed directly into a long term research trajectory which aims to fuse advanced and specialized graphics methods and visualization, while also considering perceptual and aesthetic elements.""604605,""Brophy, Michael"
"615710"	"Bui, Tien"	"Applications of Sparse Representation, Low Rank Approximation and Dictionary Learning to Image Processing, Pattern Recognition and Computer Vision"	"Human vision is good at detecting objects such as human faces under various conditions: illumination, occlusions, and poses. Training the computer to mimic human vision has occupied scientists and engineers for many years. One of the major problems is dealing with very large amount of image data. Since images such as human faces or video frames can be stored in large matrices or tensors, the research in large scale data analysis for matrices and tensors has become a hot topic. In particular, techniques such as sparse representation, low rank approximation, dictionary learning, robust principal component analysis, and multi-linear principal component analysis have brought much attention and are powerful tools in image/video processing, pattern recognition and computer vision. A common problem among these techniques is the need for efficient computational methods for optimizing the related objective functions. An efficient optimization technique will lead to a significant reduction in computational costs as well as an improvement on accuracy of the solutions. Furthermore, real data sets are often incomplete with many dimensions or elements missing. They may contain corrupted information due to measurement or communication errors. Hence the analysis of large data sets coming from real-world problems has been one of the most challenging tasks. As an example, in biometrics, face recognition under a combined effect of illumination, resolution, orientation, expression, and occlusion has been one of the major problems in computer vision. The objective of this research is to develop models and solution methods that can be applied to the problems mentioned above and common mathematical problems will be investigated. Specifically we are concerned with three main issues: (1) Development of common models and frameworks for the analysis of large scale image datasets; (2) Analysis of different factors that affect the systems such as the choice of input data for training, and the limitation of the proposed techniques; and (3) Implementation, experimentation and testing of the new models and techniques. The proposed work can have significant impact on many areas including image/video processing, computer vision, security, biometrics, telecommunications, remote sensing, and bioinformatics. One of the applications of our work is on face age progression that can be useful for the investigation of missing children.""616730,""Bui, Tuan"
"616361"	"Cameron, Kathleen"	"Graph Algorithms"	"A main direction of my research is to look for theorems which say that something that is easy to recognize always exists, and then to try to find what exists efficiently. The concepts ""easy to recognize"" and ""efficiently"" are formalized in computing theory as ""in NP"" and ""polynomial time"". Informally this research direction can be stated as: if it's easy to recognize and you know it's there, surely it's not hard to find. (Anyone who has misplaced their keys at home may not agree!) This has led me to find efficient algorithms for unrelated problems I might not have considered before.Many theorems in mathematics state that the number of something is even (or odd). Usually they are proved by counting arguments. We use a different approach: we construct an ""exchange graph"", where the ""odd-degree"" vertices correspond to the objects we want to show there is an even number of. Besides often providing a simpler proof, an exchange graph provides an algorithm for the problem: Given one object, find another. Since the number of objects is even, we know a second one exists. We are studying the efficiency of such algorithms, in particular for theorems concerning trees and cycles in graphs.Much of my research focuses on finding efficient algorithms for optimization problems on graphs such as minimum colouring and largest stable set. These problems have many applications including scheduling and in molecular biology. They are NP-hard for arbitrary graphs, which means that it is unlikely that efficient algorithms exist to solve them in general. However, graphs arising in applications often have special structure, which can sometimes be exploited to design efficient algorithms. I study specially structured graphs, where certain subgraphs are excluded or graphs with a nice intersection model. I try to discover properties of graphs in certain classes, to understand which properties are useful for solving which problems, and then use these properties to design efficient algorithms. Optimum matching is a well-solved problem with many applications including kidney exchange. Matchings in a graph correspond precisely to stable sets in its ""line-graph"". Line-graphs are a subclass of ""claw-free graphs"" which are a subclass of ""pan-free graphs"", and thus the stable set problem in either of these two specially-structured classes generalizes the matching problem. (A pan is a chordless cycle with at least four vertices together with a pendant edge.) Minty's (1980) efficient algorithm for maximum weight stable set in claw-free graphs led to much research on claw-free graphs, including the generalization of it by Brandstadt, Lozin and Mosca (2010) to pan-free graphs. Quite anomalously, these did not provide a defining system for the corresponding ""stable set polytope"". Much ongoing work focuses on finding a defining system for claw-free graphs. I am studying the structure and the stable set polytope of subclasses of pan-free graphs.""614104,""Cameron, Robert"
"616421"	"Cardinal, Patrick"	"Emotion detection from voice recordings"	"The speech signal carries several levels of information. From a voice recording, we can extract pronounced words, the speaker's identity, the spoken language or even the speaker's emotional state.In the last five years, there has been a great increase of interest in the field of emotion recognition based on different human modalities, such as speech, heart rate, etc. Building a robust emotion detection system can be very useful in several areas such as medicine and telecommunications. In the medical field, detecting emotions in general can be considered an important tool for diagnosing and following patients suffering from depression. The identification of the emotional state of a person from his voice opens new perspectives for the development of an automated dialogue system, one capable of communicating with patients at home daily and even several times a day to produce a report for the physician.Although much research has been done, actual emotion recognition systems performances are still not adequate for these real life applications. The majority of emotion detection systems have been designed by focussing on the speech modeling rather than the feature extraction aspect. Usually, feature vectors are made of a combination of classical cepstral features (such as MFCC) augmented with some prosodic characteristics (such as speech intonation). The modest results presented in the literature make us think that we should focus on improving the quality of features used for training emotion identification systems instead of focussing on the modeling aspect. For this reason, our research will focus on extracting more accurate features from the speech signal.Three main axes will be established in order to reach this objective. The first axe consists of working on the development of a DNN architecture capable of learning the classification function directly from the raw speech signal. The second axe consists of developing another DNN architecture capable of learning a normalization function in order to be able to mix different databases made of speech signals from different recording conditions. Finally, in the third axe, we will concentrate on high-level features such as phoneme durations and other evidence extracted from words said by the speaker.Improvements made during this research will be very practical for building reliable healthcare applications. Indeed, such applications would be useful for health professionals by helping them improve the quality of treatments in Canada. Moreover, our findings could be applied to several other fields of speech technologies. The approach presented would facilitate the development of command and control applications without the use of a complex and time/power consuming speech recognition engine. This would help start-ups or small businesses in Canada to add voice-based control to their applications without needing to hire a speech recognition specialist.""603152,""Cardinal, Samuel"
"615045"	"Chan, WaiYip"	"Perceptual Methods for Speech Communication"	"Speech communication is arguably the most important social activity. Speech signals are packed with verbal and non-verbal information. Humans can learn a great deal from listening to just a snippet of spoken speech: the spoken words and language; the speaker's identity, gender, accent, age, language proficiency, emotional state, health; the background environment based on the ambient sounds and noises captured; and any communications/recording media which carried the signal, based on perceivable ""signatures"" imprinted by the media on the signal. Moreover, human listeners are remarkably capable of comprehending the embedded linguistic and paralinguistic information even when the speech signal is highly corrupted. Computational intelligence, as embodied in the signal processing algorithms that run on electronic devices, is not (yet) a match for human performance. Nevertheless, the emerging Internet of Everything will further multiply the opportunities for speech communication between humans and electronic devices and between humans mediated by networked devices. Such speech communication can happen anytime and anywhere. As a result, the instances of devices picking up degraded speech signals will mushroom. To address this challenge, we propose to research speech signal processing methods that would enable machine extraction of speech information to be resilient to acoustic degradations, i.e., ambient noise and reverberation. Ideas for innovation will draw upon emerging advances in speech cognitive science and robust signal modeling and recovery techniques. New speech signal processing methods inspired by human cortical processing of acoustic speech signals will be developed. Moreover, a speech signal model ubiquitously deployed in today's cellular telephones will be revamped. A major goal is to advance the current level of machine comprehension of verbal information embedded in degraded speech signals, and by doing so also provide human listeners with highly intelligible speech. The new knowledge, methods, and algorithms anticipated from the proposed research will engender new capabilities for enhancing existing and enabling future speech communication and acoustic interface technologies. Besides training highly qualified RD personnel, the research will produce new and better tools to benefit both industry and academia. One type of tool to be produced will enable RD engineers to optimize their speech enhancement algorithms. Industry sectors that will benefit from the proposed research include communications and information technology, gaming, machine-mediated learning, hearing instruments, health and fitness, and robotics. The new knowledge and solutions are anticipated to also contribute to farther fields such as soundscape design for indoor and outdoor environments.""614124,""Chan, Warren"
"615771"	"ChanCarusone, Anthony"	"Analog Integrated Circuits for Massively Parallel Wireless Transceivers"	"Increasing demand for wireless communication has made spectrum a scarce commodity. Mobile data traffic in Canada is slated to increase 7-fold over 2014 levels by 2019. In response, Industry Canada is trying to make new spectrum available around 600MHz and 3500MHz for commercial providers, but this will require commissioning and deploying new equipment at great expense. At the same time, our use of unlicensed spectrum (around the 2.4 GHz and 5 - 6 GHz, for example) is growing even faster than that of the licensed spectrum, and therefore seeing tremendous congestion.Technologies to alleviate these spectral constraints are now known to communication theorists, but they must be adapted to permit their implementation in inexpensive integrated circuits (ICs). Specifically, ""Massive MIMO"" is a wireless technology that relies upon hundreds or even thousands of antennas at each fixed terminal (i.e. central router or base-station). All antennas transmit and receive signals in concert to hundreds of users simultaneously using the same frequencies, thereby permitting tremendous reuse of the spectrum. Orders-of-magnitude increases in communication bandwidth are theoretically achievable without increasing spectrum or radiated-power limits. The tremendous potential of Massive MIMO has made it a key part of wireless technology roadmaps worldwide, but innovation on mixed analog-digital integrated circuit design is needed to make it practical. Massive MIMO is seen as a necessary tool for meeting wireless bandwidth demands in the coming decades, and is implicit in Canada's wireless regulatory policies. However, these roadmaps and assumptions are already being questioned because the analog/mixed-signal IC signal processing required for a straightforward implementation of Massive MIMO is impractical. This proposal seeks analog integrated signal processing technologies to complement existing communication technologies and enable practical and inexpensive Massive MIMO communication.Graduate students engaged in the program will undertake system-level modeling and simulation of communication links, as well as integrated circuit design and characterization in UofT labs. Their experience and training is in high demand from Canadian companies working to establish or maintain their foothold in these markets. Hence, the proposed research program will benefit Canada via the technological leadership of its HQP, and by accelerating the wide availability of next-generation inexpensive wireless broadband.""600237,""Chandail, Mukul"
"615157"	"Chandra, Ambrish"	"Load demand management with high renewable energy penetration in distribution systems"	"Distributed Renewable Resources (DRRs) differ from conventional and fossil-fired resources in a fundamental way. Their sources (wind velocity and solar radiation in particular) cannot be controlled or stored. Further, renewable power generation often does not correlate with consumer load demand in real time due to its high intermittency. It may lead to one of the main causes of destabilizing the power system. In light of increased penetrations of these DRRs (50-60%) with the future grid, it will be difficult even for conventional power plants to participate effectively with great impact in maintaining stability of the network. In such reformed grid structure with high DRRs penetration, it will become a real challenging task to instantaneously predict and control total power generation in the entire grid. The operation of power grid will be more deliberately governed on the basis of DRRs generation rather than consumer loads demand. The main problems which may arise are increased voltage distortions in terms of flicker, sag, swell and its harmonic contents, frequency control etc. To solve such problems, solutions are presented in literature, such as, increased rating of a stand-by DG unit and employing storage system for maintaining deficit real time load demands.The demand side management is alternative solution for mitigating intermittency of the DRRs in the power system. The load power consumption is controlled such that load demand follow power generation. Load demand management methods are classified as load scheduling, use of energy storage systems (ESS), direct on-off control of non-critical loads etc. However most of these methods are suitable for load management in time frame of hours. They do not work in real time, except with the use of ESS. Recently, electric spring (ES) concept has been proposed as a smart load for demand management in real time. However, it has not been analyzed very much in terms of its design, efficient control and its worthiness under high penetration of wind and solar PV generation with distribution systems. Moreover, in near future, energy demand is expected to increase rapidly in the sector of transportation to supply plug-in electrical vehicles (PEVs). These charging stations of PEVs may be used as flexible loads to stabilize the grid. In view of these challenges, there is an immediate requirement for detailed investigation for technologies with distribution system to mitigate intermittency of DRRs. The main objectives of the proposed research project are as follows, Design, modeling, control, implementation and performance evaluation of selected topologies of smart loads to achieve loss minimization, technical upgradation, size reduction under increased penetrations of DRRs. Design, modeling and control for scheduling the dispatch of parking stations for PEVs under increased penetrations of DRRs to improve grid stability and customer satisfaction. ""612550,""Chandra, Sanjeev"
"616423"	"Chen, Liang"	"Study on Electoral College based Deep Learning and Its Applications"	"On the theoretical front, we will study the stability of the Electoral College-based Deep Learning. Its shallowest form, Electoral College, has been used for many years in US presidential elections, and applied in areas such as image processing, pattern recognition, information retrieval, and other political elections. Our model will be the first in Artificial Intelligence research that illustrates the stability of a class of Deep Learning with respect to region/prefecture size, number of layers, and overlapping rates of neighboring regions in each layer. Our theory will guide applications on how to choose an adequate number of layers, an adequate region size as well as adequate overlapping rates of neighboring regions for each layer, for using Electoral College-based Deep Learning to improve the performance of pattern recognition and information retrieval approaches.Developments made on the theoretical front will be applied in two important fields: (1) A face image set-based surveillance system will be developed. This system will combine proven active infrared illumination technology for pupil location with a mathematical approach developed in this research program to match unknown faces to known faces on watch lists. The system is expected to be a significant improvement, when compared to any appearance-based face recognition system, in recognition rate vs. false acceptance rate, and in being disguise-proof. (2) An intelligent textual information retrieval system will be developed. This system will employ a novel approach, where a document is treated as a physical object which can be described from different ""view angles"", for document representation so that each document can be divided into overlapped blocks. A vector space model will be employed as the basic strategy for matching a query with ""block"" in a document, and the Electoral College-based Deep Learning will be employed in making retrieval decisions. The system is expected to have much better performance in precision vs. recall compared to other approaches. These systems can be further developed for use in national security systems and internet intelligent document retrieval systems respectively.""613857,""Chen, Linan"
"615874"	"Chung, ChiYung"	"Power System Stability Analysis and Control Using Statistical Machine Learning Techniques"	"Simulation methods based on conventional offline models have been used widely in power system stability analysis and control design and they constitute effective tools for ensuring system stability. Effectiveness of these stability analysis and design methods is declining steadily because of the constant evolution of the power grid environment, the changes being largely attributable to increased variations in power flow and the difficulties in acquiring accurate offline models for various power-electronics-based devices. Advancement in information and communications technologies have facilitated transfer of massive data in real time and implies an opportunity for wider applications of advanced real time monitoring systems, allowing the acquisition of data of real time conditions and dynamics of various components of power systems. This makes the whole system more observable. Meanwhile, data-driven methods such as statistical machine learning techniques have developed significantly in recent times and have been successfully applied in various areas. Therefore, real time stability analysis and control using statistical machine learning techniques has become an important research direction since it aims to perceive the system's operational situation directly through real time data and provide insights into optimal operations and controls. This has the potential to resolve the problems of biased parameters when using offline models which, in most cases, do not fit real time operating conditions in the power grid. The significance of this research motivates this research program to combine statistical machine learning with domain knowledge in power systems and make them applicable to stability analysis and control in real power systems. The long-term goal of this research program is to develop new approaches for power system stability analysis and effective online model-free and self-optimization control strategies. To achieve this ultimate goal, the short-term goals are (i) to develop new approaches for prediction, control and optimization of power systems to resolve the problem of bias in offline models used in the conventional power system simulation; and (ii) to apply the general approaches proposed in this program to various problems related to power system stability and develop new online control strategies for the same. The outcomes of this research are expected to not only constitute milestones in power system stability analysis and control, but also contribute to the development of a more reliable and stable power system in the future.""601775,""Chung, Hugo"
"615228"	"Cobzas, Dana"	"Sparse methods on manifolds of images and shapes for significant anatomy detection linked to disease"	"Medical imaging plays a key role in modern medicine, being part of many diagnoses and treatments. While imaging hardware and acquisition methods have seen a tremendous development in the last decade, this is not matched by advances in image processing software. Therefore, in clinical practice, much image processing work is either not performed at all, or performed manually due to inadequate software. To realize the full benefit of these new medical imaging methods, new and better image analysis methods need to be developed.My current and planned research focuses on developing modern medical image analysis methods for studying anatomical and physiological variability in humans due to normal aging or disease. Such studies involve computational methods that extract and interpret image information to identify and quantify change. Our methods make use of anatomical similarity between humans parts and organs (such as ventricles or hippocampus among brain structures) to define a shape model. This shape model is defined in a continuous way and captures both global and local variability between individuals. We develop methods to statistically study disease-induced changes in anatomical shapes. This will provide a way to better understand the influence of certain diseases. This proposal introduces both theoretical advances of main medical image analysis methods (segmentation, shape analysis, multidimensional statistics) as well as practical modern solutions for a brain imaging study through our collaborative medical and biomedical collaborators. In addition, it opens the opportunity of training graduate students in an interesting multidisciplinary environment.""602315,""Coccimiglio, John"
"614497"	"Cohen, Robin"	"Artificial Intelligence Trust Modeling in Multiagent Systems to Streamline Social Networking"	"In this research, we aim to address the current topical but critical problem of information overload arising from the use of computers, especially in social networking environments. Our approach is to leverage artificial intelligence techniques: to develop novel paradigms for trust modeling, user modeling and multiagent cooperative coordination which will provide longstanding advances to theoretical research while at the same time forming the basis for significant breakthroughs in the management of our online existence. A companion aim is to embrace the current trend in artificial intelligence to leverage applied research towards the most valuable new insights in the development of theories and models. Towards this end, we will introduce several key application areas (healthcare, education and transportation) as motivators and as testbed areas for validation of our approach.Central to the research will be novel methods for trust modeling in multiagent systems, including user-specific trust evaluation (specific to the trustor), algorithms for trustees to engender trust, reasoning about when to solicit trust values from peers (the value of coordinated waiting), and projecting trust modeling into a specification of agent decision making.In all, we will not only advance artificial intelligence research but will also offer to Canada techniques of critical use to all organizations and individuals to enable more effective online communication and coordination between users. This in turn will translate into significant savings in time and in stress, towards an improvement of our overall state of wellbeing, to improve our overall economy. Relevant everyday contexts include massively open online courses, electronic marketplaces, online environments for social communication and peer-based networking to enable self-help. We are also interested in enabling enhanced access to online social networks for users with assistive needs (including the elderly).""617531,""CohenAdad, Julien"
"614873"	"Colpitts, Bruce"	"Harmonic Radar Systems for Insect Tracking"	"Insect tracking through the use of harmonic radar is a challenging engineering problem yielding previously unavailable behavioral information from insects. The urgency in understanding insects is driven by the need for greater food production while at the same time coping with invasive species enabled by climate change. Colony collapse disorder in honeybees is one very familiar crisis driving this work. Canada will benefit from tracking of the corn rootworm, Colorado potato beetle, and honeybee by protecting our agriculture industry while at the same time producing highly skilled radio engineers. Radio tracking of mammals and birds is commonplace providing a wealth of information on their movements. Insects are generally not capable of carrying conventional radio transmitters and thus an alternative tracking means is required. This is where harmonic radar plays an important role by moving the complexity and mass of the radio system to the human operator end leaving the simplest and lowest mass radio tag at the other, insect, end.  The first of three main objectives is to achieve insect tags of lower than the current 3 mg mass in order that new and smaller insect species can be tracked. The second is to develop a much lower mass base unit that can be flown on an unmanned aerial vehicle (UAV). The third objective is to continue engaging students in the design and development of high performance radio and related technologies for use in important ecological applications. We now fabricate 3 mg tags which are mainly composed of the copper coated steel wire used for the antenna and inductor. Tracking a 2 mg mosquito for example would require a 60 fold reduction in tag mass. There is no practical lower limit to the tag size as one can always find smaller insects needing tracking. The second objective results from numerous requests I have received for development of a system to track honeybees in particular. Collaboration with an unmanned aerial vehicle (UAV) expert will lead to a harmonic radar system that can either scan a region from the air to locate an insect or follow an insect once identified.  Finally the student's hardware and/or software project is implemented for physical testing in an operating radar system in order that they gain the very practical skills of development along with other skills such as coordinating work with others, sharing facilities and helping each other. Harmonic radar is attractive to students from the humanity perspective in that it allows them to learn and use highly technical electrical engineering skills while applying them to worldwide ecological problems. We have been building harmonic radar systems since 1999; the research has involved 17 engineering students over that time, and the systems are deployed in five countries.""608711,""Coltman, David"
"615179"	"Conati, Cristina"	"Adaptation and Personalization for Information Visualization"	"Researchin Information Visualization (Infovis) has traditionally followed aone-size-fits-all approach that does not account for user differences. Despiteincreasing evidence that user-adaptiveinteraction, i.e. interaction adapted to suit each user's specific needs andabilities, has the potential to improve users' experience while interacting with visualizations, the effects of both user differences and different forms ofadaptation in information visualization remain largely unexplored. In order tofill this gap, in the last 4 years the applicant has engaged in novel research to lay the foundations for adaptive visualization that can help tailorvisual displays to specific user needs and abilities. Through several userstudies, we showed that cognitive abilities such as perceptual speed,and visual and verbal working memory can impact user experience withdifferent visualizations, and that theireffect is mediated by task complexity. These studies also identified possiblereasons for the change in performance for users with different measures ofthese cognitive abilities, based on the analysis of user's gaze patterns collected via an eye-trackers. For instance, users with low perceptual speed werefound to spend more effort in processing the legend in bar graphvisualizations, which tends to slow down their performance. This resultindicates that these users might benefit from support in processing legends. We also showed that gaze data can be leveraged to predict, in real-time, user and task characteristics relevant for adaptation. These resultsprovided important evidence that user-adaptive visualizations are worthinvestigating, but they only scratch the surface of the know-how needed todeliver effective personalization for visualization processing. Thus, this proposal aims toconduct further research on user-adaptive visualizations that will allowclosing the so called adaptive loop.In particular, we will investigate how to design and deliver personalization that is effective and unobtrusive. Wewill also research how to detect specific user transient states that indicatewhen the user is most in need of help during visualization processing, e.g.when the user is in a state of confusion. Finally, we will investigate practical applications of the proposed approaches, for instanceproviding adaptive support for reading complex multimodal documents (e.g.,articles from the Economist) where text describesdifferent aspects of the embedded graphs.""613790,""Conciatori, David"
"614933"	"Condon, Anne"	"Advancing theory and tools for molecular programming"	"Molecular programming is the process of designing information-carrying molecules that react, or execute instructions, in purposeful ways. Nucleic acids - DNA and RNA molecules - are interesting to program because of their digital sequences and dynamic structural properties, and because they extend the reach of computational devices by naturally interacting with ""wet"" biological systems. Researchers in the field of molecular programming are tackling exciting engineering challenges such as building logic circuits, nanoscale structures and even molecular walking robots, from DNA and RNA sequences. In the coming decades, these technologies can offer significant payoffs, in transforming the ways in which we monitor and mediate molecular dynamics within the cell, or in developing ""smart drugs"" that could assess concentrations of disease indicators and respond accordingly.My research aims to uncover new and better ways to program molecules, as well as limits to molecular programming and energy-efficient computation, through the study of theoretical and empirical models.A primary goal of my research will be to address a significant barrier currently to design and analysis of molecular programs, namely the lack of good biophysical models. Particularly lacking are kinetics models that can accurately predict the rates at which nucleic strands interact through base pair formation and breakage. There is an opportunity now to make progress, by learning model parameters using valuable data from wet lab experiments of molecular programmers. This work will in part involve the creation of a database of experimental nucleic kinetics data, the first of its kind. In addition, I aim to further improve current thermodynamics models by accounting for important energy features (such as coaxial stacking and asymmetric multi-loop penalties) that are not easily incorporated into traditional dynamic programming secondary structure prediction algorithms. These contributions will support molecular programmers in scaling up current designs from hundreds to thousands of interacting strands, as well as biologists who wish to understand RNA conformational dynamics.I also want to understand the fundamental capabilities and limitations of molecular programming by studying models at higher levels of abstraction, namely chemical reaction networks (CRNs) and variants. CRNs provide a natural means for specifying and reasoning about highly distributed, asynchronous molecular programs, and a beautiful theory of what can and cannot be programmed with molecules is emerging from study of these models. I plan to contribute to this theory by developing provably fast algorithms for key problems such as leader election, and by developing models analogous to CRNs that can be useful when designing and reasoning about programs that are contained in single strands.""601114,""Condos, Tara"
"601368"	"Crawford, Eric"		"NSERC"
"615772"	"Cunningham, Charles"	"Devices to Enable New MRI-Guided Procedures"	"With excellent soft tissue contrast, multiplanar capability and lack of ionizing radiation, MRI is already used extensively for planning and performing minimally invasive medical procedures. However, there remains much untapped potential to further integrate MRI into the procedure workflow, with new types of image contrast to provide information to better target treatment, development of new devices so that MR imaging can be used to accurately place devices within complex anatomical structures, and novel treatment paradigms in which treatment is performed during realtime MR imaging to guide delivery. In this research program, novel MRIrelated device technologies are developed and tested so that medical treatments can be better delivered using MRI guidance.Minimally invasive procedures performed under MRI guidance, such as placement of biopsy needles and brachytherapy applicators, is a promising and evolving field with great clinical potential. One particular challenge of this field, however, has been how to develop safe and reliable methods for making position measurements (i.e. tracking) such devices as they are moved and manipulated to their final location. In the proposed project, this technology will be systematically developed and refined towards commercial applications in realworld medical devices.Hyperpolarized 13C metabolic MRI is emerging as a promising tool for probing metabolic changes associated with various disease states, notably for its application to cancer. We will develop data acquisition and reconstructions methods tailored to imaging hyperpolarized 13Cpyruvate and 13Clactate. Three-dimensional imaging of tumour lactate in cervical cancer patients may be useful in guiding chemo-radiation and brachytherapy. We will develop Radio Frequency coil systems to integrate real-time pelvic molecular imaging into cervical cancer therapy.This program involves the development of new technologies that together will enable new systems for treatment planning and delivery. Along with the potential for these devices to improve care and survival for Canadians with cancer of the cervix, these technologies all have broad applicability for other MRIguided procedures, and thus have significant commercial potential beyond the applications described in the research program.""613945,""Cunningham, Clifton"
"616321"	"Czyzowicz, Jurek"	"Algorithms for Mobile Agents"	"Mobile agents are devices that typically possess the following capabilities: mobility within the environment; a sensing instrument that can perceive various features of the environment ; a computing device with storage capacity; a communication mechanism that can exchange information with other agents.Typically, mobile agents are tasked with traversing the environment in order to learn it, map it and/or search for a specific target. In distributed computing, one of the fundamental problems concerns rendezvous or a gathering of agents dispersed across the environment (or some other pattern formation). Applications using mobile agents are numerous. Physical robots or humans (e.g., soldiers or rescue personnel) often perform searches in hazardous environments. Software agents migrate over a network to perform remote data collection (e.g. indexing web pages or looking for network faults). Further applications come from other domains such as wireless and ad-hoc networks, operations research, game theory, scheduling, nanotechnology, distributed spatial control, social networks etc.Most research on searching and exploration concerns a single agent. However, in various domains, the task must be performed by a set of collaborating agents. It may then prove challenging to partition the task(s) appropriately among the agents in order to ensure an adequate synchronization of their actions. For tasks requiring a large number of agents, the agents need to be produced at low cost. On the other hand, since such agents are often assigned to specific tasks, not all of their capabilities may be required. This gives rise to an important feasibility question: What is the minimum set of agent capabilities that is required to successfully complete a given task?The main objectives of my project include the study and resolution of the feasibility question for fundamental tasks assigned to mobile agents. If feasibility is determined to be no longer an issue, I will then investigate these related optimization problems:  What is the minimum number of mobile agents needed to accomplish the assigned task?  What is the minimum time needed by a team of agents?  What is the minimum energy (battery power) needed by each agent to successfully accomplish an assigned task?  What is the minimum amount of memory needed by each agent?  Can tasks be accomplished when communication between the agents becomes restricted? What is the maximum number of rogue agents of the given collection that may become disabled or destroyed, so that the remaining agents are still capable of successfully accomplishing a given task?In some cases, the agents may have no control over their movement. It is entirely determined by their interaction with the environment and can be substantiated by the laws of physics or probability. Some of my feasibility and optimization questions will be addressed for such collections of passively mobile agents.""617580,""Dacks, Joel"
"615536"	"DAmours, Claude"	"Wireless Communication Technologies to Provide Large Increases in Spectral Efficiency"	"New methods and techniques are studied to greatly increase the spectral efficiency of current wireless communication systems in order to achieve the increases in peak data rates and overall data volume that is needed for future wireless systems. Using current long term evolution (LTE) communication systems as a starting point, we will study modifications to the multiple access techniques, the effect of increased number of receive antennas to be used in massive MIMO systems, new error control coding techniques that are powerful and fast but maintain reasonable levels of computational complexity and multiuser detection techniques in order to increase peak and average data rates significantly while keeping outage probabilities low. We will determine the performance of MIMO-OFDM and MIMO-SC-FDMA in the context of massive MIMO systems and propose novel coding, antenna selection and multiuser detection techniques in order to provide very high spectral efficiency in bps/Hz. We will examine new ways to use the zeros in the frequency domain of SC-FDMA so as to carry additional information as opposed to or in addition to being used for multiple access in the goal to improve both the power and spectral efficiencies of mobile uplinks. We will also study more power efficient channel impulse response estimation techniques so as to reduce the amount of overhead needed, further increasing the spectral efficiency of future wireless systems.""603582,""DAmours, Mathieu"
"614556"	"Darnon, Maxime"	"DAMAGELESS ETCHING PROCESSES"	"Plasma etching used in micro and nano fabrication is required for precise patterning of various materials. However, it also induces damage at the surface of the etched materials. The exact impact of these modifications on devices properties is not fully understood yet.In this program, we intend to improve the understanding of plasma-induced damage and to evaluate solutions to reduce the damage. We will use silicon, III-V materials and superconducting materials as test vehicles. These materials are used for integrated circuits, high efficiency solar cells and quantum computing devices fabrication. We will base our research on an extensive characterization of the surface of materials exposed to the plasma. Plasma diagnostics will also be used to correlate materials modification with plasma properties. Based on this information, we will develop processes with minimal plasma-induced damage and we will investigate alternative plasma etching processes. This program will provide processes that will participate in the fabrication of new devices such as higher efficiency solar cells or quantum computers. In addition, it will train five highly qualified people who will be well prepared to integrate the high tech industry and develop new products which will benefit Canada's economy.""597355,""Darraji, Ramzi"
"615496"	"DeCarufel, JeanLou"	"Strategic Route-Finding and Optimal Positioning in Geometric Environments"	"My field of research is computational geometry, a field of computer science that focuses on solving problems with a strong geometric component. My long-term research objective is to uncover geometric properties in various scenarios to design and analyze efficient algorithms to solve fundamental problems in these different settings. For this research proposal, I want to focus on questions where we have to (1) find an optimal way to move in a geometric environment or (2) find an optimal placement for an object in a geometric environment.To illustrate the first class of problems, suppose that you wake up from a nap in the middle of the woods. There is only one path but you do not remember which direction you came from. What should you do? In computational geometry, this problem is known as searching on a line. The goal is to minimize the total distance we have to walk before finding the exit, in the worst case. The optimal strategy is to walk 1 kilometre in one direction and get back. Then, walk twice the distance in the other direction and get back. And repeat until you find where you came from. This power-of-two strategy is used as a black box in other fields of computer science such as in networking for online routing algorithms, in distributed computing for deterministic rendez-vous problems and in robotics for path planning. Generally speaking, ``routing problem'' refers to an agent moving to find a target of known coordinates and ``rendez-vous problem'' consists of two or more agents trying to find each other. My short term research objectives in this area are inspired by these problems. For instance, one of my goals is to characterize the trade-offs between the constraints the agents need to deal with and the information that is available to the agents.The second class of problems I focus on is related to finding an optimal location for an object in a given environment. For instance, where should we build a new hospital? What should be the trajectory for a new highway? People must have access to the facility as easily as possible, but we do not want to expropriate too many people in order to build the facility. My research work involves k-center problems where one looks for k points in the plane that minimize the distance to other points or to a frontier. I also work on optimal-enclosing problems where one wants to compute the optimal curve that encloses a set of points, with respect to a set of constraints. Another of my short term research objectives in this area is to find ways of modifying a network by adding/removing nodes or connections, in order to optimize different objective functions.Several questions related to strategic route-finding and to optimal positioning remain open. As in many other fields, partial solutions to some of these problems naturally bring additional open questions on which I will work.""603219,""deCarufel, Valérie"
"614906"	"deGuise, Jacques"	"Imagerie et modélisation forme-mouvement-déformation de systèmes biologiques"	"Mon programme de recherche se situe dans le domaine des technologies médicales et des technologies de l'information. Il porte sur des méthodes innovantes d'imagerie et de modélisation 3D géométrique et physique des structures biologiques À partir d'images médicales. L'approche classique pour accéder À ces informations repose sur deux étapes réalisées de façon généralement indépendante : 1) la modélisation 3D de la structure d'intérêt issue d'images (radiographie biplan, IRM, CT) pour obtenir sa géométrie; 2) le recalage de cette information 3D sur une séquence temporelle d'images pour quantifier le mouvement ou la déformation. Que ce soit pour l'analyse précise du mouvement d'une articulation composée de structures rigides comme les os du genou ou pour le suivi du mouvement d'organes de l'abdomen d À la respiration comme le rein ou le foie, l'accès simultané, souvent en temps réel, aux informations conjointes forme-mouvement-déformation quantifiées et précises devient maintenant essentiel aux nouvelles approches de traitement guidées par les images et les modèles.Dans ce contexte, le nouveau grand défi auquel nous désirons répondre maintenant est de caractériser finement la géométrie 3D de structures rigides ou déformables dans le temps À partir de séquence d'images 1) en exploitant simultanément l'ensemble des images utilisées pour la reconstruction d'une part et pour le suivi des déformations ou du mouvement d'autre part, 2) en intégrant des connaissances issues de modèles prédictifs comportementaux, 3) en traitant conjointement l'ensemble des structures dans les images, au lieu de les traiter de façon séparée. À terme, ces nouvelles techniques de modélisation forme-mouvement-déformation seront d'une part intégrées À des approches d'analyse du mouvement des articulations squelettiques À partir de séquences d'images radiographiques biplan et d'autre part, de guidage automatique pour le tracking de cibles tumorales rénales (rigides) et hépatiques (déformables) À partir de volumes d'images IRM et de séquences d'images IRM d'écho de gradient rapide (FLASH).Des contributions originales au domaine de la modélisation 3D, du recalage déformable et du suivi découleront de ces travaux. Puisque notre approche favorise le transfert technologique vers les utilisateurs et vers nos partenaires, l'ajout de fonctionnalités innovatrices et plus performantes devrait leur permettre d'améliorer leurs méthodes d'intervention en proposant des outils fiables et validés, et leur permettre ainsi de se positionner À l'avant-garde des systèmes d'imagerie pour le soutien aux gestes médicaux. Par exemple, la détection précise et en temps réel du mouvement des tumeurs cancéreuses et des organes de l'abdomen supérieur À risque permettra de corriger le ciblage et ainsi réduire la quantité de tissus sains irradiés dans le cadre de la radiothérapie adaptative guidée par l'image. ""616947,""deGuise, JacquesA"
"615962"	"Deschênes, JeanDaniel"	"Frequency-comb-based digital clockwork for next-generation optical clocks"	"This proposal aims to develop optical clockworks, the systems used to read out the time from an optical clock. An optical clock uses a very stable laser, referenced to an isolated atom's or multiple atoms' transition frequency to provide very fine and accurate subdivisions of time. Its precision comes from the very high frequency of oscillation of light compared to usual radio frequency oscillators, but this also makes it difficult to read the clock because the frequency is too high for conventional electronic means. Since these optical clocks already enable better time measurements and are likely to soon replace radio frequency atomic clock in time and frequency standards, it is very important that we develop the basic technology to support them. Measuring time accurately is a fundamental process, required for a wide range of applications, from answering deep scientific questions about our physical world (searching for gravitational waves, mapping gravity field distortions from the Earth and planets, testing for possible changes in what we now consider physical constants), to enabling very practical applications of accurate global navigation, high efficiency communications systems and many more. Often, the limit to the precision of these scientific and technological endeavours is linked to the performance of its clock. To build a clock, we need a stable, repeatable physical phenomenon (here, the oscillation of the laser electromagnetic field locked to an atomic transition) but also the means to count the number of these oscillations that have occurred. Every optical clock thus requires commensurate optical clockwork to count these oscillations and make the clocks' exquisite accuracy available for applications. Such clockwork is based on a type of laser emitting short pulses called frequency combs. These combs serve as a kind of strobe light for the clock: instead of counting each and every oscillation, combs enable us to very precisely pinpoint one every million oscillations, bringing the counting rate within the range of electronic techniques. Unfortunately these frequency combs are currently not robust and mature enough to operate for long term and outside the controlled environment of the laboratory. This research will use digital signal processing techniques and statistics to analyze, design and control robust frequency comb systems that can operate as reliably and accurately as optical clocks will require in the future.""612412,""Deschênes, JeanSébastien"
"614941"	"Duong, Luc"	"Motion compensation for transcatheter aortic valve implantation"	"Advanced visualization tools of medical image data are paramount to ensure safe navigation during Transcatheter Aortic Valve Implantations (TAVI). The main goal of this research program is to propose a methodology for image-based breathing motion compensation for navigation guidance during TAVI. TAVI is becoming an appealing alternative to traditional open­-heart valve surgery, reducing operating time and improving patient recovery. TAVI is currently performed under single view X-­ray angiography guidance, which does not provide any depth information. Cardiologists must evaluate the optimality of valve placement and deploy the valve with perfect timing, all under a complex motion. The predominant role of imaging has been recognized since the early stages of valve implantation. Recent advances in X-­ray angiography suites nowadays allow moving a single radiographic source freely in space to acquire images from any arbitrary views. This technique, called X­-ray rotational angiography is promising to acquire multiple images for a full 3D reconstruction prior to the intervention. First, at the beginning of the intervention, the aorta of the patient in apnea will be reconstructed in 3D using X­-ray rotational angiography. During the intervention, atlas-­based 3D reconstruction and 3D­-2D registration will be investigated to produce robust and accurate overlay of the aorta over X­-ray angiography considering breathing motion. This approach will incorporate both patient­-specific and population-­specific motion information. From a single X­-ray angiography sequence, the 3D volume computed previously and acquired in the patient's reference frame, will be updated by successive non­-rigid 3D-­2D registration. A consistency preserving approach based on dynamic time warping will be considered for modelling and learning the cyclic respiratory motion patterns in 2D and afterwards in 3D. Dynamic time warping is widely used in the speech recognition community for measuring similarity between two temporal sequences and would contribute to match actual motion pattern with motion patterns from generic motion atlas. A motion atlas, based on our previous work on monoplane stochastic motion compensation, will be learned from successive registration to represent the spatial motion in a reproducible manner from X-­ray angiography (patient­-specific). Furthermore, preoperative motion atlases will be constructed from both simulated data and from real patient datasets to better generalize the complex cardiovascular motion pattern (population-­specific). The long-term objective of this research program is to generate an online navigation system for multimodality image fusion during TAVI, with dynamic overlay of several preoperative imaging modalities on real-time per operative imaging such as X-ray angiography of transoesophagal echocardiography.""607083,""Duong, PatriciaPeiChi"
"615604"	"Eckford, Andrew"	"Theory and practice of molecular communication"	"This proposal concerns molecular communication. Molecular communication is a rapidly emerging technique with numerous applications, including nanoscale networking and communication in harsh environments. Using molecular communication, the transmitter encodes messages by releasing patterns of molecules, and conveys those messages to the receiver using physical principles such as Brownian motion. For example, a bit could be sent by releasing one or more molecules of a detectable compound such as ethanol (representing 1), or releasing nothing (representing 0), as a form of on-off keying. The compound could propagate from transmitter to receiver using free diffusion, and an ethanol detector at the receiver would measure when the molecules arrive. Noise in this example takes the form of either the random propagation process of the molecules, or noise in the detector. Many other modulation techniques have been proposed and studied in the molecular communication literature.In this project we answer the following research questions:1. What are appropriate information-theoretic models for molecular communication systems, and what are the associated information-theoretic capacities?2. Can we obtain an information-theoretic understanding of nanobiological systems?3. Can we miniaturize low-cost ""macroscale"" molecular communication testbeds, bringing them closer to the size needed for nanobiological applications?4. Can we illustrate the practical value of molecular communication by constructing demonstration systems?The first two questions are theoretical in nature, and will extend our knowledge of the ultimate limits of molecular communication. Answers to these questions will help us to understand what kind of applications are possible with molecular communications, and what kind are infeasible. Moreover, understanding nanobiological systems from an information-theoretic perspective may give us greater understanding of natural nanobiological systems, such as microbes.The last two questions are practical in nature, and will help us bring our theoretical knowledge of molecular communication to realization in practice. Answers to these questions will bring us closer to the futuristic possibilities of nanomedicine, and will open up new application areas (such as infrastructure monitoring) that are difficult to solve with conventional radio communication.""605490,""Eddenden, AlexanderMunTsung"
"615340"	"ElBoussaidi, Ghizlane"	"Maintaining architectural conformance through the mapping of architectural decisions to implementation practices and patterns"	"Architectural decisions express rules and actions that shape software architecture and have a large impact on software quality. Common architectural decisions include applying design solutions such as architectural styles, design patterns and architectural tactics, and choosing appropriate technologies. While existing technologies enable to rapidly build software systems, developers still need to understand the selected design solutions and be able to correctly implement them using the chosen technologies.In fact, despite the progress made by existing approaches in managing and documenting architectural decisions, we still lack effective support for enforcing architectural styles, tactics and patterns during development and evolution of the system. Also, recent empirical studies revealed that developers do not always understand the impact of their changes on architecture. This is partly due to the fact that developers do not know what styles, tactics, and design patterns were adopted. Even if they knew, they may not know how to implement these design solutions using the chosen technology; i.e., existing technologies implement differently the same design solutions and they have their own design practices and patterns.Therefore, the goal of the proposed research program is to study common existing technologies to figure out how they support design solutions (i.e., architectural styles, tactics and design patterns); and use this knowledge to develop techniques and tools that enforce these solutions' constraints and prevent architectural erosion. To do so, we propose to first build a knowledge base that contains explicit representations of the constraints and interaction protocols inherent to common styles, tactics and patterns; and explicit representations of technology-specific implementations of these styles, tactics and patterns including how constraints and interactions are mapped. Second we propose to leverage model-driven techniques to build appropriate approaches and tools that use the knowledge base to translate constraints and interactions into the context of specific projects, and enforce these constraints.The results of this research program will contribute to a better understanding and support of design solutions including architectural styles, design patterns and architectural tactics, and enforcing these solutions during software development and evolution. By supporting developers in understanding design solutions and their appropriate implementations using common existing technologies, we contribute to enhancing the quality of software systems, increasing productivity and reducing maintenance costs.""601289,""ElBouzidi, Salim"
"616512"	"ElEmam, Khaled"	"Advanced theory and methods for the de-identification of small cohorts, complex and composed health data"	"The demand for health data for research and public health purposes has never been so great, from the growth in clinical trials transparency initiatives, Electronic Medical Records being used to build learning healthcare systems, the development of real world evidence databases that integrate health data from multiple sources and used for observational research, and a desire to link to wearables and other monitoring devices. The collection of data is coming from providers, payers, employers, wellness programs, and even patients themselves, with increasing collaboration between academic institutions, provider organizations, health care systems and life sciences companies.We have made many critical achievements in our privacy research which represent improvements over the existing body of work. As health data uses evolve and the nature of health data that is being shared also evolves, there are important areas that require more study: - One of the best ways to allow the sharing of data for secondary purposes is to de-identify it. A key part of de-identification is the estimation of re-identification risk. Small cohorts pose a particular challenge to estimate re-identification risk. We will develop suitable estimators for small data sets. This will be important in developing successful models for clinical trials data sharing and studies on rare diseases and conditions.- With the growing number of sources of data, there is more demand to join data sets for building real world evidence databases. Often the individual de-identified data are being linked without consideration of the potential increase in re-identification risk. There is a need to develop a composition theory around re-identification risk. A composition theory would facilitate the estimation of the risk of re-identification of a linked data set using information from the source data.- Data complexity is growing rapidly; the data is constantly being updated and growing. The resulting complex data sets will require big data de-identification methods to ensure they scale appropriately. There is a need for streaming de-identification methods that are designed specifically for health data sets.- With the increasing availability of free-form medical text in EMRs, the analysis of this information is adding detail and context to structured data. More realistic evaluation frameworks for the de-identification of free-form text need to be developed that are designed specifically for the de-identification context, and then tools evaluated using that framework.Our lab has been effective in transitioning its research results into practice through standards and software that have been adopted globally. We will continue this trend as we develop new methods from this research, facilitating the sharing of electronic health information for secondary purposes while protecting the privacy of patients and the identity of providers.""593783,""Elezzabi, Abdulhakem"
"615890"	"Emadi, Ali"	"Advanced Switched Reluctance Machines for Electrified Vehicles"	"Electrified vehicles are emerging as the next generation of vehicles because electrification is an effective long-term solution for the transportation system to improve sustainability, efficiency, emissions, performance, and safety. Electrified vehicles, including electric vehicles (EVs), hybrid electric vehicles (HEVs), and plug-in hybrid electric vehicles (PHEVs), utilize one or more electric machines in their powertrains. Electrified vehicles' traction motors are expected to have a wide constant-power speed range and short overloading capability in order to match with the driving conditions. They also need to meet critical industry requirements such as reliability, low cost and weight, low maintenance, and quiet operation. Traction motors should deliver high torque at low speed for starting and hill-climbing conditions and high power to achieve high cruising speeds. Interior permanent magnet (IPM) synchronous machines are employed in most of the currently available hybrid and electric vehicles on the market. With the use of high-energy and high-coercivity rare-earth magnets, permanent magnet machines can provide high torque density and smooth operation. However, permanent magnets are sensitive to temperature. This causes a significant limitation on the operation of the machine in harsh environments. In addition, rare-earth magnets take up a significant portion of the total cost of the traction motor. When compared with the conventional vehicles, the high cost of the traction motors for electrified vehicles currently presents a challenge for their mass-production and increased market penetration. Switched reluctance machines (SRMs) do not employ any permanent magnets or conductors on the rotor. Therefore, they can operate in harsh environmental conditions and at high-speeds and high temperatures. The structure of SRMs enables a wide constant-power speed range, which matches very well with the traction requirements. Due to its simple construction, manufacturing cost of SRMs is significantly lower than the permanent magnet machines. Most significant challenges in SRMs are high torque ripple and high levels of acoustic noise and vibration. In addition, the machine demonstrates a highly nonlinear behavior due to saturation in various parts of the stator and rotor cores. This creates a challenge in terms of torque control and state estimation. Furthermore, highly saturated operating conditions can affect the efficiency of the motor. This proposed research program is focused on a novel family of SRMs with improved power density, efficiency, and performance for electrified vehicles. The proposed family of SRMs involves novel motor configurations with various stator and rotor pole combinations and advanced control techniques to improve the torque- and power-density and performance of the motor with reduced torque ripples, acoustic noise, and vibration.""605368,""Emam, Ali"
"614673"	"Evans, William"	"Geometric Representation of Graphs"	"This proposal outlines a plan to explore properties of graphs that are defined geometrically. Such graphs arise, for example, in the study of transportation, communication, and sensor networks. The focus of the proposed research is on graphs defined by some notion of contact or visibility between geometric objects. Two objects are mutually visible if there exists a line segment connecting them that does not intersect another object. They are in contact if their boundaries, but not their interiors, intersect. A main theme of the proposed research concerns simultaneous representation, where one set of objects can define more than one graph. By considering contact or visibility between objects in a limited number of directions, we obtain a limited number of different contact or visibility graphs on those objects. My goal is to understand properties of these sets of simultaneously representable graphs and to design algorithms to find such representations.The most commonly studied example of simultaneous representation is the problem of finding a planar drawing of each of two graphs where each vertex is the same point in both drawings (and edges are curves that connect their endpoints). The problem is important, beyond its theoretical interest, for its application to the visual analysis of a changing network on a common set of vertices. While point/curve simultaneous representation has received a great deal of attention, the study of alternative simultaneous representations has been much more limited. I propose to study simultaneous representation using geometric objects, such as segments, rectangles, and disks, as vertices where visibility or contact determines adjacency. For example, rectangles in the plane define one graph when visibility is vertical and another when visibility is horizontal. What pairs of graphs can be represented in this fashion? Given two graphs, what is the complexity of finding such a representation if it exists? Can the existence of such a representation aid in the solving of problems restricted to these graphs?The kinds of graphs that can be represented implicitly using visibility changes depending on the type of object and on the notion of visibility. Part of this proposed research considers visibility representations that allow some amount of ""X-ray vision"", that is, two objects are mutually k-visible if there is a segment connecting them that intersects at most k other objects. Such graphs arise, for example, when considering networks of sensors that can penetrate a limited number of walls. The model increases the set of graphs that can be represented beyond traditional 0-visibility representations, but in a way that is limited by the geometry of the representation. An objective of the proposed research is to understand to what extent permitting k-visibility impacts the class of representable graphs and to relate this class to other well-known graph classes.""603121,""Evdokimov, Andrew"
"614891"	"Falk, Tiago"	"Towards next-generation context-aware affective human-machine interfaces"	"Until recently, the ubiquitous way of interacting with digital information was via a keyboard and mouse. As mobile devices gained popularity, however, touch became the primary input modality for human-machine interfaces (HMIs), followed closely by voice. The next decade, however, will witness tremendous growth in data generation (e.g., statistics suggest a 50-fold increase in our digital footprint by 2020). New sensors and technologies will emerge and will be embedded in everyday objects, creating the so-called Internet-of-Things revolution. This data deluge will require drastic innovations in human-machine interaction, particularly as the boundaries between humans and machines, as well as machines and the environment become blurred. To achieve this ambitious goal, the proposed research program aims at developing technologies that will enhance machine intelligence by making them not only aware of their surroundings, but also of their users' affective/cognitive states. Such context-aware affective HMIs will revolutionize several key sectors of the Canadian economy, including healthcare, entertainment, education, and telecommunications.To achieve this long-term goal, six short-term objectives have been defined, grouped under three main themes. First, we propose to take existing context-aware solutions to the next level by i) enabling distributed environment awareness, ii) shifting the human-machine boundary by making the body as an input modality, and iii) giving the machine additional intelligence via an innovative Quality-of-Context metric. Voice and neurophysiological input modalities, however, are extremely sensitive to noise (e.g., room acoustics) and artefacts (e.g., due to movement), respectively. To overcome this severe usability factor, the second proposed theme aims at developing next-generation context-aware enhancement algorithms for both iv) speech and v) neurophysiological signals. Lastly, we propose to vi) build a fully-functional context-aware prototype that uses voice, gesture, and body as HMI input control signals. Validation experiments within a robot control application will be implemented, thus opening doors for longer-term applications, such as telemedicine robots that are aware of the patient's mental state and could, for example, detect the onset of depression and take necessary measures accordingly. The proposed research program will place emphasis on excellent training of highly qualified personnel (HQP). HQP will be exposed to an interdisciplinary melange of signal processing, cognitive engineering, machine learning, and hands-on prototyping, as well as to research facilities that feature state-of-the-art wearable technologies and environment monitoring sensors. In addition, via close collaboration with industry, HQP will be equipped with skills and expertise that are in high demand in today's competitive job market.""604352,""FalkDotan, Biran"
"615845"	"Feeley, Marc"	"Implementation of Dynamic Programming Languages"	"Dynamic programming languages such as JavaScript, Python and Ruby are gaining in popularity. These languages offer many features which ease the development of evolvable complex applications: a rapid development cycle, introspection, language extensibility, high level of abstraction, code migration, etc. Current implementations of these languages use highly complex multi-tier designs which nevertheless still lag in terms of performance when compared to static programming languages (C, Java, etc). The long term objectives of the proposed research program are to lower the development cost of VMs for dynamic programming languages and improve their performance.The research program will continue work on recent advancements in the field of Just-In-Time compilation. The technique of lazy Basic Block Versioning has been shown to be effective at eliminating run-time type checks from dynamically typed programs. This approach will first be extended to make it operate interprocedurally, both when the source program uses higher order functions and not. Then we will instigate the power of the approach by applying it to other compilation problems, including register allocation, memory management, function inlining and automatic program parallelization.We will also investigate the design of a self-hosted JIT compiler which can benefit from previous runs of the programs, possibly by other users at other locations, to reduce the cost of recompiling the program. We also plan to study the implementation of parallelism in the context of dynamic languages. We are interested in the parallelism model of the functional programming languages Clojure (based on transactions, agents), Erlang (based on Actors) and Termite (that offers task migration through continuation serialization).""600993,""Feero, Mark"
"615637"	"Fichtinger, Gabor"	"Process Modeling of Ultrasound-guided Needle Placement Interventions"	"Synopsis. Canadian medical universities are in the process of transitioning to a Competency- based Medical Education (CBME) model at all levels of training. Achievement of competence requires practice, and for reasons of patient safety and learner comfort, the early stages of practice are best done in a simulated environment. One of the skills that all medical doctors are required to demonstrate is ultrasound-guided needle placement (USNP), in procedures catheter port placement, anesthetic injections. The primary goal in these procedures is to insert a needle to target under ultrasound visualization, while sparing surrounding tissues. USNP is extremely difficult because it requires simultaneous mastery of multiple skillsets. While USNP is a mandatory skill for every new Canadian physician, there is a near complete lack of computing and engineering methodology and tools to support medical educators in the development of CBME programs for USNP. Research on the USNP process modeling is extremely scant, thus critically hindering medical educators in the mandatory transition to the CBME medical education model.Research program. We aspire to fill these blanks through four themes: i) generalized process model for describing USNP workflows, ii) methods for automatic segmentation of USNP workflows into elementary procedural steps, iii) framework for designing quantitative performance metrics and, iv) data management framework for secure archival and retrieval of computer-assisted USNP training information.Outcomes. The scientific contribution of the proposed program will be the development of formal analytical methodology and software tools that aims to support medical education researchers in creating CBME training programs. The societal outcome from the program is potentially significant; research that supports the development of CBME-compliant computer-assisted training systems will impact the competence of future medical doctors and as a result on the health of all Canadians. A least 20 HQP trained will be trained in engineering and computer science research.""616597,""Fichtinger, Gabor"
"614819"	"Forkert, NilsDaniel"	"Combined Segmentation and Hemodynamic Analysis of Cerebrovascular Structures using Spatiotemporal Arterial Spin Labeling MRI"	"Magnetic resonance imaging (MRI) has become an indispensable diagnostic and research tool. It is capable of visualizing the anatomy and function of organs. Time-resolved three-dimensional (i.e. 4D) MRI blood flow measurements, e.g. arterial spin labeling (ASL) MR angiography (MRA), of the brain is of increasing recent interest due to MRI technology advancements. 4D ASL MRA sequences are now capable of imaging cerebrovascular structures and blood flow with a spatial resolution better than 1 mm3 and a temporal resolution below 100 ms. Since this imaging technique utilizes blood as an intrinsic contrast agent, no exogenous contrast media is required, making it inexpensive and safe. However, the vast data volume obtained with 4D ASL MRA limits the utility of the technique, particularly if the only 2D images are only visually inspected. The broad goal of this research is to develop novel image analysis algorithms and visualization techniques for large time-resolved datasets to achieve blood flow analysis, vessel segmentation, and hemodynamic visualization techniques tailored to 4D ASL MRA datasets to enable fast, accurate, and quantitative interpretation.We will design, optimize, and evaluate various implicit and explicit models for the hemodynamic analysis of blood flow dynamics based on the ASL time-intensity curves for each voxel in a first step. A segmentation of the cerebrovascular system is required to investigate the vessel anatomy quantitatively and to visualize the results. Therefore, we will develop new advanced vessel segmentation methods for extraction of the vessels from 4D ASL MRA datasets of the brain. More specifically, we will combine the hemodynamic analysis and cerebrovascular segmentation into an integrated analysis approach. It is expected that such a coupled vessel segmentation and blood flow analysis will lead to both improved segmentation and hemodynamic analysis results. Finally, advanced visualization methods for the combined representation of the cerebrovascular system and its blood flow will be developed and evaluated. For example, it is planned to design a glyph-based visualization of the 4D blood flow as well as dynamic surface-based visualization techniques. All methods will be integrated within a novel software tool that will be made available for applied imaging researchers, thereby enhancing the practical value of 4D ASL MRA. Collectively, this work will enable a fast, accurate, and quantitative interpretation of 4D ASL MRA datasets while creating new image analysis and visualization methods with broad applications for other image processing problems.""600824,""Forman, Davis"
"615353"	"Gambs, Sébastien"	"Protecting location privacy in online and offline contexts"	"The advent of Location-Based Services (LBSs), which personalize the information provided according to the position of their users (e.g., geolocated search), has been accompanied by the large-scale collection of their mobility data. On the one hand, these mobility datasets have a high scientific, societal and economical value. On the other hand, learning the location of an individual is one of the greatest threats against his/her privacy due to its strong inference potential and the possibility of deriving a wealth of personal information. In particular in the past, I have designed inference attacks that use the location data of a user to deduce other personal information (such as the points of interests characterizing his/her mobility), to predict his/her future movements or even to perform a de-anonymization attack.The scope of my research program covers two different contexts in which the location privacy of a user should be protected. The first context corresponds to the situation in which the user is online (i.e., when he/she benefits from a location-based service in real-time). In this setting, I propose to investigate two different approaches whose objective is to enable privacy-preserving LBSs to operate while minimizing the trust assumptions: the local computation approach and the cooperative one. The second context considered is the offline setting, in which the location data of thousands of users has been collected and has to be sanitized before it is released (e.g., before opening or sharing this data). More precisely, during my discovery grant I propose to work on the design of sanitization methods for mobility mining, whose objective is to produce a data structure that can be used to derive generic mobility patterns of the population while hiding individual movements. Finally at the fundamental level, I am deeply interested in how to model and quantify location privacy in a manner that is both meaningful and useful for practitioners who need to assess the privacy risks of processing, sharing and collecting location data. Thus I propose to study how to integrate the semantic dimension in the currently existing location privacy models. The societal impact of my research program's outcomes can be important, as they have the potential to improve significantly the privacy situation of users of LBSs. In addition, the solutions developed will act as enablers by helping Canadian companies to implement privacy-preserving LBS. In particular, a major social and economic challenge is to foster the development of LBS while providing sufficient privacy guarantees. Thus, privacy-preserving LBS have to be developed to avoid the transformation of Big Data into Big Brother, and the results of my research program will directly contribute to this. Finally, the research conducted will be done in cooperation with and contribute to the formation of HQP (i.e., PhD and master students).""608986,""Gammon, Adrian"
"614980"	"Georgiou, Konstantinos"	"Efficiency Tradeoffs for Combinatorial Optimization Problems"	"One of the fundamental subjects in Theoretical Computer Science is the study of computation capabilities when resources are limited. Indeed, many real-life optimization problems admit easy-to-describe and non-efficient algorithmic solutions if one assumes unlimited computation power, e.g. no restrictions in time, and full coordination of participating processes. How well can such a problem be solved when one is restricted to use efficient algorithms with limited resources? When the limited resource is time, i.e. the number of computation steps, Theoretical Computer Science has provided a rich problem classification mostly based on deep and unresolved mathematical conjectures. According to these conjectures, a large family of combinatorial optimization problems cannot be solved exactly and efficiently, and as such one is bound to provide only approximate solutions within reasonable time. Surprisingly, a restricted and systematic algorithmic technique, based on the so-called mathematical tool of convex-programming, has given remarkable positive results in this direction. Recently, this algorithmic technique has been leveraged into a dynamic model of computation where one can provably trade efficiency for accuracy. Half of the current program will investigate the capabilities of this model of computation for the whole spectrum of efficiency notions, touching and extending on the computability predictions of famous mathematical conjectures, e.g. that of ``P is not equal to NP'' and the Unique Games Conjecture. Progress in this research program will enhance our understanding of the solvability of hard combinatorial optimization problems, will result into new algorithmic techniques, as well as will identify structural properties of input instances that are difficult to solve. Other valuable computation resources in real-life problems include the centrality of computation. For example, this is the case in the so-called search-and-fetch problems that abound in search-and-rescue operations. Indeed, recent developments in modern robotics give rise to combinatorial problems in which a number of autonomous mobile agents (drones) attempt to complete a task, e.g. locate and rescue a victim in an unknown environment. Computation in these cases is ``distributed'' among the participating mobile agents, while the centrality of computation is affected by the communication capabilities of the robots. The other half of the current research program will investigate the computation capabilities of mobile agents in search-and-fetch problems. The focus of this research will be on efficiency tradeoffs for a large spectrum of centrality notions as it is imposed by different robot communication capabilities. Progress in this area will have immediate impact to robotics applications, as well as it will deepen our understanding of the computation capabilities in distributed systems.""607376,""Georgiou, Konstantinos(Costis)"
"614839"	"German, Daniel"	"Improving License Compliance for Software Development"	"Free and Open Source Software (FOSS) is widely reused in industry. Broadly speaking, FOSS is software under a license that allows for its reuse and the creation of derivative works under a set of conditions. While reusing FOSS usually involves no monetary transaction, the organization reusing the software must comply with the conditions set by its license in order to have the right to reuse and redistribute the software.Current licensing analysis assumes a one-size-fits-all model. It presumes that the legal constraints of building a system are universal and only need to be considered once. However, this is usually not the case due to a variety of legal and technical reasons: (a) the law regarding software varies across jurisdictions; (b) the legal constraints regarding software development are often ambiguous or untested and subject to interpretation; (c) many FOSS packages offer the ability to configure the build process in order to customize what parts of the systems are built-as a consequence, different ""builds"" of the same source code might have different legal constraints; and (d) FOSS provenance information is difficult to determine-FOSS is often copied, renamed, moved, and it is not always easy to see where code originated, who the true copyright owner is, and when and how the code was copied into another system. The goal of this proposed research is to address these issues and create models and tools to assist developers and legal teams with license compliance. In particular, this grant will support the following research goals:1. Perform empirical studies to understand the challenges of organizations wanting to reuse licensed components.2. Develop a model and method that formalizes the evaluation of licensing compliance for software and a prototype to demonstrate its feasibility.3. Develop methods and techniques to improve the discovery, traceability and management of the provenance of software artifacts, as well as build tools that implement them. The management of license requirements of reusable components is a critical problem of industry for which there is a lack of research. The studies, models, methods and tools we propose will address this problem and will help industry improve its productivity by reducing the cost of the reuse of licensed components.""612546,""Germida, James"
"615365"	"Giannacopoulos, Dennis"	"Hybrid Parallel Adaptive Finite Element Analysis and Design for High-Speed Microelectronic System Interconnections"	"The objective of this research program is to develop new, highly innovative, accurate and reliable numerical methods for the efficient simulation of high-speed microelectronic system interconnections (MSI). Simulations based on circuit theory have been effective for verifying signal integrity in systems operating at maximal speeds determined by constituent devices such as logic gates and transistors. However, with today's shrinking feature sizes and increasing clock frequencies, a limiting factor for many high-speed microelectronic system designs is interconnection delays rather than device switching speeds. Further, interconnection effects such as reflection, cross-talk, dispersion and attenuation are now leading sources of signal corruption and a significant cause of system performance degradation at higher operating speeds. Standard circuit analysis and conventional simulation techniques are not sufficient for accurately predicting microelectronic system performance when these conditions prevail. Today, the state-of the-art in MSI research lies in the development of innovative numerical methods capable of accurately, efficiently and reliably simulating the interconnection electromagnetic waveforms within the sophisticated 3-D micro-fabricated structures of modern high-speed microelectronic systems. Moreover, when this information is produced effectively it can have a major impact by significantly reducing the overall time and cost of the design process.The study of the electromagnetic behaviour of MSI structures is a critical component for the synthesis and design of present and future generations of high-speed microelectronic circuits and systems. The overall goal of this research is to develop advanced high-performance computing methods capable of efficiently simulating the electromagnetic behavior of increasingly realistic models of microelectronic systems over future generations of hardware that will become available. Ultimately, this research is intended to benefit the microelectronics industry by providing practical and effective simulation tools that can be used with confidence to predict the performance of newly proposed designs to within specified engineering tolerances in a timely fashion.""606815,""Giard, Antoine"
"615980"	"Giguère, Philippe"	"Improving the Perception of Autonomous Robotic Systems through Sensing and Machine Learning"	"Recently, the field of robotics has experienced a dramatic surge, both in terms of scientific accomplishments and industrial applications. The automation of numerous tasks has been proposed, such as driving, object manipulation or warehouse operation. To mitigate the difficulties associated with uncertainties or changing conditions, improvements to the perception pipeline are necessary. In this proposal, we will improve it both at the intelligence level and at the sensing level. For the former, we will explore the use of two machine-learning techniques: Domain Adaptation (DA) and Sparse Coding (SC). Domain Adaptation aims at improving performance of a classifier trained on a data set but used on data which is distributed slightly differently. Sparse Coding tries to automate the problem of feature extraction, by finding representations where few components of a potentially long feature vector are active (non-zero). At the sensing level, we propose for example to explore the use of custom-made hyperspectral cameras.For our short term objectives, we have identified 3 key problems in robotics for which we seek to make significant contributions: visual place recognition, autonomous navigation in forests, and grasping automation. The use of DA in place recognition will improve the detection of images of the same location taken under different illumination or weather conditions. In order to remove shadows in images, better color-constant images can be generated from hyperspectral cameras. For forested environments, we will study, in parallel, the use of this hyperspectral camera and of 3D LiDAR scans for place recognition. On top of that, we will propose ameliorations to the creation process of topometric maps, used for forest navigation purposes. For grasping, we propose a richer representation of a grasping location, called hemicylindrical view. We will also increase the robustness of the 3D sensing by fusing multiple views. Finally, we will perform representation learning with Sparse Coding. Experiments will be conducted on real data or robots. For instance, we will regularly gather datasets in Quebec City, over the four seasons to test our visual place recognition methods. For forest navigation, we will use our Clearpath Robotics Husky A200 robot and its sensor suite in the forests located on Laval University campus. For grasping, we will test our algorithms with real robotic arms.We expect to make significant scientific contributions to robotics, in the form of novel applications of advanced machine learning methods or sensing approaches. For example, we do not believe that anyone has explored the paradigm of Domain Adaptation in the context of place recognition. We also expect that our research results will transfer directly to the industry. Finally, we will train 3 Undergraduates, 2 Masters and 4 PhDs with skills and knowledge that will benefit the Canadian industry.""596968,""Gil, Manuel"
"615200"	"Givigi, Sidney"	"Autonomous robotics in noisy and delayed environments"	"This research aims to develop the methods and processes that enable the development and deployment of autonomous robots in all environments wherein robots can contribute to safety and productivity, especially in time-delayed environments. The time delay comes from several different sources as sensors, computation, actuators, communication and interactions among different parts of the system, especially robot to robot (or agent to agent) interaction. We are primarily interested in the case of on-line real-time learning applied to multi-robot environments in which emergence of individual and group behaviours in collectives of robots arise and in which time-delays could lead to different, or even unstable, behaviours, individually or collectively. For the purposes of this application, individual behaviour is defined by how a single robot chooses to act (in terms of policies or strategies) given that it perceives the environment to be at a given state. The way the robot perceives the state of the environment is inherently imprecise, limited, noisy and potentially time-delayed. Also, the presence of other robots change the environment in such a way that the relationship with another single robot may affect the decision taken at any given time. Group behaviour is how the collective may act as a group when faced to conflicting roles and tasks. Group behaviours are linked to individual behaviours in a manner that is not presently completely known.This research intends to apply learning algorithms based on reinforcement learning techniques (and its derivatives) in order to produce on-line real-time capable robots that can be applied to actual applications such as the location and disruption of improvised explosive devices (IED), object identification, mine and tunnel navigation, aerial surveillance, underwater mapping, navigation in crowded environments and so on. The representation of the environment is to be done by using learning algorithms as well as decentralized control based on Model Predictive Control (MPC), especially when considering the interactions among agents (either cooperative - when the agents have a common goal or objective - or competitive - when the agents do not have an explicit representation of the goal of the others - in nature). The learning will be applied to two different layers: (i) the individual execution of a task; and (ii) the interaction among agents. The algorithms developed will be tested in real robotic platforms that are compliant with the Robot Operating System (ROS). These algorithms are needed to run in real-time with the noisy and delayed sensor readings. Furthermore, stochastic delays in the processing of the control algorithms are possible. The platforms are composed of ground and aerial vehicles and the experimental facility is already available at the Royal Military College of Canada (RMCC).""604532,""Giwa, Oyindamolaoluwa"
"614692"	"Gorbunov, Sergey"	"Securing Data in the Cloud"	"Big data and cloud computing create new privacy challenges. Our personal devices (e.g., laptops, mobile phones, tablets) have low bandwidth and can perform only lightweight computations. These devices rely on the cloud to perform search queries and other types of computations over large datasets. However, the datasets may contain sensitive information (e.g., medical, financial, personal records) that should be encrypted in the cloud, in order to maintain security and privacy. This creates a fundamental problem: our personal devices cannot use the cloud to perform computations over encrypted data. Can we run a program over encrypted data in the cloud without revealing the data or the description of the program? Can we authenticate the results of the computations performed in the cloud without re-downloading the data and re-doing the work? My long-term research program is to design and develop new cryptographic algorithms, techniques and methods for securing data and programs in the cloud. Specifically, this project will study: (1) development of new security models, methods, and algorithms for secure computation over encrypted data in the cloud, (2) development of new mathematical tools and algorithms for program and code obfuscation, and (3) advancement of techniques for verifiable computation. The algorithms and techniques can be applied to a variety of applications and help us build secure clouds. As more and more sensitive information is collected from all of us, these algorithms are essential. Moreover, during the duration of the projects outlined in this proposal, we will train a new generation of security experts. They will gain knowledge of advanced mathematics, theoretical and practical cryptography and learn how to reason about any security challenge that arises in society.""616013,""Gorczyca, Beata"
"615812"	"Hallé, Sylvain"	"Enriching system models and verdicts for testing and verification of software systems"	"Recent news are rife with headlines on incidents involving software malfunctions, with consequences ranging from the merely annoying to the dramatic. This has created a context of increased pressure for more correct software, and a growing demand for tools and techniques that may help software developers reach this goal. This research program shall contribute to the development of reliable software systems by concentrating on their specification, testing and verification. This goal will be reached by the study and development of expressive specification languages and the production of useful and testing and verification theories and automated software tools.""603222,""Hallé, Valérie"
"615153"	"Hamel, Sylvie"	"Algorithmic and combinatorial problems inspired by comparative genomics."	"Abstract: Sitting at the interface of theoretical computer science and mathematics, my area of research concerns the development of algorithmic approaches and combinatorial tools for theoretical problems inspired by biological problems, coming mostly from comparative genomics. Roughly speaking, comparative genomics aims at extracting information from the comparison of genomes of different species. This information may concern genomic sequences themselves, the order of appearance of genes in different genomes, the structure of small molecules (like RNA), etc. The general problem of extracting meaningful information from the resulting very large mass of biological data gives rise to many important problems on combinatorial objects such as words, permutations, trees or graphs. For instance, permutations partially encode the structure of genomes. The development of efficient algorithms for the manipulation of these discrete structures is also crucial in this context. One finds that, in any one of these specific contexts, many different notions of distances have been suggested to compare the relevant objects, with the consequence that we now have to clearly characterize the ``best'' distances among these. For sure, this quality measurement of distances is closely tied to the construction of efficient algorithms fulfilling the desired goal.  My research program consists in studying how one should compare and analyze such distances with respect to their principal properties, and to quantify their relevance to a given context. Concrete results of this study lead to the construction of new efficient algorithms to help solve several problems related to distances optimization. Among important uses of distances in applied contexts one looks for a way to aggregate a set of solutions of a given problem into a consensus solution which highlights common desired features, while minimizing disagreements. One original aspect of my program rests on the study of how to efficiently (algorithmically) reach these consensus solutions in various context.""613453,""Hamelin, Richard"
"615052"	"Hancock, Mark"	"Game and Interaction Science: Using Principles from Games to Design Novel Interfaces that Compel and Motivate"	"Human-computer interaction (HCI) has typically focused on tasks and functionality of software, often with attention to usability only applied once functionality has been achieved. However, both recent technological advances and current successes in the marketplace suggest that people care deeply about the interaction itself. People are attracted by interactions that are simple, natural, and intuitive; preferably even pleasurable and playful. In spite of the huge commercial interest in this area, research has remained focused on usability. In contrast, my research team and I will explore the science behind making technology motivating, attractive, and perhaps even compelling to use. This will be a deep focus on interaction, where we will explore the technologies that support physicality in touch, as well as full-body and spatial interaction, looking to unlock those aspects of interaction that can move us towards the playful and the ludic while retaining functionality.While the focus in HCI has largely been on usability, the games industry has considered elements of immersion, engagement, and enjoyment more extensively. Games and play are known for capturing attention for hours at a time, and for encouraging repeated and continued use. Indeed, in many cases, game developers base their new platforms on those known to have achieved increased engagement, such as wildly successful smartphone games like Angry Birds or Candy Crush Saga, or the previous success of Nintendo's Wii. Many researchers have investigated ways to apply motivating aspects of gameplay towards positive behaviour change, such as motivating physical activity, encouraging healthy eating, and engaging in activities that lead to learning. However, researchers have tended to focus on the design of the games themselves, rather than the technology and interaction, for the source of motivational pull.In this research, we will investigate the science of games and interaction-the use of principles from games and play to design novel interfaces that compel and motivate interaction with new technologies. The proposed research will leverage theories of motivation that have been applied to games, such as self-determination theory, flow theory, and self-regulation theory, to understand the role of interfaces and technologies in motivating interaction and compelling the use of technology.This research program will provide critical insights for commercial development and research into novel technology, allowing for the intentional invention of compelling novel interaction.""602012,""Handa, Tanya"
"600669"	"Harrison, Adam"		"FRANCE"
"615012"	"Harvey, Nicholas"	"Algorithms: Sparsification and Applications"	"Modern algorithmic research has two compelling justifications. First, the field aims to study foundational questions with significant technical depth, frequently leading to results of high scientific merit. Second, the field has developed many powerful algorithmic tools that can have great impact in applications. My passion for algorithmic research stems from a desire to contribute results and train students in both of these directions.With my MSc students at UBC, I aim to find applied areas with problems that can be attacked by modern algorithmic tools. Together we have found novel applications of discrepancy algorithms to machine learning problems, of streaming algorithms to caching problems, and of graph embedding algorithms to information visualization problems. All of this work is being published in significant computer science conferences. One of the projects was conducted together with a local startup company; that work has been patented and incorporated into their product. Technology transfer from theory to applications remains a goal for my future MSc projects.The second thrust of my research is on foundational questions in algorithms. This work focuses on a phenomenon known as sparsification: removing parts of an object to make it simpler, while approximately preserving its overall structure. Sparsification of graphs has seen dramatic progress in the past 10 years. One remarkable result shows that, in any graph, one can find a linear number of edges that preserve its cuts and its spectral structure. With my Waterloo students we have published new sparsification results in a top-tier conference and journal. In combinatorics, sparsification is known as discrepancy theory. Over the past five years there has been a revolution in discrepancy. Several problems that lacked an algorithmic solution for decades now finally have one. This work has already led to the solution of the Kadison-Singer problem, a notorious problem in pure mathematics. A major open question is to find an algorithmic version of that result. Together with my students and external collaborators, my recent and future research aims to push the boundary of discrepancy theory. Achieving the goals of this proposal will lead to theoretical algorithms of substantial scientific value, and practical algorithms with impact on other fields. My intention is to pursue the foundational questions with UBC students, and the applied questions together with local industry partners.""617142,""Harvey, Nicholas"
"614868"	"Hasan, MAnwarul"	"New Frontiers for Resource Constrained Cryptosystems"	"Security is a critical issue for both existing and emerging devices and applications, such as e-commerce, the Internet of Things, unmanned vehicles and wearables. E-commerce, for example, deals with hundreds of billions of dollars annually by facilitating online financial transactions among banks, businesses and individuals. Such transactions often occur over open networks and increasingly via wireless channels using mobile devices. All these pose a variety of security threats for user authentication, data integrity, and confidentiality. In order to provide security for communication and computing applications, various cryptographic schemes can be used. Many of these schemes rely on computations that require special kinds of representations, algorithms and hardware architectures. For practical applications, these algorithms and architectures must meet performance requirements as well as implementation constraints, e.g., limited silicon area, narrow bandwidth and low power. More importantly, they need to operate in a way so that they do not leak information related to the secret key of the cryptosystem through side channels.The main objectives of the proposed research are to investigate novel algorithms, efficient hardware architectures and robust countermeasures for resource constrained cryptosystems in light of a recent initiative towards new digital signature standards and a grand plan to transition to quantum safe cryptographic algorithms, all of which will affect many jurisdictions around the world including Canada.The proposed research will advance the state-of-the-art in data security and is expected to help, for example, increase user confidence in the emerging Internet of Things for sensitive applications, which in turn can contribute to very positive socio-economic development globally. Our work will increase Canada's innovation base and research capacity, leading to new intellectual property and publications in premier international venues. Both industry and academia lack skilled people in security, and the proposed research will provide exceptional training opportunities to highly qualified personnel in this very important area.""605672,""Hasanovich, Linda"
"601249"	"Hassanpour, Negar"	"Ontario"	"CANADA"
"615913"	"Hatami, Hamed"	"Analytic techniques in communication complexity, information complexity, and property testing"	"Communication complexity is one of the most active fields of research in theoretical computer science. It has a broad range of applications, and employs a surprisingly diverse range of techniques and tools from other areas of mathematics (e.g. linear algebra, Fourier analysis, discrepancy theory, functional analysis, additive combinatorics, information theory, etc). In addition to its applications in other subfields of complexity theory, it has real world applications via data structures and data streaming algorithms. Although communication complexity has, since its birth, been witnessing steady and rapid progress, it was not until a few years ago that a focus on an information theoretic approach resulted in new and deeper understanding of some of the classical problems of the area. This gave birth to a new area of complexity theory called information complexity. While communication complexity is concerned with minimizing the amount of communication required for two players to evaluate a function that depends on their private inputs, information complexity, on the other hand, is concerned with the amount of information that the communicated bits reveal about the inputs of the two players. The field of information complexity is deeply connected to communication complexity. Shannon, in one of the most important mathematical papers of the 20th century, introduced the notion of entropy to capture the amount of information in a random variable, and set the foundations for the era of digital communication. Shannon's setting is the simplest setting of communication where there is a one-way channel and one player wants to transmit her data to the other player. The general setting of communication complexity is more complicated as the players are allowed to interact. However as the recent results in this area have demonstrated, similar to the way that the information content of a random variable gives the asymptotics of the transmission cost, information complexity of a function provides valuable information about the communication complexity. One of the main goals of this proposal is to study protocols with optimal information cost and in particular to find a paradigm in which such protocols can be defined properly. This is important as currently we have little understanding of how the optimal protocols look like. The second goal of this proposal is to extend the information theoretic approach to other areas of complexity theory, and the third goal of this proposal is to investigate further applications of recent advances in additive combinatorics to communication complexity and the area of property testing. Additive combinatorics has seen exciting advances in recent years. Some of the tools in this field has found applications in theoretical computer science. We propose to study further applications of these techniques in the areas of communication complexity and property testing.""610561,""Hatch, Grant"
"601248"	"Hauer, Bradley"	"British Columbia"	"CANADA"
"615174"	"Hayward, Ryan"	"Computational Intelligence and Computer Search"	"For centuries, games such as chess and Go have served as an arena in which humans showed their intellectual strength. Today, such games serve as an arena in which machines show their computational intelligence. A key aspect of computational intelligence is search.The goal of this project is to find improved search algorithms. A typical problem is to find the next move in a two-player game such as chess or Go. Here, the algorithm must search a large space of possible continuations of the game and select a move which will turn out to be strong. Historically, the problem of building strong artificial players for games such as chess or Go has been a main challenge in the field of artificial intelligence.Recently, there has been tremendous progress in this field. Around 10 years ago, a new search algorithm --- Monte Carlo Tree Search (MCTS) --- that combines classical tree-search techniques with random move sampling revolutionized computer Go. I plan to continue work on fundamental problems that both explain the success of MCTS and lead to improvements for it. More recently, another algorithm --- Deep Learning --- has emerged that promises another such revolution. I plan to work on this as well.""616107,""Hazell, Tom"
"616009"	"Ho, NgaiMan(Carl)"	"Advanced architecture and interfacing technologies of real time power-hardware-in-the-loop simulation"	"Safe, renewable sources of energy, like solar and wind power, are gaining popularity due to their potential to reduce greenhouse gas emissions and dependency on fossil fuels. However, renewable energy is also often dependent on prevailing weather conditions, leading to a sporadic and unpredictable energy supply, which creates power quality issues for both users and providers of electrical power. This difficulty can be overcome by developing low voltage microgrid infrastructures that can interface directly with distributed renewable energy sources and employ smart control mechanisms to regulate the efficient use and stability of the available power in the grid.There are challenges to the design and evaluation of an intelligent microgrid. There are no appropriate simulators to simulate power and communication devices at the same time. And it is well-known that there is also a large technology gap that lies between simulation and practical systems, as there are a lot of non-ideal components in power apparatuses. A Power-Electronics-based Power-Hardware-In-the-Loop (PHIL) platform can help to overcome these challenges by providing a semi-physical system for evaluation of novel microgrid designs. Physical (not simulated) components, like power apparatuses and communication devices, could then be incorporated in a test microgrid, which would perform like an actual mircogrid installation. Energy availability from weather sources and the external grid infrastructure (e.g. medium voltage public grid networks) can be fully controlled and simulated by computers, and their responses converted and amplified by power electronics interfaces. Actual interactions of the microgrid with these external power sources will be simulated in a way that will provide superior evaluations of power quality and grid stability. The power will cycle between the PHIL platform and the GUT, which will provide high efficiency for prolonged periods of evaluation.A 30kVA PHIL system with a Microgrid under test will be constructed to run in a Lab environment. The complete evaluation platform will be small, efficient, practical, accurate and fully controllable for changing environmental data and external grid operations. The PHIL platform will bridge the technical gap between theoretical simulation and practical applications in a Lab environment. The proposed program will train 10 HQP (2 Ph.D., 3 M.Sc. and 5 USRA), they will gain experience and generate new knowledge of power electronics and power systems. The proposed system will facilitate technology and business development for both real time simulation platform manufacturers and microgrid providers in Canada. Furthermore, Canadian research centres and power apparatus manufacturers will have a cost-effective, compact, fully controllable, reliable PHIL simulator to evaluate emerging grids and power apparatuses for future small grids applications.""592775,""Ho, Paul"
"614922"	"Ho, PinHan"	"Studies on joint source-channel coding approaches for multicast of successfully refined sources"	"Studies on joint source-channel coding approaches for multicast of successively refinedsources via noisy channelsWireless multicast for media content delivery is expected to support numerous emerging mobile applications, such as mobile IPTV, electronic advertisements, and mobile gaming, due to its one-to-many transmission in nature that is scalable to the number of receivers. Envisioning its importance, the proposed DG program investigates a novel framework of coded wireless multicast, called Scalable source coding over superposition coding (SSC-SPC), which aims to create multi-resolution transmissions via an intelligent mapping between the source and channel coding. Specifically, we plan to tackle the problems of SSC-SPC multicast in a number of interesting scenarios, including a logical SSC-SPC transmission paradigm (called SSC-L-SPC) that enables simple and standard compatible design at the receivers, as well as SSC-SPC multicast over single- and multi-relay networks in the decode-and-forward (DF), amplify-and-forward (AF), non-orthogonal AF (NAF) fashion, respectively. They are considered missing pieces in the literature and interesting to the research community. The research results are expected to gain deep understanding on the performance of the SSC-SPC transmissions under various network infrastructure supports and solidly contribute to the state-of-the-art of the related research. Based on the proposed research topics, the proposed DG program accommodates abundant research activities, by which the participated students, including constantly two PhD students and a summer co-op student, will receive sufficient training in terms of network architecture design, system and problem formulation, optimization, and case studies/simulation. They will not only learn the academic research skills, but also obtain knowledge on industry standardization progress such as Multimedia Broadcast Multicast Services (MBMS) of 3GPP and Multicast-broadcast single-frequency network (MBSFN) in LTE.""602273,""Ho, SuiKa"
"615014"	"Holmes, Reid"	"Improving Software Quality With Introspective Testing"	"The long-term objective of my research program is to help industrial software engineers improve the quality and reliability of their software systems by providing them novel tools and techniques for analyzing and strengthening their test suites.Software engineers have managed to build systems of incredible size and utility that play a crucial role in modern society; however, as these systems have grown in complexity, so has our expectations for the quality of these systems. This has resulted in increased scrutiny of software quality and intolerance for system failure; failures also have a direct economic cost, estimated at in excess of $60B per year.Developers predominantly use software testing to confirm that their systems behave as they expect. Unfortunately, the tools and techniques software engineers have at their disposal for validating the 'strength' of their test suites are insufficient for modern software systems.The research methodology employed in this program will include continual industrial involvement to ensure our approaches improve shortcomings in industrial practice and scale to the complexity of real software systems. The research program will produce novel techniques for characterizing the sensitivity of software tests, a means to assess test criticality by mapping test executions to real user executions, and mechanisms for assessing the severity of any identified faults. Finally, we will develop new ways to automatically strengthen tests that represent weaknesses in existing suites. Our approaches will be evaluated in both empirical lab studies and through validations with our industrial partners (which will comprise of both quantitative experiments and qualitative case studies).Ultimately, the goal of this research program is to help software engineers assess and improve the quality of their test suites which will in turn lead to improved overall quality for the software systems we use every day.""602312,""Holmes, Reid"
"614953"	"Hoos, Holger"	"Extending the reach of automated algorithm design, optimisation and customisation"	"Challenging computational problems arise prominently in areas such as software verification, energy systems optimisation and analysis of large amounts of data. Efficient software systems for solving these problems are of crucial importance, and improvements to these systems will have considerable economic and societal benefits, e.g., in terms of more sustainable and efficient use of energy and resources. Our research aims at automatically designing, optimising and customising such software for specific application situations.Specifically, our Programming by Optimisation (PbO) approach takes broadly applicable, general-purpose software, makes it flexible and adaptable by encouraging and exposing design choices for key components, and then exploits this flexibility by automatically adapting the software to specific application situations, using advanced machine learning and optimisation techniques. PbO has already attracted much interest in academia and industry; the research proposed here aims to take PbO to the next level, with the goal of establishing this paradigm as a standard way of designing software for computational problems across a wide range of application domains.Towards this end, we will address three major challenges arising in the context of software development using PbO (and beyond). Firstly, it can be very expensive to evaluate configurations or variants of a given piece of software - too expensive to permit direct design optimisation on sets of benchmarks representing the size and difficulty of those problem instances encountered in the intended application. Secondly, e.g., in applications dealing with sensitive data, it may be impossible to perform design optimisation as part of the development process; instead, it may have to be done post-deployment, in the actual application context, using substantially more limited computational resources. Thirdly, creating, managing and assessing design choices can be rather expensive in terms of human expert time.Our methodological work on overcoming these challenges will be guided and validated using three prominent and important applications:- software verification based on state-of-the-art SAT-modulo-theory (SMT) solvers (A1);- automated design and configuration of machine learning pipelines for analysing large amounts of data (A2); and- optimisation of software systems for generation and storage of clean energy (A3). We expect our work, which combines advances in machine learning and optimisation, to take automated algorithm design, optimisation and customisation to the next level, to have transformative impact on the design of software for these and other computationally challenging applications, thus creating very significant value within the information technology sector that produces such software and in the areas that rely on their application.""615615,""Hooshyar, Ali"
"615250"	"Hu, Ting"	"Robustness and Evolvability of Evolutionary Algorithms"	"Evolutionary algorithms draw inspiration from natural evolution. A population of diverse candidate solutions is generated and compared to a desired outcome. Then, through multiple generations of variation, selection, and reproduction, such a population adapts to the selection criteria, i.e. relative distance from the desired outcome, and produces fitter solutions. Evolutionary algorithms have seen enormous progress since they were founded in the 1970s. The knowledge of natural evolution has improved profoundly in biology in the past decades. This progress has, to a large degree, not yet been incorporated into computational models of evolution and, therefore, cannot be harvested for applications.My research program investigates new developments in biological evolution and incorporates them into designing more intelligent algorithms for more ambitious application problems. Some fundamental questions on biological evolution seem particularly interesting to me. Why are living organisms evolvable? Why isn't random variation always harmful, and how can it be the driving force for adaptive evolution? Natural evolutionary systems show tremendous resilience to genetic and environmental perturbations, as well as great capabilities in generating adaptive new phenotypes. Robustness and evolvability are fundamental properties of living systems, and investigating their relationship is key to understanding core mechanisms of evolution. These have been the focus of numerous theoretical and empirical studies in biology. However, they have not yet received adequate attention in evolutionary computing, and hold great potential in advancing our algorithm design.I propose to quantitatively characterize robustness and evolvability in evolutionary algorithms. Robustness is a result of the redundant genotype-to-phenotype mapping, where different genotypes can encode the same phenotype. That is, mutations can yield both a neutral and an observable phenotypic outcome. Investigating the relationship of robustness and evolvability can inspire the design of a new algorithm with a better representation scheme that enables both high tolerance to random variations and high adaptivity to explore novel phenotypes. This can improve the search efficiency of algorithms and advance the field of evolutionary computing profoundly. The new algorithm will then be applied to complex biomedical knowledge mining problems.Meanwhile, the findings of this research can help elucidate core mechanisms of evolution in biological systems. Evolutionary algorithms provide the possibility of studying natural evolution very differently from traditional biology. In computational evolution, up to millions of generations are allowed in a practical timeline, and the evolution process is fully controllable, tractable, and repeatable. These features go far beyond experimental studies in biology.""603793,""Hu, TingHan"
"615447"	"Hung, Patrick"	"Child Privacy Protection Engine for Smart Anthropomorphic Toys"	"Children's toys have become increasingly sophisticated over the years, with a growing shift from simple physical products to toys that engage the digital world by the use of software and hardware. In this research, a smart anthropomorphic toy is defined as a device consisting of a physical toy component in a humanoid form that connects to a computing system with online services through networking and sensory technologies to enhance the functionality of a traditional toy such as Mattel's Hello Barbie and Cognitoys Dino. This new paradigm introduces a Service Oriented Architecture approach in toy computing between the toys (service providers) and children (service requestors). Referring to the direction of the United States Federal Trade Commission Children's Online Privacy Protection Act and the European Union Data Protection Directive, this research adopts the definition of a child as an individual under the age of 13 years old. Another concern with child users is that the usage patterns of children differ from that of an adult. In this research, the assumption is that children often have little understanding or regard for the privacy of their information, and are more likely to act in spontaneous ways. The usage behaviour of children indicates that they are more open to giving out private information, which makes issues of sensitive data sharing of great concern. Many studies found that anthropomorphic toys serve a purpose, as children trusted such designs and felt at ease disclosing private information. Online privacy for children has been a great concern in this environment, particularly when the child's private information is involved and can be potentially shared with other parties. For example, a new invention called ""Google Toy"" has caused many criticisms from the media as people express concern about possible privacy breaching by Google, especially to their children at home. Privacy can result in physical safety of child user, e.g., child predators. All of these risks are increased with the possibility of child predators becoming aware of the child's location or historical location patterns. While parents strive to ensure their child's physical and online safety and privacy, there is no standardized child protection engine for parental control in this paradigm. Parental control is a feature in a smart toy for the parents to restrict the content the children can provide to the toy. The main objective of this research is on developing a privacy-aware context data model for smart toys to support a standardized child protection framework with parental controls. The framework includes an alert mechanism by applying text mining techniques to identify suspicious dialogue between children and the toys. The potential impact of this research is to provide a safe smart toy computing delivery model to protect children in Canada. This research team includes 2 Ph.D. and 2 Master students in Informatoin Technology.""614604,""Hungr, Oldrich"
"616418"	"Izquierdo, Ricardo"	"Fabrication et interfaçage de microsystèmes à l'aide des techniques d'électronique imprimée"	"Il y a présentement un très grand intérêt dans le domaine de l'électronique imprimée (EI). L'idée est qu'il serait beaucoup plus efficace en termes de vitesse de production et de cot de pouvoir simplement imprimer les dispositifs et les circuits. Le développement du domaine de l'EI est basé sur les avancements dans les matériaux et les procédés d'impression. Ainsi, les conditions de procédé et l'interaction des matériaux À travers les diverses couches du dispositif peuvent avoir une grande influence sur le fonctionnement de celui-ci. L'EI permettra de produire une large gamme de composantes qui peuvent être directement intégrées par des procédés À bas cout. Des emballages intelligents, des composantes RFID, des circuits électroniques, des affichages flexibles, de l'éclairage efficace en énergie, des dispositifs de diagnostique jetables, des mémoires, des composantes pour les communications optiques et des batteries imprimées sont quelques exemples des possibilités d'applications du champ de l'EI. Dans le cadre de ce programme de recherche, nous voulons donc utiliser les technologies de l'EI pour fabriquer les microsystèmes qui sont nécessaires À l'implémentation de plusieurs applications. Les microsystèmes tels que définis ici sont des systèmes intelligents miniaturisés qui combinent des fonctions de détection ou d'actuation À des fonctions de traitement. Ces systèmes sont généralement multifonctionnels, combinant deux ou plusieurs propriétés électriques, mécaniques, optiques, chimiques, biologiques, magnétiques ou autres. La fabrication par impression de nouveaux microsystèmes intégrant plusieurs dispositifs représente un défi largement multidisciplinaire. La performance de ces microsystèmes dépendra tout autant de la mise au point de techniques de fabrication spécifiques que de la compréhension des propriétés et des diverses interactions entre les composantes aux échelles micro et nanométriques.""604567,""Izydorczyk, Conrad"
"615817"	"Jackson, Kenneth"	"Computational Methods and Software for Applications in Science, Engineering and Finance"	"I am engaged in several research projects to develop, analyze, test and evaluate numerical methods and to construct robust mathematical software. I hope these projects will ultimately lead to improvements in mathematical software, enabling scientists, engineers and financial modellers either to solve previously intractable problems or to solve problems more efficiently and/or reliably than is currently possible.During the first phase of my academic career, my research focused primarily on numerical methods for initial-value problems (IVPs) and boundary-value problems (BVPs) for ordinary differential equations (ODEs) and the application of ODE techniques to the numerical solution of partial differential equations (PDEs). I also worked on the solution of practical problems in science, such as weather and climate prediction. However, my interests have since broadened to include applications of numerical methods to problems in finance, engineering and medicine. I have a secondary interest in numerical linear algebra, particularly problems arising from differential equations.Over the next several years, I will concentrate mainly on the following three major projects. (1) Computational Finance. I have several projects in this area. My hope is that these projects will result in improved numerical methods and software that will allow financial engineers to better price, hedge and manage the risk associated with complex financial derivatives.(2) Computational Medicine. My main on-going project in this area is the development of efficient, robust, reliable numerical methods to estimate blood-flow rates from CT scans. This will help doctors diagnose cardiovascular disease and prepare appropriate medical interventions (such as surgical plans), if necessary.(3) Extension of my earlier work on ODEs and PDEs to develop better numerical methods for - stochastic differential equations (SDEs) - stochastic biochemical kinetics (SBKs) - delay differential equations (DDEs)Better numerical methods for SBKs, for example, are needed to help scientists better understand reactions involving species that occur in low numbers within a cell, such as the reactions involving a specific gene.""612929,""Jackson, Leland"
"614854"	"Jao, David"	"Post-quantum cryptography from isogenies"	"Public-key cryptography is the foundation of internet security as we know it today, allowing for two parties to communicate securely without the need to exchange confidential key material in advance. All public key cryptosystems in widespread use today are based on either the problem of factoring large integers, or the problem of computing discrete logarithms in some group. In a seminal paper from 1994, Shor showed that both of these problems would be easy to solve on a quantum computer, one which uses quantum mechanics to perform calculations faster than any classical computer can achieve. Since then, much work has been done on the topic of constructing post-quantum public-key cryptosystems which would be secure against quantum computers. Some progress towards constructing quantum computers has been made, although no quantum computers with serious computing power have yet been built. Nevertheless, we believe it is prudent to plan ahead for future needs, because it normally takes many decades to change cryptosystem deployments, due to network effects. Recent announcements by the US government of upcoming plans to require post-quantum cryptosystems for all future US government security applications have provided new impetus to develop and deploy post-quantum cryptosystems.Our goal is to develop post-quantum cryptosystems in anticipation of the future construction of quantum computers. In particular, we aim to construct more (and more efficient) post-quantum protocols based on supersingular elliptic curve isogenies, which we believe offer several advantages compared to other approaches for post-quantum cryptography. Namely, isogeny-based cryptosystems are unique among all known post-quantum cryptosystems in the following ways: their security level is determined by a simple choice of a single public parameter; they achieve the smallest possible public key size; they are based on number-theoretic complexity assumptions; and implementations can leverage existing widely deployed software libraries to achieve necessary features such as side-channel resilience. We propose a number of research objectives intended to further improve the performance and security of isogeny-based cryptosystems. In addition, we also propose to develop new post-quantum aware security models for popular protocols for which no such security models are currently available in the literature, such as authenticated encryption and authenticated key exchange.We expect that isogeny-based cryptosystems will emerge as a viable mainstream option for post-quantum cryptography. Such systems will be much easier for end users to manage than the alternatives, and in many cases represent drop-in replacements for existing (non-post-quantum) cryptosystems. Student trainees from the project will be well-positioned to play a leading role in the future development of quantum computing and post-quantum cryptography.""613303,""Jaouad, Abdelatif"
"615619"	"Jenkin, Michael"	"Sensing and perception for autonomous agents"	"Mobility is a key aspect of intelligence. Mobility enables an agent to explore its environment, to interact with its environment, and provides a realistic definition of here versus there. But mobility also introduces complex problems associated with sensing and perception. For example, mobility introduces fundamental questions such as; where is an agent in the environment? what is its pose? and questions related to change in pose: has an agent moved? and if so, how far and in what direction?What makes answering these problems difficult for an agent is that it rarely has access to some external process that can answer these questions directly. Rather the system (typically) must rely on sensors and sensing algorithms associated with the agent itself, and must integrate this information along with intent in order to obtain a best estimate for answers to these problems. Furthermore, getting answers to these questions wrong can have catastrophic consequences for the agent itself. For example, disorientation in divers, astronauts or pilots is a common problem leading to life-threatening consequences and mission failure. These fundamental problems are found across a range of disciplines, and not surprisingly have different names in different research cultures and when different sensors and sensor collections are used. For example, the task of estimating pose changes is found in egomotion estimation, motion from vection, structure from motion, and visual odometry. Similarly, robust solutions to the estimation of self-orientation and self-motion informs a wide variety of applications, from human performance in challenging environments, to virtual reality, to the design and development of autonomous vehicles. The primary research objective of this proposal is the development of robust and effective algorithms for motion and pose estimation for autonomous agents. Although biological and machine systems have access to a wide range of potential sensors and sensing modalities to address these questions, of particular interest here is in understanding how vision, and in particular binocular vision can be coupled with inertial sensing to estimate pose and motion.The basic approach to be followed in my research program is based upon a strategy that has proven particularly effective in other aspects of intelligent machines. Models of how biological systems solve similar tasks will be used to motivate solutions for machines. These solutions will then be evaluated in the lab and then in complex unstructured environments to test both the performance of the resulting system as well as to explore the efficacy of the underlying biological model.""597393,""Jenkins, Deborah"
"615177"	"Kaminska, Bozena"	"Multi-Functional, Scalable and Controllable Nano-Structured Surfaces"	"During the last decade, the boost of nanotechnology has benefited and continues to revolutionize many fields such as energy, medicine, health, biotechnology, the environment, and security. However, the performance of available nanostructures, and the lengthy fabrication time and costs are limiting progress in this field. The mechanisms behind the emergence of nanotechnology are at the heart of my current research on advanced nano-structures and functionalised materials. I research innovations in nanotechnology to develop responsive and programmable materials that display, connect, sense, process, store, and archive information to transform the surfaces of the world into interfaces, screens, and an ""Internet of Things,"" and reconfigure the nature of human-computer interaction. This fusion of the form and materials used for information display, storage, and computation is the setting of my proposed research program that focuses on the investigation of new advanced nanostructures and innovative fabrication processes for novel devices, systems, and fabric.My work on nano-optical surfaces, screens, and interfaces is part of a longer technological approach to image-making. I aim to situate my proposed novel strategies of nanofabrication within overlapping material investigations into biomimicry, information technologies, media, and art-making processes, using programmable nano-structured material as a point of contact across disciplines. I will collaborate with biomedical and new media researchers to fully understand and explore the potential of this new class of surfaces and their functionalities for seamless sensing, connecting, data storage and archiving. I will focus on the field of nano-optics that is concerned with the nano-scale interaction between light and matter and, as with any optical venture, with expansive questions of vision and perception. My program proposed here will contribute to two key areas of active research: improving the brightness of optical devices and the elimination of viewing confusion that is heavily dependent on viewing angle; and simultaneously incorporating high density data storage and sensing capabilities into optical fabric. Such fabric will be able to connect with other devices, and the proposed nano-media surfaces incorporated into other material systems.""593853,""Kaminskyj, Susan"
"614788"	"Kapron, Bruce"	"Securing the Foundations of Security"	"Cryptographic tools have become an integral part of computer and communication security. While the use of these tools is often necessary for achieving security goals, it is also the case that schemes and protocols built upon secure cryptographic primitives may themselves fail to be secure. Recent well-publicized attacks, such as Logjam and POODLE, demonstrate that security depends on a complex interaction between mathematical security guarantees for cryptographic primitives, protocol and system design, implementation, and configuration. While there is no single magic bullet which will provide security assurance in complex information and communication systems which use cryptography, it is important to develop techniques and tools which make the interaction between the various aspects of security systems described above more transparent. My research focuses on the relationship between the first two of these aspects, with the goal of developing cryptographic and formal tools which will make it easier to verify that schemes and protocols designed using secure cryptographic primitives achieve their security goals. I will do this by applying techniques from provable security, logic, probability, computational information theory, decision theory, and optimization.""603445,""Kapteyn, EmilyQuinn"
"615906"	"Kara, Nadjia"	"Design and development of smart and open virtualized communication environments"	"IT cloud services are expected to have a compound annual growth rate (CAGR) of almost 25% each year from 2013 to 2017 (IDC forecast). In order to keep up with this rapid evolution, it is essential for service and network providers to have efficient mechanisms and tools that simplify resource provisioning and management while minimizing capital and operational expenditures. In fact, cloud service is evolving from predefined to on-demand and user-driven provisioning. Because legacy networks have many limitations, cloud service environments should be significantly scalable, reliable and cost effective. Software as a service (SaaS), Platform as a service (PaaS) and Infrastructure as a service (IaaS) are increasingly considered to achieve these cloud environment objectives. However, there are several challenges that need to be considered when defining solutions for such public cloud environment. Among them, we can list definition and validation of mechanisms that enable service level agreement (SLA)-based resource allocation, resource scaling; resource availability; resource monitoring and recovery to support dynamic, efficient and on-demand resource provisioning over public cloud where both providers and customers are satisfied. This research program addresses very challenging issues related to resource provisioning and management in distributed virtualized communication environments. New mechanisms and tools will be developed to enable flexible and efficient networks and services provisioning and management within these environments. New mathematical models and algorithms will be developed that aim at satisfying multiple objectives of virtualized communication environments. New mechanisms will be also developed that integrate configuration, provisioning and optimization of resources through federation of virtualized communication environments. This research program will provide novel approaches that ease the migration towards open and interoperable communication environments while satisfying both providers and customer expectations.""610738,""Karagiannis, Jim"
"616462"	"Kashyap, Raman"	"Laser Interaction with Materials for Applications in Optics and photonics: LIMAO"	"This proposal aims to investigate the interaction of lasers and dielectrics, especially glass, one of the most interesting materials known to man. While it has several different areas of investigation, the link nevertheless remains as laser-material interaction. Hence, the proposal seeks several answers, on the noise characteristics of stimulated Brillouin scattering in optical fibre beyond the first order, the enhancement in Rayleigh scatter in optical fibre with exposure to UV radiation which we were the first to report, the implementation of ""perfect"" meter long fibre Bragg grating with ultra-narrow bandwidths and the possibility of ""tailoring"" the optical characteristics of sources by manipulating phase and amplitude, and narrow linewidth lasers, exhibiting an outstanding performance for a low cost laser. Combining this with broadband harmonic generation with in-house built periodically poled nonlinear crystals will allow ultra-narrow wavelength transposition to new regions with ease at a far lower cost than has been possible so far. Random lasers is an area of intense curiosity not only from a scientific point of view but also from serious applications such as in chaotic communication and association with scattering effects in bio tissue. One of the next frontier in photonics integration into cell-phone glass is a challenge which needs to be fully exploited. The related area of ""Photonics on a tape"" is an area that has only just begun, and one which should yield interesting application in the clinical and laboratory environment. Exploiting the nonlinearity of chalcogenide waveguides on a tape, written with fs lasers offers exciting possibilities for combining interconnects with functionality. How this technology may be harnessed for added functionality is yet to be discovered. We also seek to find the limits of cold atom guidance in hollow-core optical fibres as well as push the boundaries of laser cooling by introducing more managable materials to the currently limited high efficiency coolers. Many aspects of multiplexing of SPR sensors have to be investigated: for example, how the guided mode coupling to the SPR may be effected remotely.Optical fiber being the conduit for light, has remained a passive device; we aim to coat it with specialist coatings such as carbon nanotubes and oriented polymers to add novel functionality for optical sensing and specially coated fibre without a buffer layer, so far unavailable in the commercial world.""608681,""Kaski, Krista"
"615833"	"Khabbazian, Majid"	"Efficient and Reliable Communications in Distributed Networks"	"The applicant's long-term research objective is to make communication (wireless or wired) more efficient (in terms of consuming resources such as power), faster, and more reliable. Three out of four proposed short-term research plans are focused on broadcasting, an important communication primitive, while the fourth short-term research plan aims to improve efficiency and reliability of distributed storage systems, where we view storage as communication in time with erasures (storage node failures).Billions of devices that use radio for communication have limited power supplies (e.g., they run on batteries). Those devices can benefit from saving in energy consumption in communication primitives such as broadcast. Also, faster and more reliable communication improves quality of service, an important criteria particularly in real time applications. Our first short-term objective is to analyze the gain of energy accumulation in reducing energy consumption of broadcast. The second short-term objective is to improve quality of service of broadcast for real time applications such as real time video streaming.Distributed wired networks can also benefit from faster broadcast. In computing clusters and data centers, many-to-many data transfer (a generalization of broadcast), can take a large percentage of applications execution time. For example, in a MapReduce framework, data transfers between mappers and reducers modeled as a many-to-many transfer, can account for more than 50% of the total running time. It is, therefore, highly desirable to devise effective algorithms to alleviate the data transfer bottleneck such as those widely reported in MapReduce.Similar techniques and ideas used for addressing erasures in wireless and wired networks have been used to analyze and handle erasures in distributed storage systems. Considering the scale of today's data centers, algorithms and codes with even slightly lower storage, bandwidth and computation requirements than the existing ones can free-up significant amount of resources, resulting in, for example, saving in maintenance cost and electricity usage, and can indirectly protect our environment. The last short-term objective of the proposed research is to reduce computational complexity of coding/encoding by finding optimal binary codes within a class of erasure codes called locally repairable codes, attractive for their simplicity.""598286,""Khabbazibasmenj, Arash"
"614929"	"King, Valerie"	"Algorithm Design for Large Graphs and Communications Networks"	"Graphs are widely used to model connections between entities, such as links in communications networks, protein interactions, and social relationships. Massive graphs like the web graph or a model of the human brain may contain billions of nodes, making them hard to analyze, and where the nodes represent different agents, hard to coordinate. This puts a burden on computing resources like processing time, memory, and bandwidth in networks.One approach to reduce time has been to recognize that such graph problems are typically ongoing, with incremental changes over time. Dynamic graph algorithms store information from prior computations so that solutions can be updated quickly as the graph changes. Another approach, when the graph is too large to fit into memory, is to create a compact representation or ""sketch"" of the graph as its edges stream into the computer and to then solve the problem on that sketch. A third approach is parallel or distributed computation, where the information about the graph is spread out over many processing nodes or the distributed network is itself a changing graph with problems to be solved.Part I of this research is concerned with developing provably correct and efficient algorithms for large, changing graphs, by using ideas from dynamic graph algorithms and streaming. I am developing a type of ""hybrid"" algorithm which can maintain a solution to a graph problem quickly (and correctly with high probability) as edges are inserted and deleted, yet keeps only a sketch of the graph in memory while doing so. Representing graph information compactly is important for saving communication costs in distributed and parallel systems as well. Ideas from streaming and dynamic graphs can be used to find algorithms for distributed and parallel systems which are communication and time efficient. As both processing time and communication consume energy, understanding the possible trade-offs between these resources may enable more energy-efficient algorithms. Algorithms and lower bounds will be investigated. Part II explores fault tolerance in distributed networks. The creation of large decentralized networks of diverse agents, such as peer-to-peer networks, has created a need for protocols which are robust to the malicious behavior of some agents and can run in an asynchronous environment. We have recently made a theoretical breakthrough in a basic problem for that scenario, Byzantine agreement. It enables nodes to come to agreement without the use of secrecy or cryptography with qualitatively stronger provably correct guarantees than currently known schemes. This research will take the next steps to make this proof of concept a practical scheme. I will also explore other applications of our techniques, which include simplifying the design of randomized Byzantine fault tolerant protocols and reducing the effects of adversarial manipulation of data in a machine learning setting. ""614325,""King, William"
"616019"	"Ko, JiHyun"	"Multi-modal brain imaging analysis method for brain modeling"	"The primary goal of the proposed research program is to develop and improve the brain imaging analysis methods. The emergence of brain imaging techniques such as positron emission tomography (PET) and functional magnetic resonance imaging (fMRI) significantly contributed to broadening our knowledge about how the brain functions. For example, with injection of [18F]fluorodeoxyglucose (FDG; the most widely used radiotracer for PET), we can locate where glucose is rapidly metabolized which is related with the level of neuronal activity in particular conditions that the subject is scanned under. Although illuminating, one of the limitations of the typical FDG-PET technique is that one cannot model the region-to-region connectivity. In other words, it is difficult to model the brain as a network where distant brain regions are communicating with each other.Utilizing the functional connectivity and graph theory on FDG-PET images, I have identified ""hubs of information flow"" in human brain. Stimulating (functionally interfering) the hub brain region may have the most influence on the brain network structure and alter how the brain processes information. Here, use of fMRI will enable personalized hub identification that is not possible with FDG-PET alone. Here, the first half of my proposed research program will focus on modelling mathematical brain networks based on large brain imaging database and validate the constructed models by ""perturbing-and-measuring"" experimental approach using non-invasive brain stimulation technique. This proposal will promote a culture of mathematical theory-experiment partnership similar to that prevailing in the physical sciences which has been lacking in the field of brain imaging.In basic neuroscience research, perhaps the biggest advantage of using PET is that one can monitor the activities of specific neurochemicals that the radiotracer is targeting. However, current approaches often require arterial blood sampling to estimate how much radiotracer is actually delivered to the brain region. This is associated with risks including arterial occlusion, bleeding and infection. By simultaneously acquiring blood flow information using hybrid PET-MRI, we will develop methods that accurately delineate arteries in the brain images thereby replacing the real blood sampling with ""image-based blood sampling."" We will also develop a method that clarifies the sources of changes in the PET outcome measures which has been often obscured by concurrently changing blood flow.In sum, my team will develop methods that overcome the challenges associated with PET and MR imaging, including but not limited to the above-mentioned difficulties. One PhD student and three MSc students will be recruited. HQPs trained in the proposed research program and their research outcome will greatly contribute to the advances of brain imaging science.""605198,""Ko, Jihyung"
"616164"	"Kranakis, Evangelos"	"Mobility, Search and Communication"	"Some of the most striking developments in mobile agent computing today are occurring due to the synergy between communication, search and mobility. Equipped with locomotion mechanisms and outfitted with hardware and software for perception, mobile robots can move and perceive the world. A mobile robot is required to navigate through a well-defined domain by controlling its motion and applying its perception capabilities in the most effective and efficient manner, while at the same time communicating its findings to the rest of the robots so as to facilitate the solution of well-defined network tasks. Current research on cooperative mobile agent (robot, sensor) systems suffers from a lack of understanding of the impact of mobility in performing fundamental communication and search tasks. More specifically, we are interested in understanding the following questions. How can a swarm of mobile agents navigate efficiently through a distributed interconnected environment to complete its computing tasks? Which agent capabilities and what knowledge can be leveraged? What is special about a particular problem that makes it solvable efficiently as opposed to alternatives?This proposal will pursue three complementary research themes within the context of mobile agent computing. 1) How do critical regimes arise when agent mobility is employed to optimize various network tasks? 2) What is the cost of optimal co-operative search in graphs and geometric domains when all tasks are finished by all agents? 3) How do interactive graphs affect the stability and convergence of population protocols in networks? This project aims to advance our knowledge by pursuing these three research directions. Topics proposed are expected to play a significant role in the development of theoretical computer science and distributed computing. Our approach will employ algorithmic design methodologies as well as probabilistic and deterministic techniques in the context of distributed computing.The general guiding principles of our approach are the following. 1) Provide a deeper understanding of the behavior of interacting communicating entities within a dynamic network based on the methodologies of algorithmic distributed computing. 2) Develop research techniques using rigorous mathematical models that satisfy strict and well-defined requirements and are motivated from practical considerations in the established research space of wireless ad hoc  sensor networks, robot search  mobility, biological distributed computing, and social networks. 3) Train Highly Qualifies Personnel at the PDF, PhD and Master's level as well as gifted undergraduates in these emerging research fields.""613815,""Krass, Dmitry"
"616343"	"Kschischang, Frank"	"Coding and Information Theory for Fiber-Optic Communications"	"We live in a connected society where digital information is continuously exchanged across the globe. The vast majority of this information is carried by optical fibres for at least part of their journey. The development of long-haul fibre-optic communications is a fascinating story of invention in which various technological advances (the development of single-mode fibre, efficient laser transmitters and modulators, optical amplifiers, wavelength-division multiplexing schemes, coherent signal detection and digital signal processing have, over time yielded steady improvements in the information-carrying capacity of commercial systems. Many natural questions arise immediately. Can this progress continue indefinitely, or are optical fibres ultimately limited in their capacity to carry information? If so, how close are we to achieving this ultimate limit? Can existing transmission schemes be improved to approach the ultimate limit more closely, while satisfying implementation-complexity constraints? These are the research questions that motivate this work. The approach taken to answer these questions is centred upon a mathematical tool called the nonlinear Fourier transform (NFT), first devised by mathematicians and physicists in the 1970s. Pulse propagation in optical fibres is, to a very good approximation, governed by a partial differential equation called the nonlinear Schroedinger equation (NLSE). The NFT does for the NLSE what the ordinary Fourier transform does for linear time-invariant systems: it changes a complicated convolution operation in the time domain to a simple multiplication operation in the frequency domain. Our key idea is to encode information for transmission over optical fibres in the nonlinear spectrum of the propagating wave, a feature that is entirely preserved from one end of the fibre to the other. This technique, which we have termed nonlinear frequency division multiplexing, can be viewed as an analogue of orthogonal frequency-division multiplexing commonly used in linear channels. Unlike most other fibre-optic data transmission schemes, this technique deals with both dispersion and nonlinearity unconditionally without the need for additional compensation methods at the transmitter or receiver.Using the properties of the NFT, we hope to achieve an understanding of the ultimate information-carrying capacity of optical fibres. We plan to study a recently-discovered spatially and temporally discrete version of the NLSE called the Tsuchida Discretization, which is NFT-compatible and from which we hope to gain significant insights. We also plan to study optical and digital signal processing techniques in the nonlinear spectral domain. Our research results will guide us in the design of new communication schemes that are directly optimized for resilience against all fibre-optic channel impairments, both linear and nonlinear.""598162,""Kschischang, Frank"
"616396"	"Kumar, Shiva"	"Nonlinear effects in photonic devices and systems"	"Nonlinear effects in photonic devices play an important role in the design of fiber optic networks, imaging systems and microcavites. In applications such as fiber optic networks, these effects are detrimental whereas they can be beneficial in imaging systems and microcavities. This proposal develops techniques to mitigate them when they cause problems and to enhance them when they provide benefits. A fiber-optic network forms the backbone of the internet and as demand for video services grows, the capacity of this network must increase. Successful mitigation of the linear effects using the equalizers, transmission speeds have increased from several Mb/s to hundreds of Tb/s. Now the fundamental impediments to increases in capacity and reach that will be required to satisfy the future demands are those generated by nonlinear effects. In this proposal, digital and optical techniques will be developed to mitigate these effects in single mode and few mode fibers. We will also explore the possibility of constructing multiple-input multiple-output systems with inexpensive light sources and direct detection receivers. The subscribers have the potential to greatly reduce the cost per bit of fiber optic networks.Nonlinear imaging techniques are used for identification of molecules in biomedical applications. When a pump wave and a Stokes wave are incident on a sample, the degree of amplification of the Stokes wave provides the fingerprint by which the molecule can be identified. Typically, direct detection receivers are used in imaging systems which do not have information on the phase. We propose to combine the nonlinear imaging with coherent technology so that the phase of optical signal is available at the receiver. This enables construction of a 3-D vibrational image of a molecule. Coherent detection enables the back propagation techniques akin to those developed for fiber optic systems. These techniques have the potential to remove unwanted background and improve the quality of the image. Optical microcavities have been used for Raman lasers and sensing applications. In this proposal, we will investigate the nonlinear effects on optical modes in microcavity. The microcavity supports acoustic modes and the study of acousto-optic interaction in microcavity could lead to interesting new applications, such as acoustic lasers. We will explore the possibility of constructing ultralow threshold Raman lasers and acoustic lasers by optimizing the device geometry. As suggested by some of the proponent's previous/existing interactions with industrial partners, Canadian companies in the communications, sensing and biomedical fields could benefit from the techniques developed in this program. Furthermore, the students trained in the program will develop highly transferable skills that will be of interest not only to companies that are active in these fields, but other high technology companies, too.""594352,""Kumar, Vivekanandan"
"616111"	"Kutulakos, Kiriakos"	"Transport-Aware Imaging for Computer Vision"	"The main goal of my research program for the next five years will be to study, design, and use a new class of computational cameras whose key property is that they are ""transport-aware."" Unlike conventional cameras which record all incident light, transport-aware cameras can be programmed to block some of that light, based on the actual 3D paths it followed through a scene. Transport-aware cameras use a programmable light source for illumination and a programmable sensor mask for imaging, and were pioneered by my research group three years ago. Live video from a transport-aware camera can offer a very unconventional view of our everyday world in which refraction and scattering can be selectively blocked or enhanced, visual structures too subtle to notice with the naked eye can become apparent, and object surfaces can be reconstructed in 3D under challenging conditions well beyond the state of the art. Our initial work on harnessing their power brings together computer vision, computer graphics, numerical optimization and geometric optics to demonstrate never-seen-before capabilities with potential applications in 3D shape acquisition; robust time-of-flight imaging; visual material analysis; scene understanding; robotic navigation; industrial inspection; and medical/scientific imaging among others.Our program seeks to unlock the full potential of these cameras by (1) generalizing the concept of transport-aware imaging and advancing its mathematical underpinnings; (2) designing and building novel transport-aware camera systems; and (3) exploiting these newfound sources of visual input to solve challenging 3D and 2D visual inference tasks.""595145,""Kutz, Susan"
"616165"	"Labiche, Yvan"	"Category partition black-box software testing: theory, tool support and experiments"	"Category partition (CP) is a black-box software testing technique based on equivalence class partitioning and boundary value analysis. It applies to many contexts: e.g., C unit testing, Java unit/integration testing, system testing of use cases. It has also been combined with other techniques such as state-based testing from extended finite state machines.CP begins by identifying the parameters and environment variables of a functionality under test. Environment variables are factors in the environment of execution of the program under test that may impact its behaviour. These parameters and environment variables are then envisioned into categories, which are characteristics that are deemed important from a testing viewpoint. Each characteristic leads to the definition of choices (i.e., equivalent classes) splitting the domain of values (implicitly) defined by the characteristic. Constraints can then specify that some choices from two different categories should always be used together, can never be used together, or can only be used together under certain condition. The choices are then combined to form test frames according to a selection criterion, while satisfying constraints. A simple adequacy criterion requires that each choice appears at least once in the set of test frames. Each test frame is then associated with actual input values (for parameters and environment variables), according to the choice specifications, to produce test cases. When using CP, one needs to take many decisions that can impact the result (e.g., cost, fault detection) of the testing campaign. Obviously the selection of choices has an impact: e.g., poorly defined categories and choices will lead to faults that slip through the testing activity; Choosing an adequacy criterion matters because criteria are not all equally demanding (i.e., cost) or effective (i.e., fault detection); The technology to produce a set of test frames adequate for a criterion, while accounting for constraints, will have an impact; The procedure to identify input values for test frames may have an impact.Unfortunately, very little is known about the actual, precise impact of those decisions on the result of applying CP.The purpose of this project is to fill this gap. Tool support will be created to facilitate the use of CP and alternative decisions, on different case studies. Real and synthetic faults will be used to evaluate the impact of the many alternatives on the effectiveness of the testing technique at finding faults.The results will be of high interest to the research community: it is the first time the many alternative applications of CP will be studied. Results will also be highly relevant to industry: tool support, experimental evidence leading to better decision making. Results will be highly interesting for teaching purposes: tailoring the teaching of the technique, availability of the tool, including an online version.""600068,""Labiche, Yvan"
"615363"	"Laganiere, Robert"	"Methods and algorithms for multi-camera visual surveillance and monitoring"	"The objective of this research program is to devise new methods and algorithms for the development of intelligent surveillance systems. The research will focus on the necessary components required to build the next generation of video monitoring systems capable of extracting high-level information on occurring events and give users an easier access to the information collected by such systems. The emphasis will be on solutions that can be integrated to real-time surveillance systems. The solutions sought will perform object-based video analytics for the detection, the tracking and re-identification of specific objects in the scene and the recognition of their actions. More than the development of these individual modules, our research program aims at the seamless integration of these four vision components into real-time video surveillance and monitoring systems. In order to validate, apply and expand this research, our research will be performed in the context of an advanced surveillance system for tracking the behavior of customers in store and restaurants.The remarkable advances in object detection methods make them good candidates for object extraction in videos. In the context of video monitoring, these ones can be designed to operate at a very low false positive rate even if this leads to an important miss rate. Working with videos allow us to recover objects that were missed in previous frames. Object position in-between detections can be obtained, when required, from a visual tracker. Longer term tracking, over multiple cameras, can be achieved through a re-identification module that can re-detect a previously observed moving object. Finally, the last essential ingredient for a complete understanding of a scene's dynamic is the detection of the actions undertaken by the actors of the scene. These four components represent the fundamental building blocks of an advanced video analytics system. We foresee that the proposed research program will lead to the development of new specialized video analytics systems. Based on our current collaborations, we specifically target applications in customer tracking, or more generally event detection and behaviour analysis in public places. Although most of this research has a focus on the interpretation of videos captured by fixed cameras, we intent to also consider the case of moving cameras such as action cameras and wearable cameras.""603341,""Laganière, Robert"
"614526"	"Langer, Michael"	"Computational Perception of Surface Material and 3D Spatial Layout"	"A fundamental task in visual perception is to estimate the spatial layout of a scene, for example, the slope of the ground terrain on which the observer can move and the positions of any obstacles such an objects or walls. Standard computational models for solving this layout estimation problem have concentrated on scenes that have matte reflectance, but scenes with glossy reflecting surfaces have received relatively little attention. Important examples include navigating outdoor street scenes in the rain at night, which is challenging because such scenes are dominated by specular highlights from street lamps and car head lamps. Other scene examples with glossy surfaces include lake or ocean surfaces that are viewed from a boat or from a low flying aerial vehicle, or indoor scenes with polished floors, glossy painted or glass walls, or windows. This research will address how to interpret the specular highlights and mirror reflections that arise from large near-planar glossy surfaces. The approach will be highly interdisciplinary, addressing problem in robotic vision, human visual perception, and applied perception in computer graphics. A key goal in the applied perception problems in particular is to understand how well people perceive layout of glossy planar surfaces in virtual environments that are viewed using binocular, head-tracked, wide field of view displays such as the Oculus Rift. Such displays yield remarkable immersion effects, and it is likely that they will revolutionize how we experience real and virtual worlds. This research should provide insight into the strengths and limitations of these displays for conveying spatial layout and material information about glossy scenes.""597437,""Langerak, Robin"
"601182"	"LHeureux, Alexandra"	"Ontario"	"CANADA"
"614553"	"Li, Ming"	"Kolmogorov complexity and its applications"	"I am interested in developing a compelling theory of big data. Such a theory will depend on Kolmogorov complexity and information distance. Kolmogorov complexity is defined on one object. Information distance [C. Bennett, P. Gacs, M. Li, P. Vitanyi, W. Zurek, Information distance, IEEE Tran-IT, 44:4(1998)] is defined on two objects. This concept can be generalized to many objects. Using such a theory it is possible to optimally approximate the intuitive concept of ""semantic distance"" or closeness of two piece of data, in general. The key to this theory is to compress the data. Many ways of compressing data will be studied, including error encoding, clustering, and especially deep neural networks. Deep neural networks can be considered as ways of compressing data, especially big data. The following short-term goals are in tune with the above main theme of this research: 1) Deep learning in natural language processing (NLP). My group has trained a Convolutional Neural Network (CNN) to map natural language questions to a database structured query with a limited number of relations. This work will continue. My group also has trained a Recurrent Neural Network (RNN) for conversation or chatting. This work will be extended to context sensitive chatting. This work will have two implications with the long term goal: a) Neural network will be studied as one way to approximate semantic distance; and b) Only with big data from the internet, this approach is practically useful.2) Bioinformatics. A CNN has also been trained for protein identification as well as for peak-picking in mass spectrometry protein quantitation. These studies and methodologies will be extended to protein quantitation. This work again depends on huge amount of training data I have obtained from industry. These deep learning approaches will not be studied in isolation. They will be studied together with my theory of approximating semantic distance by information distance, experimenting with the efficiency of using deep neural networks as compression methods to deal with big data when there are no clear rules of compressing. I will also spend 8 months full time to revise his research book with Paul Vitanyi ""An introduction to Kolmogorov complexity and its applications"", that will include these new results. Several other short-term topics in bioinformatics will be studied. One is an antibody sequencing algorithm. I plan to design a new algorithm using linear programming to solve a bioinformatics industrial problem of antibody sequencing. Another problem is to apply the ideas in bioinformatics to other fields: optimal spaced seeds were invented by my group to do homology search. This has been considered one of the most influential innovations in bioinformatics during the last 15 years. I have the idea of using the optimal spaced seed idea to develop an observation theory to detect the trends in time series. Initial experiments were performed successfully.""616857,""Li, Ming"
"614862"	"Lin, Jimmy"	"Modeling Time, Space, and Networks for Effective and Efficient Information Retrieval"	"Citizens of modern societies are inundated with increasing quantities of information with no relief in sight. Search remains one of the best solutions today for combating this deluge. In today's mobile world, search systems need to understand time, space, and networks as they pertain to user needs: for example, we demand the most up-to-date facts, prefer nearby locations, and ask friends for help. Similarly, search systems need to model these important aspects of context: documents are created at specific times and locations, often with well-defined relationships to other documents (e.g., hyperlinks on the web, reply to a tweet).My research program lies at the intersection of information retrieval and big data, and aims to build tools that help users find relevant information in large text collections while tackling fundamental challenges in computer and information science. My focus is to explore the rich interactions between time, space, and networks in building search systems that are both effective (i.e., return high quality results) and efficient (i.e., exhibit low query latencies, small memory footprints, etc.). I propose an integrated approach that will make contributions to efficient and scalable indexes for querying documents that have temporal, spatial, and network metadata; rich contextual models of relevance for document ranking and predictive user modeling; and cross-platform interfaces that support real-world information seeking.These capabilities will be demonstrated in two main applications: (1) Real-time information filtering on social media streams - a journalist might be interested in ""tracking"" a stream of social media posts for updates about a particular topic, e.g., the European refugee crisis. (2) Scholarly analysis of web archives - web archives contain temporal snapshots of web content and offer valuable resources for digital historians studying the recent past or social scientists exploring how political debates evolve.The impact of this research program is many-fold: A better understanding of the roles that time, space, and networks play in search will inform the design of future search engines for complex tasks. With substantial previous experience at Twitter building production ""big data"" analytics infrastructure and user-facing products, I have a strong record of bridging the divide between academia and industry; these experiences will assist in successful technology transfer. My former students and post-doctoral researchers possess highly-coveted skills, knowledge, and experience in working with big data, and my research program will continue training valuable contributors to modern information-centric economies in Canada and around the world.""602306,""Lin, Jimmy"
"614948"	"Lis, Mieszko"	"Memory coherence for emerging datacentre applications"	"OBJECTIVESIn this project we propose to develop a family of latency-tolerant, bandwidth-sensitive, and footprint-limited memory coherence protocols. Unlike traditional protocols, which are designed for homogeneous systems with custom fast interconnects, our protocols will operate efficiently in today's complex heterogeneous systems where inter-component latencies can vary by two orders of magnitude.We will investigate latency-tolerant coherence in two emerging datacentre contexts: fine-grained programmable accelerators and oblivious computation. Because coherence is key to the adoption of both, the proposed research has the potential for significant advancement in both areas.APPLICATION DOMAINSProgrammable accelerators have already been deployed on an industrial scale in such datacentre applications as web search (e.g., by Microsoft), and Intel's recent acquisition of Altera suggests that fine-grained programmable fabrics could soon be included in commodity processors. Compared to commodity CPUs, however, accelerators are much more difficult to program; we see this as arising largely from lack of effective support for coherent shared memory, the de facto enabling technology of parallel programming. This difficulty has formed a significant barrier to adoption despite the potential for significant performance and energy advantages offered by accelerators: it is difficult to justify putting months into developing accelerators for web search (e.g., Microsoft Catapult) when the machine learning models used to rank search results change several times a week.At the same time, enterprises with cloud-computing interests are increasingly concerned about security. Trace-oblivious computing allows a client to securely compute in the cloud with the provider unable to snoop on the computation via temporal or spatial access patterns, but does so only at the cost of an order-of-magnitude penalty to performance. To overcome this cost, oblivious computation must leverage parallelism, and this, in turn, requires a secure variant of coherent shared memory.The coherence techniques we propose will (a) make emerging accelerator fabrics far easier to program and dramatically reduce design cycles; and (b) enable parallelism in oblivious computing and allow its adoption in datacentres.IMPACTDatacentres are everywhere: even an enormous cloud provider like Google accounts for less than 1 of total datacentre computing globally, with most datacentres widely distributed in diverse industrial, governmental, and academic settings in Canada and abroad. The proposed research will have lasting impact on all of them: firstly, by speeding adoption of energy-efficient heterogeneous computing fabrics, it will allow datacentres to scale existing computing products; secondly, by enabling secure computing in the cloud, it will allow datacentres to expand to new application domains.""612411,""Liscano, Ramiro"
"614771"	"Ma, Bin"	"Bioinformatics Algorithms and Software for Proteomics"	"Proteomics aims to study all proteins in an organism, and is reshaping the health research and pharmaceutical industry. One important application of proteomics is the discovery of protein biomarkers that can be used for disease diagnoses and personalized medicine. The biomarkers also become excellent drug targets. One of the most active areas in cancer treatment today is the antibody-drug conjugates that use an antibody to recognize the biomarkers on the surface of cancer cells, and deliver the drug specifically to the cancer cells.Mass spectrometry has become the primary method in proteomics for protein characterization. One mass spectrometry experiment can produce up to millions of mass spectra. Then, sophisticated computer algorithms are used to analyze the data and find out the proteins' identities, primary sequences, post-translational modifications (PTM), single amino acid polymorphisms (SAP), as well as quantities.My proposed research program aims to develop the algorithms and software tools that facilitate the in-depth proteome characterization using mass spectrometry. Many of these tools will enable new capabilities for proteomics. In particular, the ultimate long-term goal of this research program is to enable the whole proteome sequencing, where all abundant proteins in a proteome, including the PTM and SAP, can be sequenced de novo in a high-throughput mass spectrometry experiment. The breakthrough will likely come from a joint innovation that involves a vast improvement of spectrometers, a novel experimental design, and innovative computer algorithms. The achievement of this goal will fundamentally change the way scientists conduct biological and health researches - just like how genome sequencing did in the past two decades. My research group has accumulated significant expertise and built a broad collaboration network with mass spectrometry labs and industrial partners. This puts us at a good position in making critical contributions to the whole proteome sequencing goal. The success will strengthen Canada's leading position in the international proteomics research.The proteome sequencing goal may carry beyond the proposed renewal term of this NSERC Discovery program. By the end of the proposed term, we expect to have a method that can routinely de novo sequence a purified protein with mass spectrometry in minutes. This already has a profound impact to the whole proteomics field and pharmaceutical industry. Other shorter term goals include real-time PTM and SAP detection, protein quantification, as well as making the mass spectrometers smarter by result-dependent acquisition.The accomplishment of these goals will inevitably require the participation of many talented graduate students. Many HQP will be trained in a cross-disciplinary environment. I envision that many of them will contribute innovative ideas, and grow to be independent researchers in the end.""603725,""Ma, Christina"
"615539"	"Mahajan, Aditya"	"Decentralized stochastic control: information structures, communication, and learning"	"The objective of the proposed research program is to develop a systematic methodology for the optimal control of decentralized systems where all agents have a common objective. Such systems arise in almost all modern technologies, including networked control systems, communication networks, sensor and surveillance networks, environmental remote sensing, smart grids, transportation networks, monitoring and diagnostic systems, and robotics. The reason that decentralized control problems arise in these varied applications is that it is not always technologically or economically feasible to share all pertinent information with a single agent. While this is true, it is feasible to share some pertinent information among a subset of agents. In this research program we investigate how such information sharing simplifies the design of decentralized systems.This proposal maps a five year research program to address two fundamental questions: (Q1) Suppose, before the system starts running, there are resources to build a fixed number of communication links between agents. Which links should be built? (Q2) Suppose, once the system starts running, agents can communicate information to other agents at a cost. When and what should the agents communicate and to whom? To answer these questions, three directions will be pursued: (1) Identify new classes of information structures that lead to a tractable design. (2) Identify optimal transmission strategies that determine when an agent should communicate information to others. (3) Develop learning and adaptation strategies for the above two directions when the system model is not known. The proposed research program will provide broad training to 4 PhD and 3 MEng students in fundamental areas of system science and control engineering, thereby providing them with a solid foundation for their future endeavors. The results obtained in this program will find applications in networked control systems, transportation networks, smart-grids, sensor and surveillance networks, healthcare monitoring, and robotics.""601008,""Mahapatra, Chinmaya"
"616176"	"Maheshwari, Anil"	"Design and Analysis of Algorithms for Problems in Computational Geometry"	"I study geometric problems within the framework of the design and analysis of algorithms. My long-term research objectives are to design, analyze and implement algorithms for problems that require spatial information and (a) are fundamental, (b) have practical significance, (c) are theoretically challenging and have the potential to raise new questions, (d) contribute to an understanding of a methodology or concept, and (e) lead to the development of generic techniques and tools that can be applied to other problems. This requires developing efficient algorithmic solutions, establishing combinatorial and geometric properties, designing appropriate data structures, providing correctness proofs and complexity bounds, and possibly implementations. In the short term, my research focus is in three interrelated areas: geometric graphs, geometric location analysis and geometric path problems. My work in geometric graphs is centred around establishing graph theoretic and geometric properties of the underlying spatial configuration that can lead to efficient algorithmic solutions. In geometric location analysis, we design algorithms to determine an optimal location for facilities. I plan to continue my research work on weighted shortest paths by exploring variants and applications in two and three-dimensional settings.As an illustration of my research directions consider the unit-disk graphs (UDG). UDG is a well-known class of geometric graphs that is often used to model the topology of ad hoc wireless communication networks. Each node, modelling an omni-directional antennae in the network, is viewed as a point in a plane. There is an edge between two nodes if the corresponding points are within a unit distance (signal strength outside this range is considered feeble). A signal can travel from one node to another if there are intermediate nodes between them so that a signal can hop from one node to other, each within a unit distance of its predecessor until it reaches the destination node. To check the connectivity between any pair of nodes, we can use a graph connectivity algorithm, and the time required is proportional to the size of the graph. Depending on the spatial configuration of the input nodes, a UDG can be anywhere from being very sparse to very dense. Therefore, the runtime of the configurations corresponding to the dense graphs is significantly higher than those corresponding to sparse graphs. By exploiting the geometric properties, we can design an algorithm to test the connectivity of UDG and ensure that its run-time is independent of the number of edges. Fundamental questions such as how to efficiently find a shortest hop path between a pair of query nodes, how to efficiently determine the diameter, and how to efficiently determine various graph parameters related to coverage, connectivity and interference questions in UDGs and the associated wireless networks remain unresolved. ""595918,""Maheshwari, Vivek"
"615116"	"Maier, Martin"	"Bringing the Cloud to the Edge in FiWi Enhanced Networks"	"To cope with the unprecedented growth of mobile data traffic, we recently have demonstrated the significant latency and reliability performance gains obtained from unifying coverage-centric 4G mobile networks and capacity-centric fiber-wireless (FiWi) broadband access networks based on low-cost data-centric Ethernet technologies, paying particular attention to the performance-limiting impact of backhaul latency and reliability. FiWi enhanced 4G mobile networks may be viewed as an important step towards meeting the requirements of very low latency and ultra-high reliability while taking economic considerations into account and following the integrative vision of future 5G mobile networks. In our previous work, we also paid attention to the development of a decentralized routing algorithm for FiWi enhanced mobile networks by exploiting intelligence at the device rather than infrastructure side given that decentralization is another important aspect of the 5G vision. So-called edge services are expected to become increasingly important as the Internet transforms into the Internet of Things. There is a growing awareness among industry players of reaping the benefits of mobile-cloud convergence by extending today's unmodified cloud to a decentralized two-level cloud-cloudlet architecture based on emerging mobile-edge computing capabilities, thus enabling new applications that are both compute intensive and latency sensitive, e.g., cognitive assistance, augmented reality, or tactile Internet applications such as face recognition and navigation for emerging cloud robotics. Recently, a similar trend toward decentralization can be observed for the changing role of the mobile base station and the compelling opportunity for it to become a hub of service creation in order to monetize from the provision of an enriched mobile broadband experience that is characterized by low-latency, contextualized, and highly personalized unforeseen applications and services that help stimulate innovation, generate revenue, and improve the quality of our every-day lives. This research proposal explores the challenges and opportunities arising from the integration of cloudlets and FiWi enhanced networks with advanced mobile-edge computing capabilities. It investigates their instrumental role in ushering in the future Tactile Internet with a particular focus on groundbreaking cloud-enabled networked robotic services and applications. In a long-term vision, the proposed research activities aim at exploiting FiWi enabled human-to-robot communications to help merge and recombine the mobile Internet, automation of knowledge work, Internet of Things, cloud technology, and advanced robotics, which represent the five technologies with the highest estimated potential economic impact in 2025, in order to multiply their beneficial impact on society.""594304,""Maijer, Daan"
"616385"	"Makarenkov, Vladimir"	"New algorithms and software for analyzing and classifying evolutionary and biomedical data"	"My research proposal involves five major components related to the development of new algorithms and software for analyzing and classifying evolutionary and biomedical data.First, we will continue to investigate the phenomenon of reticulate evolution in the context of horizontal gene transfer (HGT). We propose to design original and effective maximum likelihood algorithms for inferring and validating statistically both complete and partial HGT events as well as for determining types (i.e., whether the transferred gene is additive, replacing or recombining) and units (i.e., whether gene transfer involves gene fragments, whole genes or entire operons) of gene transfers. The proposed algorithms will be used to estimate the impact of HGT on the resistance of bacteria to antibiotics - a topic of particular interest to the Canadian healthcare system and pharmaceutical industry. Furthermore, we will develop and maintain an up-to-date database dedicated to gene transfer networks of antibiotic resistance genes.Second, we will design a novel bioinformatics framework for estimating and validating the rates of complete and partial HGT among prokaryotes at different phylogenetic and ecological levels. It will allow researchers to determine the proportion of mosaic genes in prokaryotic genomes, to identify prokaryotic families and habitats being the major donors and recipients of genetic material, and to assess the ages of the detected HGT events.Third, we will propose a novel maximum likelihood method for identifying diploid hybridization events, including statistical validation of the detected hybrids and their parents by bootstrap analysis. This method will be extended to determine whether the relationship among the given species should be represented by a phylogenetic tree or by a hybridization network. Such methods will be of significant interest to a large community of plant and fish biologists.Fourth, we will design novel algorithms and a new statistical test for analyzing and correcting experimental high-throughput screening (HTS) data. This test will identify the type of systematic bias affecting a given HTS assay (i.e., additive or multiplicative bias). The new algorithms will be used to detect and eliminate multiplicative type of systematic bias in experimental HTS. Moreover, a novel data processing protocol for optimizing hit selection process will be introduced. The proposed methods will allow researchers to minimize the impact of systematic error in experimental HTS and thus improve the selection of potential drug candidates.Finally, we will develop open-source software to allow academic and industrial researchers throughout the world to carry out our new algorithms for detecting, validating and visualizing HGT and hybridization events, inferring and examining gene transfer networks of antibiotic resistant genes, and correcting and analyzing experimental HTS assays.""603664,""Makaro, Tyler"
"616003"	"Marchand, Mario"	"Towards more efficient machine learning algorithms: theory and practice"	"Machine learning is concerned with the development of intelligent computer systems that are able to learn and generalize from collected data. This ability is required for many important tasks that are just too complex to be explicitly programmed by humans such as recognizing voice directives on smart phones, or recognizing faces in images, or displaying the best results from a query. The best computer systems that achieve these tasks have in common the fact that their ability has been acquired by running learning algorithms on vast amounts of data. Many of these tasks are now so vital to our economy that machine learning-based technology is now commonplace. However, that technology needs to be greatly improved. Indeed, credit cards are too frequently being blocked, we are still receiving too many undesirable emails, and automatic speech recognition is still not satisfactory. Building more powerful hardware is just part of the solution as we must also find the most efficient learning algorithms. Consequently, the proposed research program aims at addressing the important problem of how to improve and modify existing learning algorithms such that they yield better predictors while using a minimal (or, at least, an acceptable) amount of resources. To meet this objective we plan to approach challenging machine-learning problems from both a theoretical and a practical perspective. Theoretical analysis is needed because we want to find efficient learning algorithms with provable guarantees. More specifically, we plan, in the short term, to improve the existing learning algorithms for structured output prediction and domain adaptation. These are currently two challenging machine-learning problems that model situations often encountered in practice but for which we do not have yet satisfactory learning algorithms. We also plan to find the most efficient boosting-type algorithms for learning from large-scale data sets. Finally, in the long term, we plan to expand the set covering machine and the decision list machine for predicting phenotypes from genomic data. These are learning algorithms (that we have proposed in the past) that produce uncharacteristically sparse predictors that are easy to interpret. Hence, these predictors can, in principle, help us to uncover the genomic cause of a phenotype. The impressive results we have obtained recently on the prediction of antibiotic resistance from bacterial genomes motivate us to consider, in the short term, the more ambitious problem of predicting cancer types from human sequences, which could be DNA, RNA, or protein sequences. It is thus expected that this research program will yield provably efficient learning methods for different machine learning-based applications that, in turn, will improve our quality of life. Moreover, it is expected that this research program will deliver the equivalent of two M.Sc graduates and four Ph.D graduates.""593066,""Marchand, Richard"
"601233"	"Mathewson, Kory"	"Alberta"	"CANADA"
"615008"	"McIntosh, Shane"	"Leveraging the Build System to Support Modern Software Release Practices"	"Context: Modern software is developed at a rapid pace. Indeed, software organizations like Google, LinkedIn, and Facebook release several times daily. To cope with the rapid rate at which modern software changes, software organizations dedicate personnel to the task of developing and maintaining tools and infrastructure that automate the software release process. This so-called release pipeline, which processes changes to the source code of a software system as input, is typically composed of three phases:(1) Integration: Code changes are assessed for risk, since riskier changes may introduce defects that can slow or halt product sales, impact the reputation of an organization, or even expose an organization to potential litigation. Risky code changes require additional effort from quality assurance teams before they can be confidently released to users or customers. (2) Build: Code changes are processed by the build system, i.e., the system that invokes software tools (e.g., compilers, interpreters) in order to update system deliverables (e.g., executables) and test them for regression in system behaviour.(3) Deployment: Official releases of software systems are made available for users or customers to interact with or acquire.Overarching goal and specific objectives: The rapid release cycle of modern software systems introduces new challenges for the teams that develop and maintain release pipelines within software organizations. The long term goal of this research program is to improve the efficiency and robustness of release pipelines by gleaning actionable insights from build systems. Through analysis of build system data, we aim to improve each of the three common phases of the release process.(1) Integration: More accurate assessment of the risk of code change integration through analysis of customer exposure, i.e., the proportion of the customer base that may be impacted by a given code change. (2) Build: Optimizing the use of dedicated build machines through build duration forecasts.(3) Deployment: An understanding of the evolution of the code that specifies how the deployment phase is executed, and its co-evolution with other release pipeline specifications.Outcomes and impact: Release engineers increase the market potential of software organizations. Investment in release engineering has allowed the relatively small Mozilla organization (the developers of the Firefox web browser) to compete with software giants like Microsoft, Google, and Apple. This research program will produce 10 HQP (2 PhD, 3 MEng, 5 BEng) through hands-on training in large-scale empirical analyses and release engineering. The HQP will be well-equipped for a research or private sector career in the field of release engineering, increasing Canada's innovation potential in this high impact area of software engineering.""603089,""McIntyre, Bryce"
"615395"	"Meger, David"	"Learning Adaptive Sensorimotor Representations"	"A core competency of any intelligent physical system is its ability to operate successfully in a wide range of environments and to adapt autonomously to changes in task conditions. Current state-of-the-art systems for robot perception and control are increasingly able to overcome challenging environmental factors, typically through the use of ""big-data"" and machine learning techniques, but these systems require extensive human engineering and data annotation and are typically not flexible enough to overcome task variations without hands-on interaction from their human designer.My research will focus on what I believe to be a core challenge of intelligent robotics - the automated learning of representations that link a robot's sensors and actuators to achieve adaptive solutions for perception and control tasks. I believe this can be accomplished by combining the state-of-the-art in learning visual representations (e.g., deep representations now used for recognizing objects) and in adaptive control (e.g., policy-gradient approaches now used to learn motor skills). The key assumption of this research direction is that an automated intermediate representation will more flexibly capture the correlations between perception and control than those chosen by a human designer. The key advantage, if the research is successful, will be robots that require drastically reduced setup and training to perform new tasks successfully.A motivational example for this work is an autonomous robotic chef. Today, we can feasibly produce robots to cook a small variety of meals in a specially-designed kitchen. One can visit Japan to find examples of this technology. However, it would require enormous effort to produce a robot that could cook even a single meal in a kitchen it had not seen before. The main challenge would be the robot's inability to generalize its knowledge to differences in the placement of items and their properties (e.g., my knife is slightly smaller and is kept in a different drawer). This research program will improve a robot's ability to handle new environments with a reduced amount of effort from human engineers. Beyond cooking, we will target applications to society, such as disaster rescue, and industry, such as mining and agriculture.""603286,""Meggison, Brett"
"615183"	"Mesbah, Ali"	"Analyzing Tests for Correctness, Adequacy, and Effectiveness"	"Software developers continuously apply code changes to add new features, improve the product, or fix known bugs. For instance, 50% of the Google code base is reported to change every month. In order to ensure that the code base remains healthy, test cases are written to verify that the production code functions as expected. Such tests also serve as continuous regression tests to ensure that previously working functionality still works, when the software evolves. Inadequate testing costs the US economy $60 billion annually. A more recent study by Cambridge University finds that software bugs cost the global economy $312 billion per year. The study also found that, on average, software developers spend 50% of their programming time finding and fixing bugs. Test cases are meant to form the first line of defence against the introduction of faults in a software product, especially when retesting modified code. Therefore, assessing the quality of a test suite is crucial to ensure that software systems are correctly implemented, which can help to decrease the overall development and maintenance cost.Test quality can be considered in different perspectives. We propose to study the correctness, adequacy, and effectiveness of test cases, which directly impact the overall test suite quality. The objectives of this research proposal are to (1) study test quality issues related to test correctness, test adequacy, and test effectiveness in practice, (2) explore novel techniques for analyzing and assessing the quality of test cases, (3) and propose new solutions for increasing the quality of test cases directly, and the production code being tested as a result. Although the quality of test code is as important as production code, unlike production code, the quality of tests written by developers has not received much attention from the research community thus far, especially when it comes to the correctness of test cases. This proposal seeks to study and analyze the quality of tests in depth. The results of this work will have positive implications for both software testers and developers. The techniques proposed will make test cases more correct, adequate, and effective and as a result the quality of the production code can be improved and the software can be released with much more confident to end-users.""594689,""Mesfioui, Mhamed"
"615494"	"Michaud, François"	"53"	"Québec"
"615458"	"Michaud, JeanBaptiste"	"Artificial intelligence, signal and data processing towards greater end-user flexibility through increased sensitivity and detection efficiency in PET imaging"	"This proposal is part of a novel vision where digital signal and data processing, artificial intelligence and distributed modular analysis can substantially improve both the intrinsic performance and end-user flexibility of Positron Emission Tomography (PET), a vision symbiotic and interdependent on the usual advances in technology, comprehension of system-level issues and socio-economic efficiency constraints. In the short term, the research program will mitigate the pressure on the flexibility paradigm emerging as side-effects of other resolution, TOF and detector breakthroughs. In the mid to long term, the vision will uniquely benefit all PET end-users through innovative complements to existing technology, helping this molecular and genomic imaging modality continue to grow in the next decades through the emergence of new radiotracers or new routine care and research protocols. Depending on scanner performance like resolution, contrast and sensitivity, the end-user faces a significant trade-off between image quality, injected dose and scan length, directly impacting the flexibility, scientific return and cost/benefit ratio of any given imaging protocol, all of primordial importance in the continued availability of the modality in healthcare worldwide. Some new protocols will require high sensitivity, others good contrast and resolution, or both: the cost effective evolution of PET requires flexibility, which an all-digital architecture and the aforementioned vision inherently provide. The program focuses on sensitivity and detection efficiency because, when considering the above end-user trade-off, these are currently the most under-studied and wide-reaching factors through which end-user flexibility can be gained quickly. Varied interwoven aspects of PET technology will immediately benefit from the proposed vision: 1) optimization of a novel artificial intelligence method I recently demonstrated to improve detection efficiency by 50 to 200%; 2) refinement of image reconstruction methods to account for the improved efficiency, minimizing its impact on image quality; 3a) algorithmic maximization of new SPAD detectors' dynamic range to improve sensitivity; 3b) implementation of the new algorithms in modular embedded power as a 3D ASIC affixed to the detector itself instead of a central computer; and 4) design of an advanced stereoscopic DOI technique to decrease localization uncertainties and gain both contrast and efficiency. Overall this vision will lead to new scientific if not commercial endeavors, allowing adaptations of digital techniques and artificial intelligence mostly unheard of in PET until now to circumvent many known problems and limitations and to open new possibilities for the modality.""608554,""MichaudBelleau, Vincent"
"615155"	"Mignotte, Max"	"Bayesian fusion models based on multi-level constraints and multiple criteria in image processing and computer vision"	"My research program investigates the use of new unsupervised (Bayesian) probabilistic or energy-based fusion models for understanding, analyzing, and manipulating still, moving and multidimensional, multispectral or multimodal images. More precisely, this research program will attempt to propose new statistical models to synergistically integrate multiple image cues (e.g., color, texture, edges, interest point or symmetry detection, Gestalt perceptual cues, etc.) with possibly different constraints (possibly expressed at different levels of abstraction) in order to better model the intrinsic and complex properties of the (image) solution to be estimated and/or to fuse several weak solutions or different complementary low-level applications (segmentation, edge map, restored image, etc.) in order to achieve either a more reliable and accurate solution or a high-level computer vision task (3D reconstruction, complex shape localization, etc.).  These models can have a wide range of applications not only in still image processing and computer vision, but also in several other fields, including multi-modal medical image applications, Geoscience imagery and more generally in all multi-camera or multi-modal recognition and reconstruction systems of the next generation.The adopted framework, for these different research models, mainly relies on the Bayesian statistical theory which allows to take into account some available prior knowledge on the information to be found and to combine this prior model with a (statistical) model describing the interactions between hidden and observed variables (likelihood model). In this framework, the proper use of the available prior information can be expressed by local prior models such as Markov Random Field (MRF) models and contextual knowledge is usually captured through the specification of spatially local interactions or recently through non local (or long-range) interactions. In addition, Bayesian theory makes it also possible to apply global prior (interactions) or constraints such as the natural variability of an object shape to be detected/reconstructed (via global parametric probabilistic prior models or global constraints expressed by the knowledge of a solution at a lower level of abstraction).""607417,""Mihailescu, John"
"616488"	"Moha, Naouel"	"Assessing and Improving the Software Quality of Mobile Applications"	"The last decade has witnessed a huge increase in the number of mobile applications (a.k.a. mobile apps) available to end users. Mobile apps invaded all areas of our daily lives not only games, entertainment, and social networking but also business, education, finance, and health. The ever-increasing user requirements and popularity of mobile apps have led mobile developers to implement, maintain, and evolve apps rapidly and under pressure. Hence, software engineers may not always follow good design and implementation practices, a.k.a. patterns, and may adopt bad practices, called by opposition antipatterns. The presence of antipatterns may lead to poor software quality, thus hindering the evolution of apps and degrading the quality of the end-user experience. I intend to contribute to the important challenge of ""Preserving and improving software quality"" in the context of mobile-oriented evolution. The long-term objective of this research program is to build a recommender system for assessing and improving the software quality of mobile apps. This work will provide mobile engineers with techniques and tools to support their evolution tasks and to deliver a better experience to end-users. We will achieve this objective by proposing and applying a methodology to detect and correct mobile antipatterns, and evaluate their impact in mobile apps relying on various mobile platforms. This program is novel and original as we target software systems of prominent importance, mobile apps, which are constantly evolving along their underlying frameworks, and are updated much more frequently than other traditional software systems. Moreover, we do not focus only on the detection but also on the correction of antipatterns, a difficult problem to tackle. Besides the correction, the other challenges to face in this research program are handling the dynamic nature of mobile apps, their constraints (e.g. CPU, memory, battery, etc.) and their various mobile platforms (iOS, Android, Windows Phone, BlackBerry, etc.). Thanks to this recommender system, we can easily envision that mobile engineers would be eager to inspect in a dashboard how the software quality of their apps evolves, while they are under the pressure of aggressively updating the apps to meet user demands. Moreover, this work will deepen our understanding of mobile antipatterns and of their impact on software quality. Indeed, given the crucial role of software evolution in mobile development and the importance of user experience, we expect that our research result will be a source of competitive advantage for the Canadian mobile industry by providing opportunities for innovation. This research program will train 11 Highly Qualified Personnel, i.e., 2 PhD, 4 MSc and 5 BSc students, who will develop an expertise in software quality improvement, and gain extensive hands-on experience with mobile development and evolution on large and complex systems.""598201,""Mohabbati, Bardia"
"615841"	"Mouhoub, Malek"	"Preference Reasoning in Constraint-based Systems"	"Constraints and preferences co-exist in a wide variety of real world problems, including scheduling, planning, vehicle routing, resource allocation and Geographic Information Systems (GIS) applications. Constraints refer to problem requirements that must be met, while preferences can be either qualitative or quantitative and reflect desires and choices that need to be satisfied as much as possible. In some applications such as urban planning and robot motion planning, these constraints and preferences can be temporal, spatial or both. In this latter case, we will have to deal with entities occupying a given position in time and space. Spatio-temporal data can be symbolic or numeric and may correspond to multiple levels of detail.The main goal of the proposed research program is to extend the features of the current constraint solving systems by developing new techniques leading to a technology that faithfully represents real world industrial applications. In this regard, we plan to develop a unique framework and its related algorithms for managing the above types of constraints and preferences in an evolving environment.Given a real world application under constraints and preferences, the main task of this framework is to return, in an efficient way, the best outcomes satisfying all the constraints and optimizing all the preferences. The current solving systems work based on idealized assumptions of environment stability. Our proposed framework will be able to maintain, in an incremental way, this set of optimal solutions anytime a constraint or a preference is added or removed. This feature will overcome the limitations of the current solving systems when tackling applications operating under highly dynamic and unpredictable environmental conditions. Through this dynamic feature, as well as the constraint and preference learning algorithms we propose, our framework will have the ability to interact with the user and allows him or her to model a given problem under constraints and preferences. This will address the challenge that users have to face when modelling these problems using the current solvers. Indeed, the current systems require logic, traditional programming skills, as well as a significant background in constraint programming for the end-user to model even simple problems.Our framework will have the ability to handle constraints and preferences with uncertainty due to lack of knowledge, missing information or variability caused by events which are under nature's control. This will be achieved by extending the probability and the possibility theories to general as well as spatio-temporal constraints and preferences.Finally, by achieving the proposed research program we will successfully address diverse complex industrial problems, including reactive scheduling, urban planning, timetabling, robotics, transportation, configuration and E-commerce.""601873,""Mouhoub, Malek"
"615732"	"Muller, Hausi"	"Software Engineering for Cyber Physical Systems"	"Cyber physical systems (CPS) are smart, distributed, software-intensive systems that control tightly integrated computational and physical components. These systems involve a high degree of complexity at numerous spatial and temporal scales and control software and physical components with networked communications. CPS technologies are becoming the key enablers for how we control and build smarter engineered systems, such as autonomous vehicles, smart cities and buildings, renewable energy systems, personalized health care, medical devices, water management systems, and food supply chains. It is imperative to position Canada at the forefront in this industrial revolution.The goal of my research program is to understand and enhance the capabilities that can be added to humans and machines with tightly integrated networked control. There are many challenges that must be addressed in CPS foundations to be able to harvest its rich economic opportunities. I will investigate scientific foundations and technologies towards a CPS control and systems science (CSS) to analyse and design CPS that are controlled through real-time, networked feedback loops. CSS will provide a platform to analyse, design, simulate, optimize, validate, and verify CPS. A CPS modifies its behaviour at runtime in response to changes within the system or its physical environment. The fulfilment of CPS requirements must be guaranteed even in the presence of adaptations. Traditionally, confidence in the correctness of a system is gained through analyses performed at design time. In the case of CPS, some assurance tasks must be performed at runtime. This calls for methods and techniques that enable continuous CPS assurance throughout its life cycle. A model at runtime (MART) is a causally connected self-representation of the associated system that emphasizes its structure, behaviour, and goals from a problem space perspective. CPS require rethinking the software life cycle for which the distinction between development and execution time stages is no longer apparent. My objective is to explore MART for different aspects of concrete application domains to deal with CPS dynamics to provide effective techniques for analysing, guaranteeing and predicting CPS properties. The societal impact of CPS is enormous. Advances in the interconnected capabilities of CPS affect virtually every engineered system. The technologies emerging from combining the cyber and physical worlds will provide an innovation and incubation engine for a broad range of industries-creating entirely new markets and platforms for years to come. CPS are advanced technology systems that require knowledge and training for their development and operation. A skilled workforce to support future CPS is a challenge in its own right and critically important for Canada. I will train a large number of highly qualified personnel (HQP) in this strategic realm of CPS.""594170,""Müller, Kirsten"
"614609"	"Murphy, Gail"	"Enabling Effective Design Decisions during Software Development"	"Software design involves activities that turn requirements for what a software system should do into an executing system that meets those requirements. The choices made during software design have a profound effect on the system, including its overall value and its correctness. Despite the significant consequences of the many design choices made during software development, little is known about the kinds of design decisions software developers face and the consequences of those decisions. The long term objective of this research program is to provide intelligent assistance to a software developer to help him or her make informed software design decisions and to enable prior decisions to be fluidly revisited and altered. This research program will enable software development to move from a sequential activity in which design decisions follow each other to allowing software developers to explore of a range of decisions. This exploration will permit the overall value of a system to be increased and choices that may have undesirable consequences, such as exposing vulnerabilities, to be minimized. Towards this long-term vision, I propose the following three objectives for the next five years of research. First, through empirical study, we will identify and catalogue different kinds of design decisions that occur and we will determine the scope of impact of various decisions. Second, we will create techniques to identify when decisions occur as a developer works with development tools, we will devise algorithms for recommending potential choices for a decision, and we will design user interfaces to effectively deliver recommendations to a developer. Third, we will create representations for the design choices made during development and we will develop techniques to enable the rolling back, and subsequent rolling forward, of a software development, to enable exploration and revisiting of different choices from a design decision point. This problem area came to light during my experiences leading a software development team in industry. Advances in this research area can provide new knowledge for software engineering and programming language and can produce techniques and software tools with immediate benefit to much of the $160B ICT industry in Canada. There is also potential for techniques developed for exploring design spaces to be used within the greater scientific community where design decisions can directly impact results on which policy and other knowledge rely (e.g., climate models).""616559,""Murphy, Graham"
"615350"	"Nabki, Frederic"	"Compact and Energy Efficient Wireless Microelectromechanical Sensing Systems"	"Like integrated circuits (ICs), microelectromechanical systems (MEMS) are a disruptive technology, enhancing systems and enabling new applications. The MEMS market is expanding at an increasing rate, projected to almost double from $11B in 2014 to $21B in 2020 [Yole, 2015]. Recently, MEMS sensors have had an increased presence, notably in wearables (e.g. smartwatches, fitness trackers) and sensor nodes (e.g. tracking, weather forecasting), where many sensors are combined. This warrants tighter integration to allow for more compact systems, and requires wireless interfaces to transmit sensor telemetry and allow for more mobility. Such sensors are suited to the upcoming Internet-of-Things, where large numbers of small and autonomous sensor nodes, independent of power and wiring infrastructure, are expected to be deployed with a much reduced deployment overhead.ICs can enable the circuitry required within such compact MEMS sensors. However, single-package integration with multiple MEMS remains difficult because of microfabrication process incompatibilities and specific design intricacies. Furthermore, energy efficiency is becoming a key challenge to system integration, as there is a strong push to deploy sensors into devices with smaller batteries, having insufficient energy capacity for the desired functionalities. Efficiency can be improved by reducing the power consumption of the wireless and interface electronics, and enhancing the sensor with energy harvesters.Accordingly, the vision of this research program is to integrate traditionally disjointed functions of sensing systems (i.e., transducer, sensing interface, wireless communication, and energy source) within a single package or, ultimately, a single die. Prof. Nabki will use his research expertise, recent progress and unique MEMS, ICs and integration technologies to attain this vision by pursuing 3 short term research objectives: i) elaborate MEMS transducers well-suited to above-IC integration and to transducer fusion; ii) investigate highly energy-efficient sensing interfaces and wireless communication ICs; and iii) explore hybrid mechanical energy harvesters to increase sensor energy efficiency.Ultimately, this research program will facilitate the creation of low-cost, small form factor wireless sensors that can be used in a variety of environments and applications, yielding advances in the fields of MEMS, energy harvesting, sensing, low-power ICs and integration, which represent significant contributions to research in Canada. These new sensors will have an overarching applicability and impact in sectors such as transportation, healthcare, environment and industrial processes, strengthening Canada's competitiveness. The 10 HQP trained during the research program (2 BEng, 3 MASc, 5 PhD) will gain valuable and marketable advanced manufacturing and design skills for the Canadian economy.""616682,""Nabki, Frederic"
"616443"	"Narayanan, Lata"	"Optimizing movement and communication for autonomous mobile entities"	"Unprecedented technological advances in a variety of fields have brought us increasingly closer to the contemporary vision of the Internet of Things (IoT). It is estimated that in the next five years, over 50 billion devices or things will be connected to the Internet. Most of these things will have embedded inside them, sensors that collect data about their environment and use wireless networking technology to communicate with each other and enable smart decision-making. In the smart cities of the future, sensors embedded in cars, roads and and parking lots will improve daily life by finding parking and avoiding congested routes on the fly; in our homes will enable reduction of energy costs; in healthcare settings will save lives by continuous monitoring of patients; in industrial and commercial settings will improve productivity, to name but a few applications. All this will be achieved with machine-to-machine communication involving little to no human interaction. The potential capabilities of autonomous devices connecting to each other seamlessly are greatly enhanced by the fact that these devices may be mobile, whether they are wearable devices, or they are robot swarms working on a construction site, or they are in driver-less cars. Billions of dollars are being invested by governments and top technology companies worldwide on realizing some of these applications. Many scientific and engineering challenges remain to be solved before the much-heralded vision of smart cities and the Internet of Things can be realized. My research centers on fundamental questions of efficient communication and collaboration between mobile autonomous devices, such as wireless sensors and mobile robots, that are critical to realizing the above applications. Communication in multi-hop wireless networks is very challenging as it necessitates both route-finding and interference-free scheduling of transmissions over multiple links. We need to study fundamental questions of how, when, and what to communicate. Adding mobility to wireless devices expands the range of possible applications but at the same time adds a whole new dimension to the challenge involved. What advantages can we gain from controlled mobility; what are the optimal trajectories to be taken by these autonomous mobile devices to achieve certain tasks? The objectives of this proposal are to focus on the aspect of movement, and to design optimal trajectories and communication strategies for collaborating mobile sensors and robots. In particular, I plan:- to design efficient centralized and distributed algorithms for pattern formation by a set of mobile entities, such as mobile wireless sensors.- to design efficient algorithms for data gathering in multi-hop networks of mobile sensors.- to design efficient trajectories for a set of autonomous mobile robots to collaborate on tasks such as searching and transporting objects of interest.""595124,""Narayanswamy, Sivakumar"
"615455"	"Nechache, Riad"	"Perovskite materials for efficient photovoltaic and light-emitting devices"	"The long-term objective of this research program is to explore the unique properties of new perovskite-type functional materials and their integration into basic optoelectronic device architectures including solar cells and light-emitting diodes.Perovskites materials offer new and remarkable functional properties (chemical, optical, optoelectronic etc.) and recent progress in their synthesis, manipulation and characterization at the nanoscale opens the way to new discoveries of functional materials and exploitation of their unique properties in several innovative applications. In the last 3 years, perovskites have yielded spectacular results and have generated a lot of attention world-wide as a promising material platform to develop high-performance solar cells produced at low costs. Here, we propose to explore two novel perovskite-type material systems: (1) multiferroic Bi2FeCrO6 oxide and (2) metal-halide perovskite MAGeI3/MA(Bi,In)3.Three key research objectives are essential to succeed in this endeavour: (1) achieve a precise control of the synthesis process and improve the quality of the resulting material systems, (2) explore, understand and control their fundamental properties, and (3) optimize their properties for energy-related applications and integration in basic solar cell (using for example multiferroic Bi2FeCrO6, MAGeI3 and MA(In,Bi)I3) and light-emitting diode architectures (using in particular multiferroic Bi2FeCrO6). As such, the requested financial support will serve to create a challenging and exciting research environment for the training of young researchers, stimulate the advancement of fundamental knowledge, and generate major technological breakthroughs in optoelectronics. In addition to a great societal impact, this program will contribute significantly to position Canada as a leader in emerging energy-related materials and technologies""609495,""Nechache, Riad"
"615442"	"Ngom, Alioune"	"Integrative Network-Based Machine Learning Approaches for Cancer Bioinformatics and Bio-Molecular Network Reconstruction"	"Network-based classifiers (NCs) that combine primary tumor data (e.g., gene expression data) with secondary data provided in the form of network (e.g., protein interaction networks or other bio-molecular networks) have been proposed for sample class prediction (e.g., cancer outcome prediction). The network data is used to identify informative groups of interacting genes, called network biomarkers (NBs), which can best separate the classes. Unfortunately, current NC methods proposed in cancer bioinformatics have produced very limited progress in terms of robust classification performance and stability of selected NBs. Current methods used for combining the primary tumor data with the network data do not well capture and summarize the biological knowledge contained within the data. The high dimensionality of the integrated data as well as the decoupling of the training of current NCs from the selection of genes hampers the stability of the NB identification. The performances of current NC methods are significantly hindered by the high level of noise and sparseness of current protein interaction networks. Network incompleteness is an important limitation in current NCs. My goal is to improve existing and devise new NC approaches which alleviate these limitations; by including additional biological data (tertiary data) useful for tumor classification, and then devising appropriate integration and learning methods which can best capture the biological information contained in the data. For a given cancer disease, finding accurate and robust predictive NBs will provide a better characterization of its subtypes, its outcomes, or its stages. The NBs will help physicians diagnose cancer more accurately, or suggest better treatment, and will also serve as potential drug targets in the future. The NBs, which describe the functional dependency between genes, can be monitored over time in the development of the disease, and hence, provide better strategies for cancer care and new strategies for the early detection of the disease.Current biomolecular networks such as Protein-Protein Interaction (PPI) networks are incomplete, contain many false-positive interactions and even many more false-negative interactions, and are very sparse with skewed degree distributions. This reduces the performance of many network-based prediction methods. I plan to propose network-based prediction methods which classify node pairs as interacting or not, in order to improve the quality of given networks. My approach will be based on the idea that ""two nodes interact if they are closer to each other""; i.e., to propose and integrate different node similarity measures within learning frameworks. The proposed methods to improve the quality of networks are novel and the reconstructed PPI networks can be used in any network-based prediction problems (e.g., protein function prediction).""603584,""NgontieNgounou, PatrickJohan"
"616241"	"Nikolov, Aleksandar"	"Computational Discrepancy Theory"	"The focus of this research is computational aspects of combinatorial discrepancy theory, including the development of applications of discrepancy to computer science, and understanding the computational complexity of discrepancy itself.First, we propose to study applications of discrepancy theory to differential privacy. Differential privacy is a recent approach to achieving strong provable privacy guarantees in the analysis of sensitive data. The accuracy of query answers computed under differential privacy must necessarily be traded off for the privacy guarantees in any non-trivial analysis. We propose to generalize the connection between discrepancy theory and differential privacy we developed in prior work in the context of linear queries, in order to characterize the necessary and sufficient error for privately answering convex minimization queries. These queries encompass a large number of primitives used in machine learning and statistics. We further propose to study applications of discrepancy theory to the design of efficient approximation algorithms for hard optimization problems. Combinatorial discrepancy has the potential to generalize and unify classical rounding methods for linear and semidefinite programs, such as randomized and iterative rounding, and was recently used to give an improved approximation algorithm for the bin packing problem. We propose to study the applicability of discrepancy-based rounding to other fundamental optimization problems. Moreover, recent progress on spectral notions of discrepancy, particularly the resolution of the Kadison-Singer problem, suggests that discrepancy based rounding may be applicable to semidefinite programs as well. Conversely, discrepancy lower bounds imply negative results for natural classes of rounding algorithms. We propose to study whether the negative results proved via discrepancy methods can be turned into proofs of integrality gaps, i.e. gaps between the optimal integral solution and the value of a linear programming relaxation.Finally, we propose to study computational questions in discrepancy theory itself. The computational complexity of many of the important measures of discrepancy remains open. Moreover, some of the most powerful methods of constructing low discrepancy structures are only existential, and do not provide efficient algorithms. Both of these concerns currently limit the applicability of discrepancy methods to algorithm design. We propose to extend tools we have developed in prior work to understand the complexity of approximating hereditary discrepancy to other combinatorial discrepancy measures. We also propose to develop algorithmic techniques to construct low discrepancy objects.""617204,""Nikolova, Natalia"
"614503"	"Nishimura, Naomi"	"Algorithms that handle change over time and space"	"My goal over the next five years is to understand how algorithms can be designed to accommodate change, where change can take place before, during, or after a problem is solved. Depending on when the change occurs, it might result in modifications to the original input or in multiple solutions to a single input. The type of change can be either inherent in or external to the problem and the inputs. The areas of parameterized complexity and reconfiguration lend themselves well to investigations of approaches to solving problems in such settings.As a real-life example, repairs to power stations might require change between two good placements of customers, placements in which the output of each station is sufficient to meet the needs of all customers it serves. To minimize disruption of service, customers are moved one by one, each change resulting in a good assignment. In general, reconfiguration models changes that occur after a problem has been solved; solutions to a specific input (here, good assignments) can be viewed as vertices in a reconfiguration graph, where an edge indicates that one solution can be transformed into another in a single step, for some definition of a step. Recent research results have considered various properties of the reconfiguration graph, such as connectivity and diameter, and include the problem of determining whether there is a path from solution S to solution T (known as S-T connectivity). A natural setting for the study of change is the framework of parameterized complexity. Here, one identifies parameters of a problem and attempts to develop an algorithm running in time polynomial in the size of the input but subject to a potentially larger function of the parameters. This approach can yield practical algorithms for otherwise intractable problems in situations when the parameters are small, or demonstrate that such algorithms are unlikely to exist. Parameters can measure how much inputs can differ, how much change can be allowed, or how much solutions can vary for cases in which change occurs before, during, or after a problem is solved, respectively. My recent results combine reconfiguration and parameterized complexity; here parameters under consideration include the length of the transformation from one solution into another, bounds on the sizes of solutions, and properties of the inputs.My two-fold approach to the study of change encompasses both general properties of classes of problems and results optimized for specific settings chosen from a wide spectrum of diverse application areas, such as fundamental problems with applications to biology, voting protocols, and communication and social networks. I plan to form a unified framework of models that captures change in both inputs and outputs. Examination of the parameterized complexity of such models may yield practical solutions to real-life problems as well as a general understanding about the nature of change.""595071,""Nistor, Ioan"
"616065"	"Paliwal, Jitendra"	"Electromagnetic imaging and infrared spectroscopy for quality monitoring and preservation of cereal grains"	"The value of Canada's annual grain crop is about $6 billion and 70% of the produced grains are exported. It is estimated that 2% of the harvested crop (worth $120 million) gets wasted during handling and storage. Most buyers specify stringent quality requirements as well as zero-tolerance for contaminants. The current method of grading grain relies heavily on manual inspection, which is time consuming, subjective and requires trained personnel. Therefore, alternative rapid, objective, accurate, and cost effective techniques are needed for grain quality monitoring in real-time that can potentially assist or replace manual inspection. The quality of grain also deteriorates during storage, which could be anywhere from a few months to over a year in Canada. With the disappearance of country elevators over the last few decades, more grain is being stored on-farm for longer periods. This has led to larger-sized bins that are difficult to monitor. Hanging temperature and moisture sensors on cables in these large bins poses many practical limitations. Thus, a non-invasive method of monitoring quality of grain stored in bins is highly desirable.The long-term goal of my internationally recognized research program is to minimize post-harvest losses and preserve the quality of grain by means of exploiting grain's optical characteristics non-destructively and in real-time. My preliminary work in this area lays the foundation for research that will be conducted in the realm of this proposal over the next 5-years. These short-term objectives are to (i) use the technique of mid infrared spectroscopy to quantify the hazardous secondary metabolites produced by grain fungi; (ii) study the microstructural changes that take place in grain kernels as a consequence of fungal infestations using soft X-ray spectro-microscopy; and (iii) adapt microwave imaging, a technique that is used for detection of cancerous tumors, to monitor the storage conditions inside grain bins.The state-of-the-art infrastructure recently acquired through a $2 million CFI grant uniquely positions my lab to conduct the research outlined in this proposal. The outcomes of these studies will have a transformative impact on the storage and handling of grains in Canada and worldwide. The optical method of detecting fungi and mycotoxins will help provide grain inspectors a currently unavailable but highly desirable tool to detect incipient infestations in real time. Using microwave imaging, Canadian producers will be able to monitor their grain bins non-invasively. The importers and consumers of Canadian grain will benefit from a safe supply of high quality cereals. The HQP trained on these studies will gain skills that are highly coveted by Canadian industries. In essence, the research will help enhance process efficiencies in grain handling and storage and also train highly skilled personnel who will contribute to the Canadian economy.""599117,""Paliwoda, Rebecca"
"615898"	"Pan, Jianping"	"Topology Control for Computer Networks"	"The world is connected and becomes increasingly so with the advances of information and communication technologies. Many natural phenomena and man-made systems can be captured, explained and improved by a better understanding of the structure and dynamics of the network topology and interactions. The proposed research program focuses on the topology control for computer networks, based on our early work on computational geometry and geometrical probability, and furthering into stochastic geometry with geographical perspectives, for both wireless communication networks and cloud computing networks. For example, in wireless communication networks, the received signal and interference strengths depend on the distances among transceivers and interferer, as well as shadowing and fading due to obstacles and multiple propagation paths. With cloud computing and distributed storage, user requests and associated data can be redirected to and stored at different locations, which also affect the service quality, system efficiency and the resilience against natural disasters and man-made attacks. Thus locations and distances play an increasingly important role in computer networks, particularly for topology control, based on which affect all upper-layer protocols and functions. Starting from a graph-theoretic perspective, our research in computational geometry and geometrical probability led to a few breakthroughs including the random distance distributions associated with rhombuses, hexagons, regular and irregular triangles, and eventually arbitrary polygons with or without a reference point, with a wide variety of applications. In the proposed research program, we will continue with high-order, joint and conditional distance distributions in both physical and logical spaces, e.g., hop distance distributions with different mobility models and routing schemes. By taking geographical perspectives into account, our proposed research will also incorporate the spatiotemporal dynamics of user requests, computation and storage capacities, and energy supplies into the topology control process. The proposed research will also continue the training of highly-qualified personnel with high-impact research contributions and high-competition job placement to benefit the economy and society in Canada and around the world.""600540,""Pan, Xue"
"615730"	"PiroozAzad, Sahar"	"Meshed Multi-Terminal Direct Current (MMTDC) Grid Protection and Control"	"In recent decades, there has been a worldwide growing attention towards climate change. To move towards the decarbonisation of the energy sector and a more sustainable energy system, the share of renewable resources in the overall energy mix should be increased. Wind energy is one of the low-cost alternatives for generating clean energy and provides a hedge against higher carbon fuel costs. Recently, significant investments have been made in wind generation projects. In 2014, 37 wind power projects, totaling a production of 1871 MegaWatts (MWs), were installed in the Canadian provinces of Ontario, Quebec, Alberta, Nova Scotia and Prince Edward Island. With this year's installation of the 270 MWs wind project in Ontario, Canada's installed wind generation has recently surpassed 10 GigaWatts. The large integration of renewable resources, in particular wind energy, poses various challenges to the operation of the electrical power system, and is driving research into changing the existing transmission infrastructure.Transmission of electrical power primarily relies on alternating current (AC) technology. Over long distances, the power transmission capacity of AC lines is drastically reduced. The alternative to high voltage AC technology is high voltage direct current (HVDC). HVDC is an environment-friendly low loss power transmission technology and over long distances, offers lower costs and visual impact and requires less space for towers and transmission lines (compared to AC). Besides the existing HVDC lines in Canada such as Quebec-New England, Nelson River and Vancouver Island, several more lines are under construction, including: Western and Eastern Alberta (expected in 2016), Labrador-Island (expected in 2017) and Maritime (expected in 2017). With the increase in the number of HVDC lines, it is envisioned that, in the long run, a meshed multi-terminal DC (MMTDC) grid will be built and overlay the AC transmission system. This grid will allow the combined AC and HVDC transmission system to absorb and economically transfer new sources of energy generation from renewable resources to remote load centres. An MMTDC grid has not been built in practice yet. There are several technical limitations that manufacturers and system operators are trying to overcome to facilitate the realization of an MMTDC grid. Protection, control and interoperability are among the most challenging questions. The proposed research will overcome these challenges and will pave the road for future realization of an MMTDC grid and all the benefits that it will bring to the society, including: improved electric system reliability, reduced converter ratings and costs, lower reserve capacity, reduced wind curtailment and increased power trading among neighboring countries. Furthermore, through this research program, several graduate and undergraduate students will be trained in specialized and sought after skills.""609706,""PiroozAzad, Sahar"
"614675"	"Poole, David"	"Representations, Inference and Learning for Complex Decision Making Under Uncertainty"	"This proposal is about decision making under uncertainty in domains where there are rich relational descriptions, such as making treatment decisions about medical patients conditioned on the detailed observations in their electronic health records, or making a decision about prospecting in a geographic region conditioned on a description of the local geology. Building on recent progress in statistical relational AI, probabilistic programming, ontological reasoning and preference elicitation, the aim of this proposal is to design a coherent expressive framework for reasoning and decision making under uncertainty.The mix of probabilistic models with relational models has become known as statistical relational AI. These models specify both the logical structure (in terms of quantified logical variables) and the probabilistic structure (in terms of conditional independence). The problem of exploiting both, called lifted inference has recently been essentially solved for the exact undirected relational case by a number of research teams. Part of this proposal is to build on the recent successors to build the next generation of relational probabilistic systems with expressive representations, efficient inference and robust learning. We will also further advance exact inference, which will also enable new ways to do efficient approximate inference.Recently there has been an explosion of scientific and government data being published with the vocabulary defined by formal ontologies. We will work on representing, reasoning and learning hypotheses that interoperate with heterogeneous data sets and rich ontologies. There are exciting challenges that arise when defining multiple hypotheses at various levels of abstraction and detail that make predictions on multiple heterogeneous data sets.Ultimately these models are used to make decisions. The other part of making decisions is utilities. We will also work on preference elicitation for utility models that interact with the relational models and ontologies, and are understandable by decision makers.Our work will build on ongoing collaboration with real-world decision makers in geology, computational sustainability, and medicine. This proposal is to develop the computational foundations to make such applications feasible.""611374,""Poole, Warren"
"614914"	"Power, Sarah"	"Task- and emotion-aware passive brain-computer interface for mental workload monitoring"	"Brain-computer interface (BCI) technologies have been traditionally defined as devices that enable movement-free communication through the direct measurement and translation of brain activity that is intentionally generated by the user for the purpose of controlling an external device. The target users of such ""active"" BCIs are generally individuals with severe motor impairments who are unable to operate more conventional interfaces (e.g., keyboards). Although this remains the focus of most BCI research, the potential of BCIs for healthy users has also been recognized and is receiving increased attention. The practical definition of BCIs is expanding to include not just those devices using brain activity to replace natural central nervous system (CNS) output in users with disability, but also those that enhance CNS output in healthy users. Rather than exploiting intentionally modulated brain activity, these so-called ""passive BCIs"" derive their outputs from arbitrary brain activity arising without the purpose of voluntary control, and are used instead for detecting the mental state (e.g., cognitive, affective) of the user. Passive BCIs have potential in many applications to enrich human-machine interaction by providing implicit information on the user's state and adapting the environment accordingly. One application domain that has received particular attention is the monitoring of mental workload, particularly in safety-critical occupations like pilots, air-traffic controllers, and other industrial operators. The ultimate goal is to derive a continuous, objective estimation of the cognitive strain on the individual from physiological signal variables, so that appropriate action can be taken to reduce the potential for error during periods of extreme demand or overload. Mental workload detection also has potential value in other domains, including gaming, adaptive training, and user interface design applications, to enhance and personalize user experience.Previous studies support the feasibility of a passive BCI for automatic mental workload detection. However, to date only very basic functionality has been explored - specifically, the automatic detection of brain activity associated with different levels of difficulty of a given task under controlled conditions. While this is a critical first step, there is still much work to be done before such systems are suitable for widespread practical application, and much opportunity remains for enhancing reliability and functionality. In this research program, we will build upon the current state-of-the-art in passive BCIs for automatic mental workload detection. Specifically, we will work toward developing a system that is both task- and emotion-aware. The enhanced capability will result in improved user-state prediction, which will in turn allow for enhanced human-machine interaction in a variety of applications.""598661,""Powers, RyanPaul"
"616010"	"Quimper, ClaudeGuy"	"Strong and efficient filtering algorithms for scheduling constraints"	"Scheduling is the process of determining in what order a collection of operations, tasks, or activities should be executed so that the resources required for their executions are not overloaded. The tasks are generally subject to a variety of constraints and the problem comes with an optimization criteria. Scheduling problems are generally NP-Hard and require special techniques to be efficiently solved. Constraint Programming (CP) is a technique issued from artificial intelligence that proved itself very efficient to solve scheduling problems.Despite recent advances, industrial problems remain hard to solve. Due to long computation times, solvers are halted before the optimal solution is found and therefore return sub-optimal schedules that can cause delays in airports or idle times on an assembly line. These inconveniences would be avoided if faster solvers were developed.The success of constraint programming for solving scheduling problems comes from its filtering algorithms that reason over the scheduling constraints to prune the search space. These algorithms apply several filtering rules based on a relaxation of the scheduling problem. If the relaxed version of the scheduling problem forbids a task to start at a given time, the solver can safely discard these solutions and spend time exploring another part of the search space. By improving the relaxation used by the filtering rules, it is possible to filter larger portions of the search space and therefore to speed up the solving process.The long-term objective of this program is to increase the speed of constraint-based schedulers to find large and complex optimal schedules in a reasonable time. This is achieved by fulfilling 3 sub-objectives.1) Designing filtering algorithms based on stronger relaxations that achieve more filtering than existing ones;2) Designing filtering algorithms based on relaxations that are aware of the objective criterion;3) Designing faster filtering algorithms.We propose a research program that will fully train 2 new Ph.D. students, 3 new master students, and allow one actual Ph.D. student to complete his thesis. Moreover, this program offers 3 internships for undergrad students.One master and one doctoral student will develop stronger relaxations that will offer a better pruning of the search space. One new Ph.D. and one finishing Ph.D. student will work on filtering rules that are adapted to the objective criterion. Finally, two master students will work on faster algorithms that enforce existing filtering rules.""597829,""Quinche, Melissa"
"616186"	"Rajan, Sreeraman"	"Non-invasive stand-off sensing for detecting, monitoring and localizing humans"	"Stand-off or distant sensing is a contactless non-invasive sensing methodology that sense either actively by transmitting waveforms (generally electromagnetic waves) such as a radar or passively (video cameras). It is useful when contact sensing is not feasible such as search and rescue, military and cross-border security applications. It is also an economical way of monitoring multiple patients in emergency departments or elderlies in home care. While majority of the existing research has focused on using single sensor or a combination of multiple sensor of the same type, the proposed Discovery Grant program will consider a suite of hybrid sensors that individually use different sensing strategies to achieve reliable sensing.A hybrid suite consisting of active sensors such as radars transmitting different waveforms, using different geometric configurations or a combination of sensors like radars, video and thermal imaging cameras is expected to significantly enhance the quality of sensed data, provide robust detection, monitoring and localization of human targets in barrier free and behind the barrier environments. However, several challenges related to type of radar sensor, transmitting waveform, sensor configuration and signal processing algorithms need to be dealt with. The proposed program addresses those challenges through theoretical and experimental analyses and provides a hybrid sensing solution along with advanced signal processing algorithms for detecting, locating humans, monitoring their vital signs and classifying various activities including speech in barrier free and behind the barrier scenarios.The long term goals of this program is to develop a robust system using a hybrid suite of active and/or passive sensors for comprehensive detection, location of multiple human being and monitoring of their various activities and physiological states. Such a system will have wide range of monitoring applications from military to health care. The short term goal will develop a hybrid suite of radar sensors that can monitor both behind the barrier and barrier free enviroments while the research emphasis will be on development of advanced signal processing algorithms to detect and locate multiple human beings, monitor and estimate of their vital signs such as breathing and heart rates, extract their vital signal waveforms, identify and classify the human movements and activities as part of robust monitoring.This program will train 6 Master students, 4 PhD Students and 2 Postdocs and 6 undergraduate students. The output of this program will lay the foundation for next generation stand-off monitoring systems for smart homes, provide patentable algorithms and technology, generate quality publications including 2 doctoral and 4 master theses and deliver a practical system that will improve the lives of Canadians and make them safe.""595006,""Rajapakse, Athula"
"616472"	"Rathore, Akshay"	"Innovative Power Electronics and Modulation Technologies for Electric, More Electric, and Hybrid Electric Transportation"	"The proposal focuses on developing a research program on transportation electrification. Major thrusts include enabling technologies for more electric aircraft (MEA) and marine electrification. Secondary thrusts are ground vehicles and vehicle-to-grid interface. This research program will develop test-benches, control platforms, software-hardware interfaces and create simulation and testing facilities for electric, hybrid and more electric transportation. The proposal focuses on design and development of innovative power electronics topologies and novel modulation techniques for electric transportation. Adoption of wide bandgap semiconductor materials like Silicon Carbide (SiC) and Gallium Nitride (GaN) and DirectFET based devices and high flux magnetic materials (nanocrystalline) and reduction in electrolytic, thermal, and filters volume are the major additions to the new design and development. The proposal is looking to investigate on power electronics circuits with reduced electrolytic capacitors.In MEA, light weight, high-density, low harmonics and EMC requirements, fault tolerance, and reliability are the major attributes. Existing conventional power electronics developed for industrial applications are not optimized for aircraft applications to meet harmonic requirements in airborne electrical system environmental standards. Therefore, the new power electronics become an essential technology supporting variable frequency electrical power system of MEA. Novel power factor correction topology with switching sequence and matrix converters will be researched.Integrated power system of electric ship is looking to adopt medium voltage (MV) DC architecture. This research program aims at developing MV modular multilevel converters and optimal low frequency modulation (50-200 Hz) techniques for marine propulsion system to reduce switching loses while not compromising on THD. It improves power handling capacity of devices and reduces thermal and cooling requirements. Transformerless architecture and matrix converter to reduce the complexity, cost, and improve density will be investigated.For ground vehicles, proposed research program will strategize to implement soft-switching modulation techniques at very high switching frequency, up to 500 kHz, to limit the cost of magnetics, and filters. Dedicated electrolytic capacitorless fault tolerant power electronics topologies with reduced magnetics and new devices (Sic, GaN, DirectFET)will be designed and developed to realize low cost, highly efficient and light weight propulsion system.Bidirectional V2G capability with communication and control layer will be developed for EVs to participate in future smart grid or power system distribution network to shave the peak demand to avail the mobile distributed energy storage (EV batteries) while parked and stationary.""604144,""Rattray, Christopher"
"615135"	"Rosehart, William"	"Integrated Energy System Analysis and Planning"	"Electrical energy systems are on the edge of a significant revolution with respect to their operation and planning. In the past, these systems and energy resources (e.g., electrical, heat, gasoline, natural gas, etc.) generally have not been planned or operated in an intentionally integrated manner. However, the impacts of different energy systems and resources on each other are becoming increasingly important. For example, with electrical systems powered by natural gas, not only is there interdependency with respect to fuel source, but a greater emphasis on energy efficiency and greenhouse gases has increased their utilization, and the future growth of combined heat and power generation. In the long term, integrated energy system planning will need to consider sizeable increases in renewable energy sources (e.g., solar, wind), energy storage, and demand response (i.e., changes in electric usage by customers). These technologies are fundamentally transforming how energy systems are planned and operated. While traditional, natural gas-based electrical generation units can respond well to variability associated with renewable resource generation, combined heat and power units operate better when generation output is more constant. The significant changes being implemented on the generation side are matched by increased load (demand) participation and responsiveness. The proposed program of research will focus on a) systems with large capacity or potential for combined heat and power; b) renewable energy such as wind- and solar-based generation; and c) energy storage. The increased interdependency in energy systems will make the planning and operations of electrical energy systems more complex, requiring increased use of specialized techniques and tools to handle multiple objectives while also accounting for uncertainty and random variables. The overall goal of our research is to develop tools and techniques for system planners and operators of 21st century power systems. This work will be essential to Canada's competitiveness in the energy sector and to keeping the ""lights on"" for future generations.""617622,""Rosei, Federico"
"616328"	"Rossman, Benjamin"	"Average-Case Lower Bounds in Boolean Circuit Complexity"	"Complexity Theory studies the nature and limits of efficient computation. The ultimate goals are lower bounds showing that fundamental problems, such as CLIQUE and STCONN, cannot be solved with limited computational resources, such as polynomial time or logarithmic memory. The subfield of Circuit Complexity seeks to prove lower bounds in combinatorial models of computation. Among these models, Boolean circuits (composed of AND, OR and NOT gates) are the most elemental and important, as they offer a concrete path to resolving the P versus NP question: one need only prove a super-polynomial circuit-size lower bound for the CLIQUE problem (or any other problem in NP). However, after over 60 years of effort, the strongest known lower bounds are only linear.With the aim of developing sharper insights and techniques, the majority of the research in Circuit Complexity has focused on restricted classes of Boolean circuits (such as formulas, bounded-depth circuits, and monotone circuits). Following a burst of seminal lower bounds in the 80's and 90's, progress slowed as the existing techniques appeared to reach their limits. The ""Natural Proofs barrier"" of Razborov and Rudich (1997) identifies a formal roadblock to further progress by showing that lower bounds against more powerful classes of Boolean circuits can only proceed from techniques that are honed to specific hard-to-compute functions, while avoiding the generic properties shared by almost all functions.This research proposal aims for a major advance in Circuit Complexity through a set of new techniques tailored to fundamental problems including CLIQUE and STCONN. One promising approach, introduced in previous work of the author, provides a combinatorial explanation of why small Boolean formulas fail to detect long paths in Erdos-Rényi random graphs. This approach has already produced groundbreaking lower bounds. The proposed research will pursue a sequence of next steps, with the ambitious goal of proving a super-polynomial lower bound against Boolean formulas. This would resolve a significant open problem in Complexity Theory (separating NC1 from P). Alongside this ambitious goal, this proposal explores other promising directions of research in Boolean circuit complexity and connections to areas such as proof complexity and communication complexity.""615547,""Rosychuk, Rhonda"
"615392"	"Ruths, Derek"	"Developing robust methods for the measurement of online social media populations"	"As human activities move online, governments, companies, and organizations want to provide optimized experiences to users of online services. Moreover, they see an opportunity to track online and offline human behavior through online user activity. All this requires knowing details about those users: e.g., their age, gender, interests, and general geographic location. Users may not provide such details for a variety of reasons. This motivates the need for methods that infer user attributes from their online content. Research in the area of demographic inference has shown that many such attributes can be accurately inferred from the textual and behavioral records left by users on a platform. The problem is that, while, researchers have made attempts to infer demographic features such as age, political orientation, income, and education, the accuracy of such methods leaves much to be desired. Few viable inference systems exist for actual use outside of research. In this initiative, we will tackle two of the major opportunities for improving the performance of demographic inference methods.First, we will develop methods for learning the nature of the relationship (e.g., family, friendship, in-person, fan) that underpin links in the online social graph. This information is both valuable in itself and also as an input for other analysis and classification tasks. Despite this, no work to date has been done on this specific version of relationship learning.Second, we will develop rich models that can accurately assess the topics users engage with. A major limitation of topical features used in existing demographic inference methods is that they are derived from the dominant language used on the platform. This inherently limits the richness of the resulting topic model. We will develop generalized topic models learned from community-curated discussion platforms such as StackOverflow and Reddit.In recent years, demand for new approaches to demographic inference have generated much research without many results. Here we return to the fundamentals of the field - constructing new and informative user features. Our work will provide generalized tools that improve the inference of a many user attributes. In addition, our work will make direct contributions to important and open problems in the fields of natural language processing and network science.""594839,""Rutledge, Robert"
"615013"	"Ruuth, Steven"	"Algorithms for continuum processes on complex, moving surfaces"	"Partial differential equation (PDE) models for continuum processes arise throughout the applied and natural sciences. There has been a great effort made to develop numerical methods for many important classes of PDEs in one, two or three spatial dimensions. However, a remarkable variety of problems involve differential equations on curved surfaces. Numerical methods for PDEs on curved surfaces are much more complicated than in the standard Cartesian coordinate spaces R2 or R3. A popular class of methods for solving such PDEs are the embedding methods. In recent research, we introduced the closest point method, which is an embedding method that decouples surface geometry from the underlying differential operators. This research is part of the work that led to the 2011 Germund Dahquist Prize. The closest point method solves rather general PDEs on smooth, stationary surfaces in an accurate manner, using standard numerical PDE methods in the embedding space. However, many practical problems involve nonsmooth, moving surfaces. In this grant, a number of geometric enhancements to the closest point method will be introduced: (1) Research will be conducted on algorithms for general PDEs on surfaces with folds, junctions and other nonsmooth features, (2) robust methods for approximating PDEs on complex, evolving surfaces will be developed, (3) research on accurate methods for general PDEs on point cloud surfaces will also be carried out. Alongside these geometric improvements, efficiency enhancements will be developed using domain decomposition and optimized time-stepping methods. Analytical research will also be conducted to provide new insight into the convergence of the method, especially in situations where its performance is unexpectedly accurate. Throughout, highly qualified personnel will participate in the development of mathematical software, the analysis of numerical algorithms, and the design of numerical methods to solve PDEs on surfaces. This provides valuable experience for a broad range of careers in academic institutions and technology-related industries.The proposed research will seek algorithms and software that are accurate and efficient, yet are simple in the sense that they compute solutions to different continuum models as uniformly as possible while making strong use of existing techniques and software in 3D. It is anticipated that the methods obtained under this grant will enable researchers and end-users to numerically investigate realistic models of general continuum processes on complex moving surfaces with great accuracy. ""594452,""Ruwanpura, JanakaYasantha"
"616190"	"Sack, JörgRüdiger"	"Dynamic Shortest Path Algorithms and their Applications"	"In many applications arising e.g., in Robotics, GIS, Navigation, Social Networks, and Sensor Networks, environments are dynamic. Objects/entities may move, disappear, appear and even change shape. Traditional shortest path algorithms, designed for static environments, would either not be applicable or require frequent recomputations and thus become inefficient.Most activity for dynamic shortest path problems has been in the area of graph (or hyper-graph). These instances allow for graph edges to be inserted and/or deleted and/or weights to be changed. Fully dynamic algorithms allow edges to be both inserted and deleted; partially dynamic algorithms allow for either insertions or deletions, but not both. Results exist for both fully and partially dynamic algorithms.Building on, and motivated by, previous work, we are interested in dynamic shortest path algorithms for solving geometric problems. For example, objects (obstacles for the shortest path computation) may only exist for a fixed time interval [Ts, Te], i.e., the object appears at time Ts and disappears at time Te. Objects may change shape over time (say grow continuously from a point to larger and larger circles - such problems arise when one wants to model uncertainties). Another interesting class of dynamic shortest paths problems arises in time-dependent graphs or networks, where the costs of edges (that is, edge travel times) vary as a function of time, and as a result the shortest path between two nodes s and d can change over time. We have already studied similarities of polygonal curves measured via the frequently used Fréchet Distance. Similarity problems between polygonal curves arise e.g. in Computer Graphics, Pattern Recognition, Clustering, GIS and structural biology, sports scene analysis, human movement, surveillance, and animal behavior. We discovered and/or improved upon some exciting and natural optimization problems involving the Fréchet Distance. The solution often involved solving particular shortest path problems. To solve several new optimization and parameterization problems using the Fréchet Distance and its variants, we will encounter geometric dynamic shortest path problems. Solutions to these are also of independent interest and will enable us to obtain also solutions to these Fréchet Distance problems.""614452,""Sadaoui, Samira"
"615186"	"Salcudean, Septimiu"	"Control and interfaces for teleoperated robots"	"Control and interfaces for teleoperated robots.The research program of the applicant is in the areas of robot teleoperation and medical imaging.Robot assistance is used in many surgical procedures. With this approach, robot-mounted instruments and a 3D camera are inserted into the patient through minimally invasive incisions. The surgeon teleoperates these instruments and the camera with two master robotic arms from a remote console showing the 3D camera view. Ultrasound is the primary real time imaging method used to guide medical interventions. Ultrasound elastography is a specific modality that images tissue deformation or stiffness. Elastography imaging can be thought of as a quantitative palpation - a form of medical examination.The objective of this program is to improve the interface between the operator and the robotic system and the operator's awareness of the environment. The applicant will carry out research on haptic feedback for the teleoperation of medical robots. This will add to the user interface in the console a feeling of the forces that the instruments encounter during surgery. To improve the interface, eye-gaze will also be tracked and used to push the robotic arms towards where the operator is looking. Applications include all areas of robotics, such as medical robotics, drones or mobile robots, or the control of robots by persons with disabilities such as ALS. The applicant will also study how to train surgeons by demonstration, using extensive and realistic data, including eye-gaze and haptics. This work could revolutionize the way mentors teach. The approach is suitable for the web-based instruction that will be prevalent in the future. Ultrasound elastography is an attractive alternative to haptic feedback in robot-assisted surgery.As a better alternative to tissue palpation and haptics, an intra-operative ultrasound tool compatible with current medical robotic systems will be used to obtain tissue deformation during surgery. Quantitative tissue stiffness images, as well as vessel structure, will be obtained through elastography during surgery, and will be incorporated in the robot interfaces. Through the proposed integrated palpation tool, surgeons will be able to see underneath the tissue to localize sensitive anatomical structure and pathology. Robotics and teleoperation are having an increasing impact on our lives. Improving the human-robot interface is an area of significant importance. The users include other researchers, medical robotics companies and ultrasound imaging companies, with ultimate benefits accruing to the healthcare system and, most importantly, to patients.""600689,""Saleh, Ahmad"
"614691"	"Salihoglu, Semih"	"Systems and Foundations For Massive-Scale Data Management"	"As the volume of data grows, and the speed of new data generation increases, many applications process their data on highly-parallel distributed data management systems. The objective of this proposal is to study distributed data management systems with a focus on two areas: (1) theoretical foundations of massive-scale data management systems; and (2) systems for processing large-scale graph data.On theoretical foundations, my focus is on understanding the fundamental limitations of distributed algorithms for answering queries over relational data and evaluate how ""good"" they are. Distributed algorithms differ in their parallelism levels, communication and computation costs, and the number of rounds of computation, i.e., machine synchronizations, they require. My goal is to derive lower or upper bounds on the costs of algorithms that perform a task over data. Specifically, I intend to study two questions: 1. Power of synchronization: How many rounds of computation are needed to answer a query? 2. Limits of parallelism: What is the maximum number of machines that can be utilized to answer a query?On large-scale graph processing, my focus is on general-purpose distributed graph systems. Large-scale graphs are at the core of many applications, such as web search, social networks, and genetic analysis. Broadly, these applications perform the following tasks on graphs: (1) batch graph algorithms; (2) machine-learning algorithms; (3) finding subgraphs; and (4) real-time analysis if the graph is evolving. Existing systems support one or two of these tasks. My goal is to build a system that supports all of these tasks and is based on the timely dataflow (TD) execution model. TD is an inherently streaming model, which can support real-time analysis, but has mechanisms to support synchronous and asynchronous computations, which can support batch graph algorithms, machine-learning algorithms and subgraph finding. Specifically, I intend to study the following issues: 1. TD operators of a general-purpose graph system. 2. Query languages and APIs that efficiently compile to these TD operators. 3. Storage models that are efficient under limited cluster memory. 4. Tools for testing and debugging graph applications running on TD.This work will establish theoretical foundations for massive-scale data processing, and produce an open-source prototype system advancing the state of the art in large-scale graph processing.""609979,""Salik, Taryn"
"615338"	"Sander, Jörg"	"Finding Groups in Big Data"	"The main objective of the proposed research program is to advance theory and practice of ""Big Data"" cluster analysis. ""Big Data"" refers to today's ever larger and more complex data sets, which are typically collected on a large scale by automatic equipment (e.g. microarray chips, sensors, logging devices). These data sets have become abundant, and they hold the potential for the discovery of new insights, which can lead to new opportunities for improved, data-driven decision making. Unsupervised, exploratory methods for knowledge discovery play an important role in realizing this potential.One of the major exploratory data analysis tasks is finding ""natural"" groups in data. Understanding the groups in Big Data allows a better organization and classification of Big Data, more efficient browsing and searching, focusing an analysis on specific groups, and discovering unknown relationships between groups.Clustering is the most common unsupervised approach to finding groups in data. Traditional clustering methods, however, face challenges when applied to Big Data, due to the typically large volume and high dimensionality of the data; and they are not designed to take advantage of properties such as the time dependence of some Big Data sets and relationships between different data sources.My proposed research program is aimed at advancing the theory and practice of clustering methods -particularly density-based clustering (i.e., where clusters are considered dense regions in the data space, separated by regions of lower point density)- applied to ""Big Data"". Based on the theoretical insights we will gain, we will develop novel and improved algorithms for clustering Big Data that overcome limitations of current clustering methods and extend the applicability of clustering to a wider range of Big Data scenarios. The research will focus on the following aspects: (a) fast methods to deal with large data volumes, (b) projected- and subspace-clustering that can address issues of high-dimensionality such as data sparseness and ""irrelevant"" attributes, (c) semi-supervised clustering based on constraints, including constrains derived from related data sources, that can guide an algorithm to a solution which is consistent with these constraints, (d) combining projected-/subspace clustering with semi-supervision, to find simultaneously the subspace(s) and clusters that are most consistent with given constraints, and (e) modelling the development of cluster structures over time to allow the discovery and tracking of relationships between different clusters.Progress with these issues will benefit a wide range and fast increasing number of application areas, in which Big Data is being collected, and which includes industrial and business application areas, as well as medical, biological, and and other scientific domains.""607381,""Sanders, Alexander"
"614519"	"Sen, Paresh"	"Performance enhancement in power electronics, converters, inverters and renewable energy systems"	"The power electronics technology has played a vital role in the evolution of power supplies, converters, inverters, electric motor drives and recently in renewable energy sources (wind and solar). It is desirable to operate these products at higher efficiency to save energy, with enhanced dynamic performance, reliability and higher power density resulting in reduced size. The proposed research is intended to achieve these goals. The following is the proposal for the five-year research program:1. Solve the lighting flickering problems and improve the efficiency of AC-DC LED bulb power supplies (LED drivers) for domestic, street, stage and other lighting applications. Circuit topologies and control strategies will be investigated to improve their performance, making them more efficient, compact, flicker free and to extend their life time.2. Multilevel transfomerless inverters for photovoltaic (PV) solar systems will be investigated to improve their performances, specially increasing efficiency, reducing Total Harmonic Distortion (THD) and eliminating leakage current inherent in such systems. 3. Investigate rectifier-inverter topology with modern robust control techniques to improve the dynamic performance of wind energy conversion systems.4. New semiconductor devices always brought revolution in power electronics technology. Silicon MOSFETs have been extensively used in power electronics products for several decades. It is predicted now that Gallium Nitride (GaN) and Silicon Carbide (SiC) devices will replace Si-MOSFETS during the next 5 to 10 years. Investigation will be underway to replace MOSFETs in LED drivers, PV systems, resonant converters and wind power converter systems by GaN/SiC devices to achieve higher efficiency, compact size and performance improvements.The following scientific approaches will be undertaken: -Evaluate the existing technologies, and identify problems associated with them.-Thorough investigation of the new proposed circuit topologies and control strategies.-Analysis, computer simulation followed by prototype experimental testing to verify the anticipated improvements.The novelty of the proposed research program will be the new concept and its implementation, resulting in improved performance of the power electronics products. The research will benefit Canadian industries, enabling them to manufacture highly efficient and high quality LED drivers, PV inverters, SiC/GaN based converters and wind energy systems. The proposed research program will contribute to the training of Highly Qualified Personnel (HQP) in power electronics technology, renewable energy sources (wind and solar) and robust control techniques. The HQP will acquire skills in theoretical analysis, simulation and building/testing hardware experimental prototypes. Such training will enable the HQP to contribute significantly to Canadian industries.""616056,""Senatore, Adriano"
"616062"	"Shafai, Cyrus"	"Micro-Systems for Power Systems and Low Loss Conductors"	"The proposed research is in the area of micro-systems with application to Smart Grid, hybrid ac/dc power technologies, and microgrid utility networks. The Smart Grid is a broad concept, which includes integration of sensors throughout the electrical distribution grid. This modernization of the electricity network has several aspects and goals, including intelligent monitoring of power flow and grid infrastructure, increased reliability, and reduced cost. The Smart Grid effort has several driving forces. Recent weather events (heat waves, ice storms) have demonstrated the public safety need for emergency resilience. Outages and power quality issues cost North American businesses more than $100 billion per year. Existing grid infrastructure is ever more stressed, due to increasing energy demand and increasing costs to deploy new infrastructure. Alternative energy sources are being added, with some sources such as wind and solar not having continuous delivery capability. The hybrid ac/dc power grid is an evolutionary transition, involving combined ac and dc networks in local installations, to maximize efficiency by reducing the need for lossy ac/dc power conversion. Hybrid networks are being driven by many forces, including dc alternative energy sources (photovoltaic, high efficiency wind turbines, battery and fuel cell storage), solid state LED lighting, electric vehicles, and consumer electronics which run on dc power. Local microgrid utility networks offer fine control of loads and generators, enabling greater reliability, cost reduction, and energy source diversification. Solid state transformers offer microgrid networks important performance benefits and small size when operating at kHz frequencies. However, high current applications require an intermediate 60 Hz stage to prevent increased resistive losses caused by skin effect on conductors, adding to their cost. Smart Grid and hybrid ac/dc networks require a new class of sensors capable of measuring dc parameters, as opposed to ac parameters. This research will develop needed micro-sensors, with focus on dc electric and magnetic field measurement, and remote voltage and current measurement. Micro-sensors offer unique solutions, due to their small size and low power needs, making them ideal for long term operation in infrastructure that is often located 100's of km from urban centres. Research will also be undertaken on novel layered metal-dielectric laminate conductors, for skin effect mitigation to enable low resistive loss kHz frequency conductors. Focus will be on conductors for microgrid circuits, kHz transmission lines, and multi-conductor cables. These new conductor topologies would be a new concept in high power utility circuits and cabling. Furthermore, they offer to be a future platform technology for the MEMS industry, with application to RF MEMS and sensor systems, and emerging TeraHertz systems.""614226,""Shafai, Lotfollah"
"616469"	"Shang, Weiyi"	"Log Intelligence: Systematically Leveraging Logs Using Development Knowledge"	"Logs are generated at run-time by logging statements that are deliberately added into the source code by developers. Logs generated during the execution play an essential role in field debugging and support activities of large software systems. Such logs are not only for the convenience of developers and operators, but have already become part of legal requirements (e.g., the Sarbanes-Oxley Act of 2002). In recent years, many companies (e.g., IBM, BlackBerry and Microsoft) have started leveraging the rich knowledge in logs to support the development and operation of their large software systems. The broad usage of logs lead to the emergence of a new market for log analysis platforms (e.g., Splunk, XpoLog, and Logstash), which support collecting, storing, searching, and analyzing the large amounts of log data.Although logs are widely used in practice, and their importance has been well-identified in prior software engineering research, logs are maintained and analyzed in an ad hoc manner. First of all, log maintenance (e.g, ., when to update a log) often depends on the gut feelings of developers. All too often, logs are too verbose or are at the wrong spots. Making it worse, developers often change logging statements without considering the needs of other stakeholders. Second, current storage and analysis techniques for logs remain very ad hoc, even with existing log maintenance and analysis platforms. Logs are typically stored as textual files. Most common analysis on logs is performed by un-scalable scripting languages and basic regular expressions. Third, log analysis techniques rarely make use of the rich run-time and development knowledge associated with the logging statements. For example, a typical log analysis is searching through logs using basic keywords like ""error"". Such an basic approach is very error-prone and fails to truly leverage the enormous potential of logs. Such challenges are also highlighted by the researchers and practitioners (from Spunk and Google) who have extensive experiences in maintaining and analyzing logs.The aim of the proposed research is to address the aforementioned limitations of the practices of leveraging logs. To improve the practice of log maintenance, I plan to design a framework for the systematic and automated guidance of log maintenance. To support the systematic log analysis, I plan to create a general analytical infrastructure for logs. Large-scale empirical studies will be performed on large open source and industrial systems, to understand the benefits and limitations of our work. The research outcomes will advance the practice of software developers and operators who depend on logs to ensure the quality of large software systems that serve millions of users worldwide. Furthermore, the proposed research will expose, train and enable five highly qualified personnel (HQP) to contribute to the state-of-the-art in software engineering research.""607188,""Shanjer, Maaz"
"616473"	"Shih, Steve"	"Microfluidics for automating and integrating synthetic biology"	"Synthetic biology has been used to engineer complex biological systems in areas such as therapeutics, bioenergy, and biological materials. However, this has been limited by two challenges: scalability and integrating automation of the synthetic biology Design-Build-Test-Learn workflow. Current research laboratories are equipped with robots that are aimed to accelerate synthetic biology and provide high-throughput solutions; however, they require more effort to instruct a robot to perform a new task than saving the researcher time in performing the task. Furthermore, they are expensive -- well outside of the budgetary reach of many scientific laboratories and have large footprints making it difficult for systems integration. Therefore, there is a need to create new automation technologies to expedite the synthetic biology workflow with the advantages of reducing costs and time while increasing the complexity of the biological systems that are created.In response to this challenge, the proposed research program is to develop microfluidic automation platforms for synthetic biology. This new tool will be powered by digital microfluidics (DMF), a fluid handling technique in which ~nL volumes can be manipulated in parallel on a patterned array of electrodes. To facilitate high-throughput applications, we will integrate DMF with droplet-in-channels - a two-phase system where it uses immiscible phases to create emulsions of monodisperse droplets that can act as microreactors containing a single cell or a cocktail of chemical reagents. To facilitate automation, these two microfluidic paradigms will be used together to create droplets containing DNA libraries that are transformed into bacteria or yeast and the functionality of these variants will be evaluated on the basis of directed evolution screening. This research program spans two streams: (1) development of a Microfluidic Automation and Characterization Station (M.A.C.S), and (2) microfluidic integration of the synthetic biology cycle. The proposed research program will enable new automation technologies with reduced costs, time, and manual intervention; in addition, the automation system will be inexpensive, will have no moving parts and a small laboratory footprint. In parallel, with developing microfluidics for synthetic biology, a M.A.C.S will enable (a) automatic reagent delivery to the microfluidics, (b) automation of droplet movement, (c) in-line detection system for rapid analysis. This system will be characterized by implementing synthetic biology to discover newly engineered microbes for biofuel production. If successful, this method could transform the field of automation and synthetic biology, and making it accessible for basic and applied scientists to solve problems that could transform other areas such as manufacturing, food production, and healthcare in Canada (and world-wide).""614210,""Shihab, Emad"
"616146"	"Singh, Karan"	"Transcended Reality: Modeling, Fabrication, Interaction, Navigation and Perception in Immersive (AR/VR) Environments"	"Augmented and Virtual Reality (AR/VR) are experiencing an unprecedented resurgence, with the onset of consumer-grade AR/VR devices and real-time computer graphics, to deliver an immersive experience where the virtual and real blend indistinguishably together. This proposal ambitiously aims to take the promise of AR/VR further, to one of TR or Transcended Reality, where our real physical environment is not only added to as in AR but subtracted from, replicated, abstracted and transformed, or more generally transcended. The dream of VR thinkers since early science fiction writings, has not been to merely recreate our physical reality, but to go beyond, indeed to transcend it.We will accomplish this by coupling a suite of devices that allow 3D shape and motion to co-exist in the digital and physical realm: combining AR/VR immersive displays (like the Oculus Rift, HTC/Valve Vive, Meta-AR or Microsoft HoloLens) and real-time 3D shape and motion acquisition devices (like the Microsoft Kinect or VICON motion capture), with robotic actuators, laser cutters and 3D printing.Specifically, the proposed research will focus on three areas within immersive environments: modeling form and function of objects, character interaction and navigation; all grounded by the perceptual capabilities of the immersive setting. Modeling and Fabrication: We expect 3D models in immersive environments to go beyond shape or form, to imbibe the functionality of physical objects, so that we may actively interact with them. In this proposal, we will thus focus on the simultaneous modeling of both 3D form and function. Digital designs using our research would thus instantly usable, both as digital models in an immersive setting, as 3D printed physical artifacts, or more interestingly as combinations of both.Interaction: As more devices in our environment become ""internet aware"", TR will be a space where humans communicate with their physical environments via the ""internet of things"". Interactive communication between man and machine in an immersive setting will be another thread of this proposal.This proposal will further explore modeling and animation techniques that produce emotionally engaging avatars, on either side of the uncanny valley.Navigation and Perception: Understanding the affordances and limitations of shape and motion perceived using stereoscopic and holographic projections continues to evolve with new AR/VR displays, and increased human exposure to these devices. Evaluating the perceptual impact of various immersive interaction and navigation techniques, is an important thread that will pervade our proposed research.""596751,""Singh, Karun"
"616460"	"Sirois, Frédéric"	"Protection of superconducting wires and cables used in high power/high field applications"	"Superconducting materials have the unique property of carrying lossless DC currents below a given critical current (Ic) and critical temperature (Tc). Superconductors considered in this proposal are called ""High Temperature Superconductors"" (HTS), because they exhibit superconductivity above liquid nitrogen temperature, i.e. 77 K (-196oC), a cheap coolant that is affordable in large-scale applications such as power devices. HTS wires come in the form of flat tapes and can carry amazingly high current densities, e.g. 2 MA/cm2 at 77 K. They are perfect for applications such as high field electromagnets, which allow reaching unprecedented field values, e.g. up to 50 T at 4 K. This could be a game changer in high-resolution magnetic resonance imaging (MRI), nuclear fusion, particle accelerators, etc. Also, since magnets typically require hundreds of kilometers of HTS tapes, they are expected to be the big market ""driver"" sought for scaling-up the production and reduce costs of HTS wires in a near future. The biggest challenge with superconducting devices is to protect them reliably against thermal instabilities that can destroy them rapidly. In this proposal, we propose to expand the application range of a concept discovered within my research group and that is called ""current flow diverter"" (CFD). This concept allows speed-up by an order of magnitude the propagation of a thermal instability when it occurs, which considerably simplifies the protection of HTS devices, in particular HTS magnets. It is seen as a potential breakthrough in the discipline, and therefore deserves serious consideration in the short term. The technology was awarded a U.S. patent in May 2015, and further I.P. could arise form short-term research, with potentially wide market if it penetrates the MRI market.Within this project, the following main objectives will be pursued:1) Find an effective and economical approach to implement the current flow diverter (CFD) concept on long lengths of commercial HTS tapes2) Understand the impact of a hot spot nucleation in high-current cables made of many HTS tapes (e.g. Roebel or TASC cables) under various experimental conditions3) Evaluate the impact of using the CFD concept combined with state-of-the-art protection systems for HTS magnetsThese objectives are aligned with the ultimate goal of my research program, which is to ""allow the emergence of a new generation of compact and efficient superconducting devices for improving security of supply in power systems and enable the emergence of high field HTS magnets"".Along the project, it is planned to train 3 Ph.D. students, 5 Master's student, and 5 summer interns. Some parts of the project will be carried out with international partners in order to take advantage of existing infrastructures and speed-up the generation of research results.     ""610819,""Sirouspour, Shahin"
"615362"	"Skillicorn, David"	"Adversarial Data Analytics for National Security"	"The goal of this research is to design algorithmic techniques and systems for adversarial data analytics, detecting the traces of bad actions in large datasets. These can be used to protect Canada in areas such as law enforcement, counterintelligence, counterterrorism, and financial enforcement. These techniques and systems also have applications in mainstream business settings, such as customer relationship management, whenever those building models and those being modelled have divergent interests. Any dataset that describes the activities of humans involves complex, non-linear phenomena, and this is especially the case when some of the humans are actively trying to conceal their activities and mislead the analysis. Data that are derived from language and social behaviour are especially useful because concealment and manipulation are difficult in these domains.The first major objective of this proposal is a deeper understanding of how to reverse engineer language to understand the mental state behind it, and how local connections in social networks reveal the power and intent of its members. A new approach to analytics, known as deep learning, has been successful in some longstanding difficult problem domains; the second major objective is to leverage deep learning ideas and techniques in the adversarial domain.This research will provide students at all levels with skills that are desperately needed in government and industry to defend Canada against other nations, international criminal organisations, and domestic crime.""614025,""Skinner, Cameron"
"614528"	"Smith, Michael"	"Development, Optimization and Implementation of New Algorithms for Improving the Resolution of EM, Acoustic and fMRI Reconstructions"	"Many people are familiar with the internal body images available through magnetic resonance (MR) imaging. Unlike photographic images, MR images are not captured directly as an image but must be calculated using a discrete Fourier transform (DFT) algorithm from signals excited from the hydrogen nuclei in the body. It has been said that ""There are more DFT algorithms used to analyze signals on any given day than any other algorithm"". This implies ""There are many research opportunities available to understand and solve the limitations of DFTs."" A basic problem is that a compromise must be made between spatial resolution, the quality of the MR image captured, and time resolution, the ability to track changes such as imaging the moving heart. False information appears in the image which confuses diagnosis. My team has recently combined our own constrained reconstruction for improving spatial resolution with proposed sparse sampling approaches that deliberately time save by gathering less data. Some unexpected preliminary results were found, with some parts of the image improving quality while others became more distorted. We are developing a new post-processing reconstruction process capable of mathematically manipulating the MR signals to produce an image with all parts having the quality to meet diagnosis needs. This requires combining the best parts of many poor quality images -- similar to recent ideas of replacing the bulky (thick), expensive, high-resolution camera lens in mobile phones with software that combines images from many flat, inexpensive, low-resolution lenses.  We have identified that our new approach has the capability of improving the capability of a wide range of signal and image reconstruction algorithms: (1) using a sparse array of acoustic sensors to gather intensity and phase information leading to industrial and community noise localization to help with our Community Acoustic Noise which has recently received considerable media attention, (2) improving the low-resolution fMRI images to provide new markers to diagnose Optical Neuritis (ON). ON is often a preliminary stage leading to Multiple Sclerosis; unfortunately common in northern countries such as Canada and (3) improving an industrial real-time lab bench embedded system's resolution in characterizing magnetic near-field effects measured using a sparse 2D hall-effect sensor array, Developing these algorithms in the laboratory is one thing, but we must be able to apply these algorithms in practical industrial, medical and community situations. We have been following up on the recent movement of successful business (enterprise) ""Agile oriented software processes"" into reliable embedded system code. The lack of literature has led us to suggest we use our proposed new reconstruction algorithms to undertake systematic investigations into using Agile ideas in a research field; an area with a potential wide audience.""611647,""Smith, Myron"
"616259"	"Soleymani, Leyla"	"Development of photoelectrochemical DNA biosensors based on plasmonic/photoelectrochemcial interactions"	"Biosensors are devices used to detect and/or quantify biological analytes such as nucleic acids, proteins, and metabolites for managing diseases and monitoring health. Handheld biosensors that bring this analytical capability to the patients' bedsides, patients' homes, local clinics, and in general, the point-of-care, are envisioned to significantly improve the quality of healthcare while reducing its cost by enabling preventive, expedited, and patient-centered clinical decision making. Nucleic acids such as DNA and RNA are important predictive, early diagnosis, and treatment selection and monitoring biomarkers, and there is an ongoing effort towards creating handheld nucleic acid biosensors. In spite of these efforts, there are currently no commercially-available handheld nucleic acid biosensors, similar in functionality to electrochemical glucose monitors, for disease management. The difficulty in creating handheld nucleic acid biosensors is that on one hand, they are required to meet the sensitivity, specificity and multiplexing required for clinical decision making; and, on the other hand, they need to offer these through portable, rapid, easy-to-use, and inexpensive platforms that are suitable for use in non-laboratory settings. Developing new knowledge in the field of photoelectrochemical signal transduction for addressing the requirements of handheld nucleic acid biosensors is at the center of this research program.Photoelectrochemical nucleic acid biosensors use photo-active materials to translate nucleic acid hybridization into changes in electrical currents. These devices offer a larger signal-to-noise ratio compared to their widely-explored electrochemical counterparts that use electrical signals for excitation and measurement. Furthermore, given that signal readout is done electrically, photoelectrochemical sensors still benefit from the ease of multiplexing and the inexpensive readout instrumentation available to electrochemical devices. We propose to develop a novel photoelectrochemical biosensor based on the newly discovered short range (20 nm) interactions between photoelectrochemical and plasmonic materials. In this sensing strategy, nucleic acid hybridization causes a plasmonic nanoparticle to be removed from the vicinity of the photoelectrochemical material, which will generate an increase in the generated photoelectrochemical current. Furthermore, built-in signal amplification strategies, based on DNA self-assembly, will be incorporated - for the first time - within this class of biosensors to enhance the generated current and the analytical sensitivity of the device. The proposed research will allow us to generate new knowledge for developing the first multiplexed nucleic acid biosensor based on photoelectrochemical/plasmonic interactions, for meeting the requirements of point-of-care biosensors.""616761,""Soleymani, Leyla"
"615136"	"Sood, Vijay"	"GRID INTEGRATION OF RENEWABLE ENERGY"	"The increased awareness for environmental preservation and the desire to reduce greenhouse gas emissions has caused a higher utilization of distributed renewable energy sources. As a result, the electrical grid is changing from utilizing large, remote fossil-fuel based generators to smaller, locally-embedded renewable energy generators. Deregulation has caused increased competition within the industry and a desire for improved efficiency. Furthermore, with the convergence of different technologies such as power systems, power electronics, information technology, communication and sensors, the grid can now be transformed into a truly smart grid capable of operating in a highly coordinated manner through intelligent management systems to realize its full potential.My research program concentrates on system studies to demonstrate the opportunities and expose the weaknesses in the smart grid implementation. The problems that will be studied relate to: - Two-way power flow in the feeders and its impact on critical grid components such as transformers which will be subjected to frequent energizing/de-energizing cycles. This will lead to reduced equipment life-span due to voltage stresses and other power quality issues (such as harmonic generation).- Smart grid components i.e. fault current limiters, transfer switches and other protection equipment needed to coordinate the integration of both legacy and modern-age equipment.- Renewable energy integration issues which will need advanced converters and their controllers to feed in power from a variable voltage/frequency into a grid that is operating at a fixed voltage/frequency. Conservation and Demand Management (CDM) issues specially taking advantage of storage components and Time-of-Use controllers. Although transformation of the legacy grid towards the new smart grid has already begun, its full implementation may take decades before reaching a desired level of operational flexibility, security and efficiency. This research will be useful for power utilities, consultants and manufacturers of electrical equipment. It will benefit the power system planners, operators and commissioning members of utilities. This research will provide ample opportunities for the training of HQP and allow university collaboration with local distribution companies (LDCs), consultants and manufacturers. The intellectual property generated will provide deeper insight into smart grid behavior and operation. There is a global shortage of HQP in this field. The next decade will see huge investments in infrastructure for the renewal of the aging legacy grid components.""605802,""Soong, Christopher"
"614777"	"Spachos, Petros"	"Wireless Sensor Networks for Smart Cities"	"Every day more computer-based devices are connected to the internet. Most of these devices have at least one sensing unit, creating opportunities for direct integration between the physical world and computer-based systems. This is the idea behind Internet of Things (IoT) and Smart Cities - a development of the Internet in which everyday objects have network connectivity, allowing them to send and receive data. Imagine a city where the street lights follows the traffic requirements in real-time and the air pollution form factories and cars is controlled with the use of data coming from tiny, cheap and energy efficient sensors. In the near future, Wireless Sensor Networks (WSNs) are expected to be integrated into the IoT and consequently to smart cities. The sensing infrastructures have a major role in smart environment and great research opportunities. Sensor nodes can join the Internet dynamically and use it to collaborate and accomplish their tasks. The future Internet, designed as an IoT is foreseen to be a world-wide network of interconnected objects uniquely addressable, based on standard communication protocols. Sensors will have a crucial role in smart cities. Green cities, which span from urban wildlife initiatives to city agriculture and green transport networks, citizen sensing and smart cities projects are emerging that attempt to realize improved sustainability through greater urban connectivity.At the same time, the sensor node market continues to grow rapidly, as the cost to build custom nodes decreases due to technology scaling. Moreover, the use of embedded sensors in smartphones, phablets, tablets and mobile devices has enabled a number of popular applications. Advances in technology, engineering, and materials science have opened the door for increasingly sophisticated sensors and sensor integration to be used in environmental and urban monitoring research.Beyond improving sensor integration, the concept of sensor in smart cities opens the door to as yet unexplored applications that cannot be conceived using specific/ devoted hardware, especially in the field of mobile devices. A flexible embedded sensor framework can be programmed and re-programmed in real-time, based on application needs, a unique characteristic that can be exploited for varied purposes, including improving application speed, power and cost. However, the security challenges of this new research area should be examined as well.I propose a research program to comfort these gaps and challenges through the development of a general framework for smart cities, starting from smart buildings. My long-term objective is to develop a multi-sensor dynamic system framework for energy efficient and secure components of the smart cities, through interdisciplinary collaborations in academia and industry. Also, I will train high- quality personnel who can satiate industry's growing demand for network engineers.""614555,""Spafford, JDavid"
"615526"	"Stebila, Douglas"	"Quantum-safe cryptography for the Internet"	"Public key cryptography is essential in securing all Internet communications. For example, the Transport Layer Security (TLS) protocol uses public key cryptography to protect every ""https"" web page for entering passwords or credit card numbers. However, all public key algorithms used in practice are based on mathematical problems-such as factoring, discrete logarithms, or elliptic curves-which could be broken by a quantum computer. The field of quantum-safe cryptography, also called post-quantum or quantum-resistant cryptography, aims to construct public key cryptosystems that are believed to be secure even against quantum computers. Ongoing advancements in physics point toward the eventual construction of large-scale quantum computers. Such future devices would still be able to decrypt present-day communications, allowing anyone to decrypt data transmitted today. Thus, it is important to start developing and deploying quantum-safe cryptography now, even before quantum computers are built.Several mathematical techniques have been proposed for constructing quantum-safe cryptosystems, including lattices (and the learning-with-errors problem), error-correcting codes, multivariate equations, and hash functions. These have been used to construct public key encryption and digital signature schemes, as well as complex functionality like fully homomorphic encryption.Unfortunately, all current quantum-safe algorithms have drawbacks that make them unsuitable for practical use. Some require larger key sizes or ciphertexts, increasing communication costs; others have slow implementations. Existing research in quantum-safe cryptography has focused on public key encryption and digital signatures, whereas most real-world protocols need authenticated key exchange. There is little research on how to efficiently and securely integrate them in applications, and their performance characteristics.The overall aim of this research program is to design cryptographic systems that are secure against attacks by quantum computers and are practical for use on the Internet, with a specific focus on authenticated key exchange protocols. We will achieve this by designing new and improved algorithms for a variety of cryptographic tasks and analysing and testing these algorithms in the context of deployed protocols to identify techniques that are suitable for long-term use.The results of this research will support the creation of Internet technologies that protect communication from quantum computers. The anticipated outcomes are practical new encryption algorithms, open source software, and recommendations for the use of quantum-safe cryptography in Internet protocols, ready for use by Internet standards bodies. Canadian Internet users will benefit from enhanced security. This research supports Canada's strategic investment in becoming a leader in quantum technologies.""597349,""Stechlinski, Peter"
"615736"	"Stege, Ulrike"	"Computational Problems & Cognitive Functions: Modeling, Characterizations, and Solutions"	"Due to rapidly advancing technology, many internet of things devices and apps now extend human capabilities, ranging from support for experts to everyday activities. As highlighted in IBM's News Room, David Stokes, Chief Executive for IBM, UK and Ireland, posits that ""We're at the dawn of a new era of cognitive computing, during which advanced data-centric computing models and open innovation approaches will allow technology to greatly augment decision-making capabilities for business and government."" Research on this topic is commonly referred to as human problem solving (HPS).To advance research in this field, an increased understanding of the how, when and what of HPS is required. Fundamental to this endeavour is the characterization of cognitive functions. My research program investigates fundamental questions: What problems can people solve successfully? Which ones cannot be solved? What are the properties of the ones that people are interested in solving? What are successful solving strategies? We might find answers to some of these questions in the properties of games and puzzles, such as Sudoku.To identify and characterize cognitive functions, we draw on methods in classical and parameterized complexity, the development of exact algorithms, as well as empirical studies and their analyses. As an effective data collection and problem solving platform for our empirical studies, we intend to utilize Lively, an open source infrastructure for Web applications programming.An improved understanding of human problem solving impacts many different application areas directly, including but not limited to: (a) The game-design industry. Methods for identifying properties of the cognitive functions used when playing games or puzzles that many people are attracted to and intrigued by while having a satisfactory playing experience will provide the industry with strategies and tools to create new games involving these properties.(b) The design of software in general and the quality of self-adaptive systems in particular. For example to design self-driving cars that are safer than human operated cars, it is beneficial to understand how humans make decisions on the road. This will aid the design software functions that have to react to human decision making in other cars. (c) Education. The knowledge gained will be informative for educators at all levels, including teaching problem solving skills.(d) Training. Highly qualified personnel (HQP) trained in this area and in my research group in particular acquire an interdisciplinary education based on solid foundations in theoretical computer science, and learn how to conduct empirical studies. These HQP are highly sought after by industry in general, and in Canada in particular.  ""615704,""Steimle, Viktor"
"614697"	"Swamy, Chaitanya"	"Algorithm Design in Strategic and Uncertain Environments"	"As information and communication systems become increasingly complex, there is a growing need for the development of efficient algorithmic tools for the design, maintenance and management of such systems, especially decentralized networks like the Internet and high-speed telecommunication networks. This research program will investigate the algorithmic questions that arise in such settings, focusing on two main problems. (a) The design of protocols that need to function in the presence of self-interested agents (e.g., autonomous systems on the Internet). In such settings, the task of algorithm-design is complicated by the fact that the data relevant to the problem is often controlled by self-interested agents. Consequently, one needs to elicit this data by cleverly incentivizing the users so that self-interested behavior leads to desirable outcomes. We shall focus on the development of techniques for the design and analysis of such efficient ""manipulation-resistant'' mechanisms. A particular emphasis will be to identify ways for ""exporting'' algorithms into manipulation-resistant mechanisms. We shall approach this task both by examining well-entrenched notions of manipulation-resistance such as truthfulness, and by considering the algorithmic Nash implementation problem of designing mechanisms whose Nash equilibria possess desirable properties. Our research will draw upon ideas from Computer Science and Economics and contribute to the thriving field of algorithmic mechanism design, an area whose importance has been recognized by leading IT companies such as Google, Yahoo, Microsoft. (b) Decision-making under uncertainty. Uncertainty is a facet of many decision environments, and for effective decision making, one needs to consider models that explicitly incorporate the uncertainty. A broad objective of our research is to explore models that abstract such settings, and devise methods for solving the algorithmic problems that arise in such models. We shall study key problems that capture the underlying difficulties in such settings, focusing on the rich class of stochastic vehicle routing problems, and devise techniques that will find use in a variety of settings. We shall also investigate ambiguous stochastic problems, wherein the distribution modeling the uncertainty is itself imprecise. The techniques we develop should be useful in the design of communication networks, which often have to handle uncertain traffic. In many cases, the algorithmic problems encountered are computationally intractable, and our approach will be to devise an approximation algorithm for the problem, that is, a polynomial-time algorithm that always delivers a provably near-optimal feasible solution. Approximation algorithms are a common thread in our research, and our research will also further the development of techniques for the design and analysis of these algorithms.""616464,""Swamy, Srikanta"
"615478"	"Tadj, Chakib"	"Ontario"	"CANADA"
"615119"	"Tatu, Serioja"	"Millimeter-wave High-performance Front-ends for Future Ubiquitous Wireless Networks and Sensors"	"Millimeter wave (mm-wave) bands are especially of interest because a massive amount of RF spectrum has been allocated worldwide for wireless communications (57-64 GHz, 71-76 GHz, 81-86 GHz, and 92-95 GHz bands), automotive radar (76-81 GHz) or imaging sensors (above 86 GHz). Intensive research efforts have been started to design compact and low-cost mm-wave transceivers for these emerging applications. In the past year, preliminary interest and discussions about a possible 5G standard have captured the attention and imagination of researchers and engineers around the world. Wireless data traffic is projected to increase 10,000 times within the next 20 years. Fundamental limits on hardware implementation and channel conditions limit the viability of the conventional option, the design of spectrally efficient systems (such as 4G LTE) in bands below 6 GHz. In order to achieve several Gb/s data-rates, the mm-wave spectrum enables the use of large bandwidths (e.g. 2 GHz) with simpler and low-cost modulation techniques, rather than relying on highly complex techniques, originally used to ensure a high spectral efficiency with smaller bandwidths. In addition, mm-wave bands ensure very high resolution for radars and other sensors. Automotive radar and imaging sensors can benefit of inherent advantages like weather independence and direct acquisition of range and velocity when compared to alternative sensors like video, laser and ultrasonic. Hot topics are also security and surveillance applications to identify weapons hidden under the clothing and for object detection which are not detected by conventional magnetometers. Technical challenges to be addressed today in the mm-wave research are unprecedented. The proposed program will take advantage of our research team's strong expertise in mm-wave circuits, systems, and interferometric techniques. Novel designs using large scale fabrication technologies, suitable for future ubiquitous applications, will be considered. The general objectives of this research program are: (i) to propose and to validate innovative and performant mm-wave transceiver architectures for emerging applications; (ii) to find new solutions for high energy efficiency and miniaturized front-end multi-chip integrated modules using low-cost fabrication technologies; (iii) to establish a permanent pool of highly qualified researchers at the INRS-EMT, for the development of RF technologies. Research efforts will be oriented towards the design of new system architectures, prototype transceiver fabrication using integrated technologies with innovative mm-wave components and advanced test-bench measurements. The new knowledge that will emerge from this program and the training of highly qualified personnel associated to the different stages of this work will certainly contribute to enhance Canada's global competitiveness in this high-technology sector. ""599863,""Tatu, Serioja"
"615427"	"Tepe, Kemal"	"Data link and network layer protocol design for wireless Internet of things (IoT), Machine to Machine (M2M) and vehicular communications"	"The Internet of Things (IoT) has evolved from Wireless Sensor Networks (WSNs) to connect smart devices to each other. IoT is the next big leap in information and communication technology and is estimated to reach a 1.7 trillion dollar (US) market size with over 50 billion IoT devices by the year 2020. IoT is likely to have an astonishing impact in areas such as energy management, transportation, industrial control, and healthcare; all of which are essential to have a sustainable development and utilization of the Earth's scarce resources.IoT is currently still in its early stages for delivering the promised impact. The challenges that remain to be resolved are in real-time and reliable communications, quality of service provisions, and interoperability among different communication technologies. For example, it is extremely difficult to achieve ubiquitous coverage with a single network technology. Therefore, interoperability among different architectures and technologies must be provided. IoT is expected to operate reliably for a number of years with batteries, which require balanced energy consumption. The protocol stacks of heterogeneous IoT networks must be able to translate the requirements between different standards to reach desirable objectives.Vehicular Networks (VNs) are essential in providing active safety and emergency applications to reduce and possibly eliminate vehicle accidents. That is why automotive and communication industries are cooperating together to develop Vehicle to Vehicle (V2V) and Vehicle to Infrastructure (V2I) communication technologies. IEEE 802.11p also known as Dedicated Short Range Communications (DSRC) standard for safety messaging was a result of this cooperation. DSRC allows message exchanges among vehicles to inform their vital mobility information such as heading, location, and speed for collision avoidance and emergency applications. However, V2V and V2I communications are still not reliable since a large packet generation rate in a relatively busy network can cause congestion, interference and processing load. There is also no provision in DSRC for multi-hop networking, where the utilization and effectiveness of the system will be significantly enhanced if V2V allows networking to extend the range of the information dissemination. Content based networking is important and will be explored for the V2V communications. Developing a research tool (middleware) to increase pace of the research in IoT is extremely important. This tool will bridge the gap between high level algorithm development environment and hardware platforms and be developed as part of this proposed research in order to verify the completeness of IoT protocols.Provided funding will enable to solve above mentioned problems related to IoT and VNs, contribute to Canada's competitiveness by developing intellectual property and contributing HQP training.""595711,""Terebiznik, Mauricio"
"614534"	"Tompa, Frank"	"Flexible and Efficient Text Storage, Management, and Retrieval"	"Applications relying on text must be built upon solid foundations of data organization similar to those that underlie conventional database systems. Thus the central long-term objective of the research program is to support document storage and management by applying sound database principles to this domain of information. The challenge is to discover how the complexity of text, with its intricate structure and diversity of expression, can be efficiently and effectively accessed and managed. In the short term, this objective will be addressed through the design and implementation of improved search engines.The research program will include three related investigations. I will explore problems of general-purpose search engines, including aspects specific to reference texts, where nested structure is central to locating data. For example, I will investigate how the mechanism of database views can be best applied to describe, design, and implement an index that provides efficient search, including support for phrasal queries. I will also examine aspects specific to providing optometry researchers with access to Electronic Medical Records, where free-text fields include many spelling errors and where ensuring patient privacy is mandatory. In the third thrust, I will concentrate on aspects specific to mathematics information retrieval, where the relative positioning of symbols is at least as important as the particular symbols used. The inadequacy of existing solutions leads to the design of improved algorithms, which I then analyze, implement, and evaluate. Standard benchmarks provide the basis for comparative evaluations, but often additional test collections must be developed to examine specific aspects of the problems from unexplored perspectives. Graduate students will participate in all aspects of this research, resulting in HQP well-prepared to contribute to this area. The centrality of documents in recording and preserving information suggests that this research remains timely and crucial to economic growth in Canada. The integration of information retrieval and database management is well-recognized as being important to Canadian business and could be equally important in promoting Canadian culture. More specifically, the reliance on text search has become a universal need, with search engines providing critical roles in gathering information internet-wide, from across one or more enterprises, or from one's own personal data collections. In addition, other text applications are found throughout business and government. Publishers, data providers (e.g., via the World Wide Web), and organizations that rely on any form of text-dominated knowledge base for conducting their internal and external business will benefit from specific tools that arise from this research as well as from the theory, which will provide a framework for designing their text management systems.""611560,""Tomy, Gregg"
"616312"	"Valaee, Shahrokh"	"Privacy Preserving Location Estimation"	"This research will investigate privacy in location-based services. The first step in providing a location-based service is to locate the user. This might compromise the user privacy. In this research, we will be developing methods in which the location of user is hidden from the location server, and the service provided is also unknown to the server. We are interested in developing location-based services where the location server cannot learn user's location, but can compose location-related services, while not being able to identify the services obtained by the user. Also, the user can only receive services pertaining to its location, without learning about the whole space of services. In the first view this objective might be a non-starter, as the location information is needed to build the service. However, recent advances in cryptography show that certain signal processing arithmetic can be performed in the encrypted domain. In this research, we will use two methods for encrypted signal processing.As the first method, we will use additive homomorphic cryptosystems. In additive homomorphic cryptosystems, the encrypted addition of two numbers can be calculated by the multiplication of their encrypted values, and the multiplication of two numbers can be calculated by taking the encrypted value of one of them to the power of the other number. Hence, a new algebra is defined in which addition is replaced by multiplication and multiplication is replaced with the power operator. Some other operations such as Euclidean distance measurement and minimization can also be performed at the encrypted domain. We will investigate private location estimation using encrypted signal processing methods. A second group of methods for privacy preserving use randomized embedding, in which data is projected onto a lower dimensional subspace while the distance between adjacent vectors remains constant. Recently, the area of randomized embedding is receiving a lot of attention from compressive sensing and big data community. Lower dimensional embedding can hide the content of data in the higher-dimensional domain. Examples of such mappings are locally sensitive hashing, and locally linear embedding. In this research, we will build localization methods that will provide privacy preserving location estimation using both additive homomorphic cryptosystems and randomized embedding. We will develop our algorithms for client-server applications, and also for multi-party cooperative localization. ""604578,""Valdez, Travis"
"615834"	"Van, Vien"	"Advanced Applications of Two-Dimensional Coupled Microring Resonator Networks"	"The proposed research is in the area of integrated photonic devices, with focus on advanced applications of coupled microring resonators (CMRs) for optical communication and optical interconnect systems. Microring resonators are travelling-wave optical resonators which have become a key building block in many photonic integrated circuits, thanks to their compact sizes, versatility, and ease of fabrication and integration. One of the most important applications of microring resonators is the construction of high-order optical filters by coupling multiple resonators together. Conventional microring filters are based on the one-dimensional (1D) coupling topology, which can only realize a limited set of spectral responses. By extending the coupling topology to two dimensions (2D), much richer spectral characteristics can be synthesized for advanced applications in optical spectral engineering, such as high-performance filters and dispersion compensators. The objective of the research is to investigate and explore novel applications of a new type of 2D CMR networks, namely those with complex field coupling coefficients. In these devices power exchange between adjacent resonators takes place via both the magnitude and phase of the electromagnetic fields, giving rise to novel effects not present in conventional CMRs with real coupling coefficients. The research aims to develop theory and techniques for designing and realizing general 2D CMR networks with complex field couplings. Novel applications such as bandpass filters with built-in dispersion compensation for Wavelength Division Multiplexing (WDM) communication systems, and the emerging class of fabrication-tolerant topologically-protected photonic devices will be proposed and experimentally demonstrated in the silicon material. The latter devices are based on the concept of 2D topological insulators in electronic systems, with unique properties such as robustness against structural disorder making these devices highly tolerant to fabrication imperfections. In addition to advancements in the field of microring resonators, the research is also expected to contribute to the development of silicon integrated photonics technology. Silicon photonics is rapidly becoming the industry standard for manufacturing photonic integrated circuits, driven by the growing demand for low-cost and large-scale integrated optics components for WDM and optical interconnect applications. The proposed research provides excellent opportunities for training highly-qualified personnel (HQP) in both the theoretical and technological development of silicon photonics. It is expected that the expertise gained, intellectual property developed and HQP trained through the research program will help accelerate the development of silicon photonics industry in Canada, as well as the broader optics and telecommunication industries.""607221,""VanAcker, Chloe"
"615150"	"Varro, Daniel"	"Model-based Design and Validation Techniques for Smart and Safe Cyber-Physical Systems"	"Motivation and Significance. Smart cyber-physical systems (CPS) will connect low-energy sensors and actuators, mobile devices and the computational power of cloud infrastructures with traditional critical embedded systems (like cars, medica devices, etc.) where a failure may result in major financial loss, severe damage or even casualties. But how can we guarantee the safety, resilience and trustworthiness of such smart and critical systems in a continuously evolving, open environment?Research Context: A smartsafe CPS needs to (i) autonomously perceive its operational context and adapt to changes in an open, heterogeneous and distributed environment with a massive number of nodes, (iii) dynamically acquire available resources and aggregate services in order to make real-time decisions, and (iv) continuously provide critical services in a safe, resilient and trustworthy way. Long-term Challenges: My research program will focus on the following long-term research challenges:(1) How to design and manage dynamically evolving smartsafe CPSs to fulfill multi-domain requirements (e.g. consistency, extrafunctional or physical)? (2) How to guarantee or validate that smartsafe CPS with multi-domain requirements deliver quality of service in an open and changing environment?Research Plan: My research program will develop innovative model-based design, management, validation and optimization techniques and tools for engineering smartsafe CPSs. Significant focus will be placed to come up with scalable solutions (applicable to industrial size systems) and incremental approaches (which exploit previously available information to make decisions) with precise semantic foundations.Research Team: This research program will contribute to the training of 6 PhD, 3 MEng and 8 undergraduate students to be carried out in an international environment. Outcome: The expected results include the (1) synthesis of consistent and large model instances with given coverage criteria to test CPS in various contexts, (2) distributed and incremental runtime monitors to detect critical situations, and (3) incremental rule-based design space exploration approaches to manage evolving CPS in a safe and optimal way.Impact: As smart devices and systems will gradually penetrate our every day life all over the world, their safety will likely be an increasing concern. Results of this research program will help guarantee that a new smart device can safely be integrated into our smart home to reduce energy consumption without undesired side effects. Connected cars may benefit from decision support provided by distributed runtime monitors to avoid accidents and traffic jams. Smart cities may increase catastrophic preparedness by using design space exploration techniques.""615405,""VarvaniFarahani, Ahmad"
"616265"	"Vaughan, Rodney"	"Smart antennas: transforming random scattered fields to controlled signals"	"Marconi's commercialization of wireless transformed communications into the most disruptive technology mankind has known. It was the demand for communications technology that spawned our discipline of electrical engineering over a century ago, and today it is still a prime driver of research - for disciplines ranging from mathematics to materials science. In electrical engineering, communications is the cornerstone of ICT (Information, Communications and Technology) and it now shapes the way we do business and socialize. This impact is accelerating and even economists concur that pervasive capability in ICT is key to a country's economic wellbeing. My discovery research is designed to create economic wealth for Canada by pioneering the world's premium design knowledge in carefully selected topics in communications. The central theme is the development of smart antennas. This choice is because the antenna is the single-most performance-limiting component in most wireless systems, and yet it is also the component with the single-most potential for improving the capacity and the energy efficiency which are critical for our future systems.A smart antenna comprises multiple-element antennas (MEAs), electronics, and communications signal processing algorithms. The idea is to suppress unwanted signals while enhancing wanted ones through adaptive control of the fields and waves. The design of smart antennas is underpinned by specialist skills and the critical ones are: the conception, design and evaluation of compact, wide-band MEA structures; the associated signal theory and processing; and the understanding and modelling of the propagation channel. These represent diverse disciplines which must be considered together for effective design: the MEAs must adapt to the radiowave propagation whose behaviour is the very reason for needing the advanced signal processing. Successful new smart antenna designs stem from a profound understanding of, and the interplay between: the scattering of waves and fields; the antenna elements; and the signal processing.This Discovery research is the backbone of my research programme. It will produce: new foundations and methods for widely-accepted MEA evaluation; the world's best MEA designs for future systems; new applications for the knowledge; and a new generation of world-class research and development engineers that will lead product development teams.The applications are wide. In communications, the applications range from short-distance links for the Internet of Things to satellite radar systems for earth resource monitoring. Other applications of the technology range from smart microwave heating of food for Canada's agricultural sector to smart spatial acoustic noise control for a more liveable environment and immersive acoustics.""603040,""Vaughan, Rodney"
"614648"	"Walus, Konrad"	"Advancing printed electronics and materials with lab-on-a-printer technology"	"The general area of printed electronics has grown very rapidly in recent years. The global market is estimated at $24B and expected to grow to $340B by 2030 with Canada's share at $1.2B by 2020. The long-term objectives of the proposed research program are to develop new knowledge, technology, techniques, and materials that hold the potential to fundamentally accelerate the development of printed electronics.One of the most important challenges in developing a new printable electronic device is that processing parameters, material composition, and geometry are coupled in complex and non-linear ways to affect structure, function, performance, durability, flexibility, and lifetime. Aspects such as the nature of solvent evaporation, self-assembly of particles or monomers, interlayer solvent interactions, and liquid dynamics in drying structures all contribute to morphology, structure, and ultimately performance. As a consequence, we find radical differences in electronic and optical properties for small changes in the process conditions. The application of combinatorial testing and optimization, an empirical method whereby many device variants are fabricated and tested to explore these complex design spaces has only recently been considered. Current printing platforms are not developed to apply this methodology economically and as a consequence there are very few reported examples. We are only now able to realize this capacity and its benefits through fundamental research and development of a targeted combinatorial testing and optimization platform based on the lab-on-a-printer (LOP) technology being explored in the lab of the PI. The key feature enabling this proposed research is that these LOP systems have the unique capability of providing software control over both the geometry and composition of the printed structure, allowing for rapid fabrication of complex device variants with both structural and compositional changes in one step. The research program to be supported by this proposed Discovery Grant will enable breakthroughs in our understanding of fundamental aspects of printed electronics technology. The objectives of the research are to: (1) understand the fundamental mechanisms and processes involved in the fabrication and function of printed electronics; (2) explore techniques to accelerate printed device development based on combinatorial device testing and optimization; (3) advance the experimental lab-on-a-printer platform technology, and (4) demonstrate and validate these approaches on printed high mobility carbon nanotube transistor and chemical microsensors that hold the potential to realize significant performance breakthroughs in printed electronics.""616595,""Walus, Konrad"
"615766"	"Wang, Jiannan"	"Crowdsourced Data Cleaning"	"Data cleaning is the process of detecting and correcting dirty (inconsistent, inaccurate, or incomplete) values from a database. Real-world data is often dirty. Analyses without data cleaning can be very risky, which may result in poor decision-making, and have a significant negative impact on applications. For example, in 2014, Statistics Canada under-reported the country's job creation in July by more than 41000, based on an analysis of dirty data. This news was reported by hundreds of TV and Internet Media outlets around the world, resulting in various negative effects on Canada.Although there has already been a long line of work on machine-based data cleaning techniques, many cleaning tasks are too challenging for machine only solutions. Recently, the advance of crowdsourcing techniques and platforms (e.g., Amazon Mechanical Turk) provides a highly promising way to involve humans and computers in solving complex problems at low cost. In view of this great opportunity, this proposal will study crowdsourced data cleaning, intelligently combining humans and computers to address challenging data-cleaning problem.This work will not only open up a new research area in the database field, but also benefit a lot of other scientific fields, such as library science or sociology, which often require to conduct data analysis on real-world datasets. Furthermore, with the rise of big data, the world is moving towards a more data-driven environment. Data cleaning has long been considered as a bottleneck for extracting value from data. Crowdsourced data cleaning, which has the potential of significantly improving data quality at low cleaning cost, will have an increasing number of applications in this new environment, such as cleaning customer information for reliable market analysis, and cleaning patients' medical history for accurate disease diagnosis.""606535,""Wang, Jiayi"
"615148"	"Wang, Xiaozhe"	"Impacts of Renewable Energy on Transient and Voltage Stability of Electric Power Systems"	"Stability is a fundamental premise for secure operation of every electric power system. As the loading demands are constantly increasing and the transmission networks are aging, the operating conditions of worldwide power systems have been pushed ever closer to their stability limits. Meanwhile, the integration of renewable energy sources has been constantly increasing. Particularly, Canada is a world leader in the production and use of all kinds of renewable energy. Although renewables bring numerous benefits, they also pose new challenges to power system stability. For example, the distinct dynamic responses of wind turbines and photovoltaic generators to disturbances may aggravate the contingencies, and thus deteriorate the stability of the overall power grid. As the penetration of renewables continues to increase in the foreseeable future, it is imperative to study the impacts of different renewables on various power system stability properties in order to enhance the stability and reliability of modern low carbon power systems.This research program intends to examine the impacts of the randomness of renewables on transient and voltage stability of electric power systems, which have not been well recognized and addressed. Indeed, the variability of renewables may affect the stability regions for transient stability and reduce the voltage stability margins. In addition, the proposed research program will develop analytical and computational tools to conduct stability analysis for the systems integrating renewables, and furthermore design control measures to enhance the stability of those systems. From the theoretical perspective, this research program will exploit concepts from advanced nonlinear system theory in the context of power systems, and extend the existing results by incorporating stochasticity and even produce novel theoretical results in their own rights. From the practical view point, the research outcomes will provide physical insights, quantitative measures and computational tools to system operators on evaluating and enhancing the stability of power systems integrating renewables in the real world. The research outcomes will advance seamless integration of renewable energy sources into power grids, which is advocated not only in Canada but also globally. The four graduate students to be trained in this program will be well-equipped to contribute to the future of the Canadian power industry. In particular, Quebec has developed a solid reputation on the world stage in hydro-electric development, and is now at the forefront of research on integrating significant amounts of wind power into its grid. In this regard, the goals of the research program comply with the interests of local industries, and the outcomes of this research program will generate various collaboration opportunities with industry partners in the near future.""610038,""Wang, Xihua"
"600475"	"Wang, XinRu"	"Nova Scotia"	"CANADA"
"614696"	"Weddell, Grant"	"Data Integration: from NoSQL to Main Memory Data Sources"	"Consider that a modern enterprise will have access to a large number of structured data sources that are managed by internal operational information systems or that can be accessed through external sites on the web. The sources will provide interfaces that can range from low-level file systems storing such artifacts as CSV files, to intermediate-level key-value systems managing very large volumes of operational data, through to high-level relational database systems providing access to structured data via SQL.Also consider that the enterprise will have an abundance of circumstances where code must to be written by developers to access many of these data sources at runtime in order to satisfy emerging integration requirements for its existing information systems, or to satisfy the requirements of new information systems currently under development. Such circumstances are instances of the general problem of data integration, the focus of this research agenda.Data integration is the problem of finding and executing query plans that manifest efficient ways to interact with data sources so as to obtain knowledge of their structured data, and to combine the results of this interaction in a way that, as efficiently as possible, yields the answers to user queries coded by developers. The problem becomes particularly challenging when user queries are expressed in a high-level non-procedural language, such as SQL, over a logical design. Such a design, sometimes called a mediated schema or ontology, is a problem or domain specific appreciation of the data entirely devoid of any knowledge of the data sources themselves. The increase in developer productivity enabled by data integration technology that provides such a developer interface is now indisputable; witness that relational technology continues to be a growing multi-billion dollar industry.There are three subproblems in data integration: (1) the selection of suitable languages for capturing so-called physical designs (physical designs augment logical designs with the source schemas of data residing at various data sources, with mapping rules relating the logical design to source schemas, and with knowledge of the capabilities of the data sources themselves), (2) the selection of suitable languages for expressing user queries and query plans, and (3) the problem of finding query plans given physical designs and user queries.This project is a continuation of my ongoing work on each of these subproblems, and therefore also adopts the hypothesis that first order logic (FOL) and so-called closed world semantics assumed by the SQL standard are an appropriate starting point or foundation for an underlying theory and technology of data integration. Indeed, FOL lies at the heart of relational database systems and the relational model of data, and has had a profound influence, via relational algebra, on the development of the SQL query language.""599266,""Wee, Ping"
"616378"	"Wigdor, Daniel"	"Enabling a Symphony of Devices"	"Imagine you are amotivational speaker roaming a stage, a politician standing at a podium, aprofessor tethered to a laptop, or a member of small group meeting. Foreseeingand designing an application to tailor itself to all of these uses, andthousands of others, is daunting. You could speak with only a projector; bothyou and your audience would only see your slides. But what if you connected aphone, and it displayed a list of slides, a timer, and your speaker notes. Ifyou added a smart watch, the display on your wrist could show you not only thecurrent time, but also the timer and the progression of your talk. Putting on ahead-mounted display, such as Google Glass would move your speaker notes,providing a private teleprompter. What if you would like your hands free? Putthe phone away and the display of available slides resizes itself and appearson your Glass. You could advance the slides by pressing a button on the watch,by speaking a command, or by tapping the side of your Glass. Each of theseconfigurations is appropriate for a different type of talk. My work focuses onhow the user can tailor an application's behavior simply by adding the rightdevices, and enabling the software to reshape and optimize its user experienceto the new topology.  Critical toachieving this future is meeting the challenge to enable software to grow andshrink to take advantage of all available hardware, without the need fordevelopers to foresee all hardware combinations in advance. Research inubiquitous computing thus far has focused on enabling transitions betweendevices, such reading a webpage or book on one device, and then moving it toanother to pick-up at the same point. This just begins to scratch the surfaceof what is possible. We are developing universal interaction methods to maximize the utility ofmultiple devices. Further, we are building tools to allow developers to definetheir applications in a way that enables portions of the UI to be automaticallyand intelligently allocated to any attached device capable of displaying UI,based on the match between software needs and hardware capabilities. Our workin this area is now expanding to enable the annexation of nearby headlesscomputing devices, such as mobile sensors, building tools to give developersincreasing levels of control. ""600733,""Wiggans, Mallory"
"615721"	"Wong, Vincent"	"When Smart Grid Meets Cloud Computing: Theory, Algorithm Development, and Evaluation"	"The smart grid is the next generation electric power grid. It aims to integrate with renewable sources and provide electricity in an efficient, reliable, clean and sustainable manner. Smart grid supports advanced metering infrastructure and real-time bidirectional communications between utility companies and their customers. This facilitates the use of demand side management, which allows the customers to schedule loads from on-peak to off-peak periods. The smart grid also supports the integration of microgrids, which have their own electricity generations and energy storage. For a successful deployment of smart grid, it is important for utility companies to (a) integrate large-scale distributed energy resources to the grid in a seamless manner; (b) develop efficient and reliable grid control and monitoring algorithms; (c) have sufficient computation capabilities and data storage to manage the large amounts of information from the smart meters. In the computer networking domain, cloud computing is the next generation paradigm for computing, information management, and data storage. Cloud computing involves many distributed data centers which are accessible through the Internet. On-demand computing resources can be rapidly provisioned to the customers. It is beneficial for the utility companies to offload some of the computational tasks and data storage to the cloud. However, data centers consume a significant amount of electricity to operate. It is important to develop energy-efficient algorithms to schedule the operations of servers in the data centers.In this proposed research program, we aim, in the long-term, to develop a cloud-based smart grid architecture, which (a) facilitates seamless integration of various types of distributed energy resources; and (b) supports real-time monitoring, forecasting, and control of distributed generators and distributed energy resources at the transmission and distribution level. We will focus on the following inter-related short-term objectives: (i) Design an energy-efficient cloud computing network architecture for smart grid to facilitate access, monitoring, data analytics, real-time control, load forecasting, and data storage; (ii) Develop cloud-based unit commitment, economic dispatch, and demand side management algorithms that can control various types of loads and heterogeneous distributed energy resources connected to the power grid in a scalable and reliable manner; (iii) Develop efficient cloud-based energy trading and pricing algorithms for energy systems and microgrids in different market types. We anticipate that our research will find practical novel solutions and applications for the benefit of the local and the global energy systems and telecommunications industries. They are expected to have a lasting impact in contributing towards the long-term competitiveness of Canada's technological innovation.""598166,""Wong, Vincent"
"614702"	"Wright, Derek"	"Design Automation Revolution for Microfluidics"	"Microfluidic systems, sometimes called biochips, are used in biomedical, biochemical, pharmaceutical, and chemical applications. The global microfluidics market is anticipated to reach $3.5B by 2018 and $7.5B by 2020. Despite this, there is no industry-standard design automation toolchain available, as for other industries; for example, the Electronics Design Automation (EDA) toolchain is used to design complex yet reliable integrated circuits. Microfluidic systems are addressing new and challenging applications such as rapid, accurate disease diagnosis, and accelerated drug discovery. As these devices address ever more difficult and diverse applications, managing design complexity becomes increasingly difficult and costly. Complete microfluidic systems can offer a competitive edge in performance and cost over traditional benchtop solutions, however, most microfluidic systems are one-off designs, the optimization, validation, and verification of which is difficult and time consuming. Engineers need a standard, integrated, and streamlined modeling and simulation process to facilitate the widespread application and development of microfluidic systems. The primary objective of this research program is to create such a toolchain for microfluidics. This will accelerate the development of the microfluidics industry, producing efficiencies consistent with those of the semiconductor industry. This will also provide rigorous design standards that ensure quality, reliability, and scalability. Crucial to the creation of this toolchain is the development of a layer of abstraction analogous to circuit simulators for electronics (lumped-parameter modeling and simulation), currently absent from the microfluidic design workflow. Today, microfluidic systems must be designed using coarse system-level Hardware Description Language (HDL) tools, or direct creation of the layouts in CAD. Activities that integrated circuit designers take for granted, like Monte Carlo analysis and global optimization, are simply not possible without such a modeling and simulation platform. This research will deliver a schematic-level, physics-based, lumped-parameter modeling and simulation framework from the device level to the system level, providing linkages and workflows across the levels of abstraction and integration. Such a framework will revolutionize the way that microfluidic devices are envisioned and created, by dramatically improving model quality, simulation fidelity, and scalability of microfluidic designs. There is commercialization potential of the validated fabrication-ready component libraries, and any proprietary software, processes, or algorithms developed. The knowledge and IP created will place the University of Waterloo at the forefront of microfluidics design and manufacture, and contribute to Canada's role as a global leader in the field.""613873,""Wright, Gerard"
"615564"	"Wu, Fangxiang"	"Algorithms for Complex Network Control and Their Applications for Drug Target Identification from Biomolecular Networks"	"Many systems of scientific interests can be modeled as complex networks. The ultimate proof of our understanding of such a complex network is reflected by our ability to steer it from an undesired state to a desired state. On the other hand, it is well acknowledged that a complex disease stems from the malfunction of some complex biomolecular networks that control its pathogenesis. Drugs are essential for steering the biomolecular networks from a disease phenotype (state) to a healthy phenotype (state). A drug target is a biomolecule which is involved in a biomolecular network that controls the pathogenesis of a specific disease and via which states of the network can be changed by combining with suitable drugs. Some drug targets may be more or less efficient than others in biomolecular networks. Identifying the most efficient drug targets is a very early and critical step in the drug design and development process as the costs of late failure are significantly higher than those of early failure in that process. A complex network can be mapped to a dynamic system and thus control issues of the complex network can be studied. As a result, we can view drug targets in a biomolecular network as steering nodes in a controlled dynamic system. By applying control signals (drugs) to steering nodes (drug targets) we desire to steer a malfunctioning biomolecular network from a disease state to a healthy state. We can thus formulate drug target identifications from biomolecular networks as some control issues of dynamic systems. The long-term goal of my proposed research is to develop advanced algorithms for some control issues of complex networks while providing bioinformatics tools for identifying drug targets from biomolecular networks. To achieve my long-term goal, three specific objectives are designed in this proposal. Objective 1: developing algorithms for transittability of complex networks and applying them for identifying drug targets from biomolecular networks; Objective 2: developing algorithms for output controllability/transittability of complex networks and applying them for identifying drug targets from biomolecular networks; and Objective 3: developing optimal control methods for complex networks and applying them for identifying drug targets from biomolecular networks. The successful outcomes of the proposed research will have considerable ramifications for drug target identifications of complex diseases. The proposed research program can foster the multidisciplinary research environment in subjects of computer science, complex network science, control systems, and pharmacology. The proposed research program will also enable effective cross-disciplinary training of high qualified personnel (HQP) in a collaborative research environment to increase knowledge, research skills, and expertise of HQP in multidisciplinary areas of the aforementioned subjects.""615586,""Wu, Gang"
"615722"	"Wyvill, Brian"	"Data driven REActive Modelling"	"Data Driven Reactive Modelling: SummaryWe live in a world where computer graphics plays an ever increasing part. Whether it is interacting with a video game or simulating a delicate surgical operation, accurate models of real world objects are required. In recent history, models that appear realistic have satisfied the movie and games industries, but for simulations of surgical procedures or fighting forest fires models that behave in a realistic fashion are necessary. Although the advent of the Graphics Processor Unit (GPU) has enabled current models to be realistically rendered in real time, the architecture of these machines is designed around paradigms for modelling that were built decades ago. Paradigms based on meshes of tiny triangles were not designed to represent objects by their volume nor by their behaviour. In this research we seek new paradigms to describe models that exist in a real or artificial world, but can react to their surroundings, behaving realistically alongside real objects in an enhanced or virtual reality situation. Such a paradigm may not fit within the pipeline that is supported by current GPU technology. Indeed, new pipeline architecture could well be a future extension of this research. Much work in computer graphics looks only at solving immediate problems to produce realistic games or movies. In this research we look ahead to future decades where accurate simulation, for example of the human body, and all the physical interactions such a simulation might involve, can be supported in real time. We wish to find new methods to ease the task of representation, design and animation of 3D models, particularly where the model has to conform to a number of physical and aesthetic constraints, and in animation, where the constraints can change in real time.The result will be a reactive model, whose behaviour takes account of the world in which it exists. The near future goal is to extend the concept of implicit modelling, in which models are represented as an iso-surface in a scalar field. Such a field can be generated from geometric and other primitives, which can be derived from data or interactive input, or a combination of the two. By designing suitable functions the combination of fields and their gradients can yield complex, reactive models.Our most recent work combined mesh and implicit modelling to achieve the most accurate, real time, automatic skin deformations to date in an animated character. We wish to extend this work as well as search for a more suitable paradigm that will bring together knowledge from several disciplines, including computer graphics, vision, HCI and artificial intelligence. We also look beyond the theoretical to application areas such as a soft tissue simulation that will enhance training in fields such as neurosurgery.""596679,""Xanthos, Foivos"
"614877"	"Xie, LiangLiang"	"Towards a New Era in Network Information Theory: A New Approach to Converse"	"Network information theory provides the fundamental guidance to the optimal design of all kinds of multiuser information systems, including wireless networks. As a theory, it has made many very successful progresses, and has dramatically transformed our view of how to develop communication systems when multiple users are present. Technologies originated from these theoretical studies include the famous code-division multiple-access (CDMA), which has become the standard for all advanced cellular networks.However, this useful theory has met with many difficult problems in its own development, some of which have been long standing for many years. Although we can already learn a lot of interesting results from this theory, many of which are to provide significant improvement for practice, the theory itself has not completely solved its own basic problems. In fact, many of the best results we know so far are still those obtained in 1970s.This is really a frustrating situation for people working in this field. For one thing, it is without doubt that this field of research is of pivotal importance in this information age, when information is such an indispensable part of our daily life, and networking among multi-users becomes such an essential character of modern societies. This pure reason has attracted many talented and hard-working people into the field. However, after decades of effort without real breakthroughs, any determined mind will doubt the worth of it. In fact, it has become no secret in the field that more and more people are turning away from the fundamental problems, to more hopeful new application topics. However, for the theory itself, when it cannot solve its own basic problems, it may not be far from dying.  Is it already hopeless? In retrospect, efforts in network information theory have been mostly concentrated on achievability results. Here, I will focus more on the converse. Instead of the classical approach of using Fano's inequality, I will develop a new approach to the converse. Actually, if one really thinks about it, one may find it a real surprise that Shannon's single user capacity result can be proved by a perfect match between two completely different approaches from both sides: typical sequences for achievability from the lower side and Fano's inequality for converse from the upper side. Is it a general rule or more like a coincidence? If this perfect match has not happened in general except in a very small number of isolated cases, one has to question the generality of this approach. The idea I have for the new approach to converse can be briefly put as follows: Develop a way of proving the converse also based on typical sequences. Once this can be done, it will no longer be a surprise that the two sides can easily match, because they are based on the same approach. The way of proving one will always hint on the other. This will provide a general solution to many basic problems in network information theory.""605486,""Xie, Vivian"
"614686"	"Yang, Enhui"	"Information Theoretic Research on Big Data Compression and Analytics: Theory, Algorithms, and Applications"	"With an explosion in data sets in our society, we are at the beginning of a big data revolution. Big data has a potential to accelerate the pace of discovery in science, engineering, and medicine, improve healthcare, finance, business, and our lives, and ultimately transform our society. To tap the opportunities afforded by the big data revolution, however, many challenges of big data have to be carefully addressed. For example, to fast access remotely, and reduce the on-going cost of maintaining, huge volumes of digital data, it is desirable to compress data as much as possible. Likewise, to glean knowledge and discoveries from huge volumes of digital data, tools and techniques based on context analytics have to be developed to organize, visualize, and manage huge volumes of digital data. Efficient data compression and accurate context analytics are of vital importance to turning big data to knowledge and discoveries to actions while achieving resource (bandwidth and power) efficiency.In the past, data compression and data analytics have been largely investigated separately in different disciplines. Although data compression has been around for many years, its existing theories and techniques, especially source coding theory from information theory, have been largely developed for stationary and/or well-structured data (such as text, web pages, etc.). In the context of big data, however, most data types are nonstationary and unstructured/semi-structured; applying existing compression techniques directly to these data types often leads to unsatisfied compression performance. Therefore, it is imperative to develop novel compression theories and techniques by incorporating data analytics into data compression to handle the lack of structure and stationarity in an elegant way. In the opposite direction, it is also beneficial to investigate how to apply information theoretic ideas, particularly source coding theory and techniques, to data analytics to develop better solutions for data analytics such as cognitive distance, clustering, and organization.Building on our early success, in this research, we will investigate and explore the interactions of data compression and analytics to advance knowledge in both fields. Our approach will be information theoretic. Three theme areas will be focused on: (1) interactions of data compression and analytics to improve lossy compression performance for nonstationary and unstructured data; (2) interactions of data compression and analytics to develop better cognitive distances and hence provide better tools for data clustering and organization; and (3) compression of and pattern discovery in large bipartite graphs. In the process of achieving our scientific objectives, we will maintain and enhance our leadership in the related areas, and train highly qualified personnel in information and communications technology for Canada.""614807,""Yang, Guangdong"
"614802"	"Yu, Alfred"	"High Frame Rate Vascular Ultrasound Innovations for Complex Flow Imaging"	"Technologies that render flow dynamics in human arteries are highly useful in the detection of vascular abnormalities. One key application area is the diagnosis of carotid atherosclerosis, a vascular disease marked by the formation of plaques that narrow the circulatory branch supplying blood to the brain. My research program will seek to make significant advances in vascular diagnostics by engineering new three-dimensional (3D) ultrasound imaging solutions that can examine, over different phases of a pulsatile pulse cycle, the spatiotemporal evolution of complex flow patterns in major arteries such as the carotid. Specifically, my team will aim to:1) Design a high-frame-rate 3D complex flow imaging framework to achieve time-resolved dynamic rendering of complex flow inside arterial volumes (sub-millisecond time resolution; well beyond the typical video display rate of 20-30 frames per second);2) Formulate new solutions for system hardware (array transducer, interfacing electronics) and real-time computing (graphics processing unit processing) that are essential to practical realization of high-frame-rate 3D complex flow imaging;3) Devise hemodynamic parameter visualization algorithms based on flow vector analysis (space-time mapping of wall shear rate, vorticity, and helicity) to derive functional insights from complex flow patterns;4) Analyze the practical efficacy of our imaging innovations by designing 3D flow phantoms (e.g. models of aneurysms and vascular trees) and performing a series of flow experimentation in vitro. Our proposed innovation endeavors will lead to the first non-invasive medical imaging solutions of their kind that can effectively trace 3D complex hemodynamics in a time-resolved manner under point-of-care settings and without using ionizing radiation. They will play an enabling role in facilitating clinicians to make more informed diagnosis of vascular pathologies such as atherosclerosis. In turn, vascular patients will be the principal beneficiaries; for instance, improved diagnosis of carotid atherosclerosis can help reduce the incidence of stroke, which is responsible for 14,000 deaths per year in Canada. This research will also benefit the grooming of Canada's RD talent pool in biomedical engineering and the strengthening of Canada's core competence in ultrasound imaging RD, establishing Canada's position as a global leader in this field.""616562,""Yu, Alfred"
"615025"	"Yu, Ming"	"Ontario"	"CANADA"
"615457"	"Zhani, MohamedFaten"	"MASTER: sMart mAnagement for futuriStic daTa cEnteRs"	"In the recent years, cloud computing has proved to be the ideal platform for hosting large-scale Internet applications. As a result, there is a pressing need to further develop cloud infrastructures and services and enhance their agility, cost-effectiveness, performance and availability. Emergent technologies like Network Function Virtualization (NFV) and Software Defined Networking (SDN) hold great promise to fulfill this need as they allow to dynamically build services as chains of Virtual Network Functions (VNFs) that are easily inter-connected using SDN capabilities.This project aims at designing scalable, efficient and dynamic cloud management frameworks for futuristic infrastructures able to satisfy requirements of large-scale Internet applications in terms of performance and availability while minimizing operational costs. This requires devising novel resource allocation schemes to manage NFV-based services and designing cloud architectures able to leverage the flexibility of SDN without incurring its potential limitations. Specifically, we intend to investigate the following key challenges:(1) Design and Management of VNF-based Cloud Services: with the success of virtualization technology, recent proposals have advocated to virtualize network functions (e.g., firewalls, routing) and offer them as VNFs to build Internet services. In this context, we intend to address challenges related to the dynamic provisioning of NFV-based services with the goal of ensuring their performance and reducing operational costs.(2) Availability-aware management in the Cloud: today's cloud users are expecting stringent guarantees on the availability of their services. However, there are currently no accurate and efficient techniques to evaluate the availability of a service running on the cloud. We therefore plan to develop models able to evaluate service availability and leverage them to design a new generation of availability-aware resource allocation schemes that satisfy the desired availability while minimizing the incurred costs.(3) Design and Management of SDN-based Cloud Infrastructures: SDN technology has recently emerged as a new paradigm offering the programmability required to dynamically configure cloud networks. Despite its benefits, the traditional SDN architecture suffers from scalability limitations. We therefore intend to investigate the challenges associated with the design of a novel SDN-based architecture that overcomes these limitations while keeping SDN benefits.The outcome of this project is expected to make significant contributions in developing novel architectures and management frameworks for next-generation cloud infrastructures and services. This project is also an excellent opportunity for graduate students to acquire expertise in technologies like NFV and SDN which are gaining significant momentum in industry and academia.""611170,""Zhao, Boxin"
