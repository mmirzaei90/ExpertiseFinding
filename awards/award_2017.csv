"id"	"researcher_name"	"application_title"	"application_summary"
"637148"	"Abdelkhalek, Maher"	"Fault Analysis and Control of Inverter-Interfaced Energy Systems for Reliable Protective Relaying"	"Green energy and smart grid initiatives are driving the move from legacy power systems toward sustainable systems with new features such as microgrids and renewable energy sources (RES). Bridging the gap between the renewable energy industry and relay manufacturers, this research will respond to growing challenges related to protective relaying to ensure the fault resiliency and reliability of next-generation power systems. The following issues will be investigated. 1. Microgrid fault analysis: Lack of accurate fault analysis algorithms for microgrids forces protection engineers either (i) to use conventional short-circuit calculation tools that fail to provide reliable microgrid protection or (ii) to employ detailed time-domain microgrid simulation models that are time-consuming and commercially infeasible. For these reasons, optimal protection coordination of directional overcurrent relays (DOCRs) has been limited to microgrids that employ synchronous generators (SGs), whereas modern microgrids are typically powered by inverter-interfaced distributed generators (IIDGs). To address this gap, sequence-component inverter fault models will be developed that include consideration of different control modes, inverter current limits, and reactive current generation (RCG) requirements, and will then be integrated into new microgrid short-circuit calculations. The resultant fault analysis algorithms will be exploited for the creation of new protection coordination indices and the optimization of DOCR operation. 2. Microgrid phase selection: To improve microgrid reliability and resiliency during faults, phase selection methods (PSMs) should accurately trip faulty phases. Significant differences between IIDG and SG fault current signatures can cause the misoperation of existing PSMs. This research will address this problem at its source, i.e., the inverter side, by developing new control algorithms in the sequence-component control frames to enable accurate operation of the PSMs used by commercial relays. 3. Distance protection of renewable energy systems: Distance relays are normally utilized as either primary or backup transmission system protection. Inverter-interfaced RES can cause unreliable relay impedance measurements, thus impairing the operation of distance relays. This research will develop new fault ride-through and RCG techniques that can provide ancillary services to distance relays, leading to accurate impedance measurements without the need for relay communication or upgrades. The fault analysis and inverter control algorithms developed through this program will be crucial for (i) fault-resilient and sustainable microgrid operation, (ii) greater RES penetration, and (iii) cost-effective protective relaying. The industrial beneficiaries include Canadian utilities, relay manufacturers, and the renewable energy industry.""632230,""Abdella, Kenzu"
"637271"	"Adachi, Michael"	"Fabrication of 2D material devices using large-area manufacturable methods"	"The proposed research program involves the fabrication of 2D materials (graphene-like materials) devices using manufacturable methods. 2D materials are crystalline monolayer materials, the most famous of which is graphene. Recently, other 2D materials such as monolayer Si (or silicene), Ge (or germanene), P (or phosphorene), and MoS2 have shown extraordinary properties: high measured carrier mobility (200 cm2/Vs), theoretical carrier mobility multiple times larger than their bulk counterpart, tuneable bandgap controlled by applied vertical electric field, high surface-volume ratio, anisotropic changes in resistance for selective gas detection, flexibility, and paramagnetic and spintronic properties. However, devices currently reported in literature mechanically transfer 2D materials in solution from a single crystal substrate, to a non-conducting substrate where electrical contacts are made, a process which is non-repeatable and not manufacturable. Therefore, innovation is required to develop manufacturing-friendly methods to prepare 2D material devices. In the next 5 years, my team will grow 2D materials using repeatable, large-area techniques which are compatible with Si manufacturing processes to enable mass-production. In parallel, we will prepare 2D material devices using established techniques (exfoliation and chemical vapor deposition) for developing tuneable infrared detectors, gas sensors, biosensors, next-generation integrated circuits, and transparent impact/shock resistance. The optical, electronic, structural, mechanical, magnetic, thermal properties of 2D materials will be characterized and newly discovered properties will be used to develop new applications. The long-term objective of this research program is to commercialize 2D material devices developed in our lab. My group will operate and maintain an existing cluster deposition tool to fabricate 2D material devices. This system is equipped with two sputtering deposition chambers and two plasma-enhanced chemical vapor deposition (PECVD) chambers. One PECVD chamber will also be upgraded to be capable of 2D material growth from vapor phase. The key advantage of a cluster tool system is that 2D materials grown in one chamber can be covered with a high-k dielectric and metal layer deposited in other chambers, which would be a unique capability in Canada. Transferring samples between chambers allows us to avoid impurity contamination and undesirable oxide formation between processes, an important feature for making high-performance devices. Application-specific devices will be tested in the lab facilities of collaborators. The proposed research program will lead to newly discovered material properties of 2D materials, and enable breakthroughs in device prototype fabrication involving industrial collaborations, which will have a direct impact on the Canadian high tech sector.""640516,""Adachi, Michael"
"637173"	"Adamiak, Kazimierz"	"Corona, Dielectric Barrier and Sliding Discharges in the Flow Control and Environmental Protection: From Fundamental Studies to Optimization of Practical Devices"	"Many processes in industry, medicine and environmental protection involve electrical discharges in gases. The most important devices for our society are probably electrostatic precipitators, which remove dust particles generated in many industrial processes so we may breathe cleaner air. Many toxic substances can also be decomposed using this process. All this is possible, because gas discharges produce free ions and active chemical species. The dust particles can be electrically charged and their motion can be controlled using electric forces, ensuring they are collected and instead of being released into the atmosphere. Moving ions generated in gas discharges can also produce gas motion due to frequent collisions with neutral molecules this is commonly called the secondary electrohydrodynamic flow, or ionic wind. Moreover, the energy transfer from ions to gas molecules can locally increase their temperature producing a shock wave, which also affects the gas flow. The design and optimization of practical devices based on gas discharges require a thorough understanding of the process and ability to predict all electrical, chemical, thermal and aerodynamic characteristics of the process. The first objective of the proposed research program is to create a set of numerical algorithms, which can be used to simulate the corona, dielectric barrier and sliding discharges, assuming different parameters of the process: geometry of the discharge gap, polarity and waveform of the supplied voltage, and temperature, pressure and composition of ambient gas. Later, appropriate models can be used to study different devices, depending on the required accuracy and available computing resources. Commercial software packages are not able to solve this problem; they are usually divergent, or at least exhibit a large numerical diffusion. Having a reliable simulation tool, it will be possible to investigate and optimize three important practical devices: a gas discharge actuator for controlling the flow boundary layer, electrohydrodynamic dryers of organic substances, and electrostatic precipitators for submicron particles. A large percentage of power in Canada is generated by coal-fired power stations, which produce enormous amounts of dust. Efficient collection of this dust is of paramount importance for the health of our society, but also to protecting our environment. Improved characteristics of actuators for the boundary layer control can tremendously affect aircraft and wind turbine industries. ""635854,""Adamic, Peter"
"636984"	"Adler, Andy"	"Computational tools and algorithms for impedance imaging"	"Electrical impedance tomography (EIT) uses measurements at body surface electrodes to calculate an image of the internal electrical properties. Such images provide useful information in medicine (i.e. blood and air flow), geophysics (i.e. ground water and mineral localization), and industrial process monitoring. The applicant is an internationally recognized expert in algorithms for EIT imaging and functional image analysis. In the past grant period, contributions have been disseminated by a) leading of consensus projects on algorithms and data analysis, b) an open-source software tool which has become the most widely used in the field, c) in the scientific literature (53 journal papers), and d) supervising 21 graduate students, 4 PDFs and 60 undergraduate students. Software developed by the research team is a core component of three companies' products. Overall, research will focus on two streams: first, a Robust Algorithms and Validation theme (funded by this grant), which will develop an image evaluation and debugging framework, and, using the core technology developed, an Applications theme (funded by collaborators and industrial partners). Despite its successes, EIT has been criticized for a lack of robustness, as the inverse problem inherent to image reconstruction is ill-conditioned, so that small data or model inaccuracies can result in large image artefacts. This research program will address this issue by developing a set of tools to analyse and correct inverse problem solutions. While several individual tools have been developed (by the applicant and others), the core innovation of this project is an integrated evaluation/debugging software framework. Three new components will be developed: a model and data validation tool, models of non-ideal characteristics of electronics hardware and electrodes (such as amplifier input impedance and crosstalk), and an image inspection tool to query the data contributing to suspected artefacts. At the same time, work will be pursued on applications of EIT technology for medical and geophysical applications. Medical applications will focus on dynamic lung imaging (for use with neonatal, intensive care, and obstructive lung disease patients) and hemodynamic monitoring (non-invasive blood pressure and cardiac output measures). Geophysical applications will focus on monitoring of groundwater and surface movement in unstable (landslide-prone) soils and permafrost regions.""639848,""Adler, Andy"
"637399"	"Aghdam, Amir"	"Reconfiguration and Cooperative Control for Multi-Agent Networks"	"There is an increasing interest in the deployment of multi-agent networks for emerging applications such as surveillance and target tracking. This proposal focuses on two research problems in the area of multi-agent networks. The first problem concerns network reconfiguration, which aims at improving the performance of a network by properly positioning its nodes and/or changing the weights of its links (e.g., by adjusting the communication/sensing power of the nodes). This is a problem which requires a strong practical background, and while the results will be developed for a general class of asymmetric networks, the applicant's prior experience in the design of experimentally validated distributed control schemes for underwater sensor networks puts him in a unique position to address some of the practical shortcomings of existing results for this type of systems. One of the main characteristics of asymmetric networks is that the graph representing them is directed, and in the case of underwater sensor networks, particularly, it is random too. However, there are not many results in the literature for the analysis of this type of graphs, due to their complexity compared to (deterministic) undirected graphs. This limits the extent of which certain observations in such networks can be justified analytically. For example, it is known that the relative positions of the acoustic nodes in an underwater sensor network can have a significant impact on data aggregation performance. In fact, similar observations have been reported in other types of asymmetric networks with no convincing theoretical justification. The applicant and his team have recently developed some theoretical results, validated by simulations, that relate the important properties of general weighted directed graphs (such as connectivity) to the configuration of the network, enabling the research community for the first timeto justify these observations theoretically, and more importantly, use the results to further improve the performance of the network. These results will be used in the proposed research to find the optimal configuration for asymmetric networks. The results can also be used in other applications such as traffic network control systems, to justify some counter-intuitive observations reported in this type of systems (e.g., negative impact of the addition of some roads on the overall traffic flow in the network). The other problem investigated in this proposal is concerned with cooperative decision making for heading control of multiple vehicles, where it is desired to coordinate a group of vehicles in order to detect, localize, track and intercept a group of objects that arrive in a protected area (mission space) at random time instants. The proposed dynamic decision making and control design is based on a reward allocation strategy which directs the vehicles toward the objects in an optimal cooperativefashion.""619531,""Agili, Hachem"
"637489"	"Ahmad, MOmair"	"Design and Implementation of Digital Signal Processing Algorithms for Communication and Biomedical Applications"	"The field of digital signal processing (DSP) has experienced explosive growth during the past couple of decades. The DSP techniques have become integral parts of the products and services that we need or encounter in our daily lives. The research efforts of the applicant in this area during the past five years have led to some very concrete results that have been shared with the international scientific community, both from academia and industry, and have given rise to new ideas and directions that need to be further investigated. The overall objective of the proposed research program is to develop efficient algorithms and architectures and to lay sound mathematical foundations for reliable processing of image, video and biomedical signals, and cost-effective implementation for communication and biomedical applications. For half a decade now, the growth in the internet of things (IoT) and the demand for machine-to-machine connections have been staggering. By 2020, the global IP networks will have to support more than 50 billion devices. In order to establish a reliable and smooth communication among such a large number of devices connected to IoT, while still maintaining efficient and sustainable power consumption, the development of versatile, smart, fast, and real-time direction of arrival (DOA) estimation algorithms is of paramount importance. Deep learning has opened up the limitless possibilities in understanding and recognition of images. Image understanding and analysis is inherently a big data problem. Hence, the use of advanced deep learning concepts can produce meaningful results in image understanding and visual tracking, which play an important role in a wide range of real-life applications such as detection and recognition, human behavior analysis, video indexing and retrieval, medical imaging, and traffic management of smart cities. Alzheimers disease is the most common type of dementia affecting elderly people worldwide. There is no cure for this disease, and the disease worsens as it progresses. Early detection is a key to preventing, slowing and stopping Alzheimers disease. Around-the-clock observation of patients is expensive, inconvenient, and in some cases, simply impossible. The technology of body area networks proposes a convenient and useful solution that promises to provide physicians access to the patients physiological data anytime and anywhere. Application of compressed sensing and statistical learning of biomedical signals can make this technology affordable. This research in the immediate future is aimed at investigating these problems and seeking their solutions by developing efficient DSP algorithms and architectures. The direct beneficiary of this research will be Canadian telecommunication and biomedical industry. This proposal will also contribute to Canadian industry and academia by enabling the training of a number of skilled personnel.""627495,""Ahmad, Raabez"
"634527"	"Alam, Omar"	"Concern-Driven Development Process"	"Model reuse remains a major challenge in Model Driven Engineering (MDE), despite the success stories in programming languages as exemplified by class libraries, services, and components. Modellers usually create models from scratch because of limited support for reuse in the current modelling tools. In addition, the crosscutting nature of software development concerns complicates the application of software engineering techniques such as information hiding, decomposition, interfaces, and abstraction in the context of MDE. Concern-Oriented Reuse (CORE) is a novel reuse paradigm that mitigates these challenges by extending MDE with best practices and techniques from advanced modularization and separation of concerns (SoC), goal modelling, and Software Product Lines (SPL). However, the research in CORE remains in its infancy and there is currently no development process based on CORE . This research programs vision is to develop a Concern-Driven Development (CDD), a next generation software development process that uses concerns as its primary artifact. Whereas classical MDE methodologies focus on models that are built from scratch with little support for reuse, CDD is a reuse-focused development process in which a concern or an application is built by repeatedly reusing other existing concerns. In CDD, a modeller would use a CORE-based modelling language that is appropriate for the current development phase and for the problem domain. Model transformations would then be applied to produce the initial set of models for the next phase. The process will continue until an execute model is produced. In each phase, the modeller should consult a repository of reusable concerns to identify and reuse concerns. One main factor that contributed to the success of programming languages is the existence of well-documented extensive libraries. We plan in short-time to extend the reusable concern library with advanced technologies to support faster update, versioning, and documentation. We envision that CDD has the potential to transform the software engineering discipline as a whole. Unlike the current development processes that often require software engineers to deal with and be an expert in many concerns simultaneously within each software development phase, CDD would enable software engineers to specialize, i.e., to become concern specialists. Companies could focus on creating long-lived concern libraries, and provide consulting services to customize concerns to specific application context, if necessary. Ultimately, this research program will increase the competitiveness and productivity of the Canadian software industry, bringing the current practices in the industry closer to other engineering disciplines.""622179,""Alam, Rejuana"
"634438"	"AlKiswany, Samer"	"Exploring a New Design Paradigm for Distributed Systems in Modern Data Centers"	"The end-to-end design principle pervades the design of virtually every modern distributed system. In its extreme form, critical functionality is implemented solely in end hosts, with a dumb but fast network to connect them. Unfortunately, such a network-oblivious approach is fundamentally inefficient, as it provides one-size-fits-all functionality to a multifarious set of applications and platforms. While this guideline has served us well in the past 30 years, technology evolution and tensions at the network interface suggest that it may be productive to revisit it. The goal of the proposed research program is to explore an alternative paradigm for designing distributed systems to leverage the capabilities of the emerging technology of software-defined networks (SDNs). The new paradigm will co-design distributed systems logic and networking support to realize more efficient, scalable, and reliable systems. Achieving this vision requires a drastic shift in our approach to building, engineering, testing, securing, and deploying systems. I plan to establish a leading research group that will explore this new paradigm for designing a complete modern data center software/hardware stack, including building, testing, and deploying systems; exploring new security, management, accounting, and QoS techniques; and investigating new programming frameworks, domain-specific languages, and testing and debugging tools. This Discovery Grant funding is instrumental in bootstrapping this research group. The immediate research context is a network-integrated design for distributed storage systems, focusing on KV stores. To this end, the research program aims to (1) characterize the current KV systems to understand their network load and consistency properties; (2) design and prototype a network-integrated KV store; (3) explore designs for end-to-end QoS support; and (4) study the impact of departing from the end-to-end principle on security properties. The innovative paradigm we will explore has the potential to have a wide influence on technology, including many Canadian companies that build KV stores (e.g., NetApp, Ericsson, and IBM) and more than 150 Canadian data centers that use these systems. In particular, the program will result in four main contributions. First, the resulting designs will provide a blueprint for future systems and present novel solutions for stagnant QoS and scalability problems. Second, this program will build network-integrated designs for core distributed system mechanisms (e.g., QoS, security, consistency, and load balancing) that are applicable beyond KV stores. Third, this program will provide timely f eedback on the suitability of the SDN API for enabling more efficient designs . Fourth, this program will provide an invaluable opportunity for training highly qualified personnel on cutting-edge networking and storage technology .""640275,""AlKiswany, Samer"
"634298"	"Aloise, Daniel"	"(Re)designing Clustering Algorithms for Big Data"	"Our current speed of data generation combined with storage capacity increases have given rise to new paradigms in computing. For example, according to the IBM's website, there are approximately 695,000 status updates and 11 million instant messages sent every minute on Facebook. However, many organizations have faced the problem of having a lot of data, but poor knowledge about them. Clustering methods help to automatically identify unobserved groups for a set of data objects, and are currently being radically transformed by the size, the variety and the nature of the available data, i.e., by so-called Big Data. This research program focuses on the development of scalable algorithms for Big data clustering. This will be achieved both by: (i) redesigning well-known successful serial algorithms for scalability, leveraging their main theoretical ideas; and (ii) developing new algorithms and heuristics using new programming paradigms associated with Big Data. In summary, the objectives of this research program are : A. Produce an extensive survey of the existing Big Data clustering methods in order to provide a complete panorama about which ones can be adapted to approach Big data. B. Redesign successful serial clustering algorithms, leveraging their main theoretical ideas to work with the new programming paradigms and computational tools from Big Data. C. Develop algorithms for semi-supervised Big Data clustering, incorporating supplementary information provided by the user into the clustering decision process. D. Develop Big Data clustering algorithms for new emerging applications. E. Provide a repository of Big Data software and clustering algorithms with guaranteed effectiveness. The software and algorithms developed in this research program are expected to constitute new benchmarks to the field, allowing larger datasets to be tackled with efficiency and effectiveness, leading to new insights in commerce, industry and academia.Moreover, given the Big Data expert shortage in Canada and worldwide, this research program will help to form and train specialists who will be able to master the scientific and technological issues emerging from the Big Data explosion. In the long term, the findings of this research program will support data mining-driven decision making in the Internet of Things (IoT) era in which huge amounts of data are generated in real-time from the most varied sources and devices (e.g. vehicles, sensors, home appliances, etc.). By combining massive data processing, machine learning techniques and algorithms, IoT devices will be able to perform clustering as well as other classification tasks on a large scale. ""637692,""AlOsman, Hussein"
"637493"	"Amer, Aishy"	"Integrating object segmentation for robust object tracking"	"Video object tracking is an active research field with applications diverse as augmented reality, self-navigation systems (drones or self-driving cars), human-computer interactions, hyper-linked social video, or athlete performance monitoring. Tracking is performed on raw video signals, captured typically from a single camera; different types of objects can be tracked, such as persons, athletes, vehicles, animals, or ships. Tracking of objects across real-world video signals has many challenges such as variations in object appearance due to scale change, occlusion, articulation, deformation, and interactions of multiple objects. Recent algorithms to object tracking have well advanced the topic in some of these challenges. In this proposal, I will address the following research challenges to advance the state-of-the-arts to single object tracking: 1) drift of object tracking: what to do when a tracker drifts from the target and how to detect such drift? 2) variable object features: how to deal with features, such as color histogram, that significantly vary during tracking? 3) scale change of objects: can we explicitly model scale change or rather implicitly such as through the integration with object segmentation and detection? To detect drifts of a specific tracker, I propose to analyze its internal state by monitoring its latent parameters over time. For tracker-independent drift detection, I will integrate object segmentation and abjectness (likelihood an image region is an object) measures. To correct drifts, I will integrate object segmentation and object detection (localization) into tracking as drift happens. The use of segmentation is motivated by recent discoveries on the human visual system which seems to rely on spatial and temporal spacing of the objects for effective tracking. I will specifically use object segmentation and localization to explore present-future (or space-time) interaction between video objects. To solve the problem of a variable feature across a video sequence, I will use machine learning, e.g., online multi-kernel learning with support vector machines, to online confirm or reject features. I will handle scale change of objects through monitoring of object states in space and time, through homography transformation, and depth as an added feature. Availability of many datasets, ground-truth data, and metrics of object tracking will enable effective evaluation of the new approaches. I anticipate that developed methods will complement existing online learning approaches, adding an extra level of robustnessto tracking-assisted video applications. Interest in widely-applicable (robust) but fast object tracking is large. With my over 20 years of research and industrial experience, I am confident to well advance knowledge and transfer it to related sectors of the Canadian video technology industries.""623174,""Ameri, Abtin"
"634356"	"Anderson, John"	"Dynamic Heterogeneous Multi-Robot Team Management in Dangerous Domains"	"Work in dangerous environments such as Urban Search and Rescue ( USAR , the post-disaster exploration and mapping of structures, searching for victims and aiding disaster response) represents an extreme challenge to teams of intelligent robots. The world is complex and unpredictable, robots can be damaged or destroyed, and communication is interfered with due to damaged infrastructure. Teams consisting of various types of robots (heterogeneous teams) are required, because the risk involved in losing units can be better amortized with larger numbers of more expendable individuals. Under such conditions, teams must form and reorganize dynamically around the mix of individuals available at any time, doing useful work with the skills available, actively looking for new individuals to supplement skills, and making use of damaged individuals to the degree practicable (including assisting these if possible). Individuals must similarly adapt to damage (e.g. make use of grasping limbs if no longer mobile) and work with a team as well as possible. Permeating all of this, risk must be actively balanced with the value of carrying out any task for the team, and these tasks must all be accomplished under rapid change and limited communication. The program of research proposed here will advance the state of the art in dynamic heterogeneous robotic teamwork in dangerous environments, incorporating and extending all of the above themes. Work has been done in these themes individually (e.g. task allocation on changing teams, recruiting agents) but a deployable decentralized solution supporting a broad collection of robots for a domain as dangerous as USAR is still elusive. In recent work my students and I have developed a framework for this purpose that includes accomplishing tasks under changing team membership, adapting to robot loss and recruiting new individuals. We have also been working on improved robot control and planning for complex domains. I will be leveraging this experience to more broadly advance dynamic heterogeneous teamwork, encompassing major issues such as managing damaged robots to the best extent possible, assisting damaged robots, balancing task risk vs. value, and developing team strategies to manage risk. The outcome of this will be a software system that will be physically demonstrated using teams of robots in the real world. We will be exploring the use of humanoid robots in particular on teams under these conditions, since the nature of this form supports a broad range of activities despite damage (e.g. humanoids can crawl or drag themselves if unable to walk). This work is applicable in a broad range of dangerous domains beyond USAR, such as mining, space exploration, and defense. The proposed research will provide valuable training for four PhD students and nine MSc students in artificial intelligence, complex software development, and robotics: skills that are vitally needed by industry. ""634494,""Anderson, Jonathan"
"621724"	"AraujoThaine, Patricia"	"British Columbia"	"CANADA"
"637342"	"Ardakani, Masoud"	"Advanced coding solutions for large-scale data storage & communication"	"Motivation The global IP traffic is growing rapidly due to increased demand for online services such as cloud storage, video on demand, live streaming, and file sharing. These online services, which are responsible for the bulk of the Internet traffic, heavily rely on distributed storage systems (DSS). Hence, around the world, the number and size of DSSs have rapidly increased in recent years. In fact, DSSs are already among the top contributors of carbon emission in the modern world. In the US, for example, they are only second to the airline industry and expected to be the top contributor by 2020. Thus, research on efficient data storage techniques for DSSs has recently attracted a lot of interest both in academia and in industry. Background Hardware and software failures in a DSS result in data loss and unavailability. Naturally, many service providers keep multiple copies of data for reliability and availability. The downside, of course, is the huge storage overhead (typically 200%) and the associated maintenance costs. As a solution, recently, error correction coding is suggested for DSSs. Unfortunately, most existing error correction codes cannot be used in these large-scale applications, because the repair traffic that they create is beyond the network capacity of the DSSs. Hence, a strong need for more efficient codes, tailored for large-scale applications, is felt. Goals In this research program, using our past knowledge of modern coding and our recent activities on coding for DSSs, we will develop efficient and scalable error correction coding solutions for distributed storage systems. We will also seek theoretical results on the fundamental relations among different performance measures (such as coding overhead, code reliability, repair traffic, update complexity, etc.). There are a large number of high-impact open problems in this area all suitable for training HQPs. Impact This research proposal follows several goals that are exceedingly important to both practitioners and theorists in the area of coding and information theory. Improving the efficiency of large-scale data storage not only improves our online experience, but also significantly reduces the carbon footprint of DSSs. We hope our results throughout this work and our contribution with other Canadian researchers turn Canada into a major scientific center in this field, which can technically and scientifically support any data center located in Canada. In fact, since cooling is a major cost in data centers, Canada because of its colder climate and its safety is considered as an ideal place to host data centers. In the past decade, the world has seen a significant increase in online activity. As a result, large-scale data storage/communication currently is and will continue to be a big engineering challenge. Our activity in this field will train HQPs that will be in high demand and involved in very high impact engineering activities.""618706,""Ardakani, Masoud"
"637425"	"Areibi, Shawki"	"Ontario"	"CANADA"
"637512"	"Asif, Amir"	"Distributed, Non-linear Signal Processing for Large Scale Engineering Applications"	"The research program lies at the intersection of three domains within Electrical and Computer Engineering, namely, signal processing, control theory, and optimization. The overarching theme that brings these three areas together is to develop smart, efficient, and robust, distributed signal processing algorithms for multi-agent systems. While signal processing for linear applications has a rich theoretical framework, non-linear, distributed signal processing lacks a universal set of tools for analysis and design. The objective of the program is to unify the study of a broad class of non-linear signal processing algorithms that emerge from statistical estimation principles for high-dimensional and distributed dynamical systems, where underlying signals arise from non-Gaussian processes. Recently implemented variants of the Kalman filter, random finite set methods, sequential Monte Carlo approaches, and particle flow methods have shown promise though for scenarios where the state space has a relatively low dimension. As the dimensions grow, the curse of state-space dimensionality and the associated high computational cost lead to a rapid deterioration in their performance. The research program will develop practical implementations of generic Monte Carlo methods for large-scale, non-linear, distributed applications and demonstrate their efficacy on real-life engineering problems. We will focus on the following inter-connected research problems in the context of geographically-distributed agent networks as representatives of large-scale, non-linear signal processing systems: (a) Diffusive fusion for large dimensional Monte Carlo approaches; (b) Event based distributed signal processing and control; (c) Weighted graph signal processing for large data sets; and (d)Derivation of the convergence properties and performance bounds of the algorithms we propose. These signal processing algorithms will be applied to practical engineering applications, including multi-target surveillance (for jointly estimating the number of targets and their states), distributed camera networks (for tracking moving objects), and electric smart grids (for estimating the operating status of smart grids in real-time). Within the signal processing community, this research will result in the development of a universal set of tools and design principles for analyzing large data sets and processing mission critical systems in a computationally efficient manner while respecting the non-linear physics of the underlying processes. Over the length of the research grant, the societal impact is achieved through HQP training of six PhD and six Masters students (in addition to fifteen undergraduate students who will build prototypes as capstone projects) in the area of non-linear signal processing, developing a range of technical skills needed to design the next generation technologies. ""623145,""Asif, Hassan"
"634290"	"Atwood, John"	"Secure Deployment of Network Configuration"	"Secure deployment of configuration information for network devices has been possible for a long time, but is relatively little used, because almost all of the management of Internet security has to be done manually. The security aspect is particularly difficult to retrofit to legacy equipment, especially in industries that use equipment with long expected lifetimes. Using a combination of novel intermediaries, careful assessment of management needs, and concepts from autonomic networks, we will investigate ways to reduce the initial and on-going costs of security enforcement, in ways that ""play well"" with existing operational practices. Our long-term objective is to formulate secure methods to manage the deployment of network configuration that are sufficiently automated that they will actually be deployed in operational networks, thus providing effective security for these networks. The research program will encompass four main areas of activity: 1) Development and validation of methods for managing the security of routing protocols, without requiring manual intervention by networking staff; 2) Exploration and validation of methods for managing mixed deployments of legacy devices and modern devices, while minimizing the disruption and maximizing the security during the transitioning of the control paradigm, thus encouraging the adoption of more up-to-date control and performance-assessment technologies, without requiring the de-commissioning of legacy equipment; 3) Demonstration of the utility of our security-management approach in the area of Software-Defined Networking, specifically on the control path between the controller and the switches, which will make it easier to ensure the security of the managed objects; 4) Assessment of selected application areas, such as power grids and industrial plants, as candidates for secure management of legacy devices, using our mixed-deployment solution. The lack of deployment for security solutions is based on an assessment (by network management executives) that the potential cost of security breaches is smaller than the cost of installing and maintaining the security solutions. The novelty of our work comes from the fact that we expect to be able to substantially lower the recurring costs of security management, to the point where the expected cost of maintaining (real) security is attractive (or at least acceptable). Governmental mandates will increase the desirability of these approaches, as they will increase the cost of not complying. The proposed areas of study will facilitate increased security in the Internet, which in turn responds to the statement by the Internet Engineering Task Force (the Standards Development Organization for the Internet) that ""pervasive surveillance is an attack"", which can only be mitigated by pervasive security. ""627888,""Au, Andrew"
"637147"	"BadrkhaniAjaei, Firouz"	"Protecting the Future Electric Power Systems"	"Direct Current (DC) grids are emerging electric power systems with remarkable potential benefits. The High-Voltage DC (HVDC) grid improves reliability and efficiency of bulk power transmission systems and also facilitates grid integration of large-scale renewable generation systems. The medium-voltage and low-voltage DC grids and the DC microgrid facilitate integration of distributed energy resources. However, unresolved protection issues have constrained utilization of DC grids. The existing protection systems (relays) do not fully protect DC grids against adverse effects of faults (short-circuits) and disturbances. The long-term objective of the proposed research program is to ensure secure operation of DC grids in the future electric power systems by developing high-speed, reliable, robust, and selective protection strategies. In the short-term, the proposed research program will address the main protection issues of the HVDC grid and the DC microgrid, as explained below. 1) HVDC Grid Protection Currently there is no commercially available technology for fault protection of transmission lines in the meshed HVDC grid. A protection strategy will be developed to enable timely and reliable fault detection and location (identification of the faulted line) in the HVDC grid. The proposed protection strategy will be applicable to HVDC grids with different configurations, grounding types, and converter technologies. In addition, a wide-area System Protection Scheme (SPS) will be developed to protect integrity of the HVDC grid by detecting and counteracting line overloading conditions that can lead to cascading outages. 2) DC Microgrid Protection The existing protection strategies do not meet the speed and reliability requirements for DC microgrid applications. The proposed research program will develop a protection strategy for high-speed and reliable fault protection of DC microgrid feeders. Besides, a SPS will be developed to protect integrity of the DC microgrid against voltage collapse. The SPS will be capable of taking timely remedial actions to counteract the disturbances that can lead to voltage collapse, under both grid-connected and islanded operation modes. The proposed research program will advance the state-of-the-art in protection of the HVDC grid and the DC microgrid, and thereby: 1) lead to development and commercialization of the first relays to protect these DC grid against faults and system-wide disturbances; 2) facilitate long-distance transmission of bulk power with higher reliability and efficiency, in large countries including Canada; 3) improve dynamics and stability of the modernized power grid by preventing widespread catastrophic failures, e.g., the blackout of 2003 in Canada and USA; and 4) increase penetration of renewable energy systems into Canadian distribution systems.""623438,""Bae, JungSeok"
"621729"	"Bai, Min"	"New Jersey"	"UNITED STATES"
"634365"	"Balakrishnan, Ravin"	"Subtle User Interfaces"	"This research is focused on exploring new user interaction and visualization techniques for a broad range of next generation input, output, and display technologies. As computing steadily moves to mobile usage environments, the computing technologies we use and interact with are becoming increasingly heterogeneous and often require a medley of interleaved interactions spanning devices, usage context, and physical space. Further, the variable environmental context of use can significantly alter the types of input and output modalities that are best suited for human communication with the technology. Unlike in the past, where more static usage scenarios were prevalent and as such a particular input/output modality could be chosen and used for a good chunk of time, we are currently moving to a situation where usage scenarios are very fluid and can rapidly change over time and space. Our current user interfaces and visualization strategies are not particularly well adapted to handling fluid switching of interaction modalities as usage context rapidly changes. The key objective in the proposed research is to develop a unified approach to interaction and visualization as users move from one device and usage context to another and back in rapid sequence, enabling smooth switching of input and output devices and modalities as these context switches happen. To support this, the research will also explore more subtle and less explicit forms of input and feedback, such as vision based sensing and micro haptic feeback, that could help tie the interface together and maintain a sense of continuity as one transitions from one form of interaction to another and back. The approach will be one of building creative new exploratory interfaces that will be used as probes in user deployments, followed by empirical evaluation of the interfaces, with the results being used to refine the designs. The resulting system artifacts will significantly deviate from the interfaces of today, and hence has the potential to alter the way we consume and interact with data on future computing devices.""617767,""Balakrishnan, Ravin"
"636991"	"Bardakjian, Berj"	"Computer models of the electrical rhythmic activities in hippocampal neurons"	"O bjectives: (a) Develop macro and micro level computer models of the electrical rhythmic activity in hippocampal neuro-glial networks under normal and pathological states exhibiting seizure-like events (SLEs). (i) Macro level models characterize system behaviour by populations of coupled cognitive rhythm generator (CRG) models (previously reported by our lab team). Each CRG consists of an input layer of neuronal modes (whose transfer functions are measured from Volterra series or alternately chosen from predetermined design functions) for sensing the input environment. The outputs of the neuronal modes layer are feed-forward to a Winfree type ring device whose variables are mapped by a static nonlinearity (representing the wave shape) into an observable output variable. The frequencies of the ring device are modulated by the outputs of the neuronal modes to provide responsive adaptation to changes in the input environment. Coupled CRGs are utilized to model pathological states and are used as testing platforms of neuromodulation strategies for abolishment of SLEs. (ii) Micro level models of traditional and ""stochastic"" Hodgkin-Huxley type models to characterize ionic transport mechanisms in ""cellular"" models of pyramidal, interneuron and glial cells. These models are used to investigate (1) the effects of astrocytes on hyper-excitability, and (2) the mechanisms of cross frequency coupling in the electrical rhythms of the neuro-glial networks characterizing their states. (b) Develop neuromodulators using coupled CRGs as therapeutic network to abolish onset of spontaneous SLEs in computer models. With the long term objective of miniaturization as electronic hardware implementation to be used as brain implants and substitutes for damaged neurons in the hippocampus. Our preliminary results on computer models demonstrated the proof of principle (previously reported by our lab team). Scientific Approach: The main hypothesis is that pathological brain states can be anticipated, detected and abolished by ""cognitive devices"" in feedback loop with the biological system. The main approach is to develop computer models of hippocampal neuro-glial networks, validated using reported biological data, which elucidate the physical basis for the phenomena of cross frequency coupling in their electrical rhythms. Significance: (a) Develop novel cognitive devices as tools for (1) neurogenic disorder therapies, and (2) in information and communication technology. (b) Provide insights into the (1) mechanisms of cross frequency coupling between low and high frequency rhythms of neuro-glial networks, (2) classification of brain states, (3) role of astrocytes in moderating neuronal firing in hyper-excitable networks underlying many neurogenic brain disorders. (c) Provide computer model platform for proof of principle testing and comparison of various neuromodulation strategies.""638542,""Bardelcik, Alexander"
"634681"	"Barron, John"	"Recovery of 3D Information from 3D Range Data and from  2D/3D Optical Flow and 3D Scene/Range Flow"	"The computation of motion via 2D/3D optical flow and 3D scene/range flow is my long term primary research interest. I have an additional primary interest in in applying these and other computational techniques to measure 3D plant growth from 3D range data in a non-invasive, non-contact manner. Over the next 5 years, my key short term primary research objectives are: (1) To measure accurate 2D optical flow at occlusion boundaries in real image data. Currently regularization approaches produce flow that tends to ""bleed"" across occlusion edges (where the images of two differently moving objects meet). We propose to compute the flow inside of closed segmented regions to produce flow that (correctly) abruptly changes at occlusion boundaries. (2) To measure accurate 3D scene/range flow (3D optical flow on surfaces) on stereo and range image sequences. This type of 3D optical flow measures the 3D motion of visible parts of a scene using depth maps produced from stereo images (scene flow) or directly measured using a range sensor. We hope to produce real time scene/range flow that would be useful in car driving, for example. (3) To measure 3D quantitative plant growth from 3D range data of growing plants in a non-invasive, non-contact manner. This will allow the quantitative evaluation of a plant's growth over its life cycle. One application would be to quantify the growth rate of wild type and genetically modified plants.""623119,""Barron, Olivier"
"634401"	"Bartram, Lyn"	"Interactive Techniques for Personal Visual Analytics"	"The Internet of Things, increase in online behavior, and a proliferation of connected mobile devices has resulted in an explosion of big data related to peoples daily lives. Concurrently governments, non-profit organizations and companies are committed to Open Data,providing ready access data that can be freely used, re-used and redistributed by anyone. These changes are extending the concepts of big data from the sphere of the expert analyst to the domain of the ordinary person,but to date there is often evidence of a gap between the provision of data and whether people understand or are motivated by the information. The research in this project will target the design and evaluation of novel personal visual analytics techniques and services for making data-based thinking more accessible to the average person:to help people integrate and use external and personal data both for their own decision making and to communicate and interact with the institutions, professionals, services and social organizations with whom they interact in their daily lives.The desktop-based, task-centric model of visual analytics and expert sense-making does not apply to the various contexts in which people can use data in their daily lives, so this research will explore novel techniques for PVA, including mobile, shared and embedded applications. It will focus on three areas particularly important to PVA: context of use; data and visual framing; and engagement and motivation. Unlike professional data analysts, people undertake data exploration for different purposes and in different physical, temporal and computing situations. They use different devices appropriate to these situations, such as mobile devices and ambient displays in their homes. Their purpose may be met with a quick glimpse of the data or a more prolonged exploration. How the data are chosen and represented presents key questions of framing, perceptual and situational constraints and appropriate context. An important aspect of this research will involve integration of different data sources (mashups) in novel ways: for example, combining personal mobility costs and house prices with projections for civic density initiatives (citizen engagement); or combining daily domestic activity records with energy use in the home to inform conservation efforts. This research will explore visualization and access techniques for flexible data mashups in common information tools like calendars and maps.Finally, visualizations that resonate with people on both an intellectual andemotionallevel promote engagement, interest, and motivation. This is particularly important when the purpose of the visualization is motivational (such as health tracking) or to promote interest in a topic or initiative ( a key issue in social engagement). We will examine the design and utility of affective visualization techniques for both communication and engagement.""622990,""Bartsch, Nicholas"
"634685"	"Basu, Anup"	"Biologically and Probabilistically guided Multimedia"	"The Human Visual System is supported by a very high resolution fovea with rapidly declining resolution in the periphery. The fovea captures details in about a 2 degree cone extending from the center of the eyes. Thus, we can only see a few letters of text clearly at a time at reading distances. In our mind, however, we think that everything is clearly visible. This perception is a result of our eyes being dynamic or active and always being guided by the brain to look at precisely what is most important at a given instant. In my basic research I introduced the concept of foveation for image, video and 3D compression. Furthermore, considering eye movements I developed the first active calibration of cameras without using any known patterns or by matching individual feature points. It is the first calibration approach that is consistent with the pan, tilt and torsional rotations of the human eye, answering some deeper questions on human vision. I also considered supporting the wide field of view of the human eyes and introduced Panoramic Stereo using one camera. These fundamental research topics have impacted the way coding standards have incorporated region of interest, and have resulted in the creation of spin-offs, like PVSI and VisionSplend, over the years by collaborators and trainees. Designing multimedia systems following biological motivation is only the first part of my approach to addressing several problems, the second part complementing this are the algorithms and their analyses. For the second part my major emphasis is on probabilistic approaches. For example, by statistical analysis of the distribution of errors I proved why my active algorithms are much more robust. Through an average case analysis I demonstrated the efficiency of our Lagrangian advection. Through probabilistic random walks I improved image fusion. I used stochastic perturbation for robust matching and registration. Finally, I also introduced the use of stochastic processes for reliable detection of brain injuries in prematurely born infants. Over the next five years I plan to introduce the following novel components: (a) incorporating foveation into panoramic stereo; (b) making further improvements to Motion Capture data compression by studying the role of attention of viewers; (c) combining saliency (or the detection of important regions in an image) with foveation for better multimedia (image, video and 3D) coding with respect to human observers; (d) making active camera calibration more robust; and (e) developing new approaches in medical and surgical image analysis. The development and analysis of my algorithms will be grounded in thorough probabilistic techniques and analysis, as in my past research. The proposed research, if successful, will have significant impact in next generation panoramic, 3D and plenoptic multimedia capture, processing, and transmission, as well as in medical and surgical innovations. ""636180,""Basu, Dipanjan"
"637082"	"BENDADA, ABDELHAKIM"	"Infrared Imaging Methods for the Thermal Characterization of Materials"	"There is an undiminishing demand for reliable measurements of the basic thermal properties of the vast range of materials in use today. The magnitudes of thermal conductivity, specific heat etc. of materials subjected to all manner of conditions are required to feed the computer models that have become the bed-rock of the modern design process. Original property determinations are required for new materials and new determinations of the properties of many of the long used materials are known to be advisable. This is particularly true for conductivity which can vary considerably with the particular composition and microstructure produced in a material during a specific production process. The current proposal aims in this context to develop new methods specifically designed for the thermal property characterization of some types of these advanced materials. Planned research will be based on thermography, which has become a well-established technique in this field. Thermography has proved attractive because it allows non-contact temperature sensing of large surfaces in real-time. Moreover, the temperature changes employed in a property determination can be very small because of the high sensitivity of modern imagers. This attribute is particularly important where measurements are needed for a material whose properties vary significantly with temperature. Nevertheless, thermography still faces some obstacles: limited frame rate, low spatial resolution, ineffective experiments. The objectives of this proposal are oriented to solve these issues. Our first objective is to propose instrumentation solutions to get rid of the limited temporal and spatial resolution of IR imagers and thereby widen the use of thermography in the field of thermal property measurement to other types of materials (i.e. thermally fast materials, microscopic imaging of the spatial variation of thermal properties across heterogeneous materials). To solve the frame rate issue, an electronic stroboscope solution will be investigated. To solve the spatial resolution issue, software improvements with super resolution methods will be considered. Our second objective is to develop a new thermography based technique for the microscopic diffusivity mapping of highly conductive materials. The technique will combine the standard flash method with the outcome of the first objective. Nowadays, the only option for such measurement is Photoreflectance Microscopy whose setup is complex and fails for low reflective materials. Our third objective is to explore Pulsed Phase Thermography (PPT) as an alternative to the well-known Thermal Wave Interferometry (TWI). TWI is a standard technique used for the characterization of coatings and multi-layers. However, it is very time consuming especially for low conductivity materials. PPT is fast and allows data processing in the frequency domain, similar to TWI frequency analysis.""625877,""Bendahan, David"
"634623"	"Bener, Ayse"	"Towards Measuring Defect Debt and Developing a Recommender System for Their Prioritization"	"Software bugs are an inextricable part of software maintenance and development activities. Due to limited time and tight budget constraints, the software development team is not able to fully resolve all the bugs that exist in the issue tracking system. The trade-off between short-term benefit of postponing the fixing of defect and the consequence of keeping the bug in the system in the long-term is interpreted as a defect debt. Typically, defect debt is defined as any kind of defect, failure or bug that is found but not fixed in the current release. In this proposal, I aim to investigate three research questions regarding the defect debt: What is the principal amount of defect debt? What is the interest amount for defect debt? What is the optimal sequence for resolving the bugs in a limited time in order to minimize the interest? In order to answer these research questions, I propose to categorize the bugs into debt prone bugs and regular bugs. The regular bugs are used to train the prediction model based on KNN-regression for estimating the principal of the debt. I propose to use graph theory analysis in order to calculate the interest. Eventually, reinforcement learning technique is recommended to prioritize the bugs based on their debt amount. In order to validate the feasibility of my proposed model, I will perform an empirical study using the bug reports collected from Mozilla Firefox project and IBM RTC project.""618803,""Bener, Ayse"
"621726"	"Bernard, Charles"	"Qubec"	"CANADA"
"637524"	"Bhadra, Sharmistha"	"Printed Electronics Technology and Its Application to Low-Cost Sensors"	"In recent years, printing technology has received significant interest for developing low-cost, large area electronic systems. Sensors have been one of the prominent areas of experimentation for printed electronic technology. Although the concept of printed sensors is not new, there are still significant challenges in developing high performance sensors with printed electronics technology. The key challenge is that the fabrication and performance of printed sensors depend very strongly on various fabrication parameters, such as material composition and process. Some of the other challenges are those that relate to the process throughput, trade-offs between cost and performance, large linewidth, poor reproducibility, and substrate distortion. If the challenges are not addressed, the performance issue will limit the commercial progress of printed sensors. The long-term objective of this research is to develop innovative low-cost printed sensor technology solutions that will push the boundaries of cost and performance. To overcome the challenges in developing high performance low-cost printed sensors, this research program will aim to 1) design simple novel sensors, 2) implement them by varying materials and their composition, and printing processes, and 3) perform extensive studies along various parameters to understand the fundamental operations and performance limiting issues, and devise high performance sensors with low-cost. In pursuit of this goal, my research program will focus on three short objectives: 1) Design and apply novel printed acoustic sensors for environmental monitoring, 2) develop fully printed RLC (resistor inductor capacitor) resonators based wireless passive electronic nose (e-nose) for food quality monitoring, and 3) develop all printed electrochemical biosensors. This research program will contribute to the training of 4 PhD, 3 MEng and 3undergraduate students. It will give supported students the opportunity to develop important skills in printed electronics, microelectronics, and electronic materials area,all of which are skills necessary for any engineers performing work in the printed electronics area. The research will be important to both academic and industry community working in sensors area. Sensors based on silicon micromachined based technology are still expensive to be deployed in everyday life of common people. Printed electronics is increasingly playing role in developing low-cost sensors. The proposed research program will ensure that the cost of printed sensors remain low while improving their performance. Furthermore, it will produce high quality and high impact research publications, effective industry engagements, and high value added services and commercial products in the field. In turn, it will bring advancement of research in printed sensors and Canadian Economy.""639560,""Bhadra, Sharmistha"
"637214"	"Bock, Wojtek"	"Novel fiber-optic sensing platforms for bio-chemical trace hazard detection"	"Since January 2016 the main direction of our research activities has centered on my SPI/NSERC Senior Industrial Research Chair (IRC) in Photonic Sensing Technologies for Safety and Security Monitoring. This is a 5-year applied research program focused on several methods for detecting bacteria and illicit drugs using fiber-optic sensors (FOSs). The principal instrumentation platform proposed for bacteria detection is the long-period fiber grating (LPG). The research program presented in this application is a fundamental extension of this ongoing IRC program within the objective of bacteria detection, with the specific purpose of exploring several novel fiber-optic sensing instrumentation platforms that have never been investigated before. Consequently, it will be a high-risk program that would not be an appropriate fit with the applied nature of the IRC program, but would, in our opinion be appropriate for funding under the Discovery Program. The two novel fiber-optic sensing and instrumentation platforms that are proposed here will be explored and developed taking advantage of our new design, fabrication and technological capabilities. These involve a femtosecond (fs) laser and an associated high-precision micromachining system, both established and operational in our labs since the end of 2015, and now fully available for the proposed research. The first concept involves fabrication of the LPG and LPG-like structures not in the optical fiber core itself (as is the case with traditional LPGs) but rather somewhere in the cladding area, between the core and the external surface of the fiber. The second approach will involve fs-laser manufacturing of interferometers based on microcavities created by drilling specific openings in optical fibers. Once these novel platforms have been developed, they will be tested in the application area of Pathogenic E.coli detection. This bacteria is one of the most dangerous agents of food-borne disease, and has been identified as a common cause of diseases related to food safety. Consumption of contaminated food or water can be deadly, especially for children and the elderly. Although E.coli infection is most prevalent in developing countries, many recent outbreaks in Europe and North America have been attributed to a strain of E.coli. Accurate routine fast testing is therefore crucial for outbreak prevention both in Canada and internationally. Currently available tests are inadequate because they require time-consuming amplification of samples. ""620722,""Boddington, Kelly"
"617768"	"Borodin, Allan"	"Design, analysis and Theory of Algorithms"	"This proposal concerns a number of different research areas. However, my recent research interests are often motivated by the question as to what can and cannot be computed by ``conceptually simple algorithms''. In this regard, my primary interest concerns conceptually simple approximation algorithms for combinatorial optimization problems. More specifically, I am studying various forms and extensions of online and greedy algorithms, primal dual algorithms, dynamic programming algorithms, local algorithms, and local search. And most recently, I have been concerned with domains where rather naive randomization can often outperform more ``principled'' and sophisticated deterministic algorithms. As examples, we can ask, what is the simplest deterministic one pass algorithm that can match or surpass the 3/4 approximation ratio achieved by various algorithms for the Max-Sat problem and the same question as to exceeding the 1-1/e approximation for online bipartite matching. In particular, I am studying parallel and multi-pass online and greedy algorithms. It has recently been shown that such algorithms can be sometimes be derived from randomized algorithms. However, so far there are only a couple of examples where randomized algorithms have been successfully de-randomized to become parallel algorithms. Furthermore, when such a de-randomization is possible, we do not know how much parallelism is required. My interest in conceptually simple algorithms has led me to various problems in the field of algorithmic game theory/mechanism design (AGT). One fundamental problem is when can good approximations be turned into good mechanisms given that self-interested agents are providing the inputs. For example, in auctions the underlying allocation algorithms needs to be conceptually simple for the auction mechanism to be adopted in practice. Perhaps the simplest type of auction mechanism is a posted price mechanism where agents (in some order) are offered prices for various bundles of goods and then must make a ``take it or leave it'' choice as to what to purchase. How such a mechanism price items or when there is no mechanism what are the eqquilibrium prices and what are the dynamics that can lead to equilibria. In the related field of social choice theory, I am also considering algorithmic questions concerning the use of various voting rules. And in other somewhat related fields, I am also interested in how influence spreads in a social network and how one achieves stable matchings given only partial (e.g. probabilistic) information as to preferences.""621244,""Borowiec, Brittney"
"634856"	"Bouchard, Kvin"	"Intelligence ambiante et domotique pour le maintien  domicile: une nouvelle approche non supervise de forage de donnes spatiotemporelles"	"Depuis quelques annes, la discipline de lintelligence ambiante connait un essor grandissant en informatique et en gnie. Celle-ci se dfinit comme une combinaison entre la domotique, soit la distribution dissimule de capteurs et deffecteurs dans un habitat, et lintelligence artificielle, le programme analysant les donnes. De nombreuses applications peuvent dcoules de lintelligence ambiante, notamment plusieurs lies au problme du vieillissement de la population. En effet, elle savre tre une solution prometteuse pour le maintien domicile des populations profil griatrique, tout en maximisant leur qualit de vie. Dans ce contexte, lun des principaux dfis se compose par llaboration des algorithmes capables de reconnatre les activits de la vie quotidienne (AVQ) dun rsident. La reconnaissance dactivits consiste recueillir les donnes en provenance de ces capteurs, les interprter sous forme de modles abstraits simples et de les associer un modle dAVQ prexistant dans une base de connaissances. Deux approches nous permettent de nous attaquer au problme de la reconnaissance dactivits. Le principal inconvnient de la premire, soit lapproche symbolique, est quelle dfinit les AVQ en langage formel et ncessite la construction fastidieuse dune base de connaissances exhaustive. En revanche, lapproche du forage, telle que nous lutilisons, construit automatiquement celle-ci. Le forage de donnes peut se dfinir comme le processus non trivial dextraction dinformations implicites, nouvelles, et potentiellement utiles, partir de grands volumes de donnes. Le prsent programme sorganisera donc autour de deux volets de recherche. Le premier volet visera amliorer en amont le forage de donnes par lexploration de caractristiques spatiotemporelles. Le principal dfi scientifique est de parvenir exploiter celles-ci tout en conservant la gnralit des solutions. Lavantage qui en rsulte sera de parvenir dfinir les AVQ avec une granularit plus fine et dainsi amliorer la prcision de la reconnaissance. Le second volet traitera dune problmatique en aval du forage : la drive conceptuelle. Ce problme apparait lorsque les proprits statistiques des variables, que les modles essaient de prdire, voluent au fil du temps. Afin dviter que ceux-ci drivent trop des modles appris, nous explorerons les techniques de forage prdictif et de forage incrmental de donnes. En plus des avantages socioconomiques noncs en introduction, le financement de ces travaux de recherche permettra la cration de proprit intellectuelle et de nouvelles connaissances scientifiques dans un domaine en plein essor. Ainsi, le transfert technologique permettra aux entreprises canadiennes dinnover et de se dmarquer face lconomie comptitive des technologies de linformation et des communications.""639622,""Bouchard, Kvin"
"634324"	"Boykov, Yuri"	"Combinatorial Optimization for Computer Vision and Biomedical Image Analysis"	"Automatic computer/robot vision for manufacturing, health care, security, and multi-media is a very active area of research, but real data performance is still far from perfect in quality, speed, or both. Due to explosion in availability of ever diverse digital media and computational resources, the number of potential applications grew significantly, but so did the size and complexities of the data. Despite significant progress in the last 10-15 years, the computer vision and biomedical image analysis communities are looking for new algorithms and mathematical models even for basic problems like segmentation, reconstruction, detection. Robustness, computational efficiency, and scalability of algorithms are crucial factors. Efficient optimization methods for high-order regularization constraints in the context large real data with high-dimensional features remain a challenge. My approach to computer vision and biomedical image analysis is largely based on discrete models and combinatorial optimization methods. My past research demonstrated many powerful combinatorial algorithms (e.g. graph cut, a-expansion, facility location, etc.) or discrete approximation methods (e.g. based on bound optimization, trust region) computing either globally optimal or provably good solutions for mathematically justified high-order graphical models in a wide range of problems in vision and biomedical imaging. These optimization methods lead to breakthrough results for difficult problems like N-D image segmentation, multi-camera stereo, texture synthesis, motion analysis, object detection/recognition, and thin-structure estimation. There are many reasons to continue research in discrete algorithms for regularization, which is my long term goal. Alternative continuous approaches require GPU implementations only to generate running times comparable to discrete methods. They are also not as stable or repeatable explaininglack of publicly available code - in contrast to combinatorial algorithms (with 20-30 daily downloads from my group's web site). Other alternative methodologies lack geometric justification making it impossible to integrate principled structural/topological constraints. The problems I plan to work on in the next five years are related to efficient optimization for high-order priors, structured partially-ordered labeling, curvature-based vessel extraction, regularization constraints for clustering in high-dimensional feature spaces, and integration of principled fast multi-object segmentation techniques with machine learning methodologies for object classification based on kernel SVM and neural networks.""633356,""Boyle, Latham"
"637262"	"Bradley, Jonathan"	"Integrated Rare Earth Lasers for Silicon Photonics"	"Lasers and photonic devices have influenced our lives in many critical ways, including enabling the high speed lightwave signals that fuel the internet. Now, after decades of research, so-called silicon photonics is poised to significantly advance internet and data communication through decreased cost and increased performance and scalability. Silicon photonics (SiP) leverages the existing multi-billion-dollar microelectronics fabrication infrastructure to provide highly compact, inexpensive, and energy-efficient integrated optoelectronic microsystems. As a result of its growing application in high-speed optoelectronic data networks, the SiP market is expected to increase to several $B in the next decade. Further, SiP promises to be ubiquitous in sensing, imaging, ranging, medical, and advanced military and space applications. Nevertheless, one of the key missing elements in SiP systems remains a monolithic laser. Silicon and SiP-compatible materials (i.e. those which can be monolithically integrated on silicon chips such as silicon nitride and germanium) provide many of the required functions in optoelectronic circuits, including signal transmission, switching, modulation, filtering, multiplexing, and detection. However, it has long been recognized that silicon is an inefficient light emitter. Currently SiP microsystems rely on off-chip lasers or bonding costly materials to silicon to deliver light. An efficient on-chip, monolithic, and scalable light source will have an enormous impact on future applications of SiP. In this research project we will develop new light-emitting rare-earth-doped photonic materials and the first fully-integrated rare earth lasers for silicon photonics. Advantages of SiP rare earth lasers, in addition to their low cost and monolithic fabrication, include their high powers, compact size, temperature insensitivity, high stability, and wide range of emission wavelengths and wavelength tunability in important communications and sensing windows. By integrating lasers based on these low-cost materials into wafer-scale silicon processes we will increase their scalability and open entirely new functions of SiP microsystems that are currently performed using more expensive or bulky optical platforms. This research will lead to specialized training of highly qualified people (HQP), have a significant impact on the growing SiP industry and information technology, and enhance overall high-tech expertise in Canada. By developing the lasers in Canada, we will continue Canadas leadership in advanced communications and photonics technologies. We will implement the methods and devices in advanced SiP microsystems for communications, sensing, and emerging applications to meet the growing information and technology needs of society.""640149,""Bradley, Jonathan"
"635002"	"Brecht, Tim"	"Understanding and Improving the Performance of Streaming Video Services"	"My research program studies problems related to measuring, understanding and improving the performance of streaming video services, including the WiFi networks that often limit their successful delivery. The goal of these efforts is to provide tangible benefits to those who deploy such systems and their end users. There are a wide variety of services that stream video over the Internet. Some of these include: Amazon, Apple, HBO, Hulu, Netflix, YouTube, major sports leagues, and television networks (e.g., CBC, CTV, Global, and TSN). Streaming video now constitutes more than 50% of Internet traffic and as these services continue to expand and supplant traditional television services, the number of servers required to supply content keeps growing. Therefore, designing, implementing and improving the performance of these services is critical to improving the user's experience and to maintaining or reducing the cost of owning, operating, and subscribing to such services. Although video and Internet Service Providers (ISPs) continue to add resources to and improve their infrastructure, many devices used to watch streaming video are limited by the WiFi network used to connect to the Internet. Recent research has shown that the performance of the home WiFi network and not the ISP link often limits the rate at which data can be obtained from the Internet. The problem is that when many access points and devices operate within the same vicinity each device suffers because contention and interference reduce the available bandwidth of the wireless channel. Improvements in streaming video service infrastructure and WiFi networks are both required to ensure a good quality of experience for users while streaming videos. Our research will break new ground in methods for improving the performance of video streaming services and in devising techniques for measuring, characterizing, understanding and improving the performance of WiFi networks. Benefits to the field include: 1) a detailed, in depth understanding of the important factors that impact the performance of streaming video services and WiFi networks, 2) significant improvements in their performance, and 3) the creation of new tools and infrastructure for use by other researchers to measure, study and further improve video services and WiFi networks. Benefits to Canada will be the efficient delivery of higher quality video streaming services at lower costs and increased demand for high-bandwidth ISP services. This will help ISPs as well as content providers (e.g., Rogers, Bell, CBC, CTV, TSN, Sportsnet and others) who want to increase their streaming video presence and to retain and attract customers. Additionally, our research to improve WiFi network performance for streaming video will also benefit the millions of Canadians who rely on WiFi networks every day for business and leisure.""631977,""Breden, Felix"
"634352"	"Brown, Michael"	"Modelling Image Formation and In-Camera Imaging Pipelines"	"My research program is focused on understanding the physical world through images captured by digital cameras. More specifically, I'm interested in what an image can tell us about the real-world environment. To this end, my research focuses on ways to model how physical light coming into the camera is transformed into the final three-channel image (red, green, blue) pixel-based images. This type of image formation is sometimes referred to as ""low-level computer vision"", which deals with how images are formed and their relationship to the physical world. Low-level computer vision often treats an image as a 2D signal, much like a communication signal. In this way, we can discuss problems such as trying to determine the true signal when it has undergone some degradation (such as sensor noise, or movement during the image process e.g. image blur due to camera motion). Such image formation models can also take into consideration environment factors, such as fog and haze, and how this can be removed through computational methods. As part of this research program, one of the key components of my work is understanding exactly how digital cameras work. While we like to think of cameras as light-measuring devices, the current design of commodity cameras includes a great deal of additional processing that is applied on board the camera (often referred to as the in-camera processing pipeline). The goal of most camera manufacturers is to make visually pleasing photographs and not necessarily to faithfully capture the imaged scene. While this is ideal for photography, this type of image manipulation is often at odds with models used in low-level computer vision. In particular, in-camera manipulation can modify colours, change local contrast, and substantially distort the original sensor response in a way that makes it challenging to determine the actual nature of the physical environment. One of my research focuses is to design new camera processing pipelines that allow the ability to produce both photographic images and images suitable for scientific purposes. Developing such ""hybrid cameras"" has the potential for significant impact, as we now are using our cameras (especially those on our mobile devices) for many non-photo-centric tasks (such as document scanning, object identification, medical imaging, colour matching). The long-term prospects of this research program are to shape the future design of consumer cameras and the applications they can be used for.""642716,""Brown, Michael"
"634341"	"Brubaker, Marcus"	"Inference and Model Building for Vision-based Estimation of Transmissive Objects"	"This proposal seeks to further the state of the art in the field of computer vision and image processing, with the goal of expanding and reinforcing the use of imaging modalities in our understanding of the world. Images are a rich source of information which are readily captured in many scenarios. The ability to process these images and extract meaning from them is the key goal of computer vision. The long-term objective of this proposal and the PIs research program in general is the theoretical and practical development of models which enable the estimation of properties of the world from images. One such problem is the estimation of detailed 3D structure of transmissive objects. Imaging and reconstruction of such objects arises in a range of domains including: optical imaging of translucent objects like clouds, glass, water or smoke, x-ray imaging of humans and animals for medical purposes, and transmission electron imaging or materials and biological specimens. The objectives of this proposal focus on transmission electron imaging with the aim to build new methods for electron cryomicroscopy (cryo-EM). Specific problems relating to the estimation of the 3D structure of biological molecules such as proteins and viruses from cryo-EM images are considered. Cryo-EM is a rapidly emerging method for experimental structure determination of biological molecules. The function of DNA has sometimes been explained by analogy as the source code of life. Continuing this analogy, proteins and viruses are the compiled programs which take the form of molecular machines that execute life at a cellular level. Determining the 3D structures of these machines and understanding how they work is not only of fundamental scientific interest, it is also critical in the understanding of disease and the development of novel drugs and treatments. The research proposed here would advance the state-of-the-art and have significant impacts on cryo-EM and consequently on many aspects of science and medicine both within Canada and around the world. By improving the reliability and resolution of estimation structures, the methods proposed here would form the basis for rapid structure determination methods which could be used to more quickly develop new drugs which would be less likely to have unexpected side-effects. They would also allow scientists to study never before seen mechanisms in the cell as the motion of these molecular machines could be captured directly. ""623260,""Bruce, Adam"
"634709"	"Brudno, Michael"	"Computational Methods for Capturing and Analyzing Personalized Genomic and Medical Data"	"My research centers on the development of Computer Science methods to help solve problems in Biology and Medicine. While my research has application to many areas of medicine and genomics, the unifying thread through all of my work is the development of novel computational methods, firmly within the domain of Computer Science, to solve these problems. In the next five years I want to focus on the development of three computational approaches that can be combined for more accurate and rapid capture and analysis of medical data. In all of these cases the effort will be primarily on developing novel computational methodology and approaches, rather than deployment of existing technologies in the medical setting, and many of the methods developed will be generalizable outside of the medical setting. Aim 1 of my grant will be on the development of mobile devices and HCI techniques for the capture of data from a patient visit. The core interaction between a patient and their doctor involved the doctor making observations, taking notes (on computer or paper), and finally writing (or dictating) a report. This report serves as the main record of the full interaction with the patient and myriad of signs and symptoms visually interrogated by the doctor (not all of which may be recorded in the notes, especially if not abnormal). We propose to develop a next generation of user interfaces, using wearable technology on the clinician (camera and microphone), combined with mobile devices (tablets),integrated into the clinical workflow, and capable of capturing the full visual and audio spectrum of the patient interaction. In Aim 2 of the grant we will work to develop ML methodology to identify concepts from the captured data audio, video, and text. We will utilize biomedical ontologies to help improve the accuracy of the methods, training models that utilize proximity in text and proximity in biomedical ontology space to help train classifiers. We will work on integrating this approach into existing speech-to-text tools to help improve audio processing, and apply these approaches to audio recording of patient exams to test their accuracy. Finally, in Aim 3 of the proposal we will work on data visualization of recorded patient data, with the aim of providing the clinician with a easy-to-understand summary of patient symptoms over time, as well as to allow for quick comparison between a patient and others with similar disorders (for differential diagnosis) and same disorder (to understand disease variability and prognosis).""635248,""Brudnyi, Alexander"
"635114"	"Butz, Cortney"	"Optimizing Inference in Deep Learning Models"	"Deep learning is currently in the media spotlight due to several impressive feats, including Google's self-driving cars, voice recognition in intelligent personal assistants (Apple's Siri, Google's Now, Microsoft's Cortana, and Amazon's Alexa), and beating a world champion in the game GO. Other notable achievements involve setting new records in image recognition, analyzing particle accelerator data, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Although there is no consensus on the definition of deep learning, deep learning involves modelling a problem domain by learning a multiply-layered network from a large amount of data using specialized computer hardware (graphical processing units rather than central processing units). One required step in deep learning is inference. Inference means updating the knowledge base according to real-world observations. Sum-Product Networks (SPNs) are a deep learning model that can perform inference in linear time. This is important, since it means that the inference step can be done efficiently. This proposal primarily focuses on further optimizing SPN inference. One objective is to perform SPN inference in sub-linear time. The probabilistic reasoning literature has shown that moving from linear time to sub-linear time can yield significant time savings in practice. Another objective is to incorporate semantics into SPNs. Currently, SPNs lack semantics. Incorporating semantics will bring meaning to the structure of the SPN. This, in turn, can be exploited in at least two ways. First, irrelevant parts of a SPN can be ignored during inference. Second, the SPN itself can be compressed. Both cases can result in faster SPN inference, which then implies faster learning. We will achieve the above objectives using our extensive history working on semantics in probabilistic inference and by exploiting Darwinian Networks ( DNs ), which are like looking at Bayesian networks (BNs) through a microscope. DNs have led to the development of Simple Propagation (SP), which is a method for BN inference and empirical results demonstrate that it tends to be faster than Lazy Propagation (LP), a standard approach to BN inference. Moreover, DNs have lead to rp -separation, which is a method for testing independence in BNs. Experimental results show that this approach is 53% faster than algorithms (Reachable and Bayes-Ball) for the same purpose. Given these exciting results, we are eager to develop methods for sub-linear SPN inference.""623769,""Buzuloiu, Paul"
"634512"	"Capretz, Luiz"	"Enhancing Software Engineering Recommender Systems with Software Analytics"	"The creation of software and its usage generate a fair amount of data about the software itself during its development, deployment, and operation. However, raw data require appropriate analysis in order to provide software practitioners and managers with feedback, insights, and analytical services about the software during its production as well as throughout its operation and maintenance. Software Analytics (SA) have been researched by the communities working on mining software repositories and predictive models, and offer great opportunities for software engineers to accumulate information about a project, predict outcomes, and make recommendations. In general, recommender systems provide software practitioners valuable recommendations or suggestions relevant to their contexts and interests. These recommendations/suggestions facilitate and foster the decision making process. Recommendation systems in software engineering (RSSE) has become a hot research topic in the last decade. RSSE has been defined as a software application that provides information items estimated to be valuable for a software engineering task in a given context. Recommendation systems for software engineers are emerging to assist developers in various activities from reusing code to writing effective bug reports.Recommendation systems may guide practitioners in decision making throughout the software development process. In this context, recommendation systems can help software managers efficiently organize their resources and identify problems by analyzing patterns on existing project data in a meaningful manner. The proposed research program aims to develop recommendation systems to be used in various phases of the software life cycle; we combine the fields of software analytics with mining of software repositories in order to empower recommendation systems. The ultimate goal is to improve the software development process, thus facilitating the management of large software projects. The research methodology follows the procedures to build a recommendation system, i.e., context extraction (trigger and treatment), development of a recommendation engine, and recommendation filtering. We will meet this goal by offering recommendation services through the development of the following: lessons-learned recommendation system, software features recommendation system, continuous calibration of software prediction, monitoring productivity of software developers, and suggesting assignment of tasks to people based on their personality traits. These systems combine many computer science and engineering methods to proactively tailor recommendations that meet users particular information needs and preferences. Through software analytics, this innovative research is expected to advance the state-of-the-art in recommendation systems in software engineering. ""618835,""Capretz, Miriam"
"634479"	"carenini, giuseppe"	"Improving, Extending and Leveraging Discourse Parsing"	"The sentences in a document are related to each other in order to express complex ideas. For instance, two sentences can be in a contrast relation is they present opposite ideas, or in an elaboration relation is one sentence is presenting more details on the idea expressed by the other. A discourse parser is a software program that given a document as input is able to extract all the relations between its sentences. The output of discourse parsing can then be leveraged to support many other useful tasks, like creating a summary of the document or determining the opinions expressed by the document. The main goal of the proposed project is to improve current discourse parsing technology in several ways. First, we aim to boost the accuracy and the speed of current parsers by applying novel techniques from artificial intelligence and machine learning. Secondly, we will study how other text processing tasks, like summarization and text mining, can maximally benefit from the output of our more accurate and faster discourse parsers. Finally, we will extend discourse parsing beyond text, to deal with the many documents, ranging from newspaper articles to scientific publications, in which text is combined with visual material (i.e., multimodal documents). In addition to scientific impact, we expect this research to have real and significant economic and social benefits for Canada. There is a rapidly expanding demand for applications that are designed to help users understand and manipulate complex bodies of textual and multimodal documents, both in the workplace and for personal use. For instance, journalists often need to analyze many documents to check facts and discover stories, while citizens may benefit from summaries of news from different sources to build more informed opinions about current events. Similarly, in the healthcare sector, summaries of patient histories and relevant medical literature could be very useful to doctors, while patients may need support in exploring on-line discussions about their condition. Finally, consider the business domain, where Canadian companies could better understand their customers and develop better products by mining online reviews. Conversely, consumers, by accessing the same information, could make more informed purchases. The proposed research has great potential to fuel these and many other applications, by boosting discourse parsing performance, extending its applicability to documents including visualizations and by turning these improvements into robust and versatile text processing technologies that can benefit both Canadian companies and citizens.""618450,""Carey, Jason"
"637510"	"Champagne, Benoit"	"Signal Prosessing Techniques for 5G Wireless and mm-Wave Communications"	"Extraordinary advances in wireless network technology during the past decade have revolutionized the way in which we use personal communications. As a result of such benefits, the user demand for throughput continues to grow at a stunning rate of 50% annually. Anticipating that current 4th generation (4G) of wireless systems will not meet the demand for future services, standardization bodies have set forth impressive target specifications for the 5th generation (5G), with multi-fold enhancements in data rate, connectivity, energy efficiency, and quality of service (QoS). Reaching such high-end performance in a spectrum congested environment calls for the introduction of new physical layer technologies along with sophisticated signal processing algorithms. The long-term goal of the proposed research is to develop and investigate new signal processing concepts, theories and algorithms, to meet the demands of emerging 5G mobile networks and beyond. Key innovative aspects include a focus on location-aware information processing, which provides the unifying theme for the proposed work, along with an emphasis on the use of multi-antenna systems operated in higher frequency bands, i.e. shift towards mm-waves. Within this exacting framework, the short-term objectives of the program are articulated around three complementary topics, namely: (1) modulation formats and waveform design; (2) space-time processing for large-scale antenna arrays; and (3) device localization in ultra-dense networks. Under the first topic, we will study new adaptive multi-carrier modulation (MCM) techniques that take advantage of location-based information to optimize spectrum efficiency over time, across the network. These new schemes will then be extended for application to massive multiple-input multiple-output (MIMO) transceiver configurations. Under the second topic, we will develop improved algorithms for the estimation of location-based parameters, including: direction of arrival (DoA) and time of arrival (ToA). We will also investigate new low-complexity 3D beamforming algorithms, with a focus on beamspace methods. Under the third topic, we will research novel on-line algorithms for the localization and tracking of mobile devices using both line-of-sight (LoS) and non-LOS (NLoS) measurements. Finally, these algorithms will be generalized to distributed forms of processing, for practical application in dense heterogeneous networks. This program of research will provide numerous opportunities for HQP training at the M.Eng. and Ph.D. levels, while the main findings will be presented at international conferences and published in highly selective journals. ""622292,""Champagne, Florence"
"635120"	"Charlin, Laurent"	"Life-Long Machine Learning for Recommender Systems"	"We, humans, are very good at decision making. When building automated methods for decision making, computer scientists including mathematicians and engineers in artificial intelligence take inspiration from humans. In particular, we are trying to build computer systems that reach human-level performance. If such methods were successful they could assist humans in making even better decisions. This could be particularly useful in situations where we are faced with a a large number of choices or situations where we do not know about the possible outcomes of each choice. Examples includes decisions about treatment options for a particular medical condition or, more mundane ones, like choosing the next book to read from a large catalogue of books or a group of stocks to add to our financial portfolio. One particular approach in artificial intelligence research is to frame such problems as a recommender system . At their core, recommender systemsfind patterns in user behaviour to improve personalized experiences and understand the environment, or the context, that they are acting in. The simplest recommender system suggests items of interests to its users based on their past behaviour (for example the next book to read or person to date). While recommender systems are an important field of current research and are widely deployed (think of Google's search suggestions or Amazon's ""Customers Who Bought This Item Also Bought"") their capabilities are limited. Of crucial importance to humans is our ability to adapt our decisions based on how our preferences evolve through time and based on changes in the environment. For example, how will my plans for the day change if it starts raining. Similarly, we, humans can reason over longer periods of time. For example, how do I pick a university major based on my values and future employment opportunities. Recommender systems and artificial intelligence more broadly do not yet offer the same possibilities. In particular, the mathematical models used in recommender systems do not have the ability for life-long learning, that is to continuously model changing user preferences and context and, accordingly, to adapt its recommendations. This research project will develop and validate novel mathematical tools using machine learning methods for life-long learning in recommender systems. Specifically, my research will use and develop novel techniques in the fast-growing fields of deep learning and reinforcement learning (RL). Deep learning can provide more accurate mathematical models of user behaviour over time as well as incorporate different sources of information. RL provides methods for turning model insights into good (sequences of) decisions. This research will contribute to Canada's position as a leader in artificial intelligence and fuel downstream applications, such as automated health assistants, that can help Canadians in their daily lives.""628288,""Charlton, Colleen"
"637189"	"Chen, Li"	"Radiation-Tolerant Microelectronics for Space and Commercial Applicaitons"	"Silicon technologies have been the dominant platform for the majority of the electronic devices in industry. The silicon technology scaling has significantly improved the performance of integrated circuits (ICs). However, due to the small device dimension and low operating voltages, nanoscale ICs have become highly sensitive to operational disturbances. These disturbances, especially those caused by single event effects (SEEs) due to energetic particles in ICs, can introduce transient pulses in logic circuit nodes or upset data in storage cells. Their impact can range from a single data corruption to a severe system crash. For example, Sun Microsystems were forced to recall their flagship servers in 2000 due to sudden and mysterious crashes caused by cosmic particles. Even for ICs in terrestrial environments, the error rates caused by SEEs can be 100 times higher than those from hard failures such as device wear out. The objectives of the proposed research program are to study the SEEs in microelectronics and, further, advise cost-effective mitigation solutions to achieve reliable operations for electronic circuits and systems. Various logic circuits and systems with SEE-hardening techniques will be designed and fabricated in test chips using advanced silicon technologies. Ion beams (protons and heavy ions) will be used to evaluate their performance in the context of SEE tolerance. An on-campus pulsed laser facility has been established and will be used as an investigation tool to characterize SEEs in the test chips, as it can precisely induce SEEs in the ICs at a designated location and time. In addition, device- and circuit-level simulation tools will be used to model and characterize the performance of the test circuits. The simulation results will be used to correlate with the laser and ion radiation results, which will provide insight, understanding and information about SEEs in ICs. The proposed program will develop SEE-tolerant technologies for the Canadian industry in order to improve the reliability of microelectronics. This program will equip a number of highly skilled personnel with the training necessary to enter this innovative field of research.""618670,""Chen, Li"
"634961"	"Chen, Yuanzhu"	"Exploring Machine-Mediated Social Interaction Mesh from a Network Science Perspective"	"The proposed research program investigates properties of the mesh of social interaction facilitated by the increasingly prevalent mobile devices and smart gadgets, which are capable of data communication through infrastructure and directly between themselves. The objective is to provide an understanding of the robustness and data-carrying capacity of such a multi-modal, time-varying, complex network. The solutions produced in this research will constitute an intermediary between the IT sector and other disciplines, such as public health and social network studies, for collaborative research. Modern mobile devices are equipped with a multitude of sensors and radios, and are capable of communicating sensed and user-generated data in a variety of modes. When they are in serviced areas, bits are transported by Wi-Fi or cellular networks. Recently, the potential of smart mobile devices is further unleashed by radically new communication methods, including mobile ad hoc networking (MANET) and delay-tolerant networking (DTN). MANET coordinates intermediate network nodes in forwarding data packets when the data producer and consumer are not directly within range, while DTN further capitalizes on the mobility of these nodes to mechanically carry data across different sections of the network. Such innovative networking schemes effectively extend the devices communication range over space and time to reach many more potential peers. Formally, the network encompassing the space and time domains is modeled as spatial-temporal graph. On one hand, users will enjoy the fact that their messages can travel a greater distance through this highly enriched mesh of smart devices. On the other hand, it is imperative for the scientific research community to understand the networks capacity in data transportation and influence spreading, and its resilience to failures and attacks. In the proposed research, I will first devise a message logging tool on smart mobile devices which form an infrastructureless wireless network using the short-range radios onboard. The tool will not only log encounters of the mobile peers, but will also pass tracing messages over the intermittent radio links as users move around. The data gathered on devices longitudinally will then be coupled with social network data during the same period to reconstruct the spatial-temporal network. To analyze such a complex network, results in network science will be extended to accommodate a multitude of link types and to portray the networks evolution over time. Through the network structure, we strive to understand its behaviour using analytical tools such as site percolation, epidemic model of disease spread, and dynamical systems generally. The acquired knowledge will help us provide innovative and reliable information sharing in the remote rural and arctic areas of Canada, and build resilient cyber systems nation-wide.""627394,""Chen, Yue"
"617738"	"Cheng, Julian"	"Wireless Fronthauling Technologies for Future Generation Wireless Networks"	"Cisco Systems predicts that mobile data traffic will exceed 30 exabytes per month by 2020. To address the skyrocketing mobile data volume, research activities in wireless network architecture have indicated two trends. The first trend is to densify the access points by massive deployment of lower-power and low-cost access points. The second trend is to separate network functionalities by shifting the computational power of a traditional base station to a base band unit (BBU) pool with centralized computing power. As a result, the cloud radio access network (C-RAN) has emerged as suitable network architecture for future generation wireless networks. In C-RAN, signal processing is centralized in the BBU pool and radio frequency processing is handled by remote radio heads (RRHs), which are low-power and low-cost access points connecting to the mobile users. In C-RAN, the links that connect the RRHs to BBU pool are called fronthaul links. Fronthauling is defined as the transportation of large user data volume from RRHs to the BBU pool. The current technology implements the fronthaul links via wired line such as optical fibers and high-speed IP backbone. However, connecting massive number of RRHs to the BBU pool via dedicated wirelines is economically infeasible and lacks the required deployment flexibility that the carrier operators desire. Therefore, fronthauling is particularly challenging for a C-RAN and has been identified as a bottleneck problem for future wireless networks. This proposal addresses various wireless fronthauling technologies for C-RAN, and these technologies include the millimeter wave based beamforming, free-space optical communications, unlicensed spectrum band transmission and caching. For each of these wireless fronthauling technology, we identify key challenging issues and propose potential solutions. We anticipate that the proposed novel fronthauling technologies can facilitate the introduction of new data-centric real-time mobile applications, which will take more than 80% of the total network traffic in 2020. The proposed vertical fronthauling configuration will lead to several new research directions. This architecture can provide high data rate and cost effective coverage for the rural areas, where about 19% of the Canadian residents live. This study can also give birth to a new research direction for defense applications. After some modifications, the unmanned aerial vehicles (UAVs) that are equipped with BBU pool can be used in a battlefield scenario when the satellite based communication is disabled. The success of the proposed research program can give birth to a new industry sector such as manufacturing of specialized UAVs equipped with sophisticated computing and communication devices, and can have the potential to revive the Canadian telecommunication industry in the new era.""637328,""Cheng, Julian"
"634726"	"Chiasson, Sonia"	"Human-oriented computer security"	"Effects of poor cybersecurity impact everyone, from individuals to large corporations, not-for-profit organizations, or government agencies. While many security incidents have a human component, these can frequently be traced back to system designs and configurations that placing unreasonable demands on users by ignore human capabilities and real-world context of use. In the same way that network protocols are resilient to network interruptions or dropped packets, I suggest that security systems should be resilient and adaptable to human behaviour. My research program relates to such human-oriented computer security. The main objectives are to identify strengths and vulnerabilities in real-world security mechanisms, to develop improved designs and understand their security and usability implications, and to identify foundational design principles applicable to the general usable security space. For this grant, the following three themes will be explored. Domain experts such as security analysts or software developers are not typical end-users and are often overlooked in terms of usable security. However, consequences of unusable or inadequate tools can be devastating for entire networks or for software deployed worldwide. A first research direction includes improving support for security code reviews by developing tools to help programmers detect security vulnerabilities, prioritize security fixes, and encourage collaborative reviewing of code, partially inspired by Agile software engineering methods. A second direction relates to security analysts who must frequently merge several data sets, find previously undetermined associations among them, and share expertise. We intend to explore such collaborative work and devise new security visualization of large data sets. We have conducted significant research into understanding the human factors and security implications of knowledge-based authentication, and will continue working towards real-world solutions. For example, we have recently turned our attention to user authentication for children, a subject which has received almost no attention in the research community. We are working towards a child-friendly authentication scheme and a parent-child password manager addressing the need for autonomy and privacy while ensuring some parental oversight. In more exploratory research, we are interested in the usable security and privacy for the Internet of Things devices and implications for Smart Cities. This emerging area has significant usable privacy and security implications for end-users, and offers opportunity to affect the implementation of new technologies while their design is still flexible.""642708,""Chiasson, Sonia"
"637688"	"Chow, Paul"	"Applications and Programming Models for Large-Scale Heterogeneous Computing with FPGAs"	"As we enter into the era of the Cloud and the Internet of Things (IoT), issues around the power and performance of computing are becoming ever more important. Computing using microprocessors is still the mainstay of all forms of computing but microprocessors are no longer enough because they are not always the most efficient in terms of power or performance. As a result, non-traditional computing approaches are being explored with more urgency. The acquisition of Altera by Intel in 2015 for USD$16.7B has woken up the world to the benefits of using Field-Programmable Gate Arrays (FPGAs), but also made the computing world more heterogeneous. For many applications, FPGAs can provide both a performance benefit and do it with lower power requirements. The problem is that they are difficult to use. The motivation and challenge of our work is to make FPGAs accessible by all application developers. This proposal focuses on the exploration of programming models and driving applications for using distributed FPGAs in high-performance computing, but many of the techniques can also be applied at the other end of the computing spectrum in the IOT world. The long-term objective of this work is to make the use of FPGAs seamless so that application developers can focus on the problem they are trying to solve, rather than struggling with making FPGAs work, as it is today. The short-term objectives are to build programming infrastructures for heterogeneous systems incorporating FPGAs and test and validate them with exemplary applications. Our overall methodology is to always build working systems because that is the only way to prove that our ideas will work, and we can make measurements that are easy to justify. While our goal is to improve the programmer experience of using FPGAs, the solution spans the entire computing platform, or computing ""stack"", putting the overall project in the area of computing systems. FPGAs have very different properties than microprocessors because of the way that they are programmed and managed, and the native capabilities that they have. This means that they do not easily fit into the existing microprocessor-based infrastructure and an important aspect of our overall work is to figure out how to fit them, or how to change the infrastructure to accommodate them well. The Intel acquisition of Altera has disrupted the computing world and broadened the interest of reconfigurable computing from a small community to the much larger computer architecture community because of the belief that there are now significant commercial benefits from using FPGAs. FPGAs are no longer a niche technology because of Intel. The most significant impact of the proposed project is to address the programmability issue and enable application developers to use FPGAs so that the constraints of performance and power can be met. The proposed work is applicable across the computing spectrum, from the IOT to high-performance computing. ""629061,""Chow, Robert"
"637641"	"Chowdhury, Sazzadur"	"High Performance CMUT Arrays"	"The proposed research will investigate current barriers that limit the full potential of capacitive micromachined ultrasonic transducers (CMUT) for use in commercial, medical, and industrial applications. Specifically, issues related to high efficiency ultrasound generation and detection, such as resonance frequency drift, electrical and acoustical crosstalk between the elements of a CMUT array, sensitivity, nonlinear dielectric charging, leakage current, and transducer insulation necessary for efficient moisture protection and patient isolation will be investigated. The investigation will include theoretical analysis, 3D simulation, statistical analysis, virtual prototyping, fabrication, and experimental methods to acquire, model, and validate new scientific knowledge that can be used to design high performance CMUT arrays. The research will advance the CMUT technology to be a powerful alternative to commercially available piezoelectric transducers for biomedical imaging and therapy, non-destructive testing, and automotive applications. This advanced technology will support the development and commercialization of many new and/or improved products along with the associated employment opportunities. The research will especially lead to the development of highly integrated transducers for new biomedical imaging modalities with higher performance that will be the technology of choice in future ultrasonic imaging systems utilizing catheter ultrasonic probes. It is expected that this will lead to the earlier and faster diagnosis of medical conditions when they are more readily curable and less costly to treat. Highly qualified personnel (Masters and Ph.D.) in the fields of microsystems, MEMS-based ultrasonic transducer design, simulation, fabrication, packaging, testing, data acquisition and software development will be trained during the project period. These skills are readily transferable to many other types of computer assisted design projects in a number of sectors important to Canada. Research results flowing from this methodology will be published in peer-reviewed high impact factor journals and international and national conferences. Any new intellectual property will be secured by patent filings through the university. This advancement and diffusion of knowledge will lead to the generation of intellectual property, paths to commercialization that can enhance the global competitiveness of Canadian firms, and provide enhanced ultrasound based health care to Canadians.""631645,""ChowFraser, Patricia"
"635030"	"Churchill, David"	"Ontario"	"CANADA"
"636952"	"Cicek, PaulVahe"	"Fully-integrated resonant micro-devices for next-generation electronic systems"	"Nowadays, in several commercial sectors such as Consumer Electronics, Automotive and Industrial, microelectromechanical systems (MEMS) have established themselves as an essential vector in enabling an effective interface between the user, the environment and a system's electronic ""brain"" . Particularly, resonant-type MEMS devices have been widely demonstrated to fulfill high-performance tasks including electrical signal filtering, inertial sensing, ultrasonic transduction and gas detection. However, MEMS technology has yet to achieve its full disruptive potential because it is still predominantly restricted to discrete integration schemes for inclusion within complex electronic systems . Indeed, the lack of viable methods for monolithically integrating MEMS and integrated circuits (IC) imposes functional, performance-related, and economic limitations on commercial products such as: the inability to incorporate several MEMS devices of similar or different class onto a single IC die; l arger system size resulting from multiple dice; high power consumption because of the electrical parasitic effects related to the interconnections between multiple dice; greater costs (e.g., materials, processing, logistics) due to separate IC and MEMS fabrication on distinct dice. Consequently, the proposed research program attempts to discover and propose answers to the question: How can current technological limitations be overcome in order to enable fully-monolithic next generation systems incorporating IC and MEMS? Founding this program on prior advances realized towards monolithic integration, we hold that solving our research question revolves around the long-term objective (LTO) of exploring optimal approaches for above-IC integration of resonant MEMS . Advancing knowledge along this LTO is expected, at term, to provide system designers with a methodology and technological framework for seamlessly incorporating MEMS as lumped-cell elements on IC . This LTO will be attained by means of two short-term objectives (STO): to propose novel topologies for optimal resonant MEMS incorporating piezoelectric actuation , and to elaborate a novel fabrication platform technology for optimal IC integration of resonant MEMS . The viability of the proposed methods will be assessed by a third STO: to demonstrate novel integrated resonant devices using the elaborated topologies and fabrication platform . With a second LTO of providing HQP with multi-disciplinary expertise in microtechnologies , this research program is expected to exert a disruptive impact on the approach, methods and techniques used for designing and producing electronics-based commercial products. As such, the outcomes of this work will provide long-lasting valueto Canada's society, industry and research community .""638063,""Cicoira, Fabio"
"637055"	"Cloutier, Sylvain"	"High-performance printable hybrid light-emitting device architectures"	"High-performance printable hybrid light-emitting device architectures Vision: The long-term goal of this Discovery Grant is to develop an internationally-recognized research program investigating the fundamental interrelations between the synthesis processing, structural and physical properties of unique printable hybrid optoelectronic architectures and their potential use to produce high-performance low-cost light-emitting diodes. Rationale: Exploring new manufacturing paradigms is essential for the Canadian economy, andprintable optoelectronics is one of these rapidly-emerging sectors. In recent years, there was an extraordinary push to develop new and better low-cost and printable optoelectronic platforms for solar-energy harvesting, lighting displays and sensing applications. Building on our research momentum from the last 5 years, we are proposing to exploit a unique electrospinning and co-jetting expertise we have developed for the aerospace industry to explore the fundamental structure-property interrelations in novel printable hybrid light-emitting device architectures. Indeed, our preliminary results show that well-controlled electrospinning (to produce nanofibers) and co-jetting (to produce microdroplets) are powerful low-cost deposition techniques, which are well-suited to produce unique film architectures using a wide range of solution-based materials. These deposition techniques are also ideal to combine seemingly incompatible materials, or to address solvent compatibility issues when producing hybrid thin-film heterostructures. Meanwhile, exciting new printable material systems have also gained tremendous momentum in the last 5 years including new sol-gel precursors to produce high-quality nano-engineered metal-oxide materials, better quantum dots quantum dot solids and methylammonium lead halide (or MALH) perovskites. As part of this program, we are proposing to take advantage of our extensive electrospinning/co-jetting capability combined with our established optoelectronic materials device expertise to explore disruptive low-cost hybrid LED architectures. Objectives: Building on promising results from the last 5 years, we wish to explore two (2) virtually unknown territories in the field of low-cost printable LEDs to achieve unparalleled control on the emission properties and reach unprecedented device performance in hybrid printable LEDs. In the next five-year, we will focus on exploring (1) combinatorial co-jetting of hybrid polymer microdroplets with and without quantum dots and (2) carbon nanotube metal-oxide (e.g. TiO 2 )nanofiber networks embedded within a perovskite matrix to achieve the desired control andlead to significantly-improved devices that can potentially transform the field of printable LEDs and other low-costdevices for light-harvesting and sensing applications. ""642623,""Cloutier, SylvainG"
"637429"	"Coates, Mark"	"Nouveau-Brunswick"	"CANADA"
"635128"	"Coleman, Thomas"	"Large-scale Application of Automatic Differentiation in Computational Finance (and beyond)"	"Summary of Proposal: Large-scale Application of Automatic Differentiation in Computational Finance (and beyond) The field of Automatic Differentiation (AD) has taken great strides in recent years. Nevertheless, AD is not heavily used for large-scale problems often due to efficiency concerns. This is true in many application areas, including economics and finance. For example, large portfolios of variable annuities can take many hours to evaluate by standard methods; hedging, typically requiring the determination of derivatives, would require significantly more time. Clearly, any rapid hedging methods based on derivatives for such portfolios is problematic. This poses a serious challenge for effective risk management. Similar challenges exist for other portfolios (e.g., Credit Value Adjustment (CVAs)). We propose research to increase the efficiency and applicability of AD to large-scale structured problems. While our proposed advances are applicable to many branches of computational science and engineering, we emphasize computational finance and risk management applications. AD technology has become more efficient in recent years. We have been involved in the adaption of AD methodology to problems with structure, working under the assumption that most practical large-scale problems are structured. A general and practical definition of a structured problem is given in ([1], 4.16) . The idea is that the function can be written as a (partially ordered) sequence of steps -- each step is a nonlinear mapping in itself and is a (sub-) function of previously defined (intermediate) variables. This structure definition captures composite function computations (i.e., a sequence of chained computations), generalized partially separable functions, and (nested) Monte-Carlo functions (and many other common structures). As indicated in previous work this structure covers many applications and is often the most natural way to express the evaluation code . Given a code that exposes this structure AD can be applied in a slice-by-slice manner to gain significant efficiency (both in space and time). Generally this gain is because the Jacobians of the component functions are either sparse (i.e., hidden sparsity) or compact (with few rows), whereas the Jacobian of the overall (original objective) function is often dense. In conclusion, the determination of derivatives is a fundamental computational task in much of computational science and engineering. The ideas we propose here can have a very significant impact across these areas - certainly with respect to applied optimization problems, as well as our target class:sensitivity problems in computational finance/risk management. [1] Automatic Differentiation in MATLAB Using ADMAT with Applications. Thomas F. Coleman and Wei Xu, SIAM, 2016. ""633070,""Coley, Alan"
"637240"	"Collier, Christopher"	"Biomedical Terahertz Systems"	"One quarter of Canadians die from cancer, being 80,000 Canadian deaths in 2015, and skin cancer is especially prevalent. In fact, skin cancer has a rate of occurrence equal to that of all other cancers combined. As early detection dramatically increases survival, there exists a longstanding need for quick, reliable, and widespread skin cancer detection. Recent advances in skin cancer detection have appeared through spectroscopy with near-infrared wavelengths. Unfortunately, these wavelengths struggle to probe subdermally and to distinguish benign and malignant lesions. However, terahertz (THz) spectroscopy is ideally suited to skin cancer detection. It can probe subdermally, from its deep THz penetration, and identify malignant lesions, from its high THz sensitivity to moisture. However, there are many challenges with oncological THz systems under development. These systems are prohibitively expensive, are much too large for widespread clinical use, and have long data acquisition times which are ineffective for in-vivo scanning of skin. These systems use conventional THz emitters, which require expensive and bulky titanium-sapphire lasers, and use conventional THz detectors, which operate through slow measurement techniques. The proposed work will develop oncological THz systems possessing the required functionality for skin cancer detection and diagnostics. This functionality will be achieved by addressing key issues related to THz emission and detection. Terahertz emitters will be developed that are compatible with inexpensive and compact erbium-doped fibre lasers (for inexpensive and compact oncological THz systems). Terahertz detectors will be developed that utilize a novel technique for short data acquisition times. The proposed work will use results from several pilot studies previously completed by the applicant. These pilot studies developed a photonic nanojet lensing technique which will enable the required compatibility between photoconductive THz emitters and erbium-doped fibre lasers. These pilot studies also developed a phase mask technique which will enable the creation of THz detectors with the short data acquisition times required for skin cancer diagnostics. Prototypes of the oncological THz systems will be developed as this technology is moved towards commercialization and widespread availability/impact. The developing oncological THz systems have great medical and commercial potential.""620120,""Collier, Scott"
"637696"	"Coulombe, Stphane"	"Checksum-aided video error correction for real-time, streaming and broadcast video applications"	"In the recent years, we have witnessed an impressively rapid deployment of video devices, applications and systems, and the growth of video will only intensify in the coming years with emerging technologies such as 4K/8K Ultra HD formats, video devices for remote monitoring and control of machinery, transport and healthcare, virtual and augmented reality, and other devices resulting from the emerging Internet of Things. However, transmission errors over unreliable networks (e.g. wireless and satellite) cause significant visual impairments or dramatically reduce the effectiveness of networks (e.g. by causing retransmissions). Such errors significantly degrade the users quality of experience (QoE) in most real-time and near real-time video applications. The main objective of this research program is to investigate low-complexity solutions to the problem of video error correction, which can be realistically integrated into communication systems. Specifically, the applicant and his team will study a new application layer video error correction approach, which has the potential to fully repair damaged video packets under reasonable error conditions. This approach consists in exploiting the protocols checksum information, currently used only to detect errors, to correct errors. The applicant and his team will achieve the main objective through three research activities. First, in the context of real-time video, they will investigate how the checksum information can be exploited in a list decoding approach, which attempts to decode various candidate corrected video packets until a valid (decodable) packet is found, to dramatically reduce the number of candidates to consider and, therefore, the computational complexity. Second, in the context of near-real-time video streaming, they will investigate how the checksum can be used to limit the number of retransmissions by combining the information of a damaged packet with its retransmitted damaged version(s) to produce a repaired packet. Finally, in the context of video broadcasting using forward error correction (FEC) (redundant information), they will investigate how the checksum can be used in conjunction with FEC to produce a repaired packet. The technologies conceived in this research program will be validated on current as well as future video standards. The advances provided by this research will bring significant improvements in the quality and efficiency of video services and enable the emergence of unprecedented media experiences. As for their previous projects, Canada will strongly benefit from the intellectual property and commercialization of these technologies and from the training of highly qualified personal in the fast growing field of video processing and applications.""637907,""Coulombe, Sylvain"
"634561"	"Czarnecki, Krzysztof"	"Model-Based Synthesis and Safety Assurance of Intelligent Controllers for Autonomous Vehicles"	"Vehicles with limited self-driving capabilities are already on the market and some car makers have promised products capable of autonomous driving in an urban setting in 2020. Self-driving cars will eventually completely transform the automotive industry, replacing private car ownership by service-based products such as robotic cabs. The deployment of large-scale self-driving vehicle fleets will reduce the number of crashes and crash severity, reduce emissions, allow commuters to use their time more effectively, and free up spaces occupied by parked cars. While current self-driving technologies have improved immensely in recent years, a major challenge is assuring safe and appropriate operation of an autonomous vehicle in all traffic situations and all road conditions. New design methods and tools are needed to address this challenge. This Discovery grant will fund a cutting-edge research program to create new methods and tools for the development of safe and intelligent computer-based controls for self-driving cars. The expected results include (1) a controller design and associated machine-learning methods to enable controllers capable of intelligent decision making and human-like driving skills and (2) methods for safety assurance of systems that rely on machine learning. The program will use the Waterloo Autonomous Driving Testbed, a drive-by-wire Lincoln MKZ with an integrated sensor suite, and take advantage of Ontarios new Automated Vehicle Pilot Program to demonstrate and evaluate the new technology on public roads. Self-driving cars will have a major impact on our society and economy over the coming decades. It is expected that self-driving cars can reduce accident rates by as much as 80%. In Canada, this would save 1,600 lives a year and over $55 billion in health costs and lost productivity. Goldman Sachs predicts a driverless technology market of $96 billion as early as 2025. The proposed research program targets current major roadblocks to a wider adoption of self-driving cars: (1) intelligent decision making in traffic that mixes driverless and human-driven cars with other participants such as cyclists and pedestrians and (2) safety assurance of such systems. The program will generate cutting-edge research results in driverless technology, establish an open test and simulation benchmark to stimulate scientific and technological progress, create commercializable technology, and train thirteen HQP in systems and software engineering for autonomous driving over the next five years. Hands-on experience on a self-driving car will give these HQP a significant competitive advantage on the job market. The automotive sector is the single biggest contributor to Canadas manufacturing GDP. Canadian innovation and HQP training in driverless technology will contribute to maintaining Canadas role as a world leader in the automotive sector.""640919,""Czarnecki, Krzysztof"
"637715"	"Dagenais, Michel"	"Reinventing the tuning and debugging tools for multi-thousand cores computer systems"	"The top supercomputers in recent years owe much of their power to the number-crunching ability of Graphical Processing Units (GPU) used for General Purpose computation (GPGPU). However, GPGPUs are also frequently used in smaller clusters and individual workstations for demanding tasks. Even in mobile phones and embedded systems, GPGPUs are an efficient medium to accomplish specialised tasks such as image processing. Furthermore, GPGPUs are at the forefront of a wider trend towards heterogeneous system architecture (HSA), where specialised co-procesors are used for a number of tasks beyond graphics and video, such as signal processing and network packet processing. The internal architecture of these complex GPGPU devices, with over 4000 cores and arithmetic units, was not well documented until recently. The architecture evolvedsignificantly in recent years with a much tighter integration with main processing CPU cores, offering shared virtual memory and user-level queues. The problem is that the programming tools for debugging and tuning these systems are severely lacking. The objective of the proposed research is to devise new more efficient algorithms and software architectures for the debugging, tracing and profiling tools, such that they can adequately cope with hardware architectures boasting thousands of cores and putting a strong pressure on the available memory and bandwidth. The challenges are at several levels: insuring that the tracing and profiling tools impose a minimal overhead on the system studied to avoid affecting its behaviour, efficiently exploiting all the available tracing and profiling hardware assistance, and efficiently interfacing the debugging, tracing and profiling tools to the operating system, device driver and application to be monitored. The significance and novelty of this work is in addressing a serious challenge currently faced by all users of such systems: having suitable tools to efficiently debug and tune their applications on these systems. It is widely recognised that the extremely rapid advances in heterogeneous parallel processing hardware availability has not been followed by an equivalent progress in the software development tools. As a result, a large number of users are not achieving the level of performance that would be attainable with proper tools, or are simply not taking advantage of such hardware, because of the difficulty in efficiently programming, debugging and tuning these devices. The proposed research program will thus have a significant impact, helping all users of sophisticated computing devices in better exploiting the available hardware.""636348,""Daggupati, Prasad"
"637630"	"Damen, MohamedOussama"	"Efficient Coding and Decoding Techniques for Hybrid Radio Frequency and Visible Light Communication Links"	"The demand for high data rate, reliable wireless communications continues to grow rapidly. The number of connected devices is expected to explode in the near future as the use of of smartphones, the Internet of Things' devices such as health and fitness wearables will vastly increase (10s of billions for 5G). As the radio frequency (RF) spectrum is becoming very crowded, alternative and complementary technologies, such as visible light communications (VLC) have recently become very appealing. The advantages of using VLC are its inherent safety, the lack of interference, because VLC does not penetrate walls, and the huge free bandwidth. However, VLC has its drawbacks such as limited indoor coverage and limited mobility, and has to be used in a hybrid manner with RF links. Despite the recent interest in VLC and the availability of some commercial products such as LiFi, there remain many open problems to be addressed in order to fully exploit the huge available bandwidth with practical schemes. For example, the development of efficient coding and modulation scheme s and their associated practical decoding algorithms . In the proposed program, we will consider efficient transceiver techniques for hybrid RF/VLC links with applications to 5G systems and the Internet of Things. For 5G systems, we will consider load balancing across WiFi, VLC, and cellular networks. After studying the degrees of freedom in such links, we will investigate the use of modified lattice codes that satisfy the modulated light intensity model in order to achieve all the degrees of freedom available. At the receiver side, we will also investigate the potential modification and enhancement of lattice decoding techniques for the non-coherent intensity detection. The studied schemes are expected to offer better alternatives to existing ones such as on-off keying or pulse position modulations when used in realistic indoor environments. We will also consider short code design for short-range communication that can have some interesting applications such as the data transfer between health and fitness wearable sensors, and smartphones and smart homes hubs. The long term objectives of this program are to make fundamental contributions to the design and test of practical communications protocols that can work in heterogenous networks, which can have a wide range of emerging applications including 5G systems, the Internet of Things and eHealth. The intellectual property resulting from this program, as well as the large number of highly qualified personnel who will be trained in 5G systems and heterogenous communications protocols, will greatly benefit these rapidly growing sectors in Canada. This is especially true for 5G systems: the 5G infrastructure rollout is expected to coincide with the time at which students will be graduating from this program, making them highly employable, and providing an advantage to Canadas workforce.""618680,""Damen, MohamedOussama"
"637217"	"Darcie, Thomas"	"Guided terahertz technology and systems"	"Our research program over the past five years has targeted the advancement of terahertz (THz) technology. This work has been facilitated through several major grants and partnerships with industry leading to considerable expertise and numerous collaborations. With this new Discovery Grant application, we seek to create new technology in several promising new directions that have been identified in our prior activities. This work is centered on Guided Terahertz Technology and Systems and comprisesthree main areas: THz sources and detectors, THz waveguide technology and THz system integration. A major objective is to develop components and systems in which THz waves are entirely confined within waveguide structures, rather than radiating as free-space beams as is typically done in todays systems. It is anticipated that this work will result in terahertz-based tools and instruments that will offer higher dynamic range, lower cost, and more widespread utility than todays free-space terahertz technology. Our recent work with sources and detectors was motivated by the requirements of higher transmitted power and better receiver sensitivity, and by the desire to work with optical pump wavelengths near 1550 nm where lasers and components developed for telecommunications are far less expensive than those commonly used shorter wavelengths. Newly proposed research focuses on the use of photoconductor devices with nano-scale plasmonic electrode structures to improve efficiency and mid-band-gap absorption in Gallium Arsenide and other materials to shift operation to 1550 nm. These approaches offer at least a factor of 10 increase in output power or receiver sensitivity while reducing cost. We have recently made substantial contributions in the area of low-loss waveguides for terahertz applications, demonstrating practical slot-line structures. An objective for future work is to incorporate these low-loss waveguides into THz systems and improve the coupling to sources and detectors to make more efficient systems. Work in THz systems targets the direct excitation of low-loss waveguides from photoconductor chips to enable terahertz systems with increased utility and dynamic range. Other novel systems concepts will be developed. Of particular interest is a novel spectrometer system that uses coherence to replace conventional time- or frequency-domain systems, with considerable potential reduction in cost. THz instruments have become important in a variety of industrial and scientific applications, as the radiation is non-ionizing (safe) and offers unique propagation characteristics through interesting materials. Present applications are found in industrial inspection, pharmacology, and imaging. Future applications in these areas and information and communications industries will be enabled through realization of the goals of this project.""640365,""DArcy, Ryan"
"634288"	"Datta, Suprakash"	"Algorithms for Efficient Wireless Communication in the Internet of Things"	"The Internet of Things (IoT) is an exciting emerging technology that proposes to network devices of everyday use, including roads, vehicles and buildings, using embedded sensors or actuators that also have networking capabilities. This would lead to new, useful services like interaction with smart appliances (possibly remotely) and buildings. Cars would also be connected to each other and to the internet in the proposed scenario, leading to better and safer driving experiences.Most sources estimate the monetary value of IoT to reach several trillions of dollars over the next decade. The underlying computer network technology needed to implement IoT poses some challenges, stemming primarily from the fact that unlike existing wireless networks (e.g. cell phones or wifi networks) there will be greater heterogeneity, size of the network, spatial density of nodes and mobility in the proposed scenario. My long term objectives are to (a) make IoT networks more efficient by designing better algorithms for key infrastructural problems, (b) train highly qualified personnel to contribute in this emerging area and contribute toward Canada's position in IoT technology. My short term objectives are to focus on challenges that would make an immediate impact in IoT technology. These include (a) topology maintenance and medium access control, (b) localization, and (c) addressing and routing. Medium access control is required to schedule the different devices as they attempt to send data on the same shared medium. Addressing and routing enable data to find their destination correctly even in the presence of high mobility.Localization is the process of computing the geographical locations of nodes; this is a challenge particularly in indoor environments because GPS is not reliable indoors. These problems are relevant in existing wireless networks but must be solved anew for IoT to handle the increased heterogeneity, scale, density and mobility. This project will produce novel algorithms that are useful to industry and train highly qualified personnel. These algorithms will enable the widespread deployment of IoT technology and bring down costs by making the networking of devices more efficient.My students and I have been working on the development of efficient solutions for similar problems for wireless networks and I anticipate that many of the approaches used in our previous work and my experience as a researcherin the area of computer networks will enable me to design solutions tothe problems proposed.""628458,""Dattani, Nikesh"
"634559"	"Dean, Thomas"	"Language Based Analysis of Software and Security"	"The main theme of my research program is the application of grammar programming, source code analysis, and source transformation to solve real software problems. As part of the ultra large scale software systems program at Queen's, I will investigate the issue of automatically migrating these applications using source code analysis and transformation to take advantage of new technologies. Ultra large scale software systems are software systems such as Facebook, Google, and Pokemon Go. These systems pose problems of scale, speed, availability, privacy and security. One example is the automated migration of applications to use NoSQL databases. Existing research has investigated the migration of the data. We will use static and dynamic analysis to assist the migration of the code that uses the data. This approach uses the map between the proposed data migration and the use of that data in the code to identify and propose changes to the application. Another example is the functionality between the multiple versions of desktop and mobile clients that access the systems. We are using source code analysis to map and compare functionality between the versions and to identify conflicts between open source components of software commonly used to build web based clients. Often source code analysis and transformation techniques provide a substantially different approach to conventional techniques in other areas leading to new insights and new blended approaches. A previous cross area I had some success in was to use grammar programming and constraints to describe network protocols for security testing. These constraints were used to mutate captured network data resulting in the detection of new vulnerabilities in well tested software. My proposed research inverts this approach. Network data that does not satisfy the grammar and constraints may indicate an attack. One of the properties of industrial control networks is the use of a limited number of protocols, unlike conventional business and academic networks. Given the limited number of protocols, we can build a complete grammar and set of constraints that defines the legal traffic on these networks. The challenge is to construct a domain specific grammar and constraint language that covers the threats of this domain, as well as a stream based constraint engine that can process a large amount of data in real time. My proposed research will train three Ph.D. and three M.Sc. students in leading edge solutions to real problems faced by Canadian companies and government agencies, increasing competitiveness and ensuring safety the of data and operations.""627459,""Dean, Yezen"
"617769"	"deLara, Eyal"	"Servlerless Mobile Edge Computing"	"Current mobile networks are not able to support next generation applications that require low latency or that produce large volumes of data that can overwhelm the network infrastructure in a carrier network. Examples include wearable applications that assist individuals cognitive impairments, as well as safety critical applications, such as face recognition applications for airport security and surveillance. These applications require more computational resources than those that are currently available on mobile or embedded devices. The use of servers on the wide-area cloud, however, is also not an option as these applications require low response times, or involve processing of large volumes of data from multiple sensors and cameras. To address these challenges the research community and the telecommunications industry are exploring ways to add computation and storage capabilities to the edge of the network. These approaches, variously referred to as cloudlets, micro data centers, or fog, augment the traditional cloud architecture with an additional layer of servers that are located closer to the end user, typically one-hop away. This proposal puts forward a new vision that generalizes edge computing into a hierarchical cloud architecture deployed over the geographic span of a network. This vision will supports scalable processing by providing storage and computation along a succession of datacenters (of increasing sizes) positioned between the end device (e.g., smartphone, IoT appliance) and the traditional wide area cloud datacenter. We explore the use a new cloud computing model know as Function as a Service (FaaS) as the deployment mechanism for running applications on a hierarchical cloud architecture along the the geographic span of the network. FaaS, also known as Serverless computing, is a cloud computing approach in which the cloud provider fully manages the infrastructure used to serve request, including the underlying virtual machines or containers, the host operating system, and the application run time. FaaS applications are composed of a collection of light-weight stateless event handlers that execute on a run time environment that is managed by the cloud provider. The small size and stateless nature of FaaS handlers makes them ideal candidates for our hierarchical cloud computing vision. ""618259,""DeLasa, Hugo"
"634529"	"deLara, Eyal"	"Servlerless Mobile Edge Computing"	"Current mobile networks are not able to support next generation applications that require low latency or that produce large volumes of data that can overwhelm the network infrastructure in a carrier network. Examples include wearable applications that assist individuals cognitive impairments, as well as safety critical applications, such as face recognition applications for airport security and surveillance. These applications require more computational resources than those that are currently available on mobile or embedded devices. The use of servers on the wide-area cloud, however, is also not an option as these applications require low response times, or involve processing of large volumes of data from multiple sensors and cameras. To address these challenges the research community and the telecommunications industry are exploring ways to add computation and storage capabilities to the edge of the network. These approaches, variously referred to as cloudlets, micro data centers, or fog, augment the traditional cloud architecture with an additional layer of servers that are located closer to the end user, typically one-hop away. This proposal puts forward a new vision that generalizes edge computing into a hierarchical cloud architecture deployed over the geographic span of a network. This vision will supports scalable processing by providing storage and computation along a succession of datacenters (of increasing sizes) positioned between the end device (e.g., smartphone, IoT appliance) and the traditional wide area cloud datacenter. We explore the use a new cloud computing model know as Function as a Service (FaaS) as the deployment mechanism for running applications on a hierarchical cloud architecture along the the geographic span of the network. FaaS, also known as Serverless computing, is a cloud computing approach in which the cloud provider fully manages the infrastructure used to serve request, including the underlying virtual machines or containers, the host operating system, and the application run time. FaaS applications are composed of a collection of light-weight stateless event handlers that execute on a run time environment that is managed by the cloud provider. The small size and stateless nature of FaaS handlers makes them ideal candidates for our hierarchical cloud computing vision. ""618872,""deLara, Eyal"
"635088"	"Denzinger, Jrg"	"Improved learning of reactive, cooperative behaviour through learning-based testing"	"The long-term vision for my research is to use machine learning toimprove the development of distributed systems software. My key idea is to modify and extend a concept for learning of cooperative behaviour to include the results of a learning-based testing component that finds negative consequences of the learned cooperative behaviour, including problems with intended functionality, efficiency and security. To learn cooperative behaviour, we see the components of a distributed system as agents that can perform actions. The learning tries to create controls for these agents, so that they together achieve the functionality the whole group of agents is tasked with. Using learning-based testing for finding negative consequences is an evolutionary learning process in which the testing component creates interaction sequences with the tested system and evaluates the sequences based on how near they come to a testing goal. The evaluation is used to learn sequencers that come nearer and nearer to creating the negative consequences the testing system is looking for. The main research challenge is how to combine the focus of learning of cooperative behaviour on trying to deal with all possible situations the system can be in with the focus of learning-based testing on finding one particular sequence of situationsrevealing a particular problem of the tested system. Possible solutions of this challenge can target the way learning-basedtesting is performed, generalizing its results, but also modifying the general learner and the architecture with exception rules.In earlier work, we have developed the shout-ahead architecture for agents and a hybrid learning method for this architecture,combining reinforcement learning with evolutionary learning. A proof-of-conceptsystem for developing control AIs for enemyunits for the game Battle for Wesnoth showed that the best learned AIs are able to beat the rather good human-created centralAI that comes with the game. We explored the usage of learning of behavior to test systems for weaknesses for severalapplications, including games. In all these applications, our test systems were able to find several weaknesses in the testedsystems, including efficiency and security weaknesses. Building on these works, we will explore our ideas for combining general learning of behaviour with learning-based testing intwo case studies, an extension of the system for Battle for Wesnoth and an automated cattle distribution center as an exampleof Internet of Things applications. The anticipated result of this research is the ability to create customized open distributed systems at lower cost than current practise and with higher quality due to automization.""626907,""Deo, Gurbind"
"635084"	"Desmarais, Michel"	"Algorithms and Applications of Learned Student Models for Adaptive Learning Systems"	"In the field of Intelligent Tutoring Systems, a critical modeling issue is to determine how the domain content relates to skills that we aim the student to learn. When this content represents tasks that can be succeeded or failed (question, exercise, etc.), the mapping of tasks to skills is the key to deliver a student skills assessment. When the content represents learning material, a tutoring system can use the mapping of skills to learning material in order to guide the student through the appropriate topics and reach specific learning goals. A first objective of this research program is to design algorithms that can help map task to the required skills. We have made great progress in developing algorithms that can infer this mapping from data, and that refine and validate expert given mappings, but there is still room from improvement and we propose a number of research avenues in that respect. A second objective of the program is to work on the mapping of learning content to skills. While the algorithms that map tasks to skill can use student performance data, such as test outcome, to infer the skills behind tasks, this type of data is not readily available for learning material. Material such as Wikipedia pages, textbooks chapters, or documents in general, is most often indexed by topics, not by the skills they can help acquire. Yet, learning objectives is best defined by skills, and the alignment of skills to concepts is not always clear, or even the one we would expect. Therefore, standard techniques based on semantic distance between content and topics is deemed insufficient to determine the specific skills addressed by a given content. They cannot readily tell if the content is too difficult or already known by a student with a specific skill profile. Nor can they determined prerequesite skills and concepts that are involved in the content. We plan to develop data driven approaches to determine the mapping of learning content to skills. These approaches will rely on textual features extraction techniques determine the related skills, and also make use of the techniques developed for tasks to skills mapping.""635868,""Desmond, Anthony"
"634843"	"Diallo, AbdoulayeBanir"	"Algorithms and Methods for small RNA studies"	"The emergence of Next Generation Sequencing provides billions of DNA elements from short sequence to full genomes. In fact, astronomic data have been and will be produced from whole genome, expressed genes from genomes regarding to their implication to either experimental, environmental, geospatial, and/or disease related conditions. The rapid and systematic functional classification, as well as the identification of the biological roles of such sequences, constitutes a major issue in bioinformatics and computational biology. My discovery research concerns the design of methods to predict from omics data the functional classification as well as evolutionary patterns of small RNAs. Such prediction methods rely on two main characteristics: the powerfulness of comparative omics methods and integrative approaches combining omics data to several related non-omics data. With the deep genomic knowledge present in the literature and the massive heterogeneous produced data, I plan to represent the knowledge, to automate the reasoning for drawing conclusions, to learn machines to adapt at new characteristics such as ancestral sequences and hidden patterns, and to recognize novel small RNA patterns. This discovery grant will permit innovative methods exploiting artificial intelligence, machine learning, graphical models, comparative genomics and integrative approaches for omics studies. The ultimate goal consisting of providing bioinformatics tools that predict and classify with high accuracy the function and regulatory mechanisms of these small RNA classes. My discovery program will focus on three main objectives: 1) design accurate methods for the identification of different classes of sRNAs; 2) develop method exploring hidden patterns and features from RNAseq data to improve the classification; 3) design methods exploiting comparative genomics and evolutionary origin of sRNA classes to improve classification. This proposal will provide new highly expected tools to be included in module constructed for main omics platforms. The outcome of this discovery program will allow a better understanding of sRNA biological functions that affect more than 60% of protein transcribed genes; improve our understanding of pathway regulation mechanisms and molecular marker development that help plant breeder to select better crops with greater tolerance to biotic and abiotic stresses; develop sRNA cancer biomarker; develop different therapy for several diseases that involved genes regulated by sRNAs such as Cardiac, inflammatory and autoimmune diseases. SRNAs will remain a main strategic target for pharmaceutical agricultural industries for this decade due to its huge economical values. This discovery proposal will provide training of high-qualified personals in bioinformatics, artificial intelligence, molecular biology as well as several papers in high impact open access journals.""636822,""Diallo, Claver"
"635036"	"Dick, Scott"	"9"	"Alberta"
"637297"	"Djerafi, Tarek"	"Smart and performances-demanding platform for millimeter-wave sensing systems"	"The smart home system based on wireless sensor networks is a combination of technologies and services that improve the life in the areas of safety, comfort and technical management. In the smart home scenario, sensors, RFID, local positioning system and decisional algorithms are combined to achieve various functionalities in complete smart and autonomous system. The smart home goes beyond the stereotype of a homeowner who can check his house on a smartphone and verify the status of interior devices such as security alarm or heating system. In addition, wearable and no-contact healthcare devices can monitor resident blood pressure, heart rate and other metrics (posture, ECG, breathing) and transmit data to the resident smartphone as part of a medical tele-monitoring system. These indoor systems should work in millimeter-wave range to be compatible with future applications of millimeter-wave technology in wireless communications such as 5G platforms, especially information sharing of sensor networks and functional interplay among wireless functionalities. System working in millimeter-wave range, should deal with some constraints in indoor environment like dynamic range, propagation blockage and noise. Reconfigurability in circuits and systems gives an additional tool to meet the stringent frequency and power requirements, even in the changing of operating environment inherent in these systems. In addition, tunable circuit responses can be made to compensate for the deleterious effects of aging and temperature changes in sensitive RF circuits. At millimeter-wave band and beyond, the actual active components which use TEM-mode waveguide are troubled by transmission loss, fabrication tolerance and other problems. Non-TEM modes such as SIW (substrate integrated waveguide) guide offers much better solutions. Therefore, the concept of true active waveguide techniques based on nonlinear media similar to the solid-state semiconductor devices should be developed. The active waveguide should present distributed features with wave interactions instead of voltage and current characterized lumped such as diodes and transistors. To do so, innovative thick- and think-films and substrate such as BST, ferroelectric and liquid crystal materials, will be integrated inside or as layer in SIW line. The strategic goal of this proposal is to develop reconfigurable circuits and waveguides to be integrated in millimeter wave systems for different indoor applications. Tunable waveguides developed in this project will be used to design distributed phase shifters based on BST that allow different modulations and smart antenna operations. The different developed components and waveguides will be integrated in the same platform to estimate the performances enhancement. Of course, such final platform will provide smart and performance demanding actions. ""639559,""Djerafi, Tarek"
"636951"	"Domingue, Frdric"	"Dispositifs RF et capteurs passifs appliqus  la dtection environnementale"	"Avec lindustrialisation rapide lchelle mondiale et la qute incessante de plus de confort et de scurit dans les domaines domestiques et industriels, le march des capteurs connat un essor spectaculaire. Rcemment, avec lavnement des rseaux de capteurs sans fil, lintrt de ces derniers a connu une ascension fulgurante dans de nombreux secteurs dactivit conomique, avec des applications de plus en plus nombreuses. Les rseaux de capteurs sans fil constituent lune des nouvelles technologies ayant eu le plus dimpact sur notre quotidien en rpondant au besoin essentiel du suivi et du contrle de phnomnes physiques dans diffrents domaines tels lenvironnement, la sant, le transport, la domotique, lindustrie, la scurit et lalimentation. Dailleurs, le march des rseaux de capteurs sans fil estim 1.2 milliard de dollars en 2014 devrait progresser avec une moyenne de 18.1% par an pour atteindre la valeur de 3.26 milliards de dollars en 2020. Le secteur environnemental est clairement identifi parmi les secteurs les plus demandeurs en technologie de capteurs. Il existe plusieurs technologies de capteurs environnementaux. Cependant, la plupart de ces capteurs ne satisfont pas aux critres de performance recherchs par lindustrie, notamment en termes de temps de rponse, de stabilit de temprature, de plage de dtection ou encore de robustesse. L'objectif de ce programme de recherche est d'tudier diffrentes solutions alternatives apportes par des technologies micro-ondes innovantes pour le dveloppement et la fabrication de capteurs environnementaux performants. Ce programme de recherche multidisciplinaire implique une exploration simultane des besoins systmiques et des moyens technologiques modernes accessibles. Les principaux volets sont : (1) ltude et loptimisation des performances des structures RF pour la dtection environnementale, (2) dveloppement des techniques de fabrication des capteurs innovants, (3) dveloppement des techniques dintgration de matriaux fonctionnels et (4) conception de circuits et systmes de dtection sans fil. Ce projet de recherche aidera btir un savoir et un savoir-faire unique et la fine pointe des technologies des capteurs environnementaux au Canada. Ce projet sinscrit dans la perspective dun transfert de technologie innovante vers le milieu industriel Canadien.""627719,""Domke, Julia"
"635111"	"Duvenaud, David"	"Efficient Inference in Compositional Generative Models"	"The objective of this program is to allow the automatic construction of complex, interpretable models directly from data, automating analysis previously done by experts. I will achieve this by further developing tools for automatically building structured deep generative models. Such models assert that the data has a hidden underlying structure, observed indirectly through complex, noisy processes. This will require novel inference, training and evaluation procedures, but will enable advances in a wide variety of machine learning applications, including automated chemical design, and model-based deep reinforcement learning. Specifically, we propose to develop a new model class having arbitrarily complex latent variable structure, and deep nonlinear observation models. This research will extend recent lines of inquiry that combine the best aspects of probabilistic graphical models (PGMs) and neural networks. These new models combine the interpretability and tractability of structured graphical models (such as clustering, time-series, or topic models) with the flexibility of neural network-based likelihood functions. This combination can sidestep the rigid modeling assumptions made by PGMs such as Gaussianity. It also sidesteps limitations of neural networks, namely their relative uninterpretability, as well as the requirement of having very large datasets. Such model classes have been previously proposed, but their usefulness was limited by slow inference methods. We outline a concrete plan for exploiting the structure of the models themselves to develop efficient inference techniques. These methods extend recently-developed variational autoencoders (VAEs) to produce structured variational autoencoders (SVAEs). These inference methods have the potential to make automatic model-building frameworks practical. In particular, we plan to extend existing matrix decomposition grammars in two ways. First, to handle non-linear observation models connecting different types of latent structures to each other and to the data. Second, to be scalable enough to handle real-world datasets. This modeling framework already contains many standard machine learning and machine vision models as special cases. Providing it with flexible neural-net observation models would mean that it would also contain most deep learning models as special cases, as well as a host of as-yet-undeveloped new model classes. Using fast recognition networks to do inference would mean that inference in this large model class would be scalable.""642724,""Duvenaud, David"
"637449"	"Eleftheriades, George"	"Next Generation Engineered Electromagnetic Surfaces and Materials"	"In this program we will develop the next generation of engineered electromagnetic materials (metamaterials) and surfaces (metasurfaces) for the ultimate control of electromagnetic fields. In this effort, fundamental science and theory will be generated, new phenomena will be explored and/or discovered, and innovative structures to enable new applications will be pioneered. The science and technology to be produced is intended for applications from microwave to optical frequencies in four main disciplines: wireless communications, super-resolution imaging and medical therapy, and aerospace and defense. Metamaterials are engineered structures that can be homogenized at the scale of the corresponding wavelength where they operate and can thus be characterized by macroscopic material parameters such as a permittivity, a permeability and a refractive index. Such engineered materials should have properties that transcend those found in natural materials (hence the prefix 'meta'). For example, nowadays we can synthesize metamaterials characterized by a negative index of refraction. Likewise, metasurfaces are engineered surfaces with properties that can be homogenized (e.g. by a corresponding spatially varying impedance distribution) and have also exotic properties; For example they can bend light negatively and without suffering any reflections. Metasurfaces can be thought of as 2D metamaterials thus providing a unified theme for this proposed research program. Particular emphasis will be placed on metasurfaces, which is an emergent research front in which my group has been one of the leading groups internationally. Primarily, a paradigm shift approach will be developed for controlling electromagnetic waves (radio waves and light) using Huygens' metasurfaces. These are engineered surfaces composed of collocated electric and magnetic dipoles, thus enabling the full control of electromagnetic waves through ""the equivalence principle"" (generalization of the Huygens' principle). The developed theory of these Huygens' metasurfaces will greatly advance the science of engineered materials/metasurfaces and electromagnetism, whereas the corresponding technology developed will enable applications in imaging beyond the limits of diffraction for industrial, medical and biological applications, advanced multiband/broadband/multifunctional antennas for the next generation of terrestrial and satellite communications, futuristic ""invisibility cloaks"" for defence and related public security problems (e.g. to avoid collisions of commercial drones with civil-aviation aircrafts), wireless power transfer devices and networks to charge laptops, TVs and future electric cars, ultrathin optical lenses to enable the next generation of smartphones, highly-efficient solar cells enabled by light-trapping metasurfaces, and small radar sensors for autonomous vehicles. ""642056,""Eleftheriades, George"
"637185"	"ElSankary, Kamal"	"Design techniques for three-dimensional integrated circuits challenges"	"The ever growing need for more complex integrated circuit (IC) has been driven by the need of complex new products packing more features and requiring faster and lower power processing. Scaling down transistors and using larger dies have been able to keep up with the market demand in the past. However, we are fast approaching the limits of continuous device scaling for planar two-dimensional (2D) IC design. Using 2D integration technologies to implement nowadays complex chips is becoming very expensive and difficult to meet nowadays design challenges. Three-dimensional (3D) integrated circuit (IC) technology, wherein IC chips are stacked in vertical 3D architectures, has emerged as a complement to silicon transistor scaling to achieve higher level of integration. Moreover, heterogeneous 3D-IC systems that integrate multiple dies, each optimized using different technologies, will offer More-than-Moore solutions for higher integration densities, lower power consumption, and higher performance.One of the most popular technologies for implementing 3D-ICs is through-silicon via (TSV) fabrication, in which multi-chip integration is enabled using TSVs to provide the vertical interconnections between dies. TSVs are smaller than off-chip wires thereby avoiding the excessive delay limitations of bonding wires. They can be used for connecting devices that reside on different dies, inter-die communications, as well as clock and power distribution. Like any new technology and despite the tremendous advantages of 3D-IC, circuits designer are faced with new design challenges particularly for clock synchronization and power delivery. The main design challenges are related to delay through the TSVs, which are susceptible to process and temperature variations. In addition, the delay through a TSV can increase significantly due to open defects leading to significant skew in clock distribution networks. Moreover, cross-die process variation limits the slack time for both within die and die-to-die paths using TSVs, thus requiring a tight constraint on clock skew and jitter . Intra-die and inter-die power distribution is another major challenge in 3D-IC design. Moreover, accurate physical characterization of TSVs represents a tremendous challenge for analog designer and an optimized layout technique is necessary to fully benefit from the advanced technology. As a result to fully enjoy the merit of 3D-IC technology, the objective of this proposal is to develop novel digital, analog and mixed signal circuit design and system techniques to address the challenges of 3D integrations.""639298,""ElSankary, Kamal"
"637149"	"ELSHATSHAT, Ramadan"	"Intelligent Microgrids-based Solutions for Optimizing Power Distribution Systems"	"Microgrid is a recently-coined term that describes a micro power distribution system composed of clusters of distributed generation (DG) sources, including wind and solar sources, that serve a group of distributed loads. The ability of the microgrid to facilitate DG sources to supply a highly localized area with reliable electricity has motivated a significant amount of research on how to exploit the strengths of the microgrid concept to transform conventional power systems into intelligent decentralized electric power distribution systems, called smartgrids. In particular, a significant question is the suitability of smartgrids to optimize the management and control of the distribution system, reducing its CO2 emissions, improving its efficiency and reliability, and ensuring its security. The successful transformation of an existing power distribution system into a smartgrid faces several technical and economic challenges related to its creation, operation, and management. These include the seamless integration of the renewable sources, and its cost-effectiveness, reliability, and long-term sustainability. This Discovery program aims to develop new algorithms necessary for optimizing the management, control, and operation of smartgrids in the presence of fluctuating renewable generation sources and system loads. In addition, this program will focus on developing new methods and techniques for pairing the strengths of artificial intelligence and graph theory with the inherent scalability and re-configurability of power grids for devising flexible and dynamically clustered smartgrids and the underlying management and operation optimization tools. The proposed research will assist power distribution system planners and operators in making decisions that ensure the cost-effective planning and proper operation of electricity distribution systems. Ultimately, it will contribute to reducing greenhouse gas emission in Canada, shifting the energy-generation portfolio from one based on fossil fuels to one based on renewable energy sources. It will also enable the effective handling of peak demands, while ensuring high reliability and power quality. Moreover, the distributed management of energy consumption and the optimized operation of small-zoned microgrid will produce significant economic benefits in terms of energy savings, more efficient utilization of available resources, and improvement of the overall efficiency of energy systems.""636919,""ElSheimy, Naser"
"621734"	"Emami, Ali"		"NSERC"
"637159"	"ErolKantarci, Melike"	"Peer-to-Peer Energy Trading over Reliable Small Cell Networks"	"In Canada only, 50% of the potentially available - non-hydro - renewable energy capacity is underutilized. It is highly challenging to integrate small-scale, distributed renewable energy generators to the utility power grid due to their intermittency. This NSERC Discovery Research Program aims to develop a Peer-to-Peer (P2P) energy trading platform along with the state-of-the-art reliable and low-latency communications network that will enable unprecedented opportunities for sharing energy among distributed consumers, generators and storage. Over the long-term, energy trading can unravel the use of renewable energy more efficiently and strengthen the resilience of the highly networked power grid in the face of disasters and attacks. The transformative research conducted under this program will contribute to the leadership of Canadian researchers in power and communication fields. In the short term, this program will lead to new analytical models and algorithms that will enhance the operation of the power grid along with its reliable, low-latency wireless communication infrastructure. This 5-year program will develop game-theoretic and stochastic multi-level optimization strategies to reach optimal trading decisions and enhance small cell networks in order to serve fast and reliable communication needs of energy trading. Multi-leader multi-follower games will be designed for players who do not have hierarchical organization. To the best of the applicants knowledge, a tri-level optimization model will be developed to allow strategic bidding of players, for the first time. This project will focus on future wireless cellular networks consisting of ultra-dense small cells, to provide the flexibility and ubiquity for the on-the-road players (EVs) as well as plug-and-play generators. Facilitating energy trading over a network with high level of densification, call for effective dynamic network planning techniques that can optimize capacity, coverage, interference, energy, and balance the network load, organize the access to random channel and handle mobility while taking into account both the traditional mobile user traffic and the traffic generated by the P2P energy trading platform. This NSERC Discovery Research program will increase Canadas competitiveness in energy and communications sectors. It will allow underutilized, distributed renewable energy generation capacity to be effectively utilized. The proposed solutions will increase resilience to disasters. It will train personnel to be experts in cross-disciplinary areas in power and communication engineering fields and to contribute to the leadership of Canada in those fields. The training provided with this program will help students to build academic success, unique and marketable skill sets and provide abilities to grow their own professional careers.""636741,""Errico, Fausto"
"634788"	"Ester, Martin"	"Data Mining in Heterogeneous Information Networks with Attributes"	"Most research in the area of data mining has concentrated on attribute data, i.e. on data where entities are represented by a set or vector of attributes. In the last decade, more and more network data has become available, which has driven the development of data mining methods for network data. However, in many scenarios, data can be represented as networks with node and/or edge attributes, with complex interactions between the topology of the network and the attributes. Nevertheless, data mining methods for attributed networks have received relatively little attention in the literature. Another limitation of existing methods is their focus on homogeneous networks, i.e. networks with a single type of objects and links, while real-life network data is often heterogeneous, consisting of multiple object and link types. Many systems can be modeled as heterogeneous information networks, for instance social networks among authors, papers and conferences, or biological networks integrating protein-protein interactions and gene regulations. Heterogeneous information networks with object and/or link attributes are characterized by complex interplays between the topology and the attributes of the network. In a social network, for instance, actors tend to connect to actors with similar attributes, while friends tend to become more similar to each other in the course of time. The long-term objective of our research is to explore data mining methods that can model and analyze the interplay of the topology and the attributes of AHINs. The proposed research program will address the limitations of the state-of-the-art and investigate several fundamental issues of data mining methods that can model and analyze the interplay of the topology and the attributes in heterogeneous information networks. The methods to be developed will have significant applications in many domains, and we will consider the analysis of social networks and biological networks as driving applications. For social network analysis, we will apply the proposed methods to the task of recommendation in social networks. For biological network analysis, we will apply our methods to the problem of network-based patient stratification and to the problem of detecting genetic causes of adverse drug reactions in gene and disease networks associated with patient records.""624947,""Etebarialamdari, Neda"
"637265"	"Fafard, Simon"	"Record Efficiency Photovoltaic Heterostructures"	"Inthe past 3 years, Fafards team made breakthrough optoelectronic device advancements, now featuring the highest optical to electrical conversion efficiency ever for any type of devices. The progress has been presented recently at several invited international presentations and late-news scientific papers. This recent development in the area of advanced III-V heterostructures allowed attaining greater than 65% optical to electrical conversion efficiencies. The current proposal is to further advance the understanding and the development with these novel III-V semiconductor phototransducers. The record-efficiency devices are based on a novel vertical epitaxial heterostructure architecture (VEHSA) design. Thanks to a precise control of thin semiconductor layers grown by Metal-Organic Chemical Vapor Deposition epitaxy or by chemical beam epitaxy, the unprecedented performance can be obtained with tailored output voltages adapted for various applications. The proposed research is therefore in the area of nanoscale p/n junctions and will reveal very interesting properties for photovoltaic devices engineered with ultra-thin bases, including photon recycling effects. The heterostructure designs yielded high external quantum efficiency (EQE) values for all the structures studied experimentally, up to 20 ultra-thin subcells (PT20 devices) to date. Conversion efficiencies greater than 60% were confirmed for all structures, including recently the PT20s. Additionally, these high efficiencies can be maintained for high electrical output powers, reaching greater than 3W for chips only a few mm 2 in area. Record high-photovoltage values for monolithic photovoltaic cells with V oc 23V have been obtained with the PT20 studied. The PT20 structure has been implemented with its narrowest ultrathin base having a remarkable thickness of only 24nm, but for future developments, the device modeling shows that the high-photovoltage and the unprecedented conversion efficiency values are expected to be achievable for a much higher number of ultrathin junctions, getting into the range where 2-dimensional quantum well effects are becoming significant. For example, we derived that the narrowest subcell of a PT60 structure would have a base as thin as 8nm, it is expected to still generate a photovoltage of 1.14V per individual subcell, and it will begin to feature 2-dimensional quantum well effects. The proposal will enable key progress in this area while leveraging the achievements accomplished in the very fruitful initial phase of the research. The thermalization losses will be minimized to reach higher efficiencies and the open circuit voltage will be further increased by adding more n/p junctionsto the VEHSA design. The devices will be built into systems that will benefit from the unique device properties. Also the VEHSA design will be implemented to other alloys, enabling longer wavelengths.""618908,""Fafard, Simon"
"634519"	"Famelis, Michail"	"Lightweight Formal Support for Software Design"	"Software systems are at the heart of the infrastructure that our societies and economies depend on for their daily function. They are the accumulated product of the myriads of design decisions made by software developers every day. Their quality is thus highly dependent on the process by which they are designed. Despite decades of innovation, formal, mathematical engineering techniques often face high adoption barriers because of the inherently creative and collaborative character of software design and the unrelenting rate of change in the contemporary software industry. Software design remains as much a craft as an engineering discipline. This research program aims to help the designers of software balance efficiency and quality in the face of the pressures in contemporary software engineering. The key lies in identifying niche design contexts in which formal techniques can be made ""lightweight"", i.e., with reduced demands on user skill and computational resources. This reduces adoption barriers, while, crucially, rich contextual information makes up for the lost expressiveness and/or analytical power. Specifically, I aim to study the context of software design in order to judiciously apply formal techniques that robustly support designers' creativity. This context characterizes the design decisions that need to be made and determines the space of possible design solutions. Therefore the proposed research program has the following objectives: (a) identification and characterization of design decisions and their context using a ""Design Dashboard"", (b) creation of a ""Lightweight Design Toolkit"" of formal techniques for managing the space of design solutions, and (c) integration of the techniques within modern software development methodologies using ""Flexible Modelling Tools"". Outcomes of this program will be context-specific theories of software design, lightweight formal techniques informed by these theories, and tools implementing them that can be deployed in various software design settings. These outcomes will benefit practitioners by allowing them to use robust engineering techniques in new development contexts. Additionally, it will further the scientific understanding of the tradeoffs involved in the creation and deployment of practical formal design and analysis techniques in modern software design settings. Ultimately, this research program will improve the state of the art in the production of quality software, while producing highly-qualified personnel with expertise in empirical, formal, and user-centered software engineering.""643005,""Famiglietti, James"
"637627"	"Fapojuwo, Abraham"	"Human Type and Machine Type Communications in 5G and beyond Mobile Networks"	"As we move forward, todays human-to-human or human type communication (HTC) will evolve to human-to-machine, machine-to-machine or machine type communication (MTC) which, using Internet-enabled devices, will culminate to the Internet of Things and Internet of Everything. Both MTC and HTC are central to the most important use cases of the next fifth generation (5G) and beyond wireless technology. Future traffic projections indicate MTC will be driven by billions of devices and, moreover, MTC is characterized by behaviors different than those of HTC. How to efficiently support both broadband HTC and massive and ultra-reliable MTC in next generation 5G and beyond mobile networks thus becomes a great challenge, overcoming this challenge forms the focus of my proposed research. My research is important to the subscribers of wireless services, wireless network equipment manufacturers, and wireless network operators and service providers. I propose to investigate innovative cell-less fog-enabled radio access network (F-RAN) architecture to support the data rate, latency and availability requirements of HTC and MTC services. An F-RAN is a collection of cloud-enhanced access nodes cooperatively serving the HTC and MTC devices within proximity. I will develop new frameworks for F-RAN functionality management and control as well as guidelines and policies for F-RAN operational optimization for efficient support of both HTC and MTC services. I shall discover efficient protocol for F-RAN access to support massive MTC. The F-RAN access protocol will be rigorously analyzed, implemented and tested to determine the critical parameters for achieving F-RAN performance optimization. In addition, efficient algorithms will be developed for joint management of radio, computation, and cache resources at the access nodes, to achieve optimal resource utilization while meeting the quality of service constraints of the HTC and MTC services. Novel theoretical techniques will be developed for performance evaluation, to gain valuable insights. Experimental results will be used to validate and refine the theoretical results. The outcomes of my research are expected to generate new knowledge that will contribute to the advancement of wireless communications field. The research outcomes will have a significant impact on the next-generation wireless technologies and on future wireless applications and services not only in information and communication technologies domain but also in other domains including healthcare, transportation, and energy and environment. My research will enhance the quality of life of average Canadians using HTC and MTC services. Furthermore, my proposed research program will provide technical training in next-generation 5G and beyond mobile networks to graduate students whose highly desired technical skills are vital for continued growth of the Canadian wireless industry. ""637430,""Far, Behrouz"
"617746"	"Fedorova, Alexandra"	"Performance comprehension of production software"	"Software developers are among the most highly paid and sought-after talent in both Canada and the US; Canada alone facing a gap of 182,000 ICT workers. Shortage can be at least partially relieved with improved productivity. Our project will address an important and often overlooked aspect of software development productivity: performance debuggingtuning. A recent survey revealed that 92% of developers faced customer-affecting performance issues during the preceding year; all in all, ~300 interviewed engineers spent 130 man-months diagnosing and fixing performance problems. The respondents further admitted that the most difficult part of performance tuning was not actually fixing the problem, but gathering and analyzing evidence and getting to its root cause. Many performance tools exist, but their primary focus is collecting performance events, such as operation timestamps or hardware metrics, and not on the analysis of the data. As a result, engineers are often swamped with swaths of data, but have little idea how effectively use it. Performance tuning is further aggravated by the fact that todays engineers deal with a lot of code they did not write: sifting through performance data without understanding the code is hardly efficient. Based on these observations, the goal of our project is to unite performance analysis and program comprehension. We aspire to design methodologies, algorithms, and ultimately build visually rich tools that will to engineers not only enable effectively navigating through billions of performance events, but also enable the developer to comprehend the flow of the program, the layout of data structures and data access patterns, while at the same time relating them to performance metrics. Our hope is that these tools will help the engineer to simultaneously understand how the program works, how it accesses data, how it performs, and why it performs the way it does.""640916,""Fedorova, Alexandra"
"634825"	"Flocchini, Paola"	"Algorithms and Foundations  for Computational Mobile Entities"	"Consider a system composed by mobile entities that interact with each other when in proximity, and whose behaviour is extremely simple: when active, an entity Looks (perceives the immediate surroundings), Computes (executes a set of deterministic rules to compute a destination), and Moves toward the computed destination. In such a system, the single entity, alone, is quite weak and cannot perform any meaningful task; the collection of entities, in its entirety, can however be capable of rather complex computations, like forming desired patterns, moving in formation, surrounding an object, searching. The general thread of this research plan is the investigation of computability and complexity issues for groups of such mobile computational entities, following two main research axes:1) entities moving on the plane, and 2) entities moving on graphs. In the direction of the first axis, the long term objective is to better understand limitations and abilities of systems of mobile entities whose perception is only local; the focus is on the minimal capabilities that allow them to perform their tasks. To achieve this goal, I plan to study some important factors like memory, communication, synchrony, and ask natural fundamental questions on the impact that these factors have on the global computational power of the entities. In the direction of the second research axis, the long term objective is to develop algorithmic tools and analysis methods for distributed (i.e., decentralized) computations by mobile entities in highly dynamic networks.To achieve this goal, I plan to first concentrate on problems like exploration, rendezvous, and map construction, and then ask general questions onwhat can be performed by the entities moving in highly dynamic networks depending on the temporal structure of the system, on other network parameters, and on the knowledge available to the entities about the environment. With these investigations, I expect togain a deeper understanding of systems of mobile computational entities,significantly advancing our knowledge on their limits and capabilities. I also anticipate that this project will lead to new algorithms for several coordination problems, as well as novel methods for the analysis of dynamic networks. In doing so it will also generate new techniques that could have a practical impact in the design and analysis of actual systems. ""625702,""flora, tanveer"
"634859"	"Fraser, Maia"	"Group actions and symplectic techniques in Machine Learning and Computational Geometry"	"Mathematical tools havebecome relevant across a broad range of scientific disciplines to an extent never before seen, as our interactions with the world become increasingly data-based and inter-connected. The rising roleof topology, geometry and statistics in Computer Science, especially in Data Science, is particularlynoticeable. Data are numbers or number-based and so often comewith underlying topological, geometric or statistical structure. Understandinghow this structure impacts algorithms and properties of interestconfers an immediate benefit to algorithmic development, but thisinnovation requires solid foundations in both Computer Science and Mathematics. My research takes steps to bridge this gap, by applyingmathematical tools related to group actions and symplectic geometry in Computer Science. In this proposal, I considersettings where transformation groups act on objects of interest in an essential way. This means that we know a group G of symmetries of the objects of interest: for example a scan of an fingerprint should be considered ""the same"" regardless of how it is rotated - the group of rotations of the plane are the symmetries. Key notions from Math that are relevant in studying group actions are invariance and equivariance.Making use of equivariance a priori allows one to define more powerful algebraic invariants but this has rarely been leveraged in Computer Science. Closely linked with the study of group actions, symplectic geometry arose as the mathematical study of equations of motion in classical mechanics: a system evolves in time in such a way that certain quantities are conserved. Contact geometry is another closely related field of Pure Math with similar origins. These areas of geometry have rarely been used in Computer Science but promising applications are now appearing in Machine Learning. I propose to bring tools related to group actions and symplectic/contact geometry to bear in three specific cases: (1) in Machine Learning - analyzing how underlying structure given by symmetries can inform more effective learning strategies, for example group-invariant feature selection, and using symplectic-geometric methods to design algorithms; (2) in Computational Geometry - investigating how topology and group actions affect algorithms and properties of triangulations in the plane and on surfaces; (3) in Contact Geometry - using equivariance to study the existence of a scale at which quantum-style flexibility gives way to classical-style rigidity in certain contact manifolds.""630036,""Fraser, Marie"
"634824"	"Friedman, Joel"	"Expanders, Sheaves on Graphs, and Applications"	"Theoretical computer science has given rise to a number of important problems that can be attacked with techniques of the subfield of mathematics known as linear algebra. Our proposal is to use a number of related techniques in linear algebra applied to ""graph theory"", to study such problems. The connection of linear algebra to such problems is well known. Our proposal seeks to strengthen the known techniques in linear algebra, with a view towards their applications, to solve problems in theoretical computer science. Such techniques can be used also to solve problems in a number of fields of mathematics; indeed, the problems in computer science of interest to us are related to a number of areas of mathematics and physics. In the other direction, these related areas have research results which we can sometimes borrow to enhance our knowledge of the parts of linear algebra and computer science of interest to us. Our proposed research is motived by two areas of theoretical computer science, one known as ""expander graphs"", another known as ""complexity theory"". The areas of linear algebra that we work with are the ""eigenvalues"" and ""spectral theory"" of certain matrices arising in to ""graph theory"", and the related notion of ""sheaf theory"". While sheaf theory is often viewed as a field of algebraic topology, our interest in sheaf theory is in ""sheaves of vector spaces,"" which is a type of linear algebra that is enhanced by the structure of a graph. Our research in ""expander graphs"" focuses on ""relative expansion,"" which is a way of building larger networks from smaller ones, in a way where one can understand expansion of the large network in terms of the smaller network and the way the large network lies ""over"" the smaller one. Recently there has been a lot of results and constructions of new networks from smaller ones. The foundations of relative expansion began in a paper of ours from 2003, motivated by work of Alexander Grothendieck in topology and algebraic geometry. Our research in sheaf theory allows us to compare structures in linear algebra that lie over the same graph; this type of sheaf theory is in its infancy. We began to study this theory motivated by complexity theory, but have used it to solve the Hanna Neumann Conjecture of the 1950's regarding group theory. This sheaf theory was inspired by work of Grothendieck and his colleagues. Hence our project exploits the fruitful connection between areas of computer science and mathematics.""625122,""Friedman, Nathan"
"635130"	"FroeseFischer, Charlotte"	"Accurate High-Performance Atomic Structure Calculations"	"The goal of this project is to improve the accuracy of results from the GRASP2K computational model by a factor of 10 for given computer resources by developing new high-performance software that is more efficient, easier to maintain, and can be modified readily for future developments in atomic theory. This requires: 1) Redesigning programs to adhere to current software engineering principles of design and using efficient algorithms for large cases. 2) Recasting programs in the most advanced scientific programming language for high-performance computing. 3) Introducing a proper modular style that anticipates future changes in the model of the nucleus, the Breit correction, and QED effects that define H . History shows clearly that accuracy can have tremendous impact on the advancement of science. An example is Tycho Brahe (1546-1601) who dedicated his life to developing tools for recording planetary positions ten times more accurately than before. His data was accurate enough for Kepler to discover that the planets moved in elliptic orbits which gave Newton the clues he needed to establish universal inverse-square gravitation theory. In quantum mechanics, the state of an electronic system is described by a wave function W that satisfies the wave equation H W = E W .Here H is the Hamiltonian of the system and E the total energy. For an atomwith N electrons, the wave equation is a partial differential equation with 3N space variables. What makes the problem challenging are the singularities that occur when the distance between the two electrons goes to zero . Observable properties of the system are expectation values of quantum mechanical operators. Thus, when H and W are known, all atomic properties can be predicted. For light atoms, H often is the non-relativistic Hamiltonian. For heavy elements H needs to be based on fully relativistic Dirac theory that includes quantum electrodynamic effects, and a finite model for the nucleus. H for superheavy elements is a current research topic. In atomic physics an accurate computed result needs to agree with an experimental result reported as a value and an uncertainty. For any given H,a challenge for the computational model are the singularities, i.e. correlation in the motion of the electrons. Test cases for the development of the software will be drawn from current research topics in physics, done in collaboration with international colleagues. The biggest challenges are presented by calculations for heavy elements or highly ionized atoms. An example is the element Astatine ( N=85 ) that is currently being considered for use in targeted cancer therapy. Experimental studies are planned in Sweden. Another critical test would be spectrum calculations for Uranium ( N=92 ) where reliable results have not been reported. In the case of superheavy elements for the search of islands of stability, the code could be an important tool for the development of new physics theory.""633357,""Frolov, Andrei"
"634854"	"Fu, Hu"	"Platforms and Internet market design"	"The e-commerce is one of the fastest growing sectors in global economies. At an extraordinary speed, it is boosting market efficiency, releasing underutilized productive forces, and expanding the markets. Two pillars are at the base of this epoch-making development: computational powers and market innovations. These two lend support and pose challenges to each other, creating a burgeoning interdisciplinary area between Computer Science and Economics. My research in Algorithmic Game Theory aims to contribute results and train students to push the frontiers both in computational methods for solving economic problems and in innovative market design that can find applications in Internet marketplaces. The first focus of the project is on designing and analyzing robust market mechanisms in complex environments. Non-traditional pricing mechanisms, such as large-scale frequent auctions and dynamic pricing, are salient features in major Internet marketplaces. Besides computational challenges to optimize such mechanisms, also crucial is their robustness, both in their reaction to observed market data and in incorporating strategizing on the part of the market participants. Building on my existing work that successfully applies approximation algorithm techniques to tackle problems in market design, I plan to further combine techniques from machine learning to enhance theoretical understanding on these mechanisms. My second focus is on economic and computational problems arising from marketplaces known as platforms. Platforms enable transactions between two other groups of agents, typically enjoying an economy of scale across the two sides. Amazon's third-party market, Uber and Airbnb are some such examples; they are playing an increasingly important role in the e-commerce. Some of my existing work pioneers in understanding design issues in such markets, and a course I taught at Caltech is among the first to focus on these markets. In an ongoing project, I collaborate with a startup company to understand the matching mechanisms on platforms and the phenomenon of algorithmic pricing in these markets. The success of such projects not only could inform the design of platforms, but may also provide insights that help shape public discussions on these markets. Besides the immediate impacts this work could have within the research community and on the industry, this research will provide key training to graduate and undergraduate students. It will produce highly-qualified personnel with skills and experience in reasoning computational systems armed with economic insights. These skills will prove to be a major asset in the expansion of Canada's growing knowledge-based economy.""640926,""Fu, Hu"
"621732"	"Fuhrman, Robert"	"British Columbia"	"CANADA"
"637461"	"Gagnon, Ghyslain"	"Advanced devices and algorithms for energy disaggregation in buildings"	"Energy disaggregation (also referred as nonintrusive load monitoring) is a combination of signal processing and pattern recognition techniques to estimate the energy consumption of individual appliances from the total energy consumption signal. It is achieved by identifying discriminating features in the aggregated signal and decomposing it into its constituent parts. Disaggregation of the energy consumption data down to the level of appliances has been largely identified as an opportunity for enhanced energy efficiency through citizen awareness, energy demand prediction tools for utilities and smart automatic control of appliances, just to name a few. This research program seeks to advance the field of energy disaggregation to efficiently determine the power consumption of individual electrical loads in real-life scenarios. This will be achieved by building on the applicants team latest developments in Hall-effect current sensor devices and weakly-supervised machine learning algorithms. Specifically, we will 1) design new energy disaggregation algorithms for residential applications using weakly labeled data from low-precision current sensors, 2) analyze the sensitivity of the disaggregation algorithms to the accuracy and quantity of sensor information, 3) design the next generation of ultra-low-power Hall-effect current sensors and 4) design new energy disaggregation algorithms for commercial and industrial applications with an optimal number of low-precision sensors. These advances will increase the accuracy, the scalability and the adaptability of existing techniques. This research program will train four HQP in domains which are in high demand in industry and academia, gaining important skills in signal processing, machine learning and microelectronics. The research results could also be the starting point of new collaborations, as energy disaggregation is gaining interest from industry and utilities, as confirmed by recent important investments in this field. Natural Resources Canada established that ""the Canadian buildings sector has a duty to use our energy resources responsibly and take up the call to action as a mechanism that will strengthen and enrich our economy for future generations."" This research program is an important step in that direction, through the development of novel technologies to monitor energy consumption more efficiently, and eventually, enabling leading-edge energy management technologies for smart buildings. ""639616,""Gagnon, Ghyslain"
"635070"	"Gagnon, Michel"	"Transformation d'une banque de documents en une reprsentation smantique pour l'extraction d'opinions par le biais de questions en anglais et en franais"	"l'heure actuelle, la consultation d'articles scientifiques se fait par le biais du Web, en utilisant un moteur de recherche ou une plateforme de diffusion numrique. Or, bien que ces moyens permettent d'obtenir des documents, ils ne sont pas adapts des besoins plus pointus, comme la rponse des questions prcises.Les questions qu'on est appel poser un tel outil peuvent couvrir plusieurs aspects trs complexes, cause d'une part de la difficult de les formaliser, d'autre part de la variabilit et de la subtilit de leur expression dans le langage. Aussi, dans bien des cas, il est ncessaire, pour y rpondre, de faire des infrences partir d'informations puises dans plusieurs segments de textes. Dans l'tat actuel des recherches dans ce domaine, nous sommes trs loin de l'atteinte d'un tel objectif. En effet, la plupart des approches proposes se limitent des factodes ou des dfinitions. Je vais donc m'intresser un problme spcifique, soit l'identification d'opinions dans un corpus d'articles scientifiques dans le domaine des sciences humaines. Il s'agit donc ici de tenter de rpondre des questions du type suivant: - Quels sont les chercheurs qui critiquent le nolibralisme? - Que sait-on sur limpact des technologies dans lapprentissage des lves du secondaire? Pour y parvenir, je propose d'utiliser les citations qu'on retrouve dans les articles. En analysant le texte qui forme le contexte d'une citation, on est en mesure de dterminer s'il s'agit d'une prise de position de l'auteur par rapport au travail cit, ou d'une relation entre les auteurs de l'article cit et certains concepts, phnomnes, thories, technologies, etc. Il faudra d'abord identifier le type de contexte de citation auquel on a affaire, puis reprsenter la relation exprime. Ceci permettra de construire la reprsentation formelle des opinions, constituant ainsi une base de connaissances qu'il faudra maintenant utiliser pour rpondre aux questions. Il s'agit ici d'un autre objectif important de cette recherche, soit la mise en relation entre la question pose et les items de notre reprsentation formelle qui permettent d'y rpondre. Ceci exigera d'une part une analyse smantique de la question, et la transformation du rsultat de cette analyse en une requte qui permet d'extraire de la base de connaissances tous les lments d'information pertinents pour rpondre la question. Les rsultats esprs pour le programme de recherche propos auront un impact trs important sur la manire d'accder l'information: la banque de documents ne sera plus perue comme un simple ensembles de textes consulter, mais aussi comme un base de connaissances qui rsulte d'une certaine interprtation du contenu de ces textes.""623354,""Gagnon, Miguel"
"634515"	"Garcia, Ronald"	"Extending the Theory and Practice of Gradual Typing"	"Programming languages are typically categorized as either statically or dynamically typed. Statically typed languages such as Java use types to ensure that a program satisfies basic behavioural guarantees before running it. Type checking catches programming errors and supports optimizations that make programs run faster and use less memory. However, programmers must add extra type annotations, and type checking often rejects programs that would otherwise run correctly. Dynamically typed languages such as Javascript and PHP use runtime checks to catch errors, so they enable flexible and fast-paced programming that is heavily used to develop web sites and mobile applications. But in trade they tend to run slower and use more memory and power, which is particularly bad for mobile apps, and deployed programs often harbour costly bugs that could have been detected right away by a type checker. Gradual typing is a new approach to language design that lets programmers seamlessly mix static and dynamic typing, combining the performance and correctness benefits of statically typed languages with the agility of dynamic languages. Industrial programming languages, such as Microsoft's Typescript (an extension of Javascript) and Facebook's Hack (an extension of PHP), now provide some support for gradual typing, while researchers in academia continue to develop its theory. My research program focuses on developing the theoretical foundations of gradual typing so as to inform the design and implementation of gradually typed languages. The results of this work have been promising, but more work is needed to make these ideas applicable to industrial-strength languages. This proposal outlines three objectives that will advance the state-of-the-art in gradual typing. First, we will develop a general theory for improving how gradually-typed languages use static type information to systematically decrease the overhead of checking programs at runtime. Second, we will extend the foundations of gradual typing to apply to type systems that guarantee not just what kinds of values computations produce, but also how computations interact with one another, assuring properties such as the confidentiality of private data and control over access to privileged resources. Third, we will develop and validate techniques for constructing high-performance and space-efficient compilers for languages that want to incorporate gradual typing by design. This research has the potential to fundamentally improve how programming languages are designed and implemented, and bridge the longstanding gap between high-confidence and agile approaches to software construction.""619974,""GarciaRamos, Felipe"
"637184"	"Gaudet, Vincent"	"Overcoming Challenges in Stochastic Computing to Enable Efficient Next-Generation Microelectronic Systems"	"High-performance microelectronic circuit design practices often focus on competing dimensions of cost, throughput, and energy consumption. The push is often towards higher precision, and design decisions are driven by worst-case analysis. However, this approach can lead to over-engineered systems that have a greater energy cost that what is fundamentally needed by the target application. Stochastic computing is a technique that traces its history back to work done by Von Neumann in the 1950s, and that counter-intuitively uses randomness to the benefit of computation, possibly leading to lower hardware cost and power consumption. Many fundamental results were reported in the 1960s, e.g., multiplication using single AND gates. Stochastic computations are performed in the time domain using streams of randomly generated symbols, where the quantity of interest is the time average of a binary sequence. In the 1970s, progress in integrated circuit technologies alleviated the need to reduce transistor count. Consequently, research into stochastic computing effectively halted from the mid-1970s until the 1990s. However, there is now a renaissance in stochastic computing, and it has become an effective technique for forward error control. Several multi-Gigabit-per-second decoders have been reported in the literature. Stochastic computations fault tolerance also makes in an attractive option for novel nanoscale technologies. Despite these advances, there are obstacles towards wider adoption of stochastic computing: (1) Stochastic computing represents quantities using time averages of random sequences of bits. Generating these sequences and maintaining desired properties of independence is costly. (2) Long sequences in the thousands of symbols are often required to reach desired levels of accuracy. This either leads to long latencies or to extremely high clock speed requirements. Furthermore, there may be high switching activity (and power consumption). (3) Because of their random nature, stochastic computing systems do not have absolute guarantees on performance and/or accuracy; this can be a stumbling block in some sensitive applications (e.g., in the biomedical sector). Through the proposed research program, the applicant and his team will work towards addressing these three challenges, and will apply the results to design efficient hardware for emerging applications in signal processing and machine learning.""626287,""Gaudreau, Mathilde"
"637174"	"Gauthier, Robert"	"Micro-Photonic Resonators: Materials / Geometries / Applications"	"Theguiding, confining and processing of light using structures with dimensionscomparable to the optical wavelength has significantly enhanced ourcommunication, sensing and control capabilities. Greater demands on deviceperformance such as speed and sensitivity along with foot-print and powerreduction motivate researchers to explore novel materials and alternategeometries. The optical resonator provides the core functionality of numerousoptical devices and is considered an optical configuration suitable to meet andsurpass forthcoming device requirements. An important step in the developmentof next generation resonator based devices is the theoretical analysis ofproposed configurations prior to costly prototyping. Numerical simulations ofoptical devices are rooted in solving Maxwells equations, an activitysupported in this research cycle. Theresearch effort builds on existing custom developed numerical solvers that are efficientin determining the optical properties of resonator states. The existence ofsymmetry (cylindrical / spherical) imbedded within the resonator geometry andexploitation of resonator state fundamental properties renders the devicedesign and theoretical analysis suitable for desk-top PC environments. Toaddress next generation resonator based devices, the material propertiesavailable for research will be extended to include anisotropy, non-linearity,gain, loss and frequency dependence. The intention is to also permit forreconfigurable geometries where external stimuli can tune material propertiessuch as through the electro-optic effect, magneto-optic effect, opticalradiation based forces, charge density, and cavity deformation. The additionalmaterial properties and geometry combinations will permit advancedconfigurations to be explored such that next generation device requirements canbe reached and surpassed. Theadvancements in the theoretical capabilities of the numerical solvers areexpected to have a significant impact on optical device research and design inareas such as plasmonics, metamaterials, environmental sensing, bio-photonics, micro-structuredfibers and automation control. Uponcompletion it is expected that advances will have been achieved in therefinement of the numerical computation techniques and extended the materialproperties available to study resonator geometries, have proposed andnumerically examined new resonator material-geometry configurations. Iterativeand perturbative computation engines will be available. Numerical computationengines will be freely available to all researchers via the internet andpromises to streamline the early stages of resonator based device developmentin Canada and abroad. Considerations will be given to extending the numericaltechniques to other areas such as acoustics and quantum mechanics as these fieldsoften overlap with photonics. ""639098,""Gauthier, Sandra"
"634748"	"Girouard, Audrey"	"Design and Evaluation of Screen-less Deformable User Interfaces"	"This proposal concerns research in the area of Human Computer Interaction (HCI). Upcoming flexible technology will enable users to squeeze a smartphone to answer a call, or bend a corner to change an eBooks page. Until now, researchers have focused on creating deformable devices, using flexible displays or simulating them using projection, to evaluate deformations as interaction techniques. We propose to study deformation input when it is not combined with a screen, as there is evidence to support the tactile nature of deformable devices alone is a worthwhile asset. In the proposed research program, we will design and fabricate specialized deformable devices, and develop software applications to analyze and use for deformable input. We will evaluate prototypes and applications to assess how bend and other deformation gestures can be used as suitable human computer interaction input techniques where no display is available on the device. We will perform comparisons between devices that include and omit a display, and assess how our devices can work in conjunction with current mobile devices. We will apply screen-less deformation to two novel domains: accessibility and creativity, to determine if deformables are best suited for specialized applications. In terms of accessibility, we will focus on visual impaired users, investigating how blind and low vision users can interact with deformable devices. The tangible nature of these interactions, such as bending an easily locatable and distinguishable part of a smartphone such as a corner, could enable visually impaired users to better control their devices compared to the use of a touch screen. For the creative domain, we will concentrate on improving tools for digital artists with a flexible stylus. This pen could provide them with a richer and more precise set of inputs to control the state of their paintbrush on the screen, for instance. This research program will uncover knowledge regarding the use of novel inputs. New technology and devices developed may benefit millions of visually impaired users in improving how they interact with their devices. The research can also modernize the tools used by digital artists everywhere. This research will be valuable to companies and institutions within Canada and abroad, by providing them with knowledge about emerging technologies, novel product development, and highly skilled workers in HCI, software development and hardware prototyping, which will stimulate growth in an economic sector where Canada is already maintaining a competitive advantage. ""628728,""Girouard, Audrey"
"634410"	"Glatard, Tristan"	"Big Data platforms for science automation"	"The objective of my research program is to automate Big Data analyses from data processing to knowledge publication. In the next years, I will focus on the following three objectives of this immense challenge. (1) Interoperability among Big Data platforms: Big Data platforms used in science currently operate in silos, which hinders open science, reduces the overall quality of platforms by limiting competition, and creates technological dependence on particular software projects. I will design the building blocks of a decentralized network connecting platforms so that data, processing pipelines and analyses can be uniformly found, accessed and reused in various platforms. (2) Reproducibility of Big Data analyses over time and space: science is going through a severe reproducibility crisis, which, if not properly addressed, will prevent various disciplines from leveraging the tremendous wealth of data that is acquired: Big Data has to be converted into trusted knowledge. I will develop methods to identify, quantify and correct reproducibility issues, focusing on the challenges that originate in the computing infrastructure. (3) Performance optimization of Big Data computations: while they have been transformational for industry, Big Data technologies remain underused in science, due to the lack of appropriate benchmarks and performance optimization studies. I will design and create an optimized, easy-to-use Big Data processing environment for large scientific datasets. These three objectives echo the notorious Vs of Big Data: interoperability addresses data Variety, reproducibility targets Veracity, and performance provides Velocity and manages data Volume. Overall, this program will speed up and improve the quality of knowledge production. It will foster cyberinfrastructure automation, by allowing scientific objects to seamlessly move across platforms, by ensuring the consistency of results computed in different platforms and by accelerating Big Data analyses. While potential applications of this research span the whole spectrum of scientific disciplines engaged in data science, I will focus on neuroinformatics, exploiting my long-standing top-level collaborations in this field and leveraging the tremendous experience acquired with the development and operation of the VIP and CBRAIN platforms. Technology transfer to the Canadian industry is also expected, in the IT (Big Data and cloud) and pharmaceutical sectors.""636573,""Glaus, Mathias"
"634926"	"Godsil, Chris"	"Quantum walks and graph spectra"	"Physicists, computer scientists and mathematicians are working to decide how we may best make use of quantum computers. Here the basic problems are to work out what we might do with a quantum computer that we cannot do equally well with the ordinary computers already on our desks, and what their limitations might be. The goal of my project is develop the theory of objects known as quantum walks, which can be viewedas subroutines in a quantum computer. These walks come in many variants, but in each case theyare based on an underlying network or graph. In the most general terms, the goal of my projectis to develop our understanding of the relation between the properties of the walk and the properties of the underlying graph. I hope to develop the theory to the point where we canproduce quantum algorithms for determining properties of the underlying graph. I expect our work will also lead us to develop limits on just what is possible using these walks. Physicistshave proposed algorithms for solving the graph isomorphism problem, an important problemin computing. I and members of my team have shown that most of these proposals do not work.We would like to go further and show that there are no natural modifications of thesealgorithms that work.""640924,""Godsil, Christopher"
"617760"	"Goel, Ashvin"	"Reliability for Fast, Persistent Memory Applications"	"This research is driven by two technology trends, persistent memory and in-memory computing. Persistent memory technology is unique in that it is accessed similar to traditional DRAM, but it provides durable storage like Flash. Several manufactures, including Intel and Micron, have announced that they plan to make low cost, high-performance, persistent memory products available in 2017. In-memory computing, in which large data sets are kept almost entirely in DRAM for fast processing, is being driven by the availability of relatively inexpensive DRAM. It enables building fast applications such as key-value stores and in-memory databases. Today, these in-memory applications form the core of the modern web infrastructure, enabling web sites to serve requests with low latency and high throughput. Persistent memory is a natural fit for in-memory applications. Persistent memory can be used to supplement DRAM, and it helps ensure durability of data. Its relatively good performance, low cost and high capacity will enable scaling these applications to much larger data sets, and running them on larger clusters of machines. In this proposal, my research objective is to address the question: how can we ensure the reliability of persistent memory applications? I am interested in exploring all the reliability challenges raised by these applications, including ensuring correctness, availability and disaster recovery. While much recent work has focused on the performance of these applications, these reliability issues are less well understood today. I plan to explore several methods for improving the reliability of persistent memory applications, including detecting bugs in these applications, providing language support for writing and testing these applications, and building reliable and scalable distributed applications. I expect that applying these methods to persistent memory applications will lead to novel research results, with significant impact. Today, computer systems are expected to work reliably and be available at all times. Yet hardware failures and software bugs are common, accounting for a significant portion of system downtime. The economic impact of these failures is estimated to be in the hundreds of billions of dollars. I expect that the results of this research will help reduce the impact of bugs and failures, thereby enabling reliable and fast in-memory applications.""634775,""Goel, Ashvin"
"637040"	"Goertzen, Andrew"	"High resolution positron emission tomography (PET) insert systems for multimodality PET/magnetic resonance imaging"	"Positron emissiontomography (PET) is an imaging modality that allows non-invasive assessment of tissuefunction at the molecular level in both clinical and preclinical settingsthrough imaging the biodistribution of radioactively labelled tracers. The highspecificity of many PET imaging tracers results in images with little anatomicalinformation, making it difficult to assign the tracer uptake to specifictissues and complicating the interpretation and analysis of the images. Thesolution to this problem has been to integrate PET with an anatomical imagingmodality, initially X-ray computed tomography (CT) and more recently, magneticresonance (MR) imaging. Integration of PET with MR for preclinical hybridPET/MR imaging of small animals can be efficiently accomplished through using aPET insert that fits within the bore of an existing high field MR system andcan operate simultaneously with the MR. During the previous funding cycle, ourgroup successfully developed and deployed such an insert for small animalPET/MR imaging using new MR compatible silicon photodetectors and a multi-layerscintillator array to preserve spatial resolution in the compact ring diameterPET system. PET insert systems for preclinical imaging are now becomingcommercially available, including one based on our design commercialized througha Canadian company. In this application I will build on ourexpertise in PET insert systems for small animal PET/MR to achieve my longterm objective of creating hardware and software technologies thatenable the use of hybrid PET/MR imaging with PET inserts as a routine imagingmodality in preclinical small animal research studies and in human neuroimagingapplication. To achieve this goal, the short term objectives of my researchprogram detailed in this proposal cover three themes: i) algorithms and hardwareto advance preclinical PET/MR imaging; ii) detector development for nextgeneration PET insert systems; and iii) creating technologies to facilitate andautomate small animal PET/MR imaging. These research objectives create multi-disciplinarytraining opportunities for graduate and undergraduate students from ElectricalEngineering, Biomedical Engineering and Medical Physics. Close collaborationwith both end users of imaging systems at the preclinical animal imaging andclinical imaging levels, together with industrial partners that manufactureimaging systems, facilitates rapid testing and deployment of new technologies, integrationof user feedback for revisions and a path for technology translation of new devices. ""625775,""Goettel, James"
"634789"	"Golab, Lukasz"	"Big data profiling: collecting data about data to support efficient and effective analytics"	"Big data are changing the way people and businesses make decisions. However, before investing time and other resources to analyze the vast amounts of available data, it is critical to ask questions such as ""Do we have the right data for the task at hand?"", ""Do we need to clean the data before they are suitable for analysis?"", or ""Is there structure in the data that can help us do effective and efficient analytics?"". These pertinent questions can be answered through data profiling: the activity of collecting metadata, i.e., data about data. Given a dataset, say, in the form of a spreadsheet, useful metadata may include quantitative information such as the number of rows, the number of distinct values and the identities of frequently occurring values, and structural information such as correlations or dependencies among columns. One could profile a small dataset just by looking at it, but automated techniques are clearly needed for big data: while the amount of data keeps growing, human cognitive processing capacity is fixed. The proposed research program will develop new methods, algorithms and software tools for data profiling, focusing on the technical challenges arising from the three Vs of big data: Volume (the growing amount of data generated by social media, the internet-of-things, etc.), Velocity (the high speed with which data are generated, e.g., sensor readings or twitter messages) and Variety (business data, numeric data, graph data such as friend/follower relationships in social media, etc.). This research will play a major role in Dr. Golabs long-term research agenda to help individuals and businesses get more value out of big data. Data profiling tools are urgently needed to make data analytics more accessible to the increasing number of experts and non-experts interested in incorporating big data into their decision-making processes. Such tools will help Canadian governments, utilities, automotive companies, healthcare companies and banks to use big data more effectively and efficiently. The anticipated deliverables will also be of interest Canadas world-renowned database companies such as IBM Toronto and SAP Waterloo: a key to improving the performance of data analytics is to exploit structural relationships in the data. Furthermore, the proposed research will be led by graduate students who will acquire sought-after skills in data science and big data engineering, which will help them to take leadership roles in Canadas increasingly data-driven economy.""634428,""Golab, Wojciech"
"635001"	"Goldberg, Ian"	"Privacy Enhancing Technologies for Censorship-resistant and Secure Internet Communications"	"In the last few years, the Internet has seen significantly increased deployment of security and privacy technologies offered by websites, messaging applications, and other forms of online communication. However, these currently deployed technologies in the area of privacy-preserving communication only solve part of the problem. Most approaches focus on protecting the contents of a message but reveal the metadata ( who communicated with whom , when , and where ). Such metadata allows the reconstruction of a person's social network, location, and activities, and by itself can reveal sensitive information such as being part of a minority or marginalized group. Furthermore, protecting communication contents and metadata is of limited utility if the communication itself is censored and the messages are not getting through at all; country-scale Internet censorship is increasingly common around the world, drastically undermining human rights such as freedom of speech. The original design of the Internet did not account for governmental parties manipulating and censoring communication and hence does not provide adequate mechanisms to guarantee availability : that the Internet services one might wish to use are in fact reachable. Similarly, the abundance of sensitive highly personal information that is currently shared digitally is not adequately protected. Adversaries, ranging from petty criminals to governmental censors, are able to observe and manipulate Internet communication. Such surveillance and manipulation affects both individuals, for instance due to leaked personal information, as well as societies, for instance due to country-wide censorship. It is therefore essential to develop technologies to resist surveillance and censorship. Unfortunately, manually designing and evaluating privacy-enhancing technologies (PETs) for secure and censorship-resistant communication is cumbersome and notoriously difficult to perform perfectly. Moreover, a single design flaw can lead to catastrophic real-world consequences for users. By enabling automated protocol construction and verification , we will ensure future research on PETs will be more efficient and have lower risk for errors. The long-term goal of our research is to develop original and innovative PETs that overcome the limitations of the existing state-of-the-art approaches and provide more extensive protection from surveillance and censorship. In light of the above discussion, we identified three short-term goals: i) protecting communication metadata, ii) automated protocol construction, and iii) censorship-resistant Internet access. Our PETs for secure and censorship-resistant communication address all three traditional aspects of security: confidentiality, integrity, and availability. In addition to advancing scientific knowledge, we focus on deploying our systems to benefit users all over the world.""633892,""Goldblatt, Colin"
"634335"	"Gong, Minglun"	"Quality-driven autonomous 3D reconstruction of large-scale scenes"	"Monitors and TVs have been the dominating display devices in the past 30 years. They are only capable of showing 2D contents, which can be easily acquired using todays digital cameras. As virtual reality (VR) devices gain practicality and popularity, allowing users to view 3D contents from selected viewpoints and directions, an important question is, therefore, how to efficiently and economically capture objects and scenes in 3D. The technology for acquiring 3D shapes of small objects is already mature. Users can scan a given object from different sides using a range scanner and then register together the 3D point clouds to generate a surface model. On this front, my collaborators and I have developed a state-of-the-art algorithm that automates the scanning process through positioning a scanner at strategically selected locations using a robotic arm. However, how to acquire high-quality models for large-scale outdoor scenes in an autonomous manner is still an open problem. 3D reconstruction for large sites has vast applications in areas such as urban planning, geological and archaeological survey, virtual tourism, and military simulation. It is an active topic in many research communities, including computer vision, graphics, robotics, civil engineering, and remote sensing. In the past, people often resorted to manned airborne LiDAR systems, which come with intimidating cost. Thanks to the rapid development of unmanned aerial vehicles (UAVs), it is now possible to scan large sites in a much more economical manner. The proposed research aims at developing novel, efficient, and autonomous techniques for large-scale 3D scene reconstruction. To achieve this goal, in the next five years my students and I will investigate: 1) how to perform quality-guided autonomous 3D reconstruction for large outdoor scenes with LiDAR-equipped UAVs; 2) how to achieve the same objective using low-cost off-the-shelf UAVs with only image capture capability; 3) the benefits of letting both types of UAVs work collaboratively and fusing together image and range data; and 4) the feasibility of segmenting moving objects from point clouds and reconstructing dynamic 4D spatial-temporal models for these moving objects. As VR devices get widely adopted, the demands for large-scale scene models are expected to increase dramatically. Hence, the techniques to be developed in this research will make broad impacts in aforementioned areas and benefit different research communities. They are likely to advance the Information Systems and Technology field through making high-quality 3D models of our cities and environments easily obtainable. A number of research questions need to be addressed, which will help to train HQPs to take up highly demanded positions in Canada's high tech industries.""629418,""Gong, SiewGing"
"634927"	"Gopalakrishnan, Sathish"	"Developing Mixed-Criticality Real-Time Systems: Analysis Methods and Tools"	"Real-time embedded computing systems monitor and, often, control artifacts or other systems that interact with the physical world. Examples of such systems include avionics systems, automotive systems and medical robotics. Such real-time embedded systems are safety critical; their failure may result in harm to people or their environment. The overarching theme of my proposed program is to avoid, or limit, failures of real-time embedded computing systems. Failures may be a result of: * design and implementation failures (at both the hardware and software layers); * operational hardware/component failures; * resource allocation problems that manifest as timing errors when the system is not able to respond within timing bounds or deadlines. I specifically intend to advance the state of the art in scheduling and timing-related problems as well as with detection, diagnosis and recovery from hardware component failures.I will also build upon work in approximate computing to make output quality tradeoffs when it is more critical to meet timing constraints. My effort takes into consideration some of the different pressures that influence the development of real-time embedded systems such as the need for reduced size, weight and power consumption (sometimes called SWaP constraints) of the computing platforms. These pressures result in the colocation of computing activities of different criticalities on the same hardware platform (e.g., on a UAV, a task that takes photographs is less critical than the task that is responsible for aircraft stability). Such designs result in mixed-criticality systems . Nevertheless, safety critical systems need to meet the requirements highlighted in safety standards such as IEEE 26262. These standards enable safety certification, and they indicate the acceptable failure rates for tasks at different criticality levels. Much of the recent work in the area of mixed-criticality real-time scheduling has focused on dual criticality systems. Tasks are either high or low criticality, and most existing work treats low criticality tasks as best-effort tasks. In other words, in prior work, researchers have ensured that high-criticality tasks meet their deadlines and low-criticality tasks are dropped even when this is not necessary. The approach taken in prior work does not also match with safety standards which indicate a failure probability or failure rate for each criticality level (so it is possible for a high-criticality task to fail, but not at the same rate as a low-criticality task). Via this grant, I intend to develop tractable probabilistic analysis methods as well runtime support for real-time mixed-criticality systems . In the area of fault tolerance, there has been some work that examines mixed criticality systems, but more work is needed in terms of platforms and analysis methods to ensure compliance with safety standards and certification processes.""640354,""Gopaluni, Bhushan"
"621735"	"Gourdeau, Pascale"	"Ontario"	"CANADA"
"637190"	"Gregori, Stefano"	"Micropower converters for internet-of-things devices"	"Thanks to the miniaturization of electronic circuits, computing, communication and sensing functions can be concentrated into smaller and smaller elements, and then integrated seamlessly into the objects around us, which in turn can become ""smart"" and interconnected in a widespread ""internet of things"" (IoT). However these physical objects have limited resources in terms of available energy, computing power, and communication bandwidth. Therefore, we must pay special attention to resource efficiency, if we want to develop IoT devices and applications that can be scaled up to fulfil the promise of the smart world. This research program will develop new micropower converters to be integrated on the same silicon chip containing other computing, communication, and sensing functions. An IoT object will need only one chip, which will be tiny, economical, and more energy efficient than conventional devices. The micropower converters will improve energy efficiency by supplying optimized voltage levels to the different functions implemented on chip, and by scaling down the supply voltage when digital circuits operate in energy-saving mode. The micropower converters will also extend the operating conditions of the IoT devices by adjusting the wobbly voltage generated by a battery or by an energy harvesting device. The limitations imposed by energy constraints are already sharply curbing the performance designers might otherwise expect. Therefore the new micropower converters will address this energy challenge by enabling distributed power processing for jointly optimizing energy and operating performance. Such optimization will lead to significantly enhanced system efficiency, battery lifetime, and extended operation on energy harvesting. In turn these improvements will be key to create sustainable IoT applications unimaginable today, which will change the way we live and will be a major engine for creating new products and services. ""630154,""Gregory, Diane"
"634972"	"Grossman, Maura"	"Evaluation of High-Recall Human-in-the-Loop Information Retrieval Technologies"	"The objective of high-recall information retrieval (HRIR) is to identify substantially all information relevant to an information need, where the consequences of missing or untimely results may have serious legal, policy, health, social, safety, defence, or financial implications. To find acceptance in practice, HRIR technologies must be more effectiveand must be shown to be more effectivethan current practice, according to the legal, statutory, regulatory, ethical, or professional standards governing the application domain. The applicant is the leading expert in this field, having first recognized the transferability of machine-learning technologies from spam filtering to electronic discovery, and then having established the scientific foundation for their acceptance in civil litigation and regulatory proceedings, both in the United States and Europe. The proposed research includes experiments seeking to enhance and extend the HRIR results achieved to date, within application domains where it is critical to satisfy an information need when it arises, and on an ongoing basis, by finding substantially all relevant information. In furtherance of these goals, four general avenues of investigation will be pursued: (1) measuring the effectiveness of HRIR technologies in practice using live data and real information needs; (2) modeling more nuanced user-system interactions and employing higher-fidelity evaluation measures; (3) addressing the technological and logistical challenges presented by massive datasets and high-volume data streams; and (4) discovering and evaluating the effectiveness of HRIR technologies in new applications domains. Such domains include, but are not limited to, distinguishing between public and non-public records in the curation of government archives; systematic review for meta-analysis in evidence-based medicine; separating irregularities and intentional misstatements from unintentional errors in accounting restatements; performing due diligence in connection with pending mergers, acquisitions, and financing transactions; and surveillance and compliance activities involving massive datasets. The proposed research promises to establish the effectiveness of HRIR technology over current practice in those domains, as well as others.""632495,""Grosvenor, Andrew"
"637576"	"Gu, Jason"	"Advanced control and application of multiple mobile manipulators"	"Mobile manipulation is a fast-growing area of robotics research. Unlike those traditional robotics research areas such as perception, motion planning, manipulation, control, cognition, and artificial intelligence, which are focused on a specific task or application, the mobile manipulation seeks to develop integrated robotic systems capable of addressing a wide range of applications in real-world environments. Mobile manipulation aims to integrate the theoretical work and experimental platform to facilitate robotics capabilities in both mobility and manipulation. Thus, they can combine the advantages of mobile platforms and robotic arms and reduce the drawbacks, to be widely applied in different industrial and productive fields, such as health, mining, construction, rescue missions or daily assistance for senior and/or disabled. The proposed research program will investigate the advanced control of mobile manipulators. The integrated adaptive robust control design of single mobile manipulator will be developed first. The novel path planning and obstacle avoidance method under nonholonomic constraints is considered, and the advanced adaptive robust control for the nonlinear mobile manipulator dynamics with uncertainty is proposed which takes into account all issues of redundancy, nonholonomy, dynamic interaction, dynamic uncertainty, and motion/force control. To achieve good coordinated motion of multiple mobile manipulators, a decentralized cooperative control approach will be developed, which takes care of the system's closed-chain dynamics and internal forces together. For the teleoperation application of mobile manipulators, a novel multiple degree of freedom haptic master structure will be designed to realize the efficient mapping to redundant slave mobile manipulator and the bilateral/multilateral teleoperation control of mobile manipulators will be developed where the time delay and the performance requirements need to be considered together. ""627529,""Gu, Minglu"
"634312"	"Gupta, Kamal"	"Toward Spatially Competent Mobile Manipulation in Indoor Human Environments"	"Our research focuses on developing a level of spatial competency, motion safety, grasping andmanipulation capability in autonomous mobile manipulators (AMM) that is well beyond that of anyexisting robotic systems. Such capabilities include navigating in indoor and dynamicenvironments with humans walking around, carrying out basic manipulation tasks and interactingwith humans in tasks such as transfer of objects from robot to human. The targetapplication for these fundamental capabilities is service robotics, e.g., a helper robot that can assistfrail or disabled persons (called Assisted Persons or APs) in independent living, helping in taskssuch as finding and retrieving objects from around the home (e.g. a bottle of pills) and handing themto the AP (hand over task), etc. to ultimately improve the quality of life in daytodayfunctioning ofan AP living independently.""637408,""Gupta, Madan"
"637679"	"Hamam, Habib"	"Protection des documents originaux contre le photo-copiage et la numrisation"	"Contexte : La fraude en matire dordonnances est la cause principale du trafic de mdicaments aux tats Unis et au Canada. Les industries du papier spcialis, et notamment celui conu pour des documents importants (ordonnance, chque, passeport, etc.), dploient un effort considrable pour limiter la fraude. Il existe diffrentes techniques pour minimiser les chances de fraude travers la reproduction illgale des documents importants. Les pantographes motif VOID prsentent l'une des techniques les plus performantes contre la reproduction de documents importants par le photo-copiage ou la numrisation. Bien quinvisible l'il nu dans le document original, le motif VOID apparat dans la photocopie ou la version numrise. Ceci indique que le document nest pas original. Problmatique : Rcemment, les performances optiques des photocopieurs et scanners sont devenues aussi bonnes que celle de l'il humain. Les photocopieurs et numriseurs (scanners) nagissent plus comme des filtres passe-bas. Lobjectif long terme de ce programme de recherche est de dvelopper une solution qui tient compte des nouvelles performances des photocopieurs et numriseurs de sorte que toute photocopie doit contenir des indicateurs divulguant quil sagit dune copie ou dune version numrise. Originalit et objectifs de recherche: Les photocopieurs et les numriseurs sont parfois plus performants que l'il humain, mais ils nont pas l'intelligence de faire la distinction entre les cts recto-verso du papier. L'il aperoit la face recto et la face verso si le papier nest pas opaque 100% alors que le cerveau interprte limage rtinienne et distingue cognitivement le recto et le verso de la feuille originale. Le premier objectif de recherche est dutiliser le degr dopacit du papier comme degr de libert et proposer une solution efficace pour protger les documents originaux contre le photo-copiage et la numrisation laide dune analyse spectrale. Compte tenu de la qualit infrieure de la version imprime de l'image au verso, nous proposons dexploiter cette imperfection. Concrtement, nous visons le dveloppement d'un algorithme qui joue sur le contraste et la luminosit du texte crit au recto et celui au verso. En outre, tant donn que les tlphones intelligents sont couramment utiliss, en particulier par les utilisateurs des documents protgs (mdecins, notaires, etc.), nous proposons dutiliser cette technologie pour authentifier des documents originaux. En ralit, il est impossible pour les photocopieurs ou numriseurs dintroduire aucun filtrage passe-bas. Un deuxime objectif est de proposer un algorithme itratif pour concevoir des pantographes adapts des frquences de coupure plus leves. La rpartition des grands et petits points sera diffrente des pantographes existants et sera optimise dune faon itrative en fonction de la frquence de coupeur. ""630775,""Haman, Franois"
"634605"	"HamouLhadj, Abdelwahab"	"Advancing Logging Practices in Software Engineering"	"Despite the advances in software development, the increasing complexity of software systems makes it difficult to produce fault free software. The residual faults often manifest themselves during system execution causing severe software failures, security breaches, and performance degradation. To diagnose the root causes of failures, software developers and system administrators frequently resort to the analysis of logs, generated during system executions. Major companies like Ericsson, Google, and Microsoft collect logs to analyze, fix, and prevent failures. Logs are also used by security providers like AlienVault and ArcSight to detect security threats and malicious behaviors. Although logging has long been recognized as an important aspect of software development, it remains predominantly an ad hoc practice. There are no standards, guidelines, or recognized best practices for logging. In addition, software developers continue to insert logs into a system without automated guidance. The lack of systematic and automated approaches for logging has a tremendous impact on the quality of logs, often hindering viable analysis. Log analysis is further complicated by the sheer volume and variability of log data. As a direct consequence, many failure diagnosis tasks are very challenging and expensive to implement, putting at risk the reliability and security of critical systems on which society depends in areas like health, telecom, public safety, and defence. The ultimate goal of this research program is to advance the practice of logging in software engineering. Much of the existing research effort in the field is devoted to the analysis of logs after they are generated. Considerably less attention is devoted to the study of the practice of logging. Over the next five years, we will focus on four priority research directions. First, we will identify systematic approaches and best practices for logging by studying the current practices in the field. Second, we will develop a context-aware recommendation system to automatically provide logging suggestions to developers. In the third project, we will facilitate the analysis of logs by reducing their complexity. Finally, we will produce techniques to support better logging decisionsbased onsystem design and architecture. The outcomes of this research program are expected to have a real impact on the software industry. Improving logging practices of software developers will result in superior failure diagnosis techniques while reducing the associated costs. Another important impact of this research is its potential for commercialization driven by the strong need from industry to have better logging tools in areas like IoT, cloud computing, and security. Furthermore, the proposed program will train several HQPs who will contribute to the growth of Canadas ICT sectoran important enabler to Canadas competitiveness in the world economy. ""631433,""Hampson, Elizabeth"
"634450"	"He, Wenbo"	"Boosting Multimedia Big Data Systems"	"Video data is considered as biggest big data. We have witnessed gigantic volume of data generated at a very fast speed in recent years. According to Cisco Systems, video data counts for around 80% of the Internet data in United States. Also, YouTube users upload more than 300 hours video clips in every minute. Besides the large volume and velocity, the irregularities and ambiguities (variety property) of the video data make it difficult to compare, understand, and annotate the data using traditional technology and systems. In the proposed research program, our objective is to systematically address the challenges in understanding the complexity of multimedia big data; to handle scalability; to make trade-off between efficiency and accuracy of various solutions; and to design and implement systems to organize, classify, search, and retrieve multimedia contents. Specifically, we will conduct the following research activities, but not limited to (1) the investigation of video-feature representation based on individual frame features while preserve the temporal and spatial correlation among frames, (2) the development of underlying operating systems and indexing structures to support parallel computation on multimedia data in multi-core and distributed environments, (3) the use of semi-supervised graphical model to downplay the dependency of the state-of-the-art deep learning models on large size of high-quality training sets by exploiting the joint distribution between labeled and unlabeled data; (4) the use of systems approach to iteratively optimize individual components and system integration, which will eventually lead to both locally and globally optimization. My research group has been working on multimedia big data since 2013, and we have made a great progress on building a testbed to measure the performance of various combinations of video features, developing a pipeline architecture to support parallel multimedia content computation based on SPARK streaming, and implementing a prototype for multimedia data storage and indexing on a single multi-core server. We will continuously apply multiple system-oriented strategies to optimize the system performance, and extend the systems on GPUs (Graphics Processing Units) servers and in distributed environments. Meanwhile, the testbed, prototype system, as well as the models and theories that we have developed in the past a few years will serve as the basis and tools in design, implement, and verify the proposed research in the future. On the frontier of multimedia big data research, the proposed research program will also provide industrially-relevant and the most up-to-date training for HQP, helping to equip them with the skills and knowledge to make an impact in the rapidly growing big data industry in Canada.""635928,""He, Wenqing"
"637234"	"Helmy, Amr"	"Nanoscale, Commercially viable, Field Programmable Photonic Device Arrays"	"This proposal describes research aimed at developing a platform for nanoscale, commercially viable, optoelectronic devices to form the basis of field programmable photonic device arrays. The platform promises to provide unprecedented capabilities and performance metrics for applications in the domains of tele- as well as data-communications, computing and on-chip sensing. Through judicious design, along with the utilization of recent discoveries reported by my group, plasmonic losses can be nearly eliminated, leaving us with plasmonic structures which only suffer from other loss mechanisms such as scattering and leakage. This in turn allows one to design optoelectronic devices that have nanometer scale dimensions in all 3 degrees of freedom with area, power consumption, bandwidth, operating wavelength range and insertion losses which surpass their all-dielectric counterparts such as Si-photonic circuits. The question that merits answering is whether these improvements fulfill unmet demands and would fuel a surge in optoelectronic functionality, utility and market share. An important example is one which relates to data communications in emerging generations of high-speed integrated circuits: interconnect latency and power consumption have become prominent bottlenecks in data routing and processing. To alleviate these limitations, optical communications based on photonic circuits and devices are integrated into conventional electronic platforms to take advantage of the high information bandwidth that the former can provide. In particular, silicon photonics have emerged as one of the major platforms in optoelectronics integration due to its relative fabrication compatibility with mature CMOS processing within a monolithic chip. However, dielectric waveguides such as silicon guide light via total internal reflection, and thus minimizing physical dimensions and increasing the integration density are restricted by diffraction limit and cross-coupling . The complex permitivity of metals at optical frequencies lead to significant losses, limiting propagation lengths to a few microns before power is attenuated below detectable levels. Due to these losses, it seems that plasmonic modes may never outperform their all-dielectric counterparts to transport light between different points despite of its potential to address many of the existing challenges encountered in Si photonics. However, if these losses are reduced beyond a certain level, through cancellation of the field in the metal layers rather than removing the field away from the metal layers, plasmonic modes may provide the optimum medium to accommodate truly nano-scale structures in all three dimensions, empowering these structures to challenge the performance and dominance of the all-dielectric Si platform used for the active and passive devices at present. ""625117,""Helpard, Luke"
"634749"	"Hoeber, Orland"	"Studying Visual Analytics Support for Interactive Information Retrieval within Complex Search Settings"	"This research addresses a problem that is becoming increasingly important in our modern information-centric lives. As we have become adept at searching the web for factual information, we are increasingly discovering the need to address complex information seeking problems within specialized knowledge repositories. While web search engines support finding single-page answers to focused questions using simple search interfaces, support for complex information seeking tasks is limited. Resolving such problems requires more than just viewing the top few documents in the search results list; the searcher may need to explore among and make sense of a potentially large number of search results, refine the query as the information seeking goal becomes more precise, and synthesize information gleaned from multiple queries and potentially over multiple information seeking sessions. The goal of this research is to study how visual analytics can support interactive information retrieval activities within complex search settings. The resulting visual search analytics research will address four problems: (1) analyze and understand the difficulties of undertaking complex search tasks; (2) design visual analytics approaches that support interactive information retrieval and the resolution of complex information seeking problems; (3) develop and iteratively refine prototype systems that implement these designs; and (4) study the benefits and limitations of such approaches. Consider the difficulty of searching within a digital library when one has limited knowledge of their search topic. In such situations, it is common to start with a vague query, and develop knowledge about the topic incrementally as documents are examined. A visual search analytics approach could use machine learning to extract key topics within the search results, and present this information in a visual format so that their relationships could be seen. This would enable the searcher to easily identify a specific interest, interactively refine the query to match this interest, and produce subsequent visualizations that enable the searcher to compare and make sense of the search results in relation to the topics. Building upon and extending the results of my previous research on using visualization to support human-centred search, this research will undertake a systematic study of visual search analytics within three specialized online knowledge repositories: digital libraries, social media, and online encyclopedias. These repositories each feature important differences in textual data and associated metadata, the types of information seeking behaviours searchers employ, and the criteria for considering the search a success. Studying visual search analytics over multiple knowledge repositories and various searcher behaviours will enable the development of a generalized framework for visual search analytics.""639089,""Hoeberechts, Maia"
"634544"	"Hogue, Andrew"	"Fundamental Challenges in User Experience Modeling for Immersive Technologies"	"The recent consumer-level growth of interest in Virtual Reality (VR) has created a demand for compelling and engaging VR content. But how does a developer create innovative yet compelling content? How does the developer know whether the content they are developing will be comfortable to the user? How do they know if their designs will impact the user negatively causing them to have headaches or nausea? Moreover, if the developer determined that a particular part of their software will negatively impact users, how do they modify the content to improve the experience? Until now, there were relatively few VR developers worldwide due to the prohibitive cost of hardware and software development processes. This allowed these specific individuals to become experts in perception, stereoscopic 3D and multi-modal effects and spend a great deal of time iteratively testing their software to ensure that it meets their particular standards. VR hardware costs have dropped dramatically and freely available (yet powerful) game engines have opened the VR industry up not only to consumers, but also to software developers who do not have the background knowledge necessary to leverage to create effective content. To ensure that VR content is compelling, we need a practical model of user experience that can demonstrate how the choice of technology can impact the user, how the physical environment itself impacts the user, and ultimately provide freely-available developer tools to visualize these effects as constraints on the content. The proposed research program will fundamentally study and develop quantitative methods that enable researchers and developers to more effectively evaluate VR systems, interactive content and the effects of design choices on user experience. To do so, the establishment of an immersive user experience model validated by user study coupled with a standardized mechanism to evaluate experimental configurations is proposed. Visualization tools will be iteratively studied in terms of their effectiveness at visualizing/predicting constraints on user experience during the content development process through the use of expert focus groups and user study. The results of the proposed program will enable researchers and content designers to quantitatively predict how new technology and design choices may affect the user enabling the rapid creation of more effective experiences in a range of application domains. Beyond contributions to basic science, the methods, models, and tools developed have wide application in the development of content for interactive digital media (a key area of Canadas ICT sector), education (immersive learning environments), health (diagnosis, collaborative healthcare), and manufacturing (diagnostics/process visualization), which are all key issues for the Canadian economy.""625352,""Hogue, Justin"
"637457"	"Hranilovic, Steve"	"Optical Wireless Communications - broadband connectivity in space, underwater and indoors"	"We take mobile communications for granted. Given the degree to which mobile devices have penetrated into our daily lives, we expect to be able to make a call or establish a broadband data link anywhere. However, there are many environments in which radio frequency (RF) communications are inefficient or impossible. Attenuation in the propagation environment, energy efficiency restrictions and RF interference can render radio systems unusable. Our research program develops wireless communication systems using optical rather than radio frequencies. Optical wireless links offer inexpensive, energy efficient broadband communication channels that are immune to RF interference or jamming and are free of spectral licensing requirements. In this project, we will develop information theory, pragmatic algorithms and prototypes for three optical wireless applications. Space-based optical wireless can provide broadband connections between satellites, to the ground and to deep space more efficiently than their RF counterparts. We will consider the channel modelling, capacity and signalling design for micro- and nano-satellite constellations that do not have precise gimbals due to their size and energy constraints. In addition, this program will develop communication algorithms and fundamental limits for blue-green band underwater optical links where RF propagation is severely limited. We will develop novel channel models, MIMO architectures and hybrid optical/acoustic communication links. Finally, we will develop energy efficient optical wireless systems that leverage existing LED lighting infrastructure. These visible light communication (VLC) systems form an important piece to enable an ultra-dense internet-of-things (IoT) where radio links are limited due to interference. We will develop novel prototypes, energy-aware uplinks and algorithms to improve the spectral efficiency of such links. This research program has the potential to provide impactful contributions given that there are few alternative technologies to provide connectivity in these challenging scenarios. In addition, the outcomes of this research program are ideally tuned to a Canadian context. For example, the use of small satellite clusters with laser communication links to extend the reach of geostationary orbiting satellites can serve as a key tool to monitor our vast Arctic or provide broadband to the peoples of the north. Given that Canada has a huge coastline rich in natural resources, underwater optical links are essential to enable security applications and marine-life-friendly data links. Visible light communications is essential to dense indoor IoT deployments and fits into Canadas tradition of innovation in communication networks. This project will provide Canada with the knowledge and HQP necessary to lead the commercialization of this next-generation broadband access technology.""640901,""Hranilovic, Steve"
"617764"	"Huang, Changcheng"	"Design and Development of New Mechanisms and Algorithms for Service-Centric Networks"	"We have witnessed the phenomenal growth of the Internet in the past two decades. This growth is increasingly driven by the introductions of new applications such as P2P computing, the Internet of Things (IoT), IPTV, online gaming, social networking, and web searching. While these new applications typically require contents being replicated and distributed over different locations, users are also becoming more mobile and multi-homed with widespread adoption of smart phones and tablets. These demands for mobility and multiplicity cannot be efficiently supported with the current core of the Internet, which is a host-centric environment that provides point-to-point connectivity between fixed attachments. Recent success on cloud computing has inspired the enthusiasm for virtualization technology. Virtualized appliances can be instantiated dynamically over shared commodity-based hardware infrastructure, leading to lower cost and customized services. While there are many benefits with virtualization technology, how to support mobility and multiplicity under virtualized environment is the top issue that needs to be addressed. Our short-term goal is to design and develop some key new mechanisms and algorithms that can enable virtualization technology to support mobility and multiplicity. In specifics, we propose to create a novel domain-dependent namespace with associated protocols and mechanisms for supporting multiplicity and mobility and develop innovative algorithms that can facilitate dynamic resource sharing and improve system performance while accommodating multiplicity and mobility. This proposed program is part of our long-term efforts to design and develop new mechanisms and algorithms for transforming the Internet into a service-centric network that can enrich peoples daily lives at large with reduced costs and numerous new services. It will provide unique and extensive training opportunities for highly qualified personnel and help Canada maintain leadership in information technology. ""624014,""Huang, Danny"
"637433"	"Huang, Weimin"	"Further Investigation of Ocean Remote Sensing Using High-frequency and Microwave Radars"	"During the past four decades, both high frequency surface wave radar (HFSWR) and microwave nautical radar (MNR) have become important tools for ocean remote sensing. HFSWR is able to provide sea surface current, wind, wave and target information over the horizon due to the strong interactions between the HF (3-30 MHz) signal and the ocean gravity wave field. Increasingly frequent marine activities (e.g. transportation, offshore oil and gas exploration) result in the necessity for installing such radar systems on floating platforms (e.g., a ship or an offshore oil platform). However, a major challenge is that the platform motion will deteriorate the radar performance by distorting the radar Doppler spectra. Another challenge to the HF radar applications is contamination due to ionospheric clutter. To improve the utility of HF radar, both problems must be addressed. Although X-band MNR, which operates at around 10 GHz on land or ship, is a sort of line-of-site sensor, it can image the sea surface with relatively high spatial and temporal resolutions. Unfortunately, its performance in sea surface sensing will be negatively affected by rain. In order for this widely deployed system to provide real-time information under all weather conditions, countermeasures for rain-contamination need to be sought. The proposal aims to solve the above challenges by developing both theoretical models and application algorithms. The theoretical models are expected to provide an improved understanding of radio wave scattering from the ocean surface, while the application algorithms will bring significant assets to both HF and MNR radar communities and can be integrated into commercial radar products that may be exported internationally. A long-term goal of this research is to build a broad coastal ocean observation network that consists of HFSWR, MNR, Global Navigation Satellite Systems (GNSS) and Synthetic Aperture Radar (SAR) in Eastern Canada and Arctic regions. Such a network will improve marine safety through providing high quality sea surface data products to meet the ongoing needs of the Department of National Defence, Fisheries and Oceans Canada, the Canadian Coast Guard and other marine stakeholders such as the shipping and oil and gas industries. With financial support from NSERC, Memorial University will continue to train highly qualified personnel to augment the radar ocean remote sensing capacity of Canada and maintain its status as one of the leaders in this field.""640895,""Huang, Weimin"
"637659"	"Ilow, Jacek"	"Interference Management in MIMO Wireless Networks"	"With the growing demands for wireless communication services to support higher bit rates, there is an ever-pressing need to increase bandwidth efficiency of communication systems through the reuse of the same spectrum by simultaneous multiple transmissions. To address this and many other issues, one of the most successful and efficient solutions is the deployment of Multiple-Input Multiple-Output (MIMO) technology. However, the multi-user (MU) MIMO systems are often designed to use non-optimized methods to deal with the interference limiting the network capacity. As a result, this proposal seeks to design signal processing strategies and wireless network protocols that manage interference in MU MIMO systems to achieve the highest network throughput. Conventional approaches to deal with interference are (i) to avoid interference by non-overlapping transmissions in time or frequency domains, (ii) lump other transmitters signals as noise and mitigate it through power control, or (iii) deploy multi-user decoding. In MU-MIMO systems, where the spatial signaling dimension is utilized to serve many users in parallel, there are opportunities for shaping the interference through linear pre-coding at the terminals and power control to minimize the effects of the combined interference at the destinations. In this proposal, we seek to develop strategies to deal with interference in MU MIMO systems by integrating signal alignment for few strong transmitters and interference cancellation for non-coordinated transmissions. Specifically, by a thoughtful design of pre-processing vectors and scheduling of transmissions, we expect that the signals with relevant information will be aligned into proper spatial dimensions according to the selected coding strategies. The transmission schemes will be also optimized to reduce computational complexity and to improve time-frequency utilization. The scope of this work will cover single-hop, multi-hop, and multi-way networks. In multi-hop systems, where a source and destination communicate through one or more other nodes that act as a relay, we plan to exploit the concept of physical layer network coding to aid the operation of the wireless network with the nodes distributed in space according to the Poisson-point process. Our focus here is on medium access (user selection) opportunities for power allocation, with emphasis on the topology control and routing-level issues for inter-relay interference cancellation. We expect here to deploy for analysis the tools from stochastic geometry which computes macroscopic properties of wireless networks, by averaging over all potential geometrical patterns for the terminals. The challenge in this work will be to evaluate the gains from the deployed signal alignment techniques where all terminals may not be synchronized or fully cooperating.""635467,""Ilten, Nathan"
"634724"	"Inkpen, Diana"	"Learning from Social Media Texts"	"Applications in the field of Natural Language Processing (NLP) have become popular in recent years. This is due to the availability of data and evaluation benchmarks, to progress in automated techniques, and finally to an increased need for these applications in our daily life and in commercial product development. I propose a thorough investigation of NLP-based techniques for user modelling in social media. There are three specific objectives: (1) Learn user characteristics from social media texts. These characteristics may include: age, gender, personality type, location, ethnicity, health issues, political views, interests, and life events. (2) Learn population distributions in various social media (e.g., Twitter, forums, Facebook) for each of these characteristics. (3) Use the extracted information, as a proof of concept, in applications for e-business, market research, health monitoring, etc. The scientific approach that I propose is based on machine learning, automatic text classification, deep learning, and information extraction techniques. We will focus our attention on social media texts, which are more challenging than regular texts due to non-standard spelling, lack of editing, abbreviations, jargon, and noise, The techniques need to be adapted to this kind of text. The various ways of adapting them include retraining, partial normalization of the messages, and adding features specific to each type of social media. In addition, I propose to integrate techniques that exploit the structure of the social network. Most of the previous work on related topics uses either only the texts of the messages or only the network structure. I believe that combining them may lead to an increase in the precision of the extracted information. No matter how detailed the study, we will also pay special attention to protecting the privacy of social media users. The novelty of the proposed work consists in a comprehensive investigation of the existing techniques, in increasing their sophistication and in developing new techniques for the proposed tasks, as well as in the development of several proof-of-concept applications that require information about users or about populations of users in social media. We anticipate that the outcomes of the project will contribute to a better understanding of human communication in social media, which will allow advances in mining the social media for market research purposes and other decision-making applications, and will strengthen Canadas position as a major player in the field of information technology.""618878,""Inkpen, Diana"
"637401"	"Ionescu, Dan"	"Next Generation Real-Time Methods for the Supervision and Contol of Industrial Environments"	"The objectives of this research application are to investigate selected theoretical and experimental topics related to the design and implementation of specific components of the next-generation of Information Technology Infrastructures for supervisory control of enterprise processes. It is common knowledge that Information and Communication Technologies are the central piece of the new digital economy due to the commoditization of computer and networking technologies. Recently, the World Economic Forum predicted that the new digital economy will be the major industrial component in any developed country. More and more, applications and data are migrating away from customer premises and into cloud environments. Reports show that an average Cloud Computing infrastructure consumes the same amount of energy like 25,000 households. The advancements made in past years in the area of distributed and cloud computing have led to the production of massive software technologies, hosted by large and extremely costly data centres, while the green computing movement promotes improving cloud efficiency in regards to the energy consumed. Despite sustained research efforts, a formal model of the dynamics of the energy consumption in Cloud Computing infrastructures, due to the computing, storage and networking resource utilization, is still lacking. We propose to investigate the modeling of resource allocation processes in Cloud Computing by Stochastic Differential Equations systems. This new stochastic model will be devised by analyzing large amounts of non-stationary cloud data for model identification. Because the manual effort needed to control a growing cloud computing infrastructure is extremely high and inefficient, making the Cloud unmanageable, we will devise an Autonomic Computing environment, protocols and control algorithms based on the new Stochastic Differential model to control cloud resources. This system will implement self-optimization, self-provisioning, and self-learning algorithms, which mimic human operator decisions and interactions with the cloud. The Autonomic Computing for cloud resource control will manage optimally the cloud resources in real-time reducing the computational power and the energy consumption. We propose to investigate the devising of complex Autonomic Computing system implementing intelligent, local and global controllers for clouds computing self-governance. We will also produce adaptive algorithms for the cloud controller to implement self-organizing strategies in order to optimize virtual servers usage. A self-awareness feature will provide information about the state of resources and the resources it links to. A light prototype will be implemented on the cloud hosted in the applicants laboratory. Recent advances on Software Defined Networking and Software Defined Services will be used for prototype implementation.""643225,""Iordanova, Mihaela"
"637448"	"Isleifson, Dustin"	"Technology Advancement for Microwave Remote Sensing of Sea Ice"	"The main objective of my proposed research program is to advance the state-of-the-art in remote sensing technologies for the engineering and scientific communities to understand the geophysical and thermodynamic characteristics of sea ice. Research will be undertaken in three areas: 1) Sea Ice Remote Sensing Experiments for Detection of Oil in Sea Ice, 2) Electromagnetic Modeling of Sea Ice Scattering, and 3) Development of New Measurement Platforms for Arctic Remote Sensing. Satellite remote sensing has historically been used to monitor the Arctic; however, lingering uncertainties remain in our knowledge of how rapid changes in sea ice geophysics and thermodynamics affect the satellite-scale scattering. As transportation through the Arctic increases in response to reduced ice conditions, compounded by plans for expansion in offshore oil exploration, drilling, and production, we have an increased risk of an oil spill in this fragile environment. Canada has a responsibility to ensure that we are in a position to detect and mitigate the risk from such an environmental disaster. Research in remote sensing is required to develop the fundamental understanding of radar wave interactions and to advance detection tools and methods. In order to interpret radar data, I rely on electromagnetic models to describe how microwaves interact with the complex sea ice medium. New modeling methods will help me to understand the mechanisms of the wave interactions and can be used to develop techniques for detecting oil that is trapped underneath sea ice. From the results of my modeling studies, I will design, build, and test remote sensing payloads to be integrated onto unmanned aerial vehicles (UAVs) with antennas that are tuned for detection of specific parameters, such as oil in sea ice. The expected major research outputs of this program will be an enhanced understanding of how radar can be used to determine sea ice geophysical and thermodynamic state, development and validation of innovative electromagnetic modeling techniques, and improved methods for detection of oil in sea ice. Antennas will be simulated, built, tested, and integrated into novel measurement platforms for remote sensing applications. Discoveries in the areas of Arctic remote sensing will help maintain Canadas status as a leader of research in technological, scientific, and economic issues related to sustainable development in the north. This research program builds on a collaborative research effort between the Faculty of Engineering and the Centre for Earth Observation Science (CEOS) at the University of Manitoba, supported by partnerships with industry. Hardware research will be conducted in the Department of Electrical and Computer Engineering, while remote sensing research will be performed at the Sea-ice Environmental Research Facility (SERF) and at the new CFI-funded Churchill Marine Observatory (CMO).""625264,""Ismail, Ameen"
"634665"	"Jacobson, Alec"	"Robust Geometry Processing for Big Dirty Data"	"Geometry processing--an extension of signal processing--interprets three-dimensional curves and surfaces as signals. Just as audio and image signal data have exploded in availability, geometric data is massively abundant. We encounter geometric data everywhere: depth scanning guides safe and effective self-driving cars, anatomical curves or surfaces enable medical visualizations and soon robotic telesurgery, and 3D printing brings customization of geometric design to the masses. However, geometry processing has been outpaced by the data. Unlike with audio and images, we are not seeing the full application of big data analytics and modern machine learning techniques to geometric data. The core roadblock is lack of robustness in our tool chest of geometry processing techniques. Geometric data is corrupted with noise, ambiguity and inconsistency. Unlike the regular pixel grid of an image, discrete geometric representations are a zoo with trade-offs in terms of efficiency, accuracy and scope. Conventional geometry processing is a direct application of continuous mathematical concepts. Its assumption of pristine input surfaces is prohibitively strict and applications suffer. Objectives Over the next five years, I will bring geometry processing up to speed with modern geometric data. I will pursue three parallel but complementary tracks: 1. Robust algorithms for recovering structure from unstructured, noisy geometric representations; 2. Robust mathematical and algorithmic foundations for solving partial differential equations (PDEs) on real-world geometric data; and 3. Robust user interfaces for direct manipulation and creation of geometric data. While solutions uncovered along the way will have immediate practical implications, the long-term impact is a multiplication of progress across all tracks. The combination of successes will unlock machine learning to geometric data, just as robust image processing has done for image data. Scientific Approach My general scientific approach is to identify and eliminate unnecessarily strict expectations of ""cleanliness"" in input data. Often this means returning to first mathematical principles and adapting traditional definitions or concepts to accommodate problems witnessed in real-world data.""642677,""Jacobson, Alec"
"634471"	"Jamali, Nadeem"	"Models and Mechanisms for Programming Interactions in Concurrent Systems"	"Although concurrent programming is more accessible to programmers today than ever, programmingof interactions between processes continues to be challenging. These interactions can be competitive or collaborative. The programming of these interaction concerns is oftenintermixed with the programming of the functions of individual processes. Proposed work will buildformal models and mechanisms to support programming of interactions in concurrent systems. In the recent past, we have addressed the problem of resource-competitive interactions between computations. We have developed fine-grained mechanisms -- based on my CyberOrgs model -- for coordinating computations' access to owned resources. These mechanisms have been used for supporting deadline assurance, for power-efficiency in multi-core processors, and for superior load-balancing for non-uniformly sized tasks. The general approach was also applied to broader contexts, such as for enabling multimedia viewers to negotiate advertisers' access to their attention, or users of email to negotiate access to their mailboxes. Most recently, we have also addressed collaborative interactions. In one effort, we identified key mechanisms underlying crowd-sourced applications, which offer services based on data collected from a ""crowd."" After prototyping these mechanisms in a middleware, which significantly reduced the effort involved in developing new applications, we are now formally studying important properties of such applications, and generalizing our results to the wider class of mobile distributed sensing-based applications. In parallel, we have studied complex inter-process communications. Although there is a growing body of work to separate these concerns, existing solutions create static structures to support protocols which enable desired interactions and/or restrict undesired ones. In contrast, the approach we take is to have first-class, self-driving, dynamically evolving communications, which are reusable and composable. This research program presents many opportunities for training of highly qualified professionals. Past graduates have foundexcellent positions in both industry and academia. Students will learn cutting-edgeapproaches for programming parallel and distributed systems, and particularly using languages andlibraries based on the increasingly influential Actor model of concurrent programming. The work on separating communication concerns promises to ease the programming of complex communications, and make communication code reusable. The solutions for crowd-sourced applications can democratize the creation of new services. Resource coordination work in these domains will enable provision of communication and service mechanisms as services. Developing support for resource coordination in languages like Scala will make them available to a large number of programmers.""634177,""James, April"
"621720"	"Jean, Sbastien"	"Qubec"	"CANADA"
"637592"	"Johns, David"	"Digital Assistance for Nanoscale Analog Circuits"	"During the past 40 years, there has been a well-known digital revolution that has made a tremendous impact in how we live, work and play. With digital transistors shrinking as predicted by Moores law, year over year integrated circuits and systems have continued to become lower in cost, higher in speed and lower in power dissipation for the same computation. What is less well known is that during these 40 years, there has also been an analog revolution that has been critical in improving the performance of electronic systems. These analog integrated circuits serve as the interface between the digital world and the physical world and without performance improvements in the analog circuits, the technology revolution we have seen would not have occurred. A recent review of analog circuit advancements has shown that over the past 10 years, there has been a 60 times improvement in performance. While much of this improvement (10 times) is due to transistors being smaller and faster, a large factor (6 times) has beendue to creative circuit innovations which is the focus of this research grant. With the above in mind, a number of questions arise. Given a technology node, how close are analog circuits to the best performance achievable? Can we continue to improve analog circuits and how is this best achieved? With the tremendous improvement in digital circuits, can we make use of their decreased size/energy to aid the performance of analog circuits? These questions drive this research in the area of analog integrated circuits. This research program is driven by the desire to enable analog circuits to be more power efficient and have higher performance when applied to numerous applications such as MEMS interface circuits, RF front-ends, high-speed signaling for serial digital communications and others. The challenge is to develop new circuits, architectures and theory that improve analog circuits and through experimental results, demonstrate the practical advantages that can be obtained.""621099,""Johnson, Amanda"
"634932"	"Jurgensen, Helmut"	"Codes, Automata, Formal Languages"	"Fundamental problems of computer science centering around codes, information encoding and decoding, and information processing even in the presence of errors are considered. In this general context, an information transmission system can be as simple as a wire and also as complicated as an abstract computing machine, even a machine which relies on uncomputable oracles. I investigate the influence of codes, encodings and decodings on the degree of computability in this very general context. For codes themselves, the main issues I plan to pursue are the error-handling capabilities of certain codes for unusual, but technically relevant error models and the relativization of codes to only the likely output messages. For automata, I study the feasability and power of computing models, including computing devices based on chemical processes. One key issue beyond modelling is the influence of input encoding and output decoding on the power of such unconventional models of computation and, far more generally, their influence on the computational power of arbitrary information transmission systems. In rewriting systems, like grammars or Lindenmayer systems, for formal languages, the information processing capabalities are hidden in the re-writing rules. They become apparent in the informational dependency between parts of the words which can be derived by the rules. My concrete long-term and short-term goals include: (1) model certain kinds of common, but not usually considered, types of errors using transducers; (2) determine the error-handling capabilities of various types of codes with respect to these error types; (3) examine the capabilities of codes under relativization to probable output message spaces; (4) develop a general framework for the complexity of computations which takes into account the encoding and decoding of information and which does not depend on any specific model of computation; (5) investigate the connection between information processing in rewriting systems and the generative capability of such systems; (6) determine the power of modularized grammars, like millstream systems, to model natural languages. ""635043,""Jurisica, Igor"
"634857"	"Kabanza, Froduald"	"Advanced Algorithms for Plan Recognition"	"Artificial intelligence (AI) presents the potential to profoundly transform our society with various applications that will positively affect the economy and our everyday life. Intelligent automated agents will soon be driving applications in many including, transportation, social media, game AI, educational tools, intelligence analysis, defense and security. The ability of these agents to effectively support and naturally interact with people will largely depend on their proficiency to recognize the goals and plans of other agents. The problem of recognizing the goals or plans of other agents is known as the plan recognition problem or the intent recognition problem. While interesting advances have been made over the years, today plan recognition algorithms are too ineffective for many applications. The objective of this research program is to explore, improve, and develop plan recognition algorithms that can cope with complex applications. The research avenues articulate around automated planning, algorithmic game-theory and machine learning.""633487,""Kabin, Konstantin"
"634603"	"Kahl, Wolfram"	"Towards  Mouldable Code as a Better Approach to Synthesis of Efficient and Correct Software"	"Important parts of the supporting infrastructure for optimising code generators, in particular in compilers, consist of analyses of graphs , especially control-flow graphs and data-flow graphs. In addition, many of the optimisations enabled by these analyses are usefully understood as graph transformations. Interestingly, the program transformation literature almost exclusively concentrates on transformation of (higher-order) abstract syntax trees . This corresponds to the usual approach in a compiler context to first parse the given programs into abstract syntax trees, and then extract from these the necessary information to construct the graphs to be used for data-flow and control-flow analyses, while still considering the abstract syntax trees as the internal representation of the program. However, many of the transformations that are used for code optimisation, in particular in the back-ends of compilers, act on patterns that can usefully be thought of as graph patterns, and the resulting transformations are frequently explained in the literature as graph transformations applied to control-flow graphs and data-flow graphs. In this research programme, I aim to create the theoretical justifications for control-flow graph transformation and data-flow graph transformations by linking the theories of control-flow and data-flow semantics with appropriate theories of graph transformation and build on these foundations a mechanised framework for nested code graph transformation. The goal of this framework is to be the first representative of a new class of mechanised environments in which experts can design special-purpose optimisation passes in a graph-based formalism that captures the intuitive graph-based descriptions of optimisation passes as they are customary in the compiler literature. However, while the graph-based descriptions in the literature are technically completely informal, the envisaged mechanised system will support not only capturing the design itself, but also proving the correctness of these optimisations at a level that is conceptually close to the design, and will finally also automatically generate correct-by-construction implementations of these optimisation passes.""621825,""Kahr, Erin"
"634790"	"Kargar, Mehdi"	"Efficient and Effective Search over Graph-like Databases"	"Much of the worlds high-quality enterprise and social data are stored as semi-structured and structured data. This includes enterprises RDBMSs, knowledge graphs, and social networks. All these data collections either are defined already as graphs or can be re-modeled as graphs. Over the past decade, we have witnessed advances in storing graph-like databases, but we have not seen much progress in search over them. As Surajit Chaudhuri (a distinguished scientist at Microsoft Research) addressed in his keynote talk at ICDE in 2015, search over graph-like databases has fallen behind search over unstructured data. While scientists and business users look for exciting, actionable discoveries from their heterogeneous datasets, the need to provide effective search is profound. In this proposed research, we focus on designing effective and efficient methods to explore graph databases. We address important problems, challenges and opportunities for improving knowledge exploration over graph-like databases. These issues arise due to the complexity, scale and massive heterogeneity of such data. First, we tackle the problem of finding relevant answers to search over heterogeneous graphs using the keyword search paradigm. Real graphs (e.g., social networks) are heterogeneous and model various types of entities and relationships. In these graphs, each node is associated with an importance value corresponding to its semantics. Previous work ranks answers using a combination of structural and content-based metrics, and ignore the type and importance of nodes. By incorporating the importance of nodes into account, we propose efficient algorithms to find relevant answers for the given query. Second, we design new algorithms to answer distance queries (i.e., finding shortest distance between any pair of nodes) over weighted graphs based on a graph indexing method called 2-hop cover. We investigate how graph partitioning can be applied to build the index and how to efficiently update the index over a stream of graph data. Third, we investigate the problem of identifying a users intention when searching over knowledge graphs. Most of the current work in this area focuses only on finding answers quickly rather than finding more meaningful answers. We investigate the problem of finding a keywords role to improve search quality. The results of this proposed research will be useful for Canadian and international businesses and government institutions. The proposed frameworks can be used by financial (e.g., TD Bank and stock market), healthcare, governmental institutions (e.g., Statistics Canada), and technological companies (e.g., IBM and Microsoft). Our program will train students in the databases and data mining area to place them in a strong position when applying for academic and industrial jobs. I expect up to twelve students (including undergraduate students) to be trained in this program.""640117,""Kargar, Mehdi"
"634808"	"Kari, Lila"	"Foundational aspects of bioinformation and biocomputation"	"In the same way we use the letters of the alphabet to write text, and bits 0 and 1 to write computer machine code, the four basic DNA units(A - adenine, C - cytosine, G - guanine, T - thymine) are used by Nature to write genetic information as DNA strands. The possibility of encoding symbolic informationon DNA, and the fact that biochemical processes such as cut-and-paste of DNA strands have been proved to be able to perform arithmetic and logicoperations, led to the development of the field of DNA computing and molecular programming. Bioinformation and biocomputation are different from their electronic counterparts in several aspects. First, biodata is not associated to a memory location but consists of infinitesimal DNA strands free-floating in solution. Second, in contrast to data in an electronic computer, which is passive, data-encoding DNA strands can interact with each other in programmable ways due to their Watson-Crick complementarity. Third, each data encoding DNA strand is usually present in millions of identical copies, and the bio-operations operate according to statistical laws. I aim to develop and investigate mathematical models of bioinformation and biocomputation that take into account such specific characteristics, as well as explore mathematical properties of naturally occurring DNA sequences, and their applications. To this end, I approach the issue of data encoding on DNA by proposing to develop a formal-language-based ""theory of bioinformation and biocomputation"" . This includes defining and investigating new concepts that capture the biological reality of DNA- and RNA-encoded information, as well as investigating properties of bio-operations, and their relationships to traditional models of information and computation. Besides its potential significance for the design of programmable DNA-based computational devices, the impact of this research is that it creates a mutually enriching link between theoretical computer science and molecular biology. Secondly, I propose an investigation into DNA self-assembly as a computational tool, the results of which could have potential implications for experimental DNA nanocomputations, and for the molecular programming of complex nanostructures. Lastly, I propose to gain insights into the mathematical properties of naturally-occurring bioinformation by investigating the connection between the syntactical structure of genomic sequences and species classification. This includes investigating Chaos Game Representations of DNA sequences as genomic signatures, as well as applications of this method to HIV-1 virus subtyping and to the classification of marine microbial eukaryotes based on their RNA transcriptomes. The potential impact of such an alignment-free universal classification method could be significant, given that 86% of existing species on Earth and 91% of species in the oceans still await classification. ""634809,""Kari, Lila"
"637042"	"KazemMoussavi, zahra"	"Modeling, Signal Processing and the Design of Novel Instruments and Algorithms Applicable to the Respiratory System"	"Using linear and non-linear signal processing and modeling techniques and instrumentation, this research program will investigate, design, develop and evaluate novel technologies applicable to respiratory system and its associated disorders, in particular obstructive sleep apnea (OSA). OSA currently affects more than 10% of Canadians, but it is also believed that there are many undiagnosed cases. Lack of diagnosis and treatment of OSA impose an immense physical and economic burden on society. The research proposed here will address this gap by investigating new ways to analyze and interpret biological data in particular breathing sounds, and design novel medical technologies that will have future applications in point of care diagnosis, treatment and monitoring of OSA; some of the proposed technologies will also have other general applications. The proposed objectives and projects are: 1) further develop and evaluate our current acoustic technology by considering the effect of confounding variables, i.e. age, smoking, height, weight, on breathing sounds and improve its sensitivity and specificity for OSA classification, 2) develop acoustic models for upper airway during both sleep and wakefulness, 3) study the relationship between breathing sounds and sleep stages continuously through an entire night, 4) design and implement an energy harvesting system using saliva to enable oral measurement of pulse oximetry overnight during sleep, and 5) develop micro-array microneedle electrodes by molding, along with a novel switching circuit for stimulating the hypoglossal nerve as a potential future non-invasive sleep apnea treatment. These objectives will be achieved through non-invasive tracheal respiratory sound analysis and design of microelectronic boards. Outcomes of this research are: to advance our understanding of the upper airway mechanism of normal breathing during both wakefulness and sleep and how it will change due to an obstruction; to develop non-invasive, cost effective and quick diagnostic tools as well as novel non-invasive treatment strategies. The main impacts of this work will be development of novel technologies that in future will improve the quality of life of the large population affected by OSA. ""637562,""Kazerani, Mehrdad"
"617759"	"Kerschbaum, Florian"	"Systems for Computation on Encrypted Data"	"The loss of control and the potential disclosure of private information to the service provider, malicious insiders or motivated hackers are seen as a barrier to the wide-spread adoption of cloud computing. Many recent scientific developments in encryption, such as homomorphic and functional encryption or secure multi-party computation, offer a viable solution by encrypting the data before sending it to the cloud. However, existing software the vast majority of code is too difficult to retrofit with these encryption techniques and does not benefit from the theoretical advances. I will overcome this challenge by researching new methods that preserve existing programming interfaces and languages, such as Java or SQL, but compile to versions running on strongly encrypted data. Existing methods from the systems community, such as CryptDB or JCrypt, are capable to execute existing programs, but use weak cryptographic mechanisms, e.g. deterministic encryption, and existing methods from the security community, such as dynamic symmetric searchable encryption or AutoCrypt, use provably secure methods, but significantly lack in functionality, e.g. not allowing range queries. I aim to bridge those two approaches in computer science. On the one hand I aim at rigorous formal models of security clearly capturing the implied leakage and on the other hand I aim at engineering testable systems that can practically and experimentally verify the theoretical performance gains. This requires the following advances beyond the state-of-the-art: researching 1) new formal security models that capture the intended functionality and security, developing 2) improved cryptographic mechanisms and systems algorithms for these models and finally 3) proving security and correctness in these models. I will target important, established programming interfaces, such as SQL and Java, which are very hard to change. For SQL I will develop improved searchable encryption schemes which have no leakage and corresponding formal models that capture the impact of leakage-abuse attacks (PhD student #1). Additionally I will develop query processing in secure computation using data-oblivious algorithms (Master student #3). For Java I will develop improved methods for verifiable homomorphic computation (PhD student #2) and develop knowledge inference for the malicious model (Master student #4). I will integrate the methods developedinto application servers and databases for an exemplary application (Master student #5). I will have been successful, if at the end of the proposal we have the scientific foundation to run existing, real-world, large, interactive applications in the cloud on encrypted data. The proposed work is of value to Canada by training HQP in computer security, demonstrating scientific excellence in a very active research field and aiming at a break-through of high industrial relevance in cloud computing. ""634526,""Kerschbaum, Florian"
"634666"	"Kersten, Marta"	"Novel display, visualization and interaction paradigms for enhanced understanding of complex, multidimensional medical images"	"Over the last decade, significant advances have been made in the fields of display technologies, computer graphics and interaction devices. This progress has opened up new possibilities for visualizing and interacting with data and has necessitated new paradigms that allow for intuitive understanding of computer-rendered images using state-of-the-art technologies. The following research program entails the development of new visualization techniques for complex multidimensional data and novel methods for viewing, displaying and interacting with 3D data in augmented and virtual reality environments. The main source of data used to test the developed techniques will come from the field of image-guided surgery. In image-guided surgery a navigation system guides the surgeon by displaying the position of real surgical tools with respect to a 3D anatomical virtual model of the patient anatomy. This is not unlike a cars GPS (global positioning system), which relates a car's position in real space to a virtual representation of it on a map. A recent survey of image-guided surgery systems showed that there has been a lack of focus on how to best visualize, display and interact with anatomical patient data. The result of this is that new technologies have had a very limited impact on clinical practice. For example, clinicians continue to rely on 2D planar reconstructions of 3D anatomy even though these do not translate well to interventional tasks that require a 3D understanding of the anatomy. Rather than using projective methods and new display technologies, clinicians continue to rely on computer monitors, which require them to look away from the patient for guidance thereby disrupting the surgical workflow. Furthermore, surgeons do not directly interact with image-guided surgery systems but require the presence of technicians in the operating room to make adjustments to visualization or view parameters. Similarly, diagnosis and treatment planning rely on 2D planar images rather than the use of novel display techniques and volumetric visualization methods. To address these limitations, the purpose of the proposed research is to develop novel methods that take advantage of the most current graphics cards, display hardware and interaction technologies in order toenhance medical image interpretation for improved diagnosis, planning and surgical treatment. Although focused on the clinical domain, the algorithms and solutions developed through this research program will be applicable to other domains such as art and museology, heritage, gaming, and aviation, and in general to the fields of data visualization, human-computer interaction and human-factors engineering. This program will not only provide news ways of visualizing and interacting with complex multidimensional data but also investigate the capabilities of the human visual system in understanding computer-generated images.""632712,""Kerton, Francesca"
"637261"	"Kim, NaYoung"	"Engineering Room-Temperature Exciton-Polgritonics"	"The demand for the rapid and secure transfer of information and for increasingly complex computational power continues to grow as societys reliance on technology increases. To develop multi-functional systems to meet these pressing demands, researchers are pursuing the creation of energy-efficient compact photonic and optoelectronic devices governed by novel physical processes in materials and nanostructures. Through the proposed program, we will engineer exciton-polaritonic devices as a scalable solid-state photonic platform. Exciton-polaritons have a dual light-matter nature, resulting from their strongly coupled cavity photons and quantum-well excitons. The matter portion bestows strong nonlinearity and spontaneous coherence arising from particle-particle interactions, and the wave portion bestows ultrafast propagation arising from its extremely light mass. These properties enable us to build ultralow power light sources and optical logic devices, with a predicted 10-100 fold power gain compared to traditional devices. The innate planar structure supports integration of other components for scalability. We aim to develop room-temperature (RT) exciton-polaritonics that offer low power consumption, small physical footprint, pristine single-mode quality and increased output power; inherent features that arise from the quantum Bose nature. RT operation eliminates the need for expensive cryogenic equipment, low power operation is required for building extremely dense integrated systems, and increased output power enables cascade and parallel processes for reaching expanded levels of computational complexity. Our approach will be to investigate transition-metal dichalcogenides as a new material candidate for RT operation because excitons in this class are stable against RT thermal energy due to a huge exciton-binding energy. We will design and fabricate suitable microcavities, in which these new materials will be embedded to form stable RT exciton-polaritons. The successful demonstration of these devices, realized through this program, will advance progress in optical communication and data storage, optoelectronics, and integrated photonic circuits with polariton-based light emitting sources, switching transistors, and sensors. Our devices will also serve as a testbed to study exotic quantum phases such as Bose-Einstein condensation, superfluidity and vortex formation at RT. Engineering and explorative aspects of the proposed research will furnish an exceptional educational platform, through which HQP will internalize comprehensive understanding of fundamental physics and acquire state-of-the-art optical and electrical techniques. This will leave them well equipped to excel as leaders in sectors that will become increasingly important to Canadas economy, such as quantum optical computation and communications.""628189,""Kim, Nicholas"
"634425"	"Kiringa, Iluju"	"Managing Data for the Internet of Things: Storage, Updates, and Transactions"	"Consider the following real-world use case presented in The Economist in June of 2016. Tom Rogers, an almond farmer in California Central Valley, must pay high costs by constantly monitoring many variables such as moisture, nutrient content, health threats, and temperature, and by taking appropriate actions to control the levels of the values of these variables. Tom wants to reduce operating costs by improving overall utilization of water and fertilizers needed to grow the water-thirsty almond nuts. Tom also wants to access real-time information about the state of his farm that he may use in making decisions to achieve the desired cost reductions. Toms problem is a perfect example for using a so-called smart farming application, an instance of a growing number of applications spanning domains as diverse as agriculture, transportation (asset management), housing (smart buildings), and infrastructure (smart power grids). What is common to these applications is the increasing use of the Internet of Things (IoT) to monitor operations in order to achieve the goal of reducing overall operating costs and making accessible real-time data about the state of the monitored entities. IoT is a paradigm of networking where the networked points (called things) are uniquely identifiable devices equipped with embedded connectivity. To achieve his stated goals, Tom installed wireless sensors throughout his nut plantation to measure the aforementioned variables. The measurements are then periodically sent to dedicated third party cloud-servers which compute the appropriate levels of water and fertilizer that should be applied to the plantation. Finally, the results are used to automatically drive the farms irrigation system by delivering precise amounts of water and fertilizers to the nut groves. The data archived on the cloud database is also processed by analytics applications. Our proposed research focuses on the design and implementation of storage, update, and transaction models for managing IoT data that flows from sensors to a cloud gateway, to a cloud database, to applications, and back. In the short term, we will investigate the architectural choices for the management of IoT data, and study data management (e.g., stream cleaning and fusion, and complex event detection) at the cloud gateway level by considering the IoT data requirements of gigantic volume, velocity and variety. In a longer term, we will study architectures for storage models of the cloud IoT distributed databases, as well as appropriate update and transactions models for IoT data management, and develop a prototype system. We anticipate that this research, which will enjoy the collaboration of a fortune 500 company as well as several renowned database and machine learning experts, will be significant with respect to new directions in the database industry, to IoT applications, as well as to the improvement of the IoT technology.""637029,""Kirk, Andrew"
"637422"	"Kirubarajan, Thia"	"Robust State Estimation in Uncertain Environments Using Point Process Models"	"The objective of state estimation is to mitigate the effects of noise in sensor measurements and extract the fixed or time-varying parameters of an object of interest using certain system and measurement models. Noise mitigation is necessary not only because no sensor is perfect, but also because our knowledge or model assumptions about any unknown system and its parameters are imprecise. The estimator considers the model uncertainties and noise statistics in order to optimally estimate the parameters of the subject of interest to some optimality criterion. While state estimation typically considers only the effects of system (or model) noise and measurement noise, in estimating the state of a moving object over time, target tracking considers additional measurement-origin uncertainties due to missing detections, false alarms, and interference from other objects of interest. In target tracking, the objective of state estimation is then to mitigate the effects of model and sensor noise and those of measurement-origin uncertainties. With the emergence of affordable sensors (e.g., cameras, sonobuoys, satellite receivers), sensor processing with the objective of state estimation and target tracking has become common. The ubiquitous and affordable nature of these sensors results in additional uncertainties that have not been addressed properly in the literature to date. In sensor processing where expensive radar systems with only one or a handful of sensors are used, systemic errors such as sensor biases, clutter, electronic countermeasures, and other interference have been effectively modeled and addressed. But, given the large number of heterogeneous sensors available, these additional sources of uncertainties have not been modeled or addressed optimally. This situation provides the motivation for the proposed work. Specifically, we will address the following problems: 1) mitigating and taking advantage of various environmental conditions to improve tracking results; 2) track-before-detect for low-observable targets in the presence of heavy clutter; 3) integration of state estimation with sensor management; and 4) constrained state estimation and prediction with the aid of uncertain external data sources (e.g., maps, terrain data). Our solution methodology is based on Point Process models and the Analytic Combinatorics (AC) formalism, which provide an efficient mechanism for working with a wide range of uncertainties in large-scale problems. To provide a comprehensive solution, we will model various forms of uncertainties that are internal and external to sensors, develop robust algorithms to minimize the efforts of sensors, and quantify the performance of the new algorithms using extensions to the AC formalism. In addition to advancing the state-of-the-art, the project will also produce a number of highly qualified personnel in areas of critical importance to Canada.""618755,""Kirubarajan, Thia"
"637220"	"Knights, Andrew"	"Materials and Device Development for Silicon-based Optoelectronics"	"The seemingly insatiable demand for increased bandwidth on communication networks is presenting the greatest technological challenges ever made on hardware and software engineers alike. Indicators of bandwidth demand include the number of devices connected to the internet and the total traffic within data-centres; both exhibit exponential growth. The former relates directly to the so-called Internet-of-Things (IoT) while the latter presents the stark reality of a roadmap with an inevitable trajectory, but for which solutions for realization are not known, or have yet to be implemented. This proposal will address the need for network hardware that is scalable and cost-effective, utilizing the powerful technology known as Silicon Photonics (SiP).At its most fundamental level SiP integrates electronic with optical quanta with remarkable performance gains; SiP will revolutionize data transfer at all distances. In the proposal I describean innovative program of research on materials and devices for SiP systems. The program is constructed as a strong training vehicle for graduate engineers. I will explore characteristics of defects engineered via ion implantation in silicon waveguides. Such defects find utilization (for example) in monolithic detectors. This is of particular importance at wavelengths around 2000nm where a new communication window is under development. I will subsequently fabricate a complete SiP platform for 2000nm and demonstrate an optical link operating at 25Gb/s. I will develop ion beam techniques which show promise for selective synthesis of germanium volumes in silicon for applications requiring strain engineering. Using silicon microring modulators I will expand my previous work on highly integrated WDM transceivers. I will explore methods for microring stabilization and post-fabrication trimming with the aim to dramatically reduce the associated power budget. I will expand on my previous work on dispersion compensation using silicon microring modulators to the use of compounded elements such as dual rings, or ring assisted Mach-Zehnders which will provide link-dispersion tuning. I will pursue the concept of sub-carrier modulation with the aim of realising data transfer rates approaching 1Tb/s for a single unit when combined with WDM. Tangible benefits to Canadians will include: a) strategic support for the Canadian SiP industry; b) direct training of HQP for a vital industrial sector c) realization of new SiP-enabled products and services. In a broad sense, this work will form part of the unfolding global information revolution which is experienced by Canadians who download information, take part in social networking, access information on their environment, require rapid medical diagnosis, and utilize consumer electronics.""642059,""Knights, Andrew"
"637150"	"Kocar, Ilhan"	"Development of accurate identification techniques, smart models and methods for future power systems"	"Electric energy is essential to progress and social well-being. It must be produced and delivered each time in larger quantities and under increasingly stringent conditions of reliability, sustainability, cost and environmental impacts. This forces a number of changes not only in the physical infrastructure of power systems but also in the way they are operated. In-depth simulation of power systems is required to identify the operational and safety limits. Furthermore, new simulation tools and models are necessary to account for new technologies in engineering studies. The future power systems will be dominated with power electronics based control, generation and transmission systems with special cable and transmission line configurations. There is an accelerated growth in the integration of renewable powered plants with power electronic circuits into the electric grid resulting in scientific and technical challenges. One major challenge is related to the accurate evaluation of short circuit contributions and development of models and methods for system protection and stability studies. Although Electromagnetic Transient Tools provide a precise platform to study transients in power systems, they are rarely used for protection studies due to performance and modeling effort concerns. Practicing engineers typically use phasor tools that have limitations in terms of modeling complex control and dynamic behavior of renewables. This program will establish new, accurate and adaptive approaches for dynamic studies compared to the existing methods and tools. This will be achieved by effectively using multiport network equivalents, adaptive line/cable models, reduced equivalents and adaptive renewable models. The renewables have significant impact on the behavior of power systems. This requires proper characterization of them and fundamental changes in conventional tools. This proposal will advance the knowledge in the field of power systems and provide a step forward in their simulation using adaptive techniques that manage accuracy either imposed manually or adjusted automatically. New identification methods will be developed in order to obtain a unified mathematical modeling frame that can adapt to different precision requirements. The identification methods will be used for the modeling of transmission lines, cables, network equivalents and transformers. Transmission lines and cables are key components in power systems and have significant impact on computing time and precision of power system simulators. The wideband modeling of these components is becoming more important with increased integration of power electronics based components in power systems, HVDC connections, long HVAC cable installations and wind parks connected through cables. Improved performance and precision of line and cable models will help improve efficiency of the system and reduce design costs.""638769,""Koch, Charles"
"637061"	"Koleilat, Ghada"	"Carbon Nanotubes: Building Blocks for Power Textiles"	"According to the US National Renewable Energy Rooftop solar energy has the potential to deliver 39% of the US energy demand using current solar technology; in fact, if we convert the radiant solar energy hitting the earths surface for only one hour, we would meet the worlds yearly energy demand. However, largely around the world, the present cost of standard photovoltaic modules based on crystalline silicon technology is not competitive enough to replace the dominant but slowly depleting fossil fuels resources. Canada needs to develop a solar technology that is at the same time efficient and low in cost. Furthermore, wearable technology in health monitoring, and entertainment have led to high-tech applications and are growing at an exponential pace. With todays society needs in mind, wearable devices must survive vigorous external stresses caused by flexing, twisting, stretching and exposure to random conditions. Rigid circuit boards in wearable technology are reaching the limits of their practical feasibility. Our research group is focusing on solution-processed semiconducting carbon nanotubes (CNTs). We propose to use this technology to address these two challenges using an innovative materials processing strategy which will allow CNTs to be used as building blocks for low cost high efficiency photovoltaics that can be fully integrated as a power source in textiles, i.e. power textiles. The properties of CNTs are highly dependent on the direction and the degree of twist they are wrapped in and are thus extremely tunable. CNTs can either be metallic or semiconducting, with their bandgap ranging from 0 to 2 eV. With just as little as 80 nm thick films, CNTs can absorb the entirety of the suns radiation spectrum. In addition to their exceptional electrical, and optical properties, CNTs possess unmatched mechanical properties. We believe that we are well placed to be the first to fabricate a highly efficient scalable carbon nanotube based solar junction. We also plan to leverage the mechanical compliance of carbon nanotubes for the first time to fabricate the first scalable textile-integrated photovoltaic junction, i.e. power textiles. This project will generate scientific insights on the potential of carbon nanotubes in light conversion applications and will lead to innovations in the field of solar energy and wearable electronics. It will support 7 highly qualified personnel who will be trained in an inclusive interdisciplinary environment with potential international collaboration.""621175,""Kolesar, Tiffany"
"635069"	"Kondrak, Grzegorz"	"Overcoming Data Sparsity in Machine Translation"	"Canada is a multicultural society. A large percentage of Canadian residents report a mother tongue that is distinct from either English or French. In addition, Canada is home to a rich variety of indigenous languages, some of which have also been granted official status. Everyone has the right to get all official federal government services, publications and documents in both English and French. Important information for new Canadians is often provided in multiple languages and scripts. Increasing the availability of texts in aboriginal languages increases their prestige, and thus helps preserve them. As a consequence, there exists an acute need for accurate and rapid translations, not only between English and French, but also into other languages. Human translation is slow and expensive, and requires highly-skilled experts. Computer translation programs, known as machine translation, have the potential to fill the gap. Unfortunately, the current technology is far from perfect. The quality of translations involving smaller languages is often poor, and even between major languages, it is sometimes inadequate for technical applications. Two of the reasons for the low quality of machine translation are the scarcity of bilingual texts for low-resourced languages, and the prevalence of infrequent words, such as certain verb inflections in French. The dominant statistical machine translation approach, which is used in web programs such as Google Translate, struggles to properly translate words that occur only rarely in bilingual texts. The objective of this proposal is to improve the quality of machine translation by improving the handling of infrequent words. The principal research directions are the incorporation of the state-of-the-art morphological techniques into the translation process, the development of lexicon induction methods, and the translation of out-of-vocabulary words based on the cutting-edge algorithms for cognate identification, name transliteration, and decipherment. In the current global economy, the enormous demand for fast and freely-available translations can only be satisfied by the machine translation programs. The solutions that I outline in my proposal will not only improve the quality of machine translation, but also influence the research on other aspects of natural language processing, thus accelerating the progress towards the goal of making computers understand human language.""622732,""Kondro, Douglas"
"637678"	"Kunz, Thomas"	"SDN-Enabling Multihop Wireless Networks"	"Multihop wireless networks (MWNs) have established themselves in a range of domains: vehicular ad-hoc networks, mobile ad-hoc networks, wireless mesh networks, wireless sensor networks, home networking, or even as extensions in cellular networks. The co-operative nature of the nodes and the absence of a fixed infrastructure make them very attractive where fast deployment, low cost, or support for strong mobility is required. Networking protocols for such networks have been designed with the assumption that nodes need to co-operate to achieve network-wide goals, with all nodes involved in operating and managing the network. At the same time, wired networks are seeing a paradigm shift towards Software-Defined Networking (SDN). SDN proposes a network architecture with a (logically) central controller that collects information about the whole network. This central entity makes network-wide decisions about routes and resource usage. The network routers and switches become much simpler and consequently cheaper. They are instructed by the SDN controller how to forward packets, removing the requirement for them to be involved in any network-wide decisions. The goal is to facilitate the introduction of new and innovative services, to simplify network management, to allow for better/more optimized use of resources, and to reduce the costs of offering communication services. This SDN paradigm is fundamentally different from the distributed network protocols prelevant in todays MWNs. However, SDN is quite promising for such networks as well. WMNs, for example, are severely resource-limited (bandwidth, energy, etc.). The central controller, having global knowledge of the network, can optimize resource usage network-wide. Yet efficiently supporting this paradigm over high-latency, lossy, and bandwidth-constrained wireless links is not trivial. In our research, we are exploring how to best deploy the SDN architecture in multihop wireless networks. The key idea is to apply various clustering approaches to reduce overhead, improve responsiveness, and provide robustness. We will demonstrate the advantages of our proposed designs in a number of ways. We will apply and evaluate popular SDN techniques such as traffic engineering or network slicing. We will also design and implement new global optimizations that address the unique challenges of MWNs. The research outcomes will simplify the deployment and increase the flexibility of future MWNs. It will enable us to apply many of the advances made in wired SDNs to MWNs. As a result, it will become easier for ISPs to offer rural mesh networks that provide Internet access in remote communities or for vendors to offer new services in home networks. Furthermore, managing MWNs via SDN will make it easier for telcos to integrate them as access networks with next-generation core networks that are similarly based on SDN. ""635602,""Kunze, Herb"
"637479"	"Labeau, Fabrice"	"Signal and Information Processing for distributed and networked applications"	"The power of signal processing as a discipline is that it develops the algorithms and techniques that form the backbone of todays Internet, cellular networks, or human-machine interfaces; it is constantly extending its reach to other domains, including healthcare, power and energy or social networks. This proposal is rooted in this tradition and proposes to examine several specific situations of application of signal processing to new and emerging areas, characterized by the distributed and/or networked nature of the signals or information considered. These areas include the Internet-of-Things (IoT), Healthcare and the Energy Sector. The proposed research program will tackle projects that are motivated by some of concrete problems in these application domains, but will also strive to provide contributions that can be extended to other application domains. This program will develop new technologies and algorithms (i) to enhance the lifetime of wireless sensor networks through carefully managed deployments of nodes and Radiofrequency Power harvesting; (ii) to enhance the security and resistance to cyber attacks of wireless data gathering mechanisms, including in the Smart Grid and healthcare; (iii) to monitor large streams of data created by networked sensors for data integrity and to improve networked environmental monitoring. The program will provide training to at least 13 trainees at several levels (from undergraduate to PhD), in an environment that is connected to industry and rooted in international and national collaborations. The program will hence provide a stream of innovative solutions to serious challenges in networked systems while producing the next generation of highly skilled workers in the critical application domains cited above.""624775,""Labecki, Adam"
"617726"	"Labeau, Fabrice"	"Signal and Information Processing for distributed and networked applications"	"The power of signal processing as a discipline is that it develops the algorithms and techniques that form the backbone of todays Internet, cellular networks, or human-machine interfaces; it is constantly extending its reach to other domains, including healthcare, power and energy or social networks. This proposal is rooted in this tradition and proposes to examine several specific situations of application of signal processing to new and emerging areas, characterized by the distributed and/or networked nature of the signals or information considered. These areas include the Internet-of-Things (IoT), Healthcare and the Energy Sector. The proposed research program will tackle projects that are motivated by some of concrete problems in these application domains, but will also strive to provide contributions that can be extended to other application domains. This program will develop new technologies and algorithms (i) to enhance the lifetime of wireless sensor networks through carefully managed deployments of nodes and Radiofrequency Power harvesting; (ii) to enhance the security and resistance to cyber attacks of wireless data gathering mechanisms, including in the Smart Grid and healthcare; (iii) to monitor large streams of data created by networked sensors for data integrity and to improve networked environmental monitoring. The program will provide training to at least 13 trainees at several levels (from undergraduate to PhD), in an environment that is connected to industry and rooted in international and national collaborations. The program will hence provide a stream of innovative solutions to serious challenges in networked systems while producing the next generation of highly skilled workers in the critical application domains cited above.""639181,""Labeau, Fabrice"
"637039"	"Lacefield, James"	"Ultrasound Signal Processing for Quantitative Microvascular Imaging"	"This Discovery Grant will support our research developing improved methods to use ultrasound toimage blood flow in networks of small vessels. This research is important because current medical imaging technologies are not sufficiently effective for measuring the subtle changes in blood flow thatcan bethe earliest evidence that a patient is or is not being helped by treatments for conditions such as cancer, cardiovascular disease, and diabetes.Our researchwillimprove three important ultrasound technologies used for vascular imaging,namely power Doppler, ultrafast imaging, and contrast-enhanced ultrasound.Power Doppler is an inexpensive,widely available technology for imaging vessel networks, but the images are strongly affected by settings chosen by the operator of the ultrasound system. Ourinnovation is to automate the selection of some of those settings, which will improve the accuracy of comparisons among images of different patients or among images of an individual patient that are acquired on different days. Ultrafast imaging is a state-of-the-art Doppler method that improves imaging of the smallest vessels by allowing more measurements to be used toproduce each image. Ourlab is developing a new ultrafast technique named ""spread-spectrum Doppler"" that will maximize the number of blood-flow measurements made withoutreducing image quality. We also plan to combine spread-spectrum Doppler with our automated method for choosing scanner settings, which should increase the effectiveness of both techniques. Contrast-enhanced ultrasounduses microscopic bubbles to increase the signal strength from small vessels above the levels that are possible with Doppler. Ourlab is developing new methods to statistically analyzecontrast-enhanced ultrasoundimages with the goal of increasing the technique's sensitivity to changes in how well organized or disorganized a vessel network is, which is valuable information toobtain about vessel networks in cancer tumours and other diseased tissues. Ourwork is also expected to yield new insights into the physical relationships between the structure of blood vessel networks and the information contained in ultrasound images of those vessels that will be valuable to other medical imaging researchers. This research will provide training opportunities for undergraduate and graduate students in biomedical engineering, electrical engineering, and medical physics that will help prepare those students for industry or academic careers designing and developing medical devices.""628523,""Lacelle, Denis"
"635121"	"LacosteJulien, Simon"	"Robust and Efficient Structured Prediction"	"Machine learning is widely used in science and technology with deployed tools like automatic spam classification for emails or face detectors in digital cameras. Yet todays partial solutions to the fundamental problem of structured prediction , that is, learning to make multiple interrelated predictions (e.g. predicting the sequence of translated words for an input sentence in machine translation), lag far behind those available for binary classification in term of accuracy and scalability. Progress has been fragmented, and the theory is almost nonexistent, preventing the widespread adoption of the technology to new areas. The key challenge in structured prediction is the combinatorial explosion of choices for the output, requiring a radical new marriage of computation and statistics . The objective of this research program is to elaborate a general theoretical and algorithmic framework for robust and efficient structured prediction . The goal is to bring structured prediction to a level of maturity and usability similar to that of binary classification in modern machine learning. I plan to attack this problem through the tools of statistical consistency of surrogate losses. Our theoretical work will provide the groundwork to build new robust structured prediction models that can handle weak supervision. Radically more efficient structured prediction machines will be obtained through our algorithmic work on advanced convex and combinatorial optimization. The applicability of the framework will be demonstrated through applications in computer vision, natural language processing and computational biology. More specifically, my research program will address the following four scientific challenges: 1) Provide a unified theoretical analysis of structured prediction models. 2) Propose novel structured prediction models that enjoy good theoretical properties, are tractable, and address the particularities of the field such as weak supervision . 3) Design efficient algorithms that solve the underlying large-scale convex or non-convex optimization problems. 4) Demonstrate the applicability of the framework in several application areas. Breakthrough progress on structured prediction will have high impact on statistical machine learning research, notably by providing a new solution to the open problem of making robust interrelated predictions. Moreover, the developed methodology will directly impact numerous application areas in science and technology by enabling the widespread adoption of advanced structured prediction.""630411,""Lacroix, Christian"
"637044"	"Lane, Pierre"	"Ultra High-Resolution Optical Imaging Catheters"	"Optical coherence tomography (OCT) is a volumetric imaging modality used to assess the subsurface structure and function of soft tissue. It is the optical analog to ultrasonography with improved resolution but albeit decreased penetration depth. High resolution OCT (10 um) has enabled several new clinical imaging applications in ophthalmology and cardiology. Ultra-high resolution OCT (1um) has demonstrated the cellular resolution required to detect cancer and its precursor lesions in tissue. There are technical challenges involved with the shift to broader spectral bands and shorter center wavelengths required to achieve ultra-high resolution. The paramount challenge is the management of dispersion. While dispersion in free-space optical system can be managed though careful interferometer design and the selection of optical elements to compensate dispersion, management of dispersion in (fiber-based) imaging catheters is significantly more challenging. The goal of this program is to develop new fiber-optic imaging catheters that will enable ultra-high resolution OCT in small luminal organs such as the lung, gastrointestinal tract, and fallopian tubes. Four technical objectives have been identified to achieve this goal. The first is to develop a testbed OCT system to evaluate candidate high-resolution imaging catheters. The second objective is to develop miniature common-path fiber-optic catheters. Designs will include the ability to adjust the sample-reference amplitude split ratio and the position of reference delay at the time of fabrication. The third objective is to develop miniature optics for extended depth-of-focus. This is required to help maintain lateral resolution over a longer ranging depth. The final objective is to develop novel scanning methodologies. Approaches will include the translation of a rotational drive into forward and side-looking scans, and scanning using multiple beams for NURD correction. The design, fabrication and validation of imaging catheters is an iterative process. In-house fiber-processing capabilities enable this design cycle to iterate quickly. Trainees do the initial design work using in-house optical and mechanical CAD software. OCT imaging with cellular resolution has been demonstrated using benchtop systems that employ free-space optics but has not yet transitioned into fiber-based imaging catheters suitable for luminal organs. This work will enable fiber-based OCT imaging of the small peripheral airways of the lung, and will for the first time, enable cellular resolution images of lung epithelium. This may lead to the diagnosis of cancer without the removal of tissue samples.""626141,""Lane, Robyn"
"635068"	"Langlais, Philippe"	"DeFacto: Acquiring, Curating, and Using a Bilingual Domain Aware Commonsense Knowledge Base"	"Automatically extracting knowledge from a large set of mostly unstructured documents (such as the Web) and organizing it into a knowledge base (KB) is a key challenge in artificial intelligence. Intuitively, such KBs should directly impact the quality of many NLP applications such as question answering, information retrieval or Text Analytics. Open information extraction, the task of extracting knowledge from texts without much supervision (especially not a prescription of the kind of information to mine), has brought new hope for such an endeavour. Despite a number of well-designed components are nowadays widespread and readily available for extracting facts and relations (so-called tuples) from texts, tapping information in large collections of texts still raises a number of issues. The technology embedded in a typical knowledge extraction pipeline is fraught with shortcomings: coreference resolution, named-entity resolution and parsing errors are collapsing so that many tuples (if not the vast majority) are simply useless. Also, most works are targeting very frequent entities and relations, which exclude a large quantity of information on domain specific texts that are pervasive over the Web. Our long term objective consists in developing the necessary expertise in populating, curating, maintaining and using a KB. Our proposal departs from several existing initiatives by a number of key factors. First, since specific domains are prevalent over the Web, we want our technology to be domain aware. Second, since today's world is multi-lingual and because not everything is written in English, we further want our technology to be multi-lingual in nature. Last, most works are devoted to develop fully automatic technology for assisting humans. In our proposal, we are interested in measuring how much gaming with a purpose can make humans assist the computer. In order to succeed, we target in this proposal the development of deFacto, a multi-domain, bilingual KB (French -- English) acquired iteratively from texts mined over the web, with the help of feedback collected from users via serious gaming.""639585,""Langlais, Philippe"
"634416"	"Larose, Benoit"	"Refined complexity of constraint satisfaction problems"	"In a constraint satisfaction problem (CSP), one must assignvalues to variables that must satisfy various constraints; typicalreal world examples include scheduling problems, database queries,image-processing, and frequency assignment problems. In general,determining whether a CSP admits a solution is an algorithmicchallenge, but it often happens in practice that the constraintsare of a very restricted form, allowing the use of efficientmethods to solve the CSP. Our long-term goal is to classifyprecisely what kinds of restrictions lead to these tractableCSP's. Our approach is based on an unexpected and fruitfulconnection between CSP's and universal algebra that was uncoveredin the late 90's, and which has led to major breakthroughs in our understanding of the complexity of CSPs over the past 20 years. In short, every family ofconstraints is transformed into a mathematical object whosealgebraic properties reflect the difficulty of solving theCSP. Several precise conjectures have been formulated, predicting which equations should lead to solvability with given time and space restrictions. The goal of this program is to investigate and solve these conjectures in various important special cases.The investigation of special cases of the refined dichotomy conjectures is bound to provide insights into an eventual solution of the full L- and NL- conjectures. This will give us a complete classification of the complexity of CSPs of bounded width, and hence a much deeper understanding of the complexity of non-uniform CSPs, which are ubiquitous in the theory of computing, with wide-ranging applications from artificial intelligence to database theory.""623614,""LaroseBeaudoin, Charles"
"634508"	"Lawrence, Ramon"	"Embedded Databases for the Internet of Things"	"The Internet of Things connects billions of devices that collect, store, and analyze data about our environment, business and industrial processes, health and activities, and our society. Developing and deploying these devices is a multi-billion dollar industry, and there is a wide diversity of the technologies used and their capabilities. Typically, these devices have very limited hardware resources, and developing software for them is highly customized, time-consuming, and error-prone as software is developed specifically for each hardware platform and application use case. The objective is to research techniques to dramatically reduce the cost and time to develop data collection software for embedded devices and implement and deploy these techniques as open source software for the worldwide community to use. A secondary objective is to deploy and evaluate the data collection systems in industrial and environmental monitoring applications and collaborate with researchers in these domains where the data analysis is critical. The scientific approach is to investigate and evaluate data management techniques that are specifically designed for low-resource, low-power embedded devices that optimize their behavior for hardware limitations and unique features of flash memory. Our previous work has developed LittleD, the smallest, SQL relational database for embedded devices, that runs on as little as 4 KB of memory, IonDB, a key-value store for embedded devices, and novel optimized file systems and overwriting techniques for NOR flash. These technologies and contributions will be fully integrated and expanded to produce the smallest, most energy-efficient database for embedded devices with between 1 and 100 KB of memory and less than 256 KB for code space. There are no systems currently available in this space with the smallest database, SQLite, being too large for these devices. Our goal is for IonDB to be the ubiquitous, open source system for embedded devices like SQLite is for cell phones. The significance of the research is that it addresses a pressing industrial need for databases to work on embedded devices where there is no current system available. The system will make it more efficient to develop and deploy embedded devices. Our research group will continue to work with partners such as the City of Kelowna and vineyards on applications of the system for irrigation management and environmental sustainability as well as other applications in environmental, health, and industrial data collection.""636547,""Lawryshyn, Yuri"
"637365"	"Lee, Daniel"	"Communication networks with a simple physical topology and smart-city technologies"	"With the grand vision of connecting sensors, devices, infrastructure, and humans, the proposed research program will investigate a new paradigm of network protocols, which takes advantage of a simple physical topology of nodes. My trainees and I will study fundamental aspects of the new protocol paradigm and seek mathematical relations between the protocol design variables and performance metrics. The program will also seek how to best utilize the physically distributed processors for the case in which the nodes are each a combined unit of a sensor and a processor. The research results will provide a guide on how to configure protocol parameters to maximize energy efficiency and how to design network applications under a given constraint of limited communication capacity. The proposed research is important because, in future applications, a very large number of devices, sensors, machines, vehicles, and buildings will be connected and networked through widely different kinds of channels. The proposed research will increase fundamental knowledge about these massive networks for future network designers. In particular, the idea of utilizing a special physical topology is anticipated to provide new insights in the field of wireless protocol design. Also, energy efficient network protocols and network control and planning methodologies will significantly contribute to reducing energy consumption of future systems. The research will create fundamental knowledge applicable for realizing the vision of smart cities and even broader smart infrastructure. In particular, network protocols that take advantage of linear physical arrangements of sensors will be applicable for monitoring and managing Canadas large-scale assets such as railroads and oil pipelines.""625676,""Lee, DongJu"
"634510"	"Lemire, Daniel"	"Faster Compressed Indexes On Next-Generation Hardware"	"Software indexes accelerate applications in business analytics, machine learning, and data science. They often determine the performance of big-data applications. Efficient indexes not only improve latency and throughput, but they also reduce energy usage. Many indexes make parsimonious use of internal memory so that critical data remains close to the processor. It is also desirable to work directly on the compressed data, to avoid potentially harmful decoding passes. Thus, we use lightweight compression strategies, optimized for speed. We are interested in bitmap indexes. We find them in a wide range of popular systems: Oracle, Apache Hive, Apache Spark, Druid, Apache Kylin, Apache Lucene, Elastic, Git and so forth. They are an integral part of systemssuch as Wikipedia or GitHubused by millions of people every day . Our long-term plan has three axes of research: (1) Pursue the optimization of existing bitmap indexes, as they are used in current systems. Many of these systems rely on either Roaring or EWAH bitmaps, two formats we produced. We plan to multiply the performance of some of these indexes on processors supporting advanced SIMD (single instruction, multiple data) instructions such as those of the AVX2 and AVX-512 families. (2) Continue to break speed records with our integer-compression techniques. We focus on sorted lists of integers, as they frequently appear in B+-trees, inverted indexes and compressed bitmap indexes. In recent years, we showed that we could decode billions of integers per second while maintaining compression ratios close to the limit given by Shannon's entropy. Yet we used all but a fraction of the features of the latest processors. We will establish new performance records on recent server processors while compressing even better. We will also accelerate applications with rank, select, merge, update and insert functions operating directly over the compressed data. Finally, we will apply this work to database engines (e.g., upscaledb ) and big-data systems (e.g., Apache Parquet, Druid, Apache Spark). (3) Develop a novel bitmap index format that outclasses the state of the art in both memory usage and raw speed. Currently, one of the best formats is Roaring: it is widely adopted, fast and it uses little memory. We seek to design a new format that uses even less memory than Roaring while improving the speed. Though it is relatively easy to offer better compression, doing so while improving query performance represents a real challenge. This research axis builds directly on the first two axes.""639578,""Lemire, Daniel"
"634725"	"Leung, CarsonKaiSang"	"Mining interesting patterns from big data"	"In the current era of big data, high volumes of a wide variety of valuable data of different veracity (e.g., imprecise or uncertain data like sensor data and medical lab test results, in which the contents are uncertain due to factors like inherited measurement inaccuracies or sampling frequency) can be easily generated or collected at a high velocity. Consequently, we are drowning in data but starving for knowledge. In order to be able to make sense of these data, data science solutions for big data management and big data mining (which discovers implicit, previously unknown potentially useful knowledge that might be embedded in data) are in demand. Over the past few years, I--together with my HQP--have developed algorithms that use probabilistic approaches for finding frequent sets of items from uncertain data. The algorithms are enhanced with a few optimizations, including some in-memory tree structures for capturing important contents in the data. Along this direction of my current research program, I plan to broaden my research work with an objective to build a more efficient, user-friendly, and powerful data science system for mining interesting patterns from big data. Specifically, I plan to (1) adapt the developed algorithms to take into account the user preference and push these user constraints inside the mining process so that the resulting algorithms only output interesting patterns and no post-processing step is needed; (2) mine integrate heterogeneous data sets from multiple related sources, and incorporate prior posterior knowledge about these data; (3) further improve performance so as to provide users with real-time responses by exploring other optimizations and techniques (e.g., Apache Spark, Scala) and further reduce the memory requirements (e.g., by adapting Bigtable); (4) explore other quantitative qualitative approaches to capture analyze more data information; (5) explore real-life applications (e.g., mining social networks, Web, telecommunication data, agricultural data, meteorological data, news feed, tweets, blogs) and find other interesting patterns such as trends, sequences subgraphs (e.g., social networks/graphs); and (6) develop data visualization visual analytics tools that enable users to visualize and analyze data, to change the user-specified mining parameters, and/or preferences based on the visualized information mined from the data. This would make the resulting system more interactive and exploratory.This, in turn, helps users to enrich their knowledge so that they could promptly take appropriate actions or to make the best (business or military) decision, which would consequently have significant positive impacts on the improvement of human life and the benefits to Canadian economy as well as national defense security.""622996,""Leung, Chelsea"
"634262"	"Levin, David"	"Big Data for Fast and Accurate Numerical Simulation of Mechanical Structures"	"Numerical simulations of physical phenomena such as large and small deformations are a crucial tool for everything from building design to 3D printing. The knowledge of how something will perform in the real-world has a tremendous impact on the design process. However, even today, state-of-the-art algorithms are still several orders of magnitude too slow to be used interactively, especially when we consider constraints imposed by desired accuracy and computational challenges introduced by the high-resolution, multi-material nature of advanced additive manufacturing techniques. The problem becomes more daunting when one considers that next-generation interactive design tools for buildings, airplanes, cars and even characters in blockbuster films desire ""in-the-loop"" simulation. Such a setup has two principal benefits; first, designers can receive feedback on the effect of design changes instantaneously and second, ultra-fast simulation opens the door to intelligent, optimization-based suggestion schemes -- ones which can perform background exploration of the design space in order to find non-intuitive designs which satisfy designer constraints. Currently, numerical simulations are treated as disposable, thrown away once the desired structural analysis or animation has been completed. But why should this be the case ? What could we do with a large database of simulation data? Could we use it to accelerate a broad range of simulations without requiring the tedious and expensive precomputation on a case-by-case basis? In this research project I will explore the implications of this question and develop simulation algorithms which use prior information extracted from such a database to avoid the performance/fidelity trade-offs of traditional methods. Such algorithms could have a plethora of benefits for any domain in which physical simulation is used. In order to do this I will focus on three main areas 1.) Compact, geometry independent representations for storing simulation data 2.) Using stored data for fast, runtime numerical coarsening 3.) Algorithms and devices with which to quickly and accurately capture material and geometry parameters necessary for simulation 4.) New algorithms for solving coupled systems of linear and nonlinear equations which exploit both of the above. Accomplishing these four goals will push us towards a new era of high-performance physics simulations driven by Big Data. Just as how online databases have revolutionized areas such as computer vision, I envision a similar change will occur in the numerical physics and computer animation communities. I believe that this work, essentially building the google image search for simulation data, is crucial for bringing this to fruition.""618294,""Levin, David"
"617749"	"LeytonBrown, Kevin"	"Data-Driven Market Design"	"Markets are institutions that facilitate the exchange of goods and services via binding contracts; they thus run according to rules. Sometimes these rules arise organically. This can work well when buyers and sellers have little trouble finding each other, when it's not very important who does the buying and selling, and when the market produces simple contracts (e.g., I'll sell you this can of Coke for $2). Otherwise, it can be harder for markets to determine effective rules (e.g., to produce contracts like I'll rent you my spare room tomorrow for $60, but you have to be quiet after 11 PM). When effective markets do not arise organically, rules can instead be crafted explicitly, an idea championed by the fields of market design and mechanism design (recognized by Nobel prizes in 2012 and 2007 respectively). The goal is to prove that a market achieves desirable outcomes (e.g., matching up buyers and sellers who gain the most by trading) under the constraints imposed by a problem and under reasonable assumptions about the behaviour of participants. These assumptions are typically game theoretic: roughly, that participants are fully informed about how a market works and act rationally within it to best serve their interests. This is a powerful approach; it has yielded both elegant, general theory and deeply impactful applications as varied as search-engine keyword auctions and kidney exchanges. It has also had a profound impact on artificial intelligence, providing practical, theoretically grounded techniques for addressing longstanding challenges like information fusion and task allocation in multiagent systems. This approach has a critical flaw, which is more egregious in 2016 than it was when the field's foundations were being laid in the mid-1900s. This flaw is that market design is almost entirely an analytic (i.e., mathematical) exercise: once one has committed to a game theoretic model of the world, there is little room left for responsiveness to real-world observations. In contrast, computer science is currently undergoing a data science revolution: we now think of computer systems not as static artefacts, but as evolving services that remember user interactions and adapt to them. It is becoming a truism that the more data one has about user interactions with a system (a self-driving car; a speech recognition system; a search engine) the better it should work. The proposed research will help market design to become part of this paradigm shift, enabling markets to draw jointly on actual interactions with users and on game theoretic analysis.More specifically, it will develop data-sensitive techniques for modeling human behavior in markets, building heuristic clearing algorithms, and analyzing adaptive mechanisms. The result will be market designs that can be optimized to different settings and that can adapt after being deployed, just like other modern computer systems.""632637,""Leznoff, Daniel"
"634442"	"Lin, Man"	"Learning-based Energy Management for Cyber-Physical Systems"	"With the advance of engineering and networking technology, many embedded devices are now connected into a network or with the Internet, and are emerging into cyber-physical systems. A cyber-physical system can be found in various applications for control and monitoring, such as automotive, aerospace, health care, transportation, building and process control, and entertainment. Unlike desktop systems, many cyber-physical systems operate with batteries that have limited energy supplies. Thus, energy efficiency is one of the inherent requirements of cyber-physical systems, as low-energy consumption yields better battery life, which is especially important for applications involving implanted medical devices. With more and more cyber-physical systems around us nowadays, low-energy consumption problem of cyber-physical systems becomes more critical. The research challenge to minimize the energy consumption of CPU or devices, while still meeting the constraints of the real-time systems, has attracted much attention in the past decade. With the variety of system configurations and task characteristics, a scheduling arrangement with Dynamic Voltage/Frequency Scaling (DVFS) and/or Dynamic Power Management (DPM) that is energy efficient for one system configuration might not be appropriate for another. Therefore, it is important to design scheduling algorithms that can be adapted to various system configurations and task characteristics. The objective of this project is to develop adaptive efficient algorithms for scheduling co-design problems for various types of cyber-physical systems subjected to various timing and resource constraints. The plan is to adopt learning-based methods that are able to learn an implicit model for voltage selection or scheduling strategy selection for the underlying cyber-physical system based on scheduling history. This method is especially useful when the task features and architecture model are unknown to (or too complex to be considered by) the DVFS scheduler. The problem of extracting good features to serve as input for an implicit model for the learning-based method, that can best represent the model of an underlying cyber-physical system, will also be explored. Currently, Q-learning, Double Q-learning and Deep Double-Q-learning will be explored. Evaluation of the framework will also be studied extensively. The framework designed will be used to explore systems with various types of tasks (dependent, independent, periodic or non-periodic, etc.), various types of scheduling policy (earliest deadline first, fixed-priority, etc.), various types of system configurations (single-core, multi-core, GPU, NOC-based, wearable type of devices, etc.).""624394,""Lin, Ruoyi"
"637725"	"Lina, JeanMarc"	"Rhythmic and arrhythmic brain activity: from advanced signal processing tools to new electromagnetic neuroimaging methods"	"The main objective of this research program is to provide neuroscientists and neuro-clinicians with a new generation of signal processing and imaging tools based on the electromagnetic signals produced by the brain. Direct measurement of the electrical activity produced by the brain provides time-resolved signals which reflect the brain in action, i.e. the synchronization of neural populations, either during cognitive tasks or resting state. Sleep is also a particularly important period during which the brain generates and processes information, and undergoes plastic reorganization of the interconnected synaptic networking in order to be more efficient during the forthcoming awake time. As a matter of fact, the characterization of spontaneous brain activity during resting states or sleep in humans is among the most challenging scientific issues. Electromagnetic (EM) signals produced by the brain can be non invasively measured in two complementary ways: the usual EEG signals (Electroencephalography, i.e. electric potential measured on the scalp) and more recent MEG recordings (Magnetoencephalography, i.e. magnetic field measured around the head). As for most of electrophysiological signals, EM recordings are a complex mix of short lasting and narrow-banded oscillations (rhythmic activity), with more long-range and broadband dynamical processes without characteristic time scale (arrhythmic activity). Usual spectral analysis mostly focus on the rhythmic component, i.e. the oscillations that are usually categorized into frequency bands (delta:0.1-4 Hz; theta:4-8 Hz; alpha:8-16 Hz; beta:16-32 Hz; gamma: 32 Hz). Although the arrhythmic component has been generally modeled by power-law processes and self-similar processes without functional interpretation with respect to the brain activity, recent works has demonstrated the functional relevance of this spontaneous arrythmic signal that reflects neural activity and non linear interactions between neural populations with implication in cognition. However, this model that relies on a unique spectral exponent, cannot account for the full complexity of the real data. Based on the recent development in multifractal analysis, the main objective aims at validating new advanced signal analyses methods and numerical tools that offer new and robust spectral descriptors elucidating the coupling between rhythmic and arrhythmic components of EM neuro-physiological signals, and to localize in the brain specific neural processes associated to those components. The specific (but not restricted) field of application will be a quantitative assessment of the sleep quality and integrity based on dynamical neuro-processing tools in the healthy and aging population.""628196,""Lindberg, Melissa"
"634439"	"Liu, Xue"	"Deduplication-aware Systems for Cost-efficient Cloud Storage"	"Cloud storage systems serve as an important infrastructure for emerging applications including Big Data Analytics and Internet of Things (IoT). A key challenge is how to handle a massive amount of data in real-time in a cost-efficient way. Explosive growth in the volume and complexity of data exacerbates this challenge. Further, many cloud computing systems are networked and distributed, thus making storage system management more complex and costly due to the limited bandwidth. Data deduplication is an efficient data reduction approach that not only reduces storage space by eliminating duplicate data but also minimizes the transmission of redundant data even in low-bandwidth environments. However, conventional deduplication schemes suffer from high computation complexity in chunking and large storage overhead for storing block indices, thus failing to offer real-time and cost-efficient storage services. This research program aims to address the most important challenges facing the performance optimization of cloud storage systems targeting big data applications. We will conduct innovative research to overcome the current limitations of deduplication-based methodologies. To this end, we will investigate adaptive multi-granularity deduplication schemes to significantly reduce the amounts of data to be processed and improve the overall system performance. We plan to propose a new methodology in deduplication granularities to meet the needs of handling different data scales. Implementation techniques, including locality-aware hashing, deduplication, and compression synergization and pipeline scheduling, will be investigated, evaluated, and validated in real cloud storage platform.""617748,""Liu, Xue"
"634600"	"LopezHerrejon, RobertoErick"	"Mining Software Repositories and Information Visualization for Empirically Robust Testing of Variable Software"	"In modern software systems ""one-size fits all"" does not hold because they must be customized to fit the needs of diverse and large populations of users. Variable software refers to this type of software systems that can be configured in a large number of ways, and that stem primarily from the adoption of generator-based techniques, full-blown Software Product Line (SPL) approaches, advanced modularization paradigms, highly-configurable systems, or ad hoc reuse practices collectively called clone-and-own. Testing variable software is specially challenging because of the typically large number of configurations that must be tested, a fact that makes it unfeasible to test each individual configuration. Combinatorial Interaction Testing (CIT) has been advocated as a paradigm to address this challenge because of the diverse set of algorithms available which can be exploited to compute representative groups of configurations for testing. There are however several open problems that this research program aims to tackle. First, there is a critical lack of publicly-available case studies of variable software on which to empirically evaluate and validate CIT techniques. Second, there is a stark need of a thorough empirical and comparative evaluation of different CIT techniques that have been applied to variable software. Third, there is no adequate tool support for testing of variable software. The current practice is an ad-hoc collection of uncoordinated and incomplete assortment of plug-ins and stand-alone tools. The objectives for this research program are: 1) Exploit techniques for mining software repositories to gather publicly-available case studies of variable software systems for a thorough empirical evaluation of CIT techniques applied to variable software. 2) Perform a systematic assessment of CIT techniques applied to variable software. 3) Leverage information visualization techniques to better convey the large amount of information present in variable software case studies. 4) Provide robust tool support that allows software engineers to design, implement, and carry out test of variable software. The first objective will be achieved by studying open source projects available in repositories such as GitHub and developing tools that mine configuration and fault data. For the second objective we will systematically collect and catalog tools for CIT testing of variable software, and evaluate them with the case studies data of the first objective. For the third objective we will explore and evaluate different visualization techniques to convey all the distinct types of information that must be considered for making adequate engineering decisions when testing variable software. For the fourth objective, we will develop a framework that allows flexible addition of testing tools and visualization interfaces to provide software engineers with robust tool support of testing tasks.""632550,""Loppnow, Glen"
"637581"	"LoVetri, Joe"	"Algorithms and Systems for Electromagnetic and Ultrasound Inverse Problems"	"This NSERC Discovery Grant proposal aims to advance the core scientific knowledge and practical technologies that form the foundation of quantitative electromagnetic wave imaging (EWI) and ultrasound imaging (USI). Both are wavefield imaging techniques that create quantitative images of particular properties of an inaccessible object or region of interest. Properties such as the permittivity and conductivity of a region are imaged when one utilizes electromagnetic waves, e.g., microwaves, whereas ultrasonic properties, such as the compressibility, attenuation factor, and mass density, are imaged when using ultrasound waves. These property images can be utilized in a wide variety of diagnostic applications. Under my direction, the Electromagnetic Imaging Laboratory at the U. of Manitoba has made significant contributions to particular EWI and USI algorithms and weve built imaging systems applicable to various biomedical and industrial applications. The primary biomedical focus has been on building systems and technologies for the detection of breast cancer as well as for the frequent monitoring of breast cancer treatment. These same techniques have also been incorporated into stored-grain imaging systems that are used for the monitoring of grain quality so as to provide the early detection of spoilage. The quantitative wavefield imaging techniques weve advanced, based on solving the nonlinear inverse scattering problem, can be utilized in imaging applications wherein wave-type energy can be introduced as an interrogating field and the scattered-field outside the region can be accurately measured. Our system design research enables the practical manifestation of this interrogation/measurement process and our software development, which incorporates the imaging algorithms, allows us to produce the property images. Scientific and technological challenges limit the accuracy and resolution of images obtained using wavefield imaging. These challenges have limited the adoption of EWI for breast imaging but there is reason to believe that research will lead to improvements. Accuracy, rather than resolution, is more important for grain imaging where no competing technology that produces property images exists. The overall long-term focus of this research program is to improve and adapt EWI and USI algorithms, technologies, and systems to the point where these modalities become commercially viable modalities for both the breast imaging application as well as for the grain-imaging application. The main methodological theme is to simultaneously develop algorithmic advances while developing, incorporating, and utilizing system design features that aid the imaging process. The success of this research will open-up new biomedical and novel industrial applications (e.g., non-destructive evaluation). A total of seven graduate students will be involved in this research.""631480,""Lovic, Vedran"
"637296"	"Loyka, Sergey"	"Analysis, design and optimization of robust and secure wireless communication systems"	"Wireless communications are an essential part of modern society and the way it functions at different layers. Wide availability of multi-media content puts a great demand on the supported rates and quality-of-service. A large number of sensitive transactions require a high degree of security offered by such systems. A high mobility of users makes the wireless propagation channel highly dynamic/unpredictable and hence dictates an adaptive and robust design of systems. These 3 key challenges will be addressed in the proposed project. Specifically, methods for analysis, design and optimization of secure and robust wireless communication systems will be studied. To address the requirement for high rates, wireless multiple-input multiple-output (MIMO, or multi-antenna) systems are widely adopted by academia and industry due to their high spectral efficiency. The broadcast nature of wireless channels stimulated significant interest in their security aspects. While cryptography provides valuable tools for ensuring the secrecy of communications, it also comes with a number of drawbacks (leaked keys etc.). Physical-layer security has recently emerged as a powerful approach, which is complementary to cryptography and which allows to address a number of the shortcomings of the latter. Hence, the physical-layer security approach to MIMO channels will be studied using the tools of information and communication theory. In particular, optimal (adaptive and robust) transmission techniques will be developed, which will provide high data rates over highly-variable channels ensuring the secrecy of the transmitted data at the same time. Real-world (non-stationary, non-ergodic) nature of the wireless propagation channel as well as its estimation inaccuracy will be taken into account. The powerful tools of convex optimization will be used. Overall, the project will address three key and inter-related aspects of secure wireless communications: its information-theoretic foundations, optimal signaling strategies and practical implementation issues (e.g. robustness to channel uncertainty). ""621082,""Lozano, Alexander"
"637362"	"Lu, Ning"	"Real-Time Scheduling for Internet of Vehicles"	"Internet of Vehicles (IoV) is envisioned to enable next-generation vehicular applications, such as preventive maintenance/diagnostics, feedback-based routing, speed and fuel management, and advanced active safety, among others. As a mission-critical system, IoV is required to guarantee the Quality of Service (QoS) requirements of various services/applications, especially for real-time services with deadline constraints either at packet level or application level. Serving real-time applications is therefore considered as a key component of IoV and many other cyber-physical systems. Despite significant research effort on IoV technology, a mature science to support real-time applications of high-confidence IoV is still missing, and traditional analysis tools/algorithms are unable to cope with the full complexity of IoV or adequately predict system behavior, due to great challenges that arise from the high mobility and intrinsic heterogeneity of such systems. The long-term goal of this research program is to advance the development of efficient and low-complexity scheduling algorithms for delivering heterogeneous services in IoV. The short-term objectives in the next five years are to model and characterize the impact of vehicle mobility on real-time scheduling, to develop a rigorous and systematic framework for design scheduling policies that can achieve optimal performance, and to develop efficient real-time scheduling algorithms considering various engineering costs and constraints. A heterogeneous vehicular connectivity will be considered in this program, where vehicles are capable of connecting to cellular network base stations, Wi-Fi access points, and other vehicles. The proposed research includes three thrusts: (1) Intra-cell scheduling: the development of throughput/utility-optimal scheduling algorithms for enhancing todays cellular network or for next-generation cellular network to serve location-specific IoV services in a real-time manner; (2) Network-wide scheduling: the development of optimal distributed scheduling algorithms for packets with hard deadlines in a vehicular ad hoc network, taking account of both the long-term and short-term guarantees, and flow-level dynamics due to vehicle mobility; (3) Application-level scheduling: the development of optimal application-level scheduling algorithms in favor of cellular traffic steering subject to application delay constraints. It is anticipated that the proposed research program will generate significant scientific, technological, and social impacts in providing the scientific research foundation for supporting real-time applications in IoV. This program will also contribute to the training of highly qualified personnel, providing the trainees with solid knowledge and research background in RD of IoV, and enabling them to contribute to Canadian high technology industry.""617955,""Lu, Paul"
"637065"	"Lu, ZhengHong"	"Fundamentals in Organic Light Emitting Diodes"	"An organic light-emitting diode (OLED) is made of a stack of organic semiconductor layers sandwiched between a transparent conducting anode and a reflective metal cathode. Under an external electric bias between the anode and the cathode, holes, or positive charges, from the anode and electrons from the cathode are forced into the organic semiconductor layers to form excitons (tightly bound hole-electron pairs). The annihilation of an exciton may release its energy radiatively to emit light or non-radiatively to generate heat. Some of the emitted light may be trapped inside the device due to optical waveguide and other optical effects. The percentage of the emitted light coming out of an OLED (i.e. optical coupling factor) is related to the orientation of organic molecules in these thin-films. The devices external quantum efficiency is determined by these three processes: electron-hole balance factor, excitonics, and optical coupling. For electron-hole balance factor, the research will focus on understanding energy barriers at electrode-organic interfaces and organic hetero-junctions by using X-ray photoelectron spectroscopy (XPS) and ultra-violet photoelectron spectroscopy (UPS). For excitonics, time-resolved PL and EL will be used to quantify several competing excitonic processes: migration and radiative recombination, non-radiative recombination, and exciton energy transfer from one emitter molecule to another. For optical coupling, the research objective will be understanding of materials science governing emitter molecular orientations in guest-host systems. In additional to application as display modules in smart phones and televisions, OLED technology has been identified as the greenest possible technology for energy-efficient lightings: no toxic mercury used in the product, no toxic gases used in device fabrication. According to a recent report released in September, 2016, by the US Department of Energy, a 75% energy saving will be reached by year 2035 if there is a broad societal adoption of energy-efficient lightings. This proposed research program on OLED fundamentals bridging device functions and materials science (energetics and molecular orientation) will lead to innovations in future OLED technology. A successful research outcome is expected to accelerate commercial adoption of OLEDs as a new generation of green and energy-efficient light bulbs. Moreover, there are numerous solid-state lighting companies and OLED technology players in Canada. This proposed NSERC Discovery program will provide an exceptional opportunity to train a steady stream of HQPs (eight Ph.D. and ten Master/MEng students) for Canadian companies having business interests in energy, environment, and information and communication technologies.""642519,""Lu, ZhengHong"
"637081"	"Lynch, Alan"	"Nova Scotia"	"CANADA"
"634440"	"Mago, Vijay"	"Reliable and efficient real-time tools for collecting and analyzing large health datasets"	"Background Research suggests that more and more people rely on online sources for health information including symptoms, treatments and general health-related advice. Moreover, the user behaviour of millions of users currently active on social media demonstrates an openness to share facts related to their current health status. Such data could be used to provide real-time tracking and prediction of the spread of disease and other health concerns, or provide vital information about the effectiveness of the public health awareness strategies of health agencies such as Health Canada, the Centre for Disease Control or the World Health Organization. However, our current understanding of online health data produced through social networks is limited in important ways: (a) existing databases are project specific and data gathering mechanisms are time-constrained; (b) existing health-tracking tools depend on single interfaces such as Facebook, Twitter or Instagram, and (c) there is a lack of capacity for real-time mapping of health care issues. Specific Aims of Research Program Data collection: We will design an infrastructure (software and hardware) to continuously collect data from the social media handles of health agencies and medical associations, storing data on network storage systems. Social media strategy effectiveness: Data points created by health organizations to inform the public can be correlated to real effects in the general population. The influence of these organizations can be studied by observing the level of penetration of their media content and overall effort of their communication strategy as compared to the unfolding of public health events without following general users social media accounts. This requires a new approach of understanding the effectiveness of such campaigns by measuring the attractiveness of the social media content by the volume of data being shared among different types of users (medical/health organizations, national laboratories, etc.). Validating the predictive models: Social media data has been used to predict various healthcare behavioural issues and infectious diseases. The major challenge in these predictive models is to define the ground-truth. One way is to use crowd-sourcing but this limits the evaluation to one particular problem or model. To overcome this shortcoming, the proposed research program will develop ground-truth communities algorithms using multiple social media datasets. Real-time analysis of social media: The amount of data captured from various social media platforms could be daunting and requires large and scalable computational powers to incorporate variations in the volume of data streams. It will be necessary to use distributed computation, so the proposed research program aims to use and build new algorithms that can be ported onto a Hadoop cluster which is available through the High Performance Computing Lab.""629124,""Magor, Bradley"
"634522"	"Maheswaran, Muthucumaru"	"Fog Machine: A Fault-Tolerant System and Programming Framework for Cloud of Moving Things"	"Cloud computing and mobile computing have been two dominant drivers of computing trends in the last few years. One of the challenges of using cloud computing to power mobile applications is the significant network distance between the server resources and the clients and the consequent problems in delivering guaranteed qualities of services. To address these problems edge computing paradigms such as fogs that bring clouds closer to the clients have been proposed. With the impending boom of Internet of Things such as connected cars and smart homes, edge computing is poised to play a larger role in any computing infrastructure of the future. The three-layered distributed computing architecture formed by such a cloud-fog-device system introduces many new challenges. Chief among them is the programmability of the system. The first objective of this research is to develop a model for Cloud of Moving Things (CoMT) system for the three-layered cloud-fog-device that provides (a) adaptive device-to-fog associations based on the locality of the devices, (b) efficient handover of mobile devices across the fogs, (c) system reconfiguration to meet the reliability goals. The second objective is to develop a simple yet flexible programming model that allows the programmer to control various aspects of failure management, consistency, and performance in a CoMT system. Fogs are used to provide low latency and localized access to processing capacities. For fault tolerance purposes, we need to associate multiple fogs with a device so that quick failovers are possible with fog or network failures. The fogs that are associated with a device needs to be selected carefully to minimize latencies and should be from the appropriate locality. With mobility, the situation can get quite complicated; the devices need to change associations to maintain the locality and low latency access. In a rapidly moving scenario like vehicular networks the handover times from one fog to another need to be very small. In CoMT, the devices can call upon the fog or cloud to perform various types of processing on data they have captured. Therefore, the computing system formed by the combination of devices, fogs, and clouds needs to be reliable and maintain state in a consistent manner. A simple way to add fault tolerance is to insert redundancy into the system. In the case of the three-layered CoMT, we can make redundant associations between the devices and fogs and between fogs and clouds. However, making too many such associations come with a cost. In particular, the time to complete the basic operations (e.g., reads and writes) on fogs increase with increasing number of fogs. Therefore, a careful trade-off analysis need to be performed between the benefits of increasing redundancy and increasing costs of maintaining a consistent machine.""624570,""Maheux, MarcAntoine"
"637260"	"Majedi, Hamed"	"Hybrid Graphene/Superconducting Optoelectronics on Silicon Photonic Crystals"	"Electronic and photonic technologies have become part of our daily life. They are the ubiquitous backbone of a multitude of applications, from smartphones and computers to medical instrumentation and telecommunication systems. Integrating electronics and photonics on a single platform, known as integrated optoelectronics, holds great promise not only to enable a greater degree of device miniaturization, higher speed, and better performance but also for emerging applications in quantum and nanotechnologies. Although silicon and complementary metal oxide semiconductor (CMOS) technology have dominated electronics for more than four decades and research on silicon photonics has progressed significantly, a variety of other semiconductor materials are used in photonics to expand the wavelength range of silicons operation mainly for optical communication systems operating at 1320 and 1550 nanometer wavelengths. Finding CMOS-compatible materials that extend silicons optical operation wavelengths that enable new functionalities and leap across the classical domain to quantum regime has therefore become a task of technological need and industrial importance. Graphene, an atomically-thick carbon layer, with exceptional electronic and optical properties proves not only to enhance the performance of silicon devices but is also compatible with CMOS technology. Through the proposed program, we aim to develop two classes of new integrated optoelectronic platforms that will combine silicon photonic crystal structures with graphene for classical applications and their combination with superconducting Niobium Nitride (NbN) for quantum applications. The proposal will address the device physics, engineering design, fabrication, and characterization of such hybrid structures. The existing fabrication facilities at Quantum NanoFab in Waterloo and well-established characterization infrastructure in Majedis group will be used fully to ensure success of this program. The major outcome of this proposal is to develop optoelectronic devices, mainly optical modulators, switches and detectors, that are beyond silicon photonics optical operation wavelengths, with better performance metrics in comparison with competing technologies. The developed devices and circuits greatly impact areas of application ranging from optical interconnects and computing processors to biomedical sensors. The hybrid graphene/superconducting devices on silicon photonic crystals is introduced as a new quantum photonic platform integrating single photon devices where photons are individually processed and detected on a single chip, a technological challenge that has not yet been achieved. The wealth of multidisciplinary research, breadth of expertise, and technical capabilities to which HQP will be exposed to; attract talented students and provide a unique environment for innovation.""625971,""Majeed, Hamnah"
"634712"	"Mannan, Mohammad"	"Data security through trusted execution and comprehensive analysis framework"	"In this proposal, I primarily target two complementary long-term objectives. (1) I will develop next generation data security mechanisms by leveraging existing and new hardware-based trusted computing features (e.g., trusted execution modes of modern CPUs, and security primitives implemented in chipsets, firmware, dedicated security chips) in three environments: laptops and client-end PCs, servers, and mobile devices. (2) I will develop new systematic and comprehensive security analysis frameworks for improved characterization of security failures due to implementation bugs and design flaws in real-world, complex software systems. I will also use trusted computing technologies to address security issues uncovered by such analysis frameworks. Trusted computing technologies are largely under-utilized in current security solutions. Although such techniques are not new, and several academic proposals also exist, I believe real-world adoption is low due to many proposals being too narrow, i.e., solving only part of a complex problem, and processing secure user input and output is difficult. I will design more complete solutions, and present several target problems that have not been explored yet, specifically, problems that are too expensive or cumbersome to solve only through cryptographic/systems means. On the other hand, mechanisms for security analysis are in many cases adhoc, i.e., applicable to a certain piece of software or vulnerability. My goal is to develop frameworks that will be reusable (i.e., used for different software systems), and frameworks that can provide clear directions to improve security for system designers and product developers. I believe such frameworks and software tools (which I will open-source) will help researchers and developers to evaluate their intended systems more frequently, and more systematically. The long-term vision of this proposal will be materialized through several short-term, concrete projects. I will explore security-critical applications (apps) in multiple platforms (desktops, servers, and mobile devices), including: protecting data against ransomware attacks; securely deleting data for device repurposing, and mitigating theft/coercion; securing in-memory confidential data against memory-extraction attacks; analyzing TLS implementations, and measuring real-world TLS interception; analyzing evasive malware, and consumer/enterprise applications. The target problems are broad, affecting many high-impact practical systems, and difficult to solve with current approaches. Overall, the use of trusted computing in the design of verifiable solutions, and being able to validate real-world systems through comprehensive security frameworks will increase trust for everyday/enterprise users, and encourage researchers to leverage trusted computing and rigorous analysis techniques from this proposal.""627638,""Mannar, Dhiraj"
"621722"	"MarciaSoaresAlmeida, Hayda"	"British Columbia"	"CANADA"
"636986"	"Masani, Kei"	"Control of dynamic standing using functional electrical stimulation"	"People with neurological injuries such as spinal cord injury or stroke have impaired movements of their arms and/or legs due to paralyzed muscles. However, we can stimulate those muscles using electrical impulses to generate muscle contractions and create various limb movements to facilitate functional activities such as walking and standing. In this program, I aim to design a pattern of electrical stimulation for the artificial control of standing by mimicking the way people use their muscles, with the aim of a person with an SCI riding on a Segway Personal Transporter , a personal vehicle that is operated in a standing posture (http://www.segway.ca). In the first project, I will investigate how healthy people ride on the Segway and design a controller for controlling leg joints force. I will measure body movements when the healthy people ride on the Segway. Based on the findings, I will design a controller that monitors body movements of a rider on the Segway and modify the joint force in an appropriate manner. The controller will be tested in a computational simulation. In the second project, the best electrical stimulation technology will be investigated to effectively stimulate leg muscles. As the number of muscles is much larger than the number of leg joints, there are various ways to make one joint exert a certain force. Thus, the best way to control leg muscles to exert a given joint force will be investigated experimentally and computationally. In the third project, a prototype controlling the joint force and muscle activities will be designed and will be computationally and experimentally tested. This program will create a device for electrically stimulating leg muscles of people with paralysis in their leg, which will allow them to stand and ride the Segway. This will provide a highly beneficial assistive technology for those patients, which, in future, will improve their quality of life. Additionally, multiple engineering and scientific outcomes will be obtained, related to body movements and muscle physiology under electrical stimulation. Further, this program will train two Ph.D. students, a Postdoctoral Fellow, and ten undergraduate students who will promote bioengineering in Canada. ""632986,""Mascher, Peter"
"634542"	"Mcheick, Hamid"	"Conception et adaptation des applications contextuelles"	"De la mme manire que ltre humain adapte continuellement son comportement relativement aux nouvelles situations, les systmes informatiques ubiquitaires proposent dsormais des applications contextuelles (AC) ayant la capacit de recueillir des informations (environnement, sant, etc.) et de sadapter en consquence. Les chercheurs tentent d'intgrer ces informations (contexte) dans l'informatique ubiquitaire afin de rpondre ces besoins volutifs en temps rel. Nous considrons le domaine d'application mdical pour deux raisons : a) mdicale, car 1,5 million de Canadiens sont atteints des maladies pulmonaires obstructives chroniques (MPOC) assez dangereuses et souvent ne ncessitent pas lhospitalisation, et b) scientifique, grce la complexit des systmes biologiques, dont la matrise exige une combinaison de modlisation et d'observations empiriques. Un contexte peut tre dfini comme tant lensemble d'informations permettant de comprendre ltat actuel du patient et prdire les changements potentiels. Les AC de MPOC mesurent un contexte CT (taux doxygne SO2) qui peut tre insuffisant pour valuer le risque en fonction des mesures initiales et de l'historique. Elles mesurent alors d'autres paramtres comme la temprature T (CT = {SO2, T}) ayant de bonnes chances dtre utiles. Sachant que lon ne peut pas tout mesurer tout le temps, considrer trop ou peu de paramtres de CT peut induire poser une mauvaise valuation de risque [29]. Par consquent, lAC doit identifier les paramtres pertinents, les changements et sadapter dynamiquement au contexte. Ceci soulve deux problmes majeurs : i) Prdiction du contexte pertinent pour le domaine mdical : Quels paramtres pertinents doit-on surveiller pour valuer ltat des patients? ii) Conception dun cadre architectural pour grer ladaptation des AC, la connectivit et la puissance de calcul des infrastructures disponibles en profitant de la proximit des objets connects, et de la capacit de calcul disponible dans le Cloud. Ce programme de recherche consiste rsoudre cette problmatique en gnral, et pour le cas de la surveillance de patients atteints de maladies (comme MPOC) ncessitant de la surveillance continue, mais pas d'hospitalisation. Il propose une mthodologie et un cadre architectural innovants des mesures par anticipation. En effet, les techniques de prdiction de contexte pertinent, le cadre architectural adaptatif et lintgration du concept Fog constituent manifestement des approches innovatrices. Notons que Fog est un paradigme des middlewares de lIoT qui peut offrir la distribution de charge et lamlioration de la performance [40]. Une plateforme d'adaptation des AC, intgrant ces concepts et techniques, sera ralise permettant ainsi de renforcer la reconnaissance prcoce de risque des patients et de rduire la liste d'attentes des patients et le cot de l'hospitalisation.""635085,""McIlraith, Sheila"
"637380"	"Menezes, Alfred"	"New Directions in Public-Key Cryptography"	"Public-key cryptography is a critical technology for securing communications over the internet and other communications channels. The two public-key technologies in widespread use today are RSA and elliptic curve cryptography (ECC). It has been known since 1994 that both these technologies are completely insecure against attacks by computers that are capable of exploiting quantum mechanical effects. However, despite intensive efforts around the world, it is still not known whether such large-scale quantum computers can actually be built. Nonetheless, many advances have been made and some researchers are making somewhat optimistic predictions of the timeline for building large-scale quantum computers. Motivated in part by these predictions, the demand by governments and industry for public-key cryptographic technologies that withstand attacks by quantum computers is growing. The goal of my research is to contribute to the development, analysis, implementation, standardization and commercialization of these quantum-safe public-key cryptographic technologies. I will also continue working on two ongoing long-term research programs. The first is the study of the security, efficiency and functionality of so-called pairing-based protocols. These protocols have been a major theme in cryptographic research since 2000. However, in the past three years several powerful attacks have been discovered which calls for a reassessment of their security and efficiency. The second ongoing long-term research program aims to get a deeper understanding of the concrete security guarantees actually provided by reductionist security proofs for cryptographic protocols, including protocols that have been designed to be quantum safe. The proposed research is inherently multidisciplinary in nature drawing from the disciplines of mathematics, computer science, and engineering. Graduate students who work on the research programs will be well prepared to enter the academic profession and the security industry and contribute towards making the internet quantum safe. ""629768,""Meng, Baozhong"
"634525"	"Merlo, Ettore"	"Automated detection, explanation, and remediation of security inconsistencies in Web application access controls using program analysis"	"The proposed research aims at improving the quality and the security integrity of software, while reducing its development cost in the perspective of application security. Today's large deployment of Web and mobile applications, cloud services, and cyber-physical systems demand frequent and short release cycles or continuous releases. This puts even more stress and time constraints on quality assurance in general and on application security. I propose to design, implement, and evaluate automated and scalable methods for the early fault localization and automated repair of security inconsistencies and vulnerabilties in access controls in software applications. I propose to localize faulty security code by investigating counter-examples from violated policies in security models and to synthesize human-usable explanations. In this proposal, I want to address and investigate the automated repair of detected security inconsistencies by reasoning on the localized faults that correspond to executions that violate some role-privilege policies. For example, missing checks could be repaired by automatically inserting proper authorization checks to restore the desired security reachability and accesses to security sensitive resources. Two problems appear when path based security repair is sought: (a) selection of code fragments implementing appropriate security checks to be inserted, deleted, or modified to repair the detected inconsistencies. (b) where to insert the checks along the possibly many paths that violate the security reachability constraints. In this proposal, I want to address first the problem of automated security repairs and second the problem of optimal placement of required new security checks. I want to determine the categories of security problems that can be automatically repaired, thus relieving the developers from this burden. I want to investigate their significance in large industrial or open source systems. When automation cannot be completely achieved for some inconsistency category, I want to investigate an interactive and recommendation-based strategy to support the developers during their manual repair of inconsistencies by supplying explanations and suggestions. The proposed research on automated repairs will prevent detected and repaired inconsistencies from being released. Software systems will be more secure and less vulnerable to attacks. The overall process from detection to repaired release will be shorter. Therefore, the window of opportunity for attacks will be dramatically reduced. Results from this research will be methods and tools available to researchers for automatically analyzing and repairing large applications in the perspective of security. Findings about the effectiveness of automated detection and repair of inconsistencies in large and popular open source applications will also be produced.""631712,""Merrill, Evelyn"
"637326"	"Michelson, David"	"Wireless Channel Sounding using Distributed and Synthetic Arrays"	"As wireless systems have adopted increasing complex methods and techniques to achieve greater capacity, reliability and energy efficiency, their performance has become increasingly sensitive to a broader range of propagation impairments and channel models have become evermore complex.We propose to develop novel and improved methods for characterizing wireless channels using either synthetic or distributed arrays and thereby support the design and deployment of next generation wireless systems in new and challenging scenarios. Anticipated outcomes include: 1) more cost effective channel sounder architectures, 2) better methods for extracting channel characteristics from raw signal data, 3) better methods for capturing the channel characteristics in the form of channel models. Distributed Array Channel Sounders . We propose to replace the mechanical switches used in current distributed antenna channel sounders with optical-delay-line multiplexers. By eliminating the high cost, slow switching speeds and poor reproducibility of fast mechanical switches and providing simultaneous sampling, ultra-fast multiplexing (microsecond delays) and excellent reproducibility, such channel sounders have the potential to revolutionize work in this area. We will focus on developing better methods for extracting channel characteristics from raw signal data produced by such a channel sounder and better methods for capturing the measured channel characteristics in the form of DAS channel models. Ultimately, more effective DAS channel sounders and standardization of DAS channel models will facilitate fair comparison between alternative architectures and protocols and spur advances in DAS performance enhancement and cost reduction. Synthetic Array Channel Sounders . Spatial channel models play a crucial role in predicting the performance of multiple-input multiple-output (MIMO)-based wireless communication systems in mobility environments including high speed rail. Current approaches to characterizing spatial channels require either mechanically steerable directional antennas or virtual / physical antenna arrays at both the transmitter and receiver, but these are generally unsuitable for use in mobility environments. We suggest that a spatial channel measurement system suitable for mobility environments can be simplified considerably by casting it as a bistatic synthetic aperture radar (BiSAR) with a stationary transmitter and a mobile receiver that moves with respect to the environment. The hardware required is trivial compared to conventional spatial channel sounders and lends itself to routine use by operators. Ultimately, more efficient and effective spatial channel sounders will improve both academic and practical understanding of the factors that affect MIMO system performance.""629127,""Michnick, Stephen"
"637415"	"Miller, Daniel"	"Adaptive Control of Time-Varying Systems"	"In systems control, the objective is to make a physical system(the plant) act in a desired manner through the use of an(automatic) controller, e.g. an autopilot (the controller)is used on an aircraft (the plant) to maintain speed,altitude and direction. The first step in control system design is to obtaina mathematical model of the plant, and then one designs a controller,described by a mathematicalequation, which is typically implemented in software.A common stumbling block in this process is uncertainty in theplant model, which can be caused by such things as modelling error,changing parameters due to wear and tear, changing operatingconditions (such as the altitude of an aircraft or thechanging mass of a payload in a robotic system), or systemsfaults (common in industrial systems). If theuncertainty is large, then a simple proportional-integral-derivative (PID)controller cannot be used, and a more sophisticated approachmuct be adopted. One powerful approach is that of adaptive control,wherein the controller adapts itself to the plantas it learns more and more about it over time. This approach has its roots in the 1950s, and it has grown more and more sophisticated over the years. The increased computational power of computers allows more complicated control algorithms to be implemented in real-time, so it is making inroadsin areas like robotics and aerospace. However, there are still unanswered questionsin the field, and in my proposed research the goal is to answer severalimportant ones: (i) Is it possible to design adaptive controllers which not only provide goodperformance asymptotically, but also provide it in the short-run,while the adaptive controller is still learning about the plant? (ii) Is it possible to handle rapidly time-varying parameters as well as the morecommon case of very slow time-varying parameters? (iii) Is it possible to redesign classical adaptive controllers to make themmore powerful: more robust, more noise tolerant, and better at tolerating time-varyingparameters? The answers to these questions will enhance the state-of-the-art of adaptive control, and reduce the gap between theory and practise. It will provide control engineers with new algorithms; it will benefit a team of graduate students by engaging in advanced technological training; and it will enhance Canada's position as a leading proponent of advanced automation.""640758,""Miller, Donald"
"637596"	"Mintchev, Martin"	"Advanced Electronic Microsystems for Biomedical and Oilfield Instrumentation Applications"	"The goal of my proposed research program is to study, design, implement, and test innovative advanced electronic microsystems for biomedical and oilfield applications, develop patent applications around them, and commercialize them as soon as these ideas mature enough to reach industrial applicability. My plan in the next 5 years is to research, develop and test two innovative electronic microsystems, the In Situ Lab Aid (ISLA), a new wristwatch-like device for painless, wireless, single-use blood analysis for early breast cancer screening; and a miniaturized, self-calibrating, mobile inertial microsystem for downhole navigation in lateral drilling from existing unproductive oil wells. The idea behind ISLA is quite innovative: after a blood sample is extracted with an in-plane MEMS actuator based on shape memory alloys (SMA), it is subjected to an immediate, intermittent interrogation in situ by a miniature high-intensity super-luminescent light emitting diode and the resulting fluorescence is recorded until the blood sample coagulates completely. The hypothesis is that these fluorescence data will be extremely useful for the early diagnosis of breast cancer. A recent (2015) study on blood fluorescence spectroscopy for breast cancer screening revealed the sensitivity and specificity of this approach to be 80.4% and 100%, respectively. Having a minimally-invasive, wrist-watch like device to perform this blood fluorescence test in situ will be a tremendous advancement in early breast cancer detection. In the area of inertial navigation in downhole oil and gas exploration, we recently suggested a radical new error-reducing approach using an innovative method called ""In-Drilling Alignment"" (IDA). My idea now is to apply the technique of IDA for lateral drilling from existing unproductive wells. The goal is to miniaturize the IDA setup to fit in a 12 mm by 100 mm enclosure, in which the IDA-required controlled mechanical motion for error compensation will be provided by the drilling fluid dynamics in the narrow lateral pipe itself. In addition, a relay-based wireless communication downhole is an essential part of this proposal and involves research on utilizing the existing downhole oil-drilling pipes as waveguides for xbee-based autonomous units optimally positioned within the pipes at their junctions (minimal distance of 10 meters between each xbee unit) and employing the ZigBee communication protocol. The benefits of the proposed approach are twofold: (A) unproductive oil wells could be re-explored laterally for adjacent oil or gas deposits, thus reducing the environmental impact of new well development; and (B) novel, wireless, and much faster communication of downhole measurements will be developed, which will be posed to replace the existing mud-pulse telemetry, an obsolete downhole data transmission technology based on creating pulses in the drilling mud by mechanical valves.""637213,""Mirabbasi, Shahriar"
"634853"	"Mishna, Marni"	"Singularity analysis and the large scale behaviour of combinatorial structures"	"Many problems from the natural sciences and computing theory are modelled using discrete combinatorial objects, such as trees, sequences, and random walks. Interesting applications require an understanding of the properties of these objects when they are extremely large. The models aid in predicting the run-time of algorithms, properties of genomes, and phase transitions in chemical processes. Naive strategies are quickly overwhelmed by the sheer size of the objects. Formal power series (known as generating functions in this context) have proved to be an efficient and effective way to study enumerative questions, offer insights into distributions of important parameters, and other large scale behavioural questions notably when tools like exhaustive generation are no longer feasible. The proposed research program develops theory and applications for an important class of generating functions in order to answer enumerative questions, and to develop algorithms to randomly generate objects. This kind of information is key to evaluate the choice of a combinatorial model in a given application. Efficient uniform random generation gives a glimpse of what a typical object of large size looks like. Asymptotic enumeration formulas are simple enough to test against. This program develops new methods and techniques by focussing on lattice walks and grammars. Lattice walks are a basic, yet customizable object: one controls the allowable steps and the region of interest. Grammars are a formalism for describing objects. This is an ideal context from which to study generating functions. The key novelty of this program is the analysis of multivariable series by extracting relevant subsidies. This is done through a study of integrals, with special properties. Computer algebra, algebraic geometry and complex analysis all intervene to unravel structure and provide insight. When a combinatorial class can be written using a grammar, there exist efficient strategies for random generation. We investigate this in the case of some graph classes. When no grammar (provably) exists, they are still useful: It suffices to find a combinatorial class that is generated by a grammar, that contains the desired class and not much else. In this case rejection algorithms are provably efficient. This research is important to anyone that studies combinatorial models-- from natural sciences to pure mathematics. New strategies to understand the large scale behaviour of combinatorial classes has the potential to advance any field manipulating big data.""630510,""Mishra, Ram"
"634314"	"Mitchell, Ian"	"Numerical Algorithms for Verification, Design, Analysis and Operation of Shared Control Cyber-Physical Systems"	"Objectives: The proposed research is part of a multi-year program to develop numerical algorithms and implementations for the verification, synthesis and design of safe controllers and planners for cyber-physical systems subject to uncertainty arising from the influence of external agents whose behaviour is revealed at run-time. Examples of this often non-random uncertainty include human-in-the-loop control and systems using legacy or independently designed black-box controllers whose safety properties cannot be guaranteed. The techniques developed will maintain safety and support pursuit of the agent's goal in a manner that aligns with user expectations. The research will be informed by shared control applications in powered wheelchairs for cognitively impaired older adults, automated delivery of anesthesia, unmanned aerial vehicles, and collaborative manufacturing. Approach: The foundation of this research is development of numerical algorithms for approximating reachability of systems with continuous or hybrid state models, and in particular algorithms which can generate results robust to model uncertainty. The goal of this algorithm development is not just verification, but rather characterization of the set of control inputs which lead to correct behaviors. The second element of this research program will be to design interventions that use this information to maintain safety or assist the agent to attain its goal, despite uncertainty about that goal and system state. The affective outcome of these interventions on the system's user will be a key feature in their design Expected Significance: Unexpected and incorrect operation can lead to a host of negative consequences, and the potential significance of bad behaviours and/or outcomes is increasing as we transition to human-in-the-loop scenarios and/or compose controllers with poorly understood safety characteristics into complex systems. By developing formal verification methods which can treat online input sources in a black-box fashion, my research program will increase the confidence of designers and users that these systems will perform correctly. By considering how these problems are best solved in a diverse set of application areas, approaches which are more broadly applicable can be identified and subsequently extended to other domains in which cyber-physical systems are prominent. Trainees will gain experience in numerical algorithms, robust software development practices, health care technology development and/or robotics, which are all areas with positive job prospects. ""629011,""Mitchell, Jennifer"
"637447"	"Mousavi, Pedram"	"Multifunctional Antenna Systems for 5G Applications"	"The unprecedented emergence of new wireless applications in recent years pervades all aspect of our life. It is predicted that by 2020, the number of connected wireless devices will be 34 billion. The emerging 5G network brings the Internet of Things (IOT) to reality by connecting billions of devices. 5G IoT devices promise to create the next wave of Internet-enabled innovation with significant potential to reshape many industrial and consumer environments. They will enable many applications such smart home; wearable devices for health monitoring and performance assessment; environmental monitoring; education, entertainments, and smart appliances. To host all these applications and accommodate the future ones in a single platform; scalable, flexible and multi-purpose energy smart wireless devices are required. These devices need to sense environmental conditions, adapt to user requirements, and decide how to communicate among themselves in a self-organized manner for hyper-connected Internet of everything. Traditionally, these features are implemented at higher physical layers, which are far away from the antennas which have direct interaction with outside environments. This approach leads to bulky, complex and expensive systems with numerous components, which are not amenable to the high level of miniaturizations and integrations required by modern wireless applications. The systems also require significant digital signal processing capacity and have high power consumptions. These issues prevent them of being widespread commercial products. These challenges highlight the need for a new conceptual approach that will redefine the way that these wireless devices interact with their environments. We propose to transfer the implementation of main features such as sensing, intelligence and adaptability from higher physical layers to the level of antenna apertures and investigate new topologies in which these systems become integral to antenna structures. This approach enables antennas to act as energy efficient multi-functional systems with unparalleled level of integration with much less required processing power. The proposed concept has a transformational effect on the future of 5G IoT systems and devices. It is anticipated that many Canadian industries such those in wireless communications, oil and gas, constructions, smart homes, and environmental monitoring significantly benefit from the outcomes of this proposal. The HQPs are the integral part of this proposal. With the unique training setting, they will be ready to tackle the future cutting edge challenges in this field.""619356,""Mousavi, Pedram"
"637156"	"Musilek, Petr"	"Data-driven Methods for Integration of Distributed Energy Resources"	"The balance between demand and supply is crucial to the safe, reliable and efficient operation of electric power systems. With the advent of distributed energy resources (such as photovoltaic panels and battery storage systems), maintaining this balance becomes more and more challenging. This is caused by the unprecedented complexity of modern power grids, and by the limited control over the energy produced by intermittent energy sources, such as wind and solar. This research program will address these challenges by exploiting energy system data (including data on generation and loads, weather forecasts and energy market conditions) for the design, monitoring and control of electric power grids. This research will leverage and expand the successful results developed during the previous discovery funding cycle, and under related collaborative RD projects with industry. It will develop new solutions for managing energy in a variety of contexts, from residential buildings, through community energy storage systems, to grids powering extensive urban and rural areas with a high penetration of dispersed generation and renewable energy sources (such as roof-top solar panels or small wind turbines combined with battery energy storage). The use of a data-driven approach is a distinguishing aspect of this proposal. It means that the safe, reliable and efficient operation of modern electric power systems will be driven by local measurements of how the system is behaving now and has behaved in the past. The data-based predictive energy management methods and agent-based balancing protocols will effectively integrate smart grid components with forecasting, analytic and control functions that will also be developed in this research. This will lead to a new type of distributed, reconfigurable systems capable of local optimization while providing global efficiency and reliability through coordination. In practice, this will mean lower power bills for consumers and improved profitability for energy companies . The developed technology will be essentialin realizing the expected environmental, reliability and economic benefits of modern electrical grids. This is crucial as the implementation of renewable generation technologies is expected to double by 2020, also doubling the current Canadian investment in renewables of C$10B per year. In addition to ensuring return on this investment, the research outcomes will also bring more direct economic benefits through commercialization, technology transfer and spin-offs . Over the course of five years, this research program will also train a number of highly qualified professionals (HQPs) including up to 4 PhD and 6 MSc graduates and 10 BSc students. They will be equipped with the expertise to design, plan and operate the future smart power systems, and conduct related advanced research in an academic or industry setting.""640672,""Musilek, Petr"
"634513"	"Nadi, Sarah"	"Mining Software Repositories to Infer Software Product Line Migration Strategies"	"A 'one size fits all' software system is no longer practical; customizations to fit various hardware and user requirements are often necessary. For example, Hewlett-Packard produces over 40 printer models, and the Linux kernel supports 32 computer architectures that are included in devices ranging from mobile devices to enterprise servers. Software is often designed for a specific product in mind, and when new requirements arise, the old product is copied and modified to fit these new requirements. This copying approach is cumbersome, and managing separate related products is error-prone and can lead to redundancy (e.g., applying the same bug fix to different products). Software Product Lines (SPLs) avoid exactly this situation. An SPL is a shared, configurable platform that provides a set of features (units of functionality) that can be combined to create a specific software product, while respecting given feature dependencies documented in a variability model .Advantages of SPLs include an easier approach to produce tailor-made software with reduced cost, improved quality, and reduced time to market. Unfortunately, software systems are often conceived as individual products; only later, after multiple related products have been produced, does it become obvious that they should have been better developed as part of a single coherent SPL. However, migrating a set of related software products into an SPL is costly and requires high manual effort. The long-term goal of my research program is to create methods and respective tools that provide automated support for SPL migration. My overall methodology relies on leveraging code evolution and process knowledge mined from software repositories (e.g., version control systems) along with insights gained from quantitative and qualitative empirical studies involving practitioners. My proposal integrates two research activities, each targeting a set of problems that address the above goal. Re-engineering related products: What re-engineering steps can consolidate related products into a single shared configurable system? Depending on the programming language and initial design of these products, what is the most suitable architecture for supporting variability? Reverse-engineering variability models: Can we automatically determine feature dependencies and explain them to stakeholders? Can we automatically infer dependencies that stem from domain knowledge? The contributions of this research program will enable more systematic reuse of existing software, saving software architects valuable time spent in manual migration and allowing software producers to cope with the increasing demand for software. The HQP trained through this research program will gain experience in designing and conducting quantitative and qualitative empirical studies, mining software repositories, and analyzing source code and other software artifacts.""642671,""Nadi, Sarah"
"637152"	"Narimani, Mehdi"	"Advanced High-Power Systems for Medium-Voltage Motor Drives"	"To meet the growing demand for electrical energy while reducing environmental impact, electrical systems of the future will need to be much more energy efficient. High-power electric motor drives are the major consumers of electrical energy and they have significant potential for improvement in terms of energy saving. The key to this improvement is advances in power electronics technology. The proposed research will focus on advancing the state-of-the-art in power electronic technologies that operate in the medium-voltage (MV) range (2.3 to 15kV) for motor drive applications. High-power MV drives have a wide range of applications in the oil and gas, chemicals, mining and metals, cement, power generation and utilities, traction, and food and beverage industries. Energy efficiency, power quality, cost, size, reliability, fault protection, and easy installation and maintenance are the main challenges in the development of power electronics for high-power MV motor drives. Although existing MV power electronic converters based on the concept of multilevel structure have advantages over the conventional converters, their voltage level is still limited to 6.9kV. This is because of the limitation in voltage rating of the available power semiconductors in the market and the structure of the current multilevel MV power converters. The existing solutions for higher voltage need to employ a complex transformer, which is not efficient, or use a modular topology with a significant increase in the number of components which is not cost-effective and reliable. The proposed research program will focus on a novel family of modular and scalable power conversion structure for further expansion of MV drives for higher voltage (6.9kV) applications with improved efficiency, performance, cost, size, weight, reliability, and fault detection capability. It is expected that the theoretical results and the successful implementation of this proposed research program will provide a new set of power electronic topologies and advanced control techniques to introduce new technologies in the area of high-power MV drives which will produce significant energy savings for all sectors using industrial electric motors. This research program will also help to educate the next generation of power engineers with required knowledge and technical skills to contribute to local and national businesses and industries.""638267,""Narine, Suresh"
"637310"	"Nguyen, Ha"	"Design and Analysis of Communication Techniques for Future Wireless Networks"	"The explosive growths of wireless devices (smart-phones, tablets, machines, sensors, etc.) and mobile data traffic are driving demand for wireless access far beyond the capabilities of the current generation of wireless networks. Future wireless networks (5G and beyond) are expected to dramatically increase data rate, network capacity and energy efficiency and lower the latency. To meet these expectations, significant technological innovations will be required. Today, there is no single technology representing 5G networks and beyond. Instead, there are several promising candidate technologies, including in-band full-duplex (FD) radio, massive MIMO, millimeter-wave communications (mmWave), new multicarrier modulation, and relaying. The general consensus is that the integration of these candidate technologies will be required in future wireless networks. Among these emerging technologies, FD communication is very promising since it offers the potential to double the spectral efficiency relative to the conventional half-duplex systems by allowing a node to conduct simultaneous transmission and reception over the same frequency band. The concept of FD communication is not new but was long considered impractical given the very high level of self-interference that an FD device's transmitter causes to its own receiver. The renewed interests in FD are largely due to the fact that network providers have shrunk cell sizes to enable increased spatial reuse. Such architectural change towards shorter-range links permits reduced transmission power, thereby lessening the self-interference problem and making practical FD communications closer to reality. The long-term objectives of the proposed research program are to use tools and insights from information theory, communications, signal processing and optimization to advance understanding and innovations in providing more efficient communication services over wireless networks. The research program will be advanced to such long-term objectives through the pursuit of several short-term objectives concerning the design and analysis of FD communications with new multicarrier waveforms, namely generalized frequency-division multiplexing (GFDM) and circular filter-bank multicarrier (C-FBMC). These multicarrier waveforms are being seriously considered as the air interface in 5G networks and to replace the orthogonal frequency-division multiplexing (OFDM) waveform widely used in 4G networks. This is because GFDM and C-FBMC can resolve the major limitations of OFDM in requiring strict synchronization and generating high out-of-band emission. The outcomes of the proposed research program, both in research innovations and HQP training, will continue to help Canada remain internationally competitive and influential in the rapidly growing wireless communication industry.""621819,""Nguyen, Hiep"
"637363"	"Nguyen, KimKhoa"	"Optimized Orchestration of Massive Next-Generation IoT Applications"	"New standards recently set by 5G communications that many Internet of Things (IoT) applications are expected to follow, namely, massive throughput, massive low latency, massive sensing, massive energy efficiency, massive safety and security, and massive fractal heterogeneity, have driven new research on integrated platforms. Creating, deploying, and orchestrating applications across this multi-tenant, multi-service, and multi-speed ecosystem is very challenging because of the variety of diverse technologies, incompatible protocols, heterogeneous user profiles, and particularly the millisecond latency requirement of 5G applications. Also, new green technologies are required to offset the explosive increase of energy consumption of massive demands. The convergence of five enabling technologies - cloud computing, 5G, software-defined networking (SDN), network function virtualization (NFV), machine-to-machine (M2M) communication, and big data/machine learning will provide an opportunity to create IoT application platforms that offer unprecedented technical capabilities, scalability, energy efficiency, security, flexibility, and economics. The combination of these technologies, holistically investigated in this research, enables next-generation applications that are both context and environment aware. The goal of this research is to discover the potentials and push the boundaries of real-time optimization, dynamic resource allocation, and data transmission paradigms to meet new requirements of massive next-generation IoT applications, both in terms of quality and environmental emissions. Our research program will create new virtualization, routing and scheduling algorithms, optimization models, cognitive decisions, and computational game theory to orchestrate a platform for massive IoT applications at scale. In particular, this research will underpin two of the most challenging realms in the field: i) seamlessly migrating services to the edge in real-time through long distances, and virtually breaking the speed of light to achieve millisecond latency, and ii) real-time orchestration of unprecedented number of smart objects (order of billions) in vast forms of relationships and connections, as well as a multitude of temporal, spatial, economic, and environmental requirements. The impact of this research will be significant, both in terms of fundamental methodologies and advancement of novel technologies. It empowers knowledge enhancement through emerging theoretical models delivered with consistent practical experiences and enabled by sustainable use-cases. The platform, models, and paradigms resulting from this research will serve as the leading force in the development of next-generation IoT applications to unlock vast innovations in our networked-society and sustain economic growth and a high quality of life.""639583,""Nguyen, KimKhoa"
"637072"	"OLeary, Stephen"	"Semiconductor Materials for Future Device Applications:  Materials Issues and Device Implications"	"Semiconductors are the active electronic materials that are present within electron devices. A great deal of progress in electronics has been achieved through an understanding of the basic material properties of the semiconductors used within such devices. The proposed research aims to deepen our understanding of this class of materials and to explore the possible device applications. Specifically, the research to be performed by the researchers within my group will: (1) enrich a simulation tool within which Monte Carlo simulations of the electron transport may be pursued, and (2) use this tool in order to obtain electron transport results corresponding to a number of semiconductors of current interest. The device implications of these results will then be explored. The overall goal of these research endeavors is to advance our understanding of semiconductors, to provide researchers in the field with tools for the analysis of these materials, and to equip emerging areas of the electronics field with engineering methodologies for device design and optimization. The development of novel techniques for the analysis of these materials, and the use of these techniques for the purposes of device optimization, is the common theme underpinning these research endeavors. The semiconductor materials to be examined will include, but are by no means limited to, the III-V nitride semiconductors, gallium nitride, indium nitride, and boron nitride, and the II-VI semiconductor, zinc oxide, and its alloy with magnesium oxide. Other novel semiconductors, such as gallium oxide, will also be considered. Traditional semiconductor materials, such as silicon and gallium arsenide, will be considered, albeit for benchmarking purposes; Si and GaAs are mature materials, and thus, provide a point of reference for our electron transport studies. The proposed research program offers abundant opportunities for advancements in the basic fundamental science underlying semiconductor materials and in exploring the possible novel device applications that are thereby engendered. The training programs that are being supported through this research program will help supply the Canadian workforce with highly qualified personnel who can work in a variety of research and development capacities. The research that is to be performed, and the graduates who emerge from the group, will help nurture the growth and development of the electronics industry here in Canada, an industry that has been estimated by some to have the potential to exceed $1.7 Trillion in sales worldwide (per annum) by the year 2017. ""640415,""OLeary, Stephen"
"634549"	"Osgood, Nathaniel"	"Cross-Leveraging Computational, System and Data Science in Support of Computational Epidemiology in the Era of Big Data"	"Despite the rapid growing demand for -- and our contributions demonstrating the great potential of -- integration of ABMs with big data, existing simulation infrastructures provide poor support for model integration with incoming data. While our work has demonstrated that online computational statistics techniques such as Particle Filtering (PF) and Particle Markov Chain Monte Carlo (PMCMC) can form highly effective tools for integrating simulation modeling and incoming big data -- such as that from our popular iEpi system -- application of such techniques is at best awkward, and is often infeasible because of the high computational costs or high degree of implementation effort involved. To address this application area in which interdisciplinary teamwork is of central importance, we have secured strong success with our existing Frabjous domain-specific functional reactive ABM programming platform to significantly enhance ABM transparency, concision, and modularity. Despite these contributions, current ABMs commonly lack publishable specifications, are typically quite opaque to non-technical stakeholders and often confusing even to technical team members, raise significant performance barriers to scenario-based exploration by policy makers and analysts, and poor support for collaborative interaction and sharing across teams. We propose here to address these challenges using a multi-pronged strategy that starts with a port of Frabjous to the Scala language (FrabjouS) -- a language which offers strong support for modularity, parallelization, domain-specific language design and interoperability, and which we have used for many other tools. Work following this port is divided into four relatively autonomous streams. The first focuses on integrating language support for key computational statistics algorithms PF, PMCMC, and MCMC and for streaming interfaces using the streaming component of the popular Spark data science platform. The second seeks to greatly enhance ABM performance via multi-level parallelization (exploiting both distributing computing, multi-cores and GPUs at a finer-grained level), including via integration with the Spark platform. The third interface uses monadic composition to both ease common modeling tasks, and to empower model end-users to undertake analysis tasks traditionally requiring programmer support. Finally, in collaborations with a leading researcher in this the area of Human Computer Interaction (HCI) and Computer Supported Cooperative Work (CSCW), we will adapt techniques successfully used in our existing collaborative model mapping tools to implement a graphical specification language for FrabjouS as well as a collaborative tool supporting multi-user exploration, running and modification of FrabjouS models. Finally, across each phase of the work, we will evaluate model success with user studies.""637720,""OShaughnessy, Douglas"
"637569"	"Pahlevaninezhad, Majid"	"Novel Control Architectures for Power Electronic Converters in Micro-Grids"	"The limited supply of fossil fuel-based energies, and growing global concerns about the imminence of climate change are a leading cause as to why wind and solar are the fastest growing sources of energy in Canada. As renewable energy production increases, the stability of the grid system will experience detrimental impacts due to the intermittent nature of wind and solar energy. The irregularity of renewable energy can cause fluctuations in the grid voltage and grid frequency, and these fluctuations may jeopardize system stability. For instance, in Germany, once solar energy production reached 8% of their total energy production, they began to see a significant amount of fluctuations in their grid voltage and grid frequency. In North America, wind and solar energy penetration is low compared to other sources of energy. Currently, wind energy is responsible for producing only 3.5% of electricity, and solar energy accounts for less than 1% in Canada. Since the penetration of wind and solar energy has been limited in North America as of yet, harmful effects on the grid system have been negligible. However, in the near future these adverse effects will no longer be negligible, because of the urgency to replace mainstream power generation with renewable energy. Thus, the integration of renewable energy into the grid system is posing a greater challenge as renewable energy installations continue to grow. In order to avoid instability and absorb the grid fluctuations, the common approach has been to install high power grid stabilizers in the transmission and distribution network. However, this approach has multiple disadvantages: the infrastructure is very costly, inefficient, not scalable, and requires a substantial amount of maintenance. The main objective of the proposed research program is to provide solutions that will actively absorb grid fluctuations and stabilize the grid system through the use of power electronic converters. Power electronic converters are requisite components of renewable energy systems, and operate as a grid interface. Since power electronic converters are a part of the existing infrastructure of renewable energy systems, the cost and scalability of this approach is a vast improvement over the common approach. This research aims to deliver more intelligent next generation power electronic converters for renewable energy systems that can address the aforementioned difficulties at the site of origin. These next generation power electronic converters will utilize new control systems that enable them to support the grid system. Thus, the main objective of this research program is to develop novel advanced identification algorithms and adaptive control schemes to achieve these goals. The proposed research program will directly benefit the Canadian energy sector by seamlessly integrating renewable energy into the grid system, without adding costly grid infrastructure.""642701,""Pai, Dinesh"
"634648"	"Pai, Dinesh"	"Computational Models of Humans"	"My long term goal is to create useful computational models of how humans move and physically interact with their environment. The focus is not on black-box modeling of movement data. Rather, it is on modeling the underlying system itself, as a biological machine: the motors (muscles), sensors (esp. mechanoreceptors in muscle and skin), structures (skeleton and connective tissues), and software (neural control). By modeling the underlying sensorimotor system we could achieve better generalization to novel situations and gain fundamental insights into biological principles. In the near term, we could create more believable computer animations of human characters. Specifically, I propose to focus on the following. (1) EULERIAN SOFT TISSUE SIMULATION. Eulerian discretization has shown great promise in my previous work, but constitutes a radical departure from the standard Lagrangian approach. I will systematically explore its strengths and limitations, and identify the significant advantages that could make it the future standard for soft tissue simulation. (2) REALISTIC SKIN MODEL. It is not sufficient to have good algorithms; a model must also reflect real human physiology. My goal is to develop a simulation-ready structural and statistical model of the mechanical behavior of real human skin, based on in vivo measurements. (3) HAND SYSTEM. Modeling the hand as a sensorimotor system with many interacting parts will provide deep insights into why it is so effective. I will focus on the hand since it is our primary effector for interacting with the physical world (including computers) and the key role of skin and soft tissues in hand function. Such virtual human models are required in an enormous range of applications. A central goal of computer animation is the synthesis of human characters interacting with their environment. Realistic models of human movement are also important in biomechanics, neuroscience, and medicine, and could lead to new robot designs based on biological principles. Students and postdoctoral trainees are involved in all aspects of this research, and will acquire a broad range of skills in an interdisciplinary environment, including development of advanced simulation software systems, measurement and analysis of human soft tissues with world class instrumentation, biomechanics, and computer graphics.""628842,""Pai, Emil"
"635105"	"Pawlak, Miroslaw"	"Machine Learning for Signal Analysis and System Modeling: Sparse and Event Driven Strategies"	"The proposed research is in the area of machine learning (ML) and its applications to signal processing and system modelling. ML is one the fastest growing fields of computer science, with far-reaching applications. These diverse applications demand for new ML algorithms able to cope with massive data sets of high dimensionality. In this proposal, I describe my plan to extend my current research on nonparametric/semiparametric learning methods to ML problems that are driven by two concepts: sparsity and events. This challenge includes not only the development of the new basic methodology, but also to verify it in the framework of signal and system analysis. Testing of the proposed methods in concrete applications within the areas of power engineering and biological signal processing is also planned. My research proposal relies on the idea of blending the modern nonparametric/semiparametric learning methodology with the concepts of sparsity and events. The sparsity approach to ML is based on the observation that real-world signals and systems are well characterized by a relatively small number of relevant parameters when compared to the dimension of their original space. Sparse modeling is deeply rooted in the ancient principle of parsimony and can be related to our daily lives. The key problem is to discover a sparse representation and its form in a given setting. The current research on ML with sparsity is mostly confined to finite-dimensional linear models. In this set-up the sparsity refers to the condition that most variables are close to zero. Efficient solutions for sparse linear models are based on the convex relaxation of the penalized least-squares criteria yielding the celebrated Lasso algorithm and its extensions. The event-driven approach can be viewed as a specific form of obtaining sparsity by generating a representation of the infinite-dimensional object based on observations that are acquired only when the event triggering condition holds. As a result, the object is represented by sparsely and randomly distributed instances. In signal analysis concrete event-based representations can be obtained from a sequence of events like level crossings and local extremes. Various event types may result in different sparse representations. Research is planned to select or combine various event schemes for the maximum accuracy and efficiency of the event-based learning. The challenge of this proposal is to give thorough examination of the sparsity of infinite-dimensional objects generated by classical and event-based sparse representations. I believe that the proposed sparse and event driven ML paradigm can substantially enlarge a scope of ML applications and improve the existing algorithms within the context of examined case studies. Over this 5-year cycle I propose to train 6 PhD students (3 current + 3 new) and 3 MSc students (1 current + 2 new).""637754,""Pawlik, Marek"
"634774"	"Pei, Jian"	"Querying Dynamics in Evolving Graphs and Networks"	"In many applications, such as social network analysis, fraud detection, and network quality diagnosis, a large amount of complex data are modeled as graphs or networks. Graphs and networks enable us to analyze sophisticated behavior that cannot be understood comprehensively in the past, such as influence propagation and group collaboration in a social network. Such dynamics are a beauty in graph and network data. Moreover, due to the scale and the complexity of data modeled as graphs and networks, changes and evolutions are inevitable. Analyzing and making good use of dynamics in evolving graphs and networks, on the one hand, provide us unprecedented power and tools to conquer big data, and, on the other hand, post grand technical challenges. This proposed research is to embrace the opportunities and address the technical challenges. The project will investigate a series of novel queries about dynamics in evolving networks and develop effective and efficient algorithms to answer those queries. Moreover, we will develop a graph database to integrate our inventions and support distributed analysis of dynamics in huge evolving networks. Last, we will conduct case studies to assess the algorithmic design and also appraise the effectiveness and feasibility of our research in real applications. This research is important to academia since it will advance the frontier in the fast-growing area of graph databases. It is also important to industry and applications since the tools developed will bring users critical capability in analyzing dynamics in evolving networks. Moreover, the HQP training component will prepare a group of graduate students for their future professional careers.""624173,""Pei, Zhenning"
"637714"	"Pellizzoni, Rodolfo"	"Many-core Platforms for Time-Critical Systems"	"Cyber-Physical Systems (CPS) are the next generation of smart, interconnected embedded systems. Advanced CPS, such as autonomous vehicles, rely on complex sensor processing and sensor fusion capabilities; hence, they require significant computational power. To address such needs, many-core architectures are increasingly considered attractive choices. Proposed platforms include both scaled-up versions of traditional multi-core architectures, as well as complex systems-on-chip comprising single-instruction-multiple-data accelerators such as GPU, which are especially suitable for video processing and similar highly parallel tasks. CPS are safety-critical systems exhibiting real-time requirements: a failure to compute required actuation commands within a precise time window (deadline) can lead to catastrophic results, including loss of human life. For this reason, computing platforms used in CPS must be timing predictable, i.e., able to provide strict timing guarantees and performance isolation between subsystems with different criticalities. Unfortunately, current Commercial-Off-The-Shelf (COTS) many-core architectures are mainly targeted at either the consumer or high-performance computing markets, and thus are not designed with such objectives in mind. We argue that the key to guarantee predictable timing for safety-critical applications is to control accesses to shared resources, including the memory hierarchy and on-chip interconnections, and coordinate their usage between all processing cores. The main goal of this project is to develop, implement and demonstrate software-based, predictable execution models for real-time tasks on many-core platforms. An automated compiler toolchain will be used to analyze, compile and optimize parallel programs based on a parallel programming API (OpenMP). Novel thread allocation and scheduling schemes will be introduced to optimize the usage of memory and communication bandwidth, while at the same time facilitating timing analysis. The proposed research will make essential contributions to the way high-performance, time-critical CPS are designed. We believe that this is a strategic area where fundamental research, as enabled by the Discovery program, is sorely needed to achieve initial prototype status. We will demonstrate the validity of our toolchain and algorithms through both architectural simulations and implementation on existing many-core COTS platforms. Our techniques will be integrated and tested in the context of an autonomous vehicle demonstrator, developed collaboratively at the University of Waterloo.""636747,""Pelot, Ronald"
"621733"	"Peng, XueBin"	"Ontario"	"CANADA"
"634827"	"Pesant, Gilles"	"Exploiting the Combinatorial Structures Found in Constraint Programming Models as Multivariate Distributions"	"Many sectors of the economy rely on our ability to solve complex combinatorial problems in order to plan, organize, or schedule their activities in the best way possible. For example: scheduling nurses in a hospital according to their individual skills and preferences so that they cover theforecast demand in patient care, and assigning patients to them so as to achieve a balanced workload; or planning the construction of a forest road network in order to harvest and then transport the lumber, designing truck routes between harvesting sites and mills, and building synchronized schedules for trucks and loaders. Among the computerized methods to accomplish this, Constraint Programming represents the problem using a formalism that exposes much of its structure. This research proposes to use that structure in a deeper way in order to improve our ability to solve combinatorial problems. Any progress towards that general goal can apply to each of these sectors and therefore can have a significant impact on our productivity and well being. ""628057,""Peshimam, Adnaan"
"634595"	"Petrenko, Alexandre"	"Fault Model-Based Testing"	"The proposed research focuses on a model-based testing from Finite State Machine (FSM) and Input/Output Transition System (IOTS) models, including their extensions. It advocates fault model-based testing, which draws constantly growing interests of both, researchers and test practitioners. One of the reasons for this is the fact that the program/specification coverage does not provide any guarantee for fault detection. The dictum of Dijkstra could be mentioned here: testing can be used to show the presence of bugs, but never to show their absence. The proposed research is aimed at finding an answer to the question: can testing at least guarantee that no faults of certain types are left? The challenge then is how faults should be modeled. We define a fault model as a tuple of a specification, fault domain, and conformance relation. A fault domain as a set of all possible submachines of a given nondeterministic machine, called a mutation machine of a given specification machine. The mutation machine contains the specification and extends it with a number of mutated transitions modelling potential faults. In our recent work, we developed an approach for test generation which is based on logical encoding and SMT-solving. Considering this approach as a starting point, we plan to investigate a general fault model-based testing framework to deal with a variety of state-oriented models, from classical FSM and IOTS models to their symbolic extensions reaching the level of extensions found in state-oriented models used in model-driven development of critical applications, such as Simulink Stateflow and UML state machines as well as activity and sequence diagrams. The goal of this project is to develop scalable testing approaches (supported by prototype tools) for developing tests with a coverage guarantee for fault models chosen by the domain experts based on the history of a product development, companys specifics and their test assumptions.""627696,""Petrick, Heather"
"637414"	"Petriu, Emil"	"Intelligent Tactile-based Dexterous Robot Hand Control"	"The main objective of the proposed research is to develop novel real-time computational-intelligence tactile feedback-based control algorithms for a human-like multi-finger robot hand able to dexterously explore, grasp, and in-hand manipulate 3D objects. Dexterous object exploration, grasping and manipulation is a complex area of robotics in which multiple fingers cooperate to carry out the task. Controlling a dexterous robotic hand poses major design challenges for the tactile sensors that are the first line of contact, for processing the sensor data, detecting object geometry, slippage and contact pressure, and then for the tactile and kinaesthetic feedback-based coordinated control of all finger movements and applied forces. The short-term objectives are (1) development of new algorithms for the fusion of tactile- and kinaesthetic-sensor data to provide a coherent multisensory feedback for the real-time control of the dexterous robot hand, (2) development of new neuro-fuzzy reactive-behavior control algorithms for concurrent position force control of hand fingers during object exploration, and (3) development of task-driven intelligent control algorithms for dexterous object grasping and in-hand manipulation The new intelligent tactile-based hand control techniques to be studied will have a significant impact on an emerging range of robotics applications requiring advanced dexterous object exploration, grasping, and manipulation capabilities such as: (i) autonomous or remotely controlled robotic handling of materials in hazardous environments, high risk war-zone and security operations, or difficult to reach undersea and outer space environments, (ii) tele-surgery and tele-medical diagnostic, (iii) a new generation of humanoid robots for eldercare assistance and social robotics.""620364,""Petrone, David"
"634558"	"Pientka, Brigitte"	"Beluga: Building Trustworthy Software Systems through Programming Proofs"	"Software systems are an integral part of our infrastructure and we are increasingly dependent on them: Software manages our financial assets, assists in driving our cars, and plays a central role in electronic voting. At the same time, security vulnerabilities and computing errors can lead to massive disruptions of our infrastructure and safety-critical failures in services. Thus, ensuring that software systems are more trustworthy and reliable is not only essential to the continued success of the computing industry, but to our economy and society at large. Over the past decade we have made significant progress towards building a trustworthy computing infrastructure by verifying compliance of software components with a formal safety policy and eliminating software ""bugs"" that can lead to security vulnerabilities and computing errors in compilers and operating systems. However, the cost of verification remains exorbitantly high and we often concentrate on establishing only shallow safety guarantees about sequential programs. The objective of my research is to develop a type-theoretic foundation for verifying deep properties about sequential, distributed, and reactive programs that provides abstractions and primitives for common and recurring concepts and makes mechanizations modular, and easier to maintain and reuse. This can have a major impact on the effort and cost of mechanization: By factoring and abstracting over low-level bureaucratic details, it reduces the time to prototype formal systems and their meta-theory, and avoids errors in manipulating low-level operations. More importantly, it can make a substantial difference to automatic verification. Concretely, we plan to address three major research challenges: 1) Extend the existing type-theoretical foundation to be able to verify deep properties about sequential programs such as full abstraction, which states that two components are indistinguishable in any valid program context. 2) Develop a foundation to establish properties about programs that are intended to run continuously, i.e. distributed and reactive systems. 3) Design and develop tools that automatically discharge as many proofs as possible to make it routine work for programmers to specify and automatically verify complex properties of their programs, as writing out proofs by hand can quickly become overwhelming to programmers. My long-term goal is to bring down the cost of verification and make it common practice to communicate, exchange, and verify proofs to ensure that software is reliable and safe. This is essential to building a trustworthy and reliable computing infrastructure. ""622867,""Pierce, ColinEllison"
"617739"	"Pineau, Joelle"	"Deep Reinforcement Learning for Dialogue Systems"	"In the space of a few years, machine learning has moved from an academic discipline pursued by a small group of dedicated researchers, to a mainstream industry, poised to solve any and all problems where data is available. At the core of this revolution are a set of flexible, robust and powerful algorithms, many of them based on neural network and deep learning architectures. One particularly promising direction for the deployment of deep learning technologies is in building language understanding and generation systems. Our work aims to extend deep learning methods to tackle challenging tasks in natural language understanding and generation by exploiting the reinforcement learning paradigm. Reinforcement learning (RL) provides a rich mathematical and algorithmic framework for tackling the problem of sequential decision-making with an unknown (or partially known) system. This project focuses primarily on the development of new reinforcement learning methods for conversational systems, and in particular for deployment in Dialogue Management systems. This domain is chosen due to the sequential and interactive nature of the task, and the possibility for feedback to occur at different time points, which are a good match for the RL setting. In Dialogue Management, the main goal is to build an AI agent that can converse with a human user using natural language. Such systems can be used to allow different types of interactions between a machine and user: social chatting, acquiring information (e.g., customer service), conducting transactions (e.g., online banking or reservation system). Eventually, a complete system can incorporate several of these aspects, allowing a rich and frequent pattern of interactions. In terms of our objectives, the Dialogue Management task is a classic example of an RL problem, as it requires the machine to learn a good strategy for producing a sequence of dialogue acts (responses) throughout the conversation. Our goal over the next five years is to develop a range of RL models and algorithms suitable for deployment in dialogue systems, with specific examples and benchmark results of their performance quality. We also aim to develop the next generation of ML researchers with graduate-level training at the intersection of reinforcement learning, deep learning and NLP.""635067,""Pineau, Joelle"
"617766"	"Pitassi, Toniann"	"New Directions in Complexity Theory"	"Computational complexity studies the inherent limits of computation, in terms of resources such as time,space, randomness, and nondeterminism, and how these resources arerelated.Proof complexity (the study of propositional proofs) istightly connected to complexity theory: methods fromcommunication complexity and circuit complexity have been heavilyused to obtain proof complexity lower bounds, and conversely lower bounds in proof complexity can be used to rule out largeclasses of natural algorithms for solving optimization problems. Ipropose to attack several new directions in complexity theory and proof complexity.In recent work we have developed the hardness escalation method in communication complexity, and used this method to prove a variety of new circuit lower bounds (i.e., the best known monotone depth lower bounds, as well as the first exponential-size lower bounds for monotone span programs and secret sharing schemes.) This has been quite successful so far, and many problems remain to be studied. For example, we would like to prove a hardness escalation theorem for randomized (BPP) communication, which should lead to new circuit lower bounds.Secondly, I want to prove super-quadratic lower bounds for branching programs and comparator circuits, breaking the longstanding barrier from the 1960's.Thirdly, in proof complexity, I want toprove size lower boundsfor Cutting Planes proof systems (and related matrix cut systems),and I want to further develop a new connection between algebraic circuit complexity andproof complexity that I initiated with Grochow. Using this approach, I hope to prove lower bounds for AC0[p]-Frege systems,a long-standing open problem in proof complexity. My second research area aims to understand andto develop mathematical solutions for several inter-related issues that arise from large data:privacy, fairness, and controlling false discovery in adaptive data analysis.The first topic is data privacy -- how can sensitive but important data (such as census data or medical records) beused for the good of society, while simultaneously protectingprivacy? Differential privacy is a promising approach and has resulted in a toolbox ofalgorithms for privatizing data. I propose to continue to work in this area, to make the methodsmore usable in practice as well as to discover links to related areas where stability is important (such as machine learning, and adaptive data analysis). A related issue is fairness : with increasing reliance on learning algorithms to make decisions,how can we be sure that these decisions are fairand not a result of an underlying bias?We have made partial progress on some ofthese questions (fairness in classification, and generalization in adaptivedata analysis) but have only begun to scratch the surface. ""631702,""Pitcher, Tony"
"637423"	"Plataniotis, Konstantinos"	"From adaptive signal processing to brain-inspired cognitive systems"	"The long term research objective (vision) of this program is to significantly advance research in signal processing by introducing a unifying and powerful information processing framework. To achieve this vision the program introduces a model-based framework which is comprised of local, processing units, an inference/adaptation engine, and a control unit.This brain-inspired, cognitive framework employs a measurement module to maximize information gain from the environment, uses memory-based attentional mechanisms to process information, deploys a reasoning/decision making engine to identify intelligent choices in an uncertain environment, and utilizes feedback control to interact with the environment in an efficient and cost effective manner. The research program will demonstrate that the proposed framework decomposes the signal/information processing problem into a set of considerably reduced complexity processing tasks which are far easier to design and implement. Thus, it constitutes a natural setting for studying challenging problems in unsupervised learning, optimal estimation, decision and control. Moreover it will be shown that the framework can be applied to both well-studied linear problems as well as complex learning problems characterized by non-linear, non-Gaussian, non-stationary conditions.In addition, to having significant theoretical implications, the results of the program will contribute to the advancement of the state-of-the-art in interdisciplinary areas such as neuroscience data processing, and advance technological solutions which promote well-being and enhance quality of life. To that end, two open research problems (challenges) from the field of EEG-based brain-machine interaction (BMI) will be used to demonstrate the frameworks utility. Namely, the framework will be used to: (i) reconcile and unify spatial filters used in cognitive electrophysiology, and (ii) show-case how these spatial filters can be used in the so-called zero-training BMI interfaces for rehabilitation systems. The programs intellectual merit is the development of an all-encompassing cognitive information processing framework. It will produce novel results of importance to the advancement of methodology, theoretical understanding of brain-inspired information processing, and useful solutions in its practical applications. Innovations in the areas of cognitive signal processing will have applications far beyond the engineering community with profound impact on quality of life and healthcare. Lastly, the program will provide a platform for training and development experience for the students involved, and it will be a major inspiration for further studies in cognitive systems. It will help spring new communities of research contributors, and developers, around specific interests in cognitive information processing. ""618713,""Plataniotis, Konstantinos"
"634773"	"Pottinger, Rachel"	"Improving Recommending and Understanding Schemas and Their Provenance"	"We are drowning in a sea of data. Increasingly, jobs that used to rely on other skills now rely on the ability to interpret, understand, and trust data. For example, many biologists now spend less time in the lab and more time trying to analyze the data that they have created. Increasingly, thanks to improved tools for creating data, they are expected to manage and understand this data by themselves. Unfortunately, this data is often exceedingly difficult to understand and trust, even for experts in the field. There are many reasons for this. One reason is that the representation of the data is often designed for computers, not for people (i.e., storage, not accessibility). Another reason for this is that users may spend a lot of time using systems where data is combined from multiple sources, leaving them stuck having trouble integrating data that they neither understand nor trust. The lack of understanding and trust can cause problems both large and small. For example, the recent financial crisis was partially caused by the inability to track money across multiple sources (e.g., mutual funds are in a different database than savings accounts). Because it was impossible to understand and trust the data that was spread across various sources, regulators did not realize some of the problematic flows of money until after the crash. On a smaller scale, without being able to find all of the data needed in order to evaluate building design choices, it is impossible to easily explore building design alternatives, which leads to stifled innovation and less-efficient buildings. My proposed research will improve the ability of users to access data that they need and trust that this data is well curated enough to do what they need it to do. This will build on my current work that has looked at case studies in economic data and civil engineering, and will work with the techniques that I have learned in working with recommender systems to help recommend data items to users. In this proposal I describe a number of specific directions that I intend to work on with my students in that space, including: (1) using query logs to help recommend parts of the data that users may be interested in; (2) helping users understand the complex connections that appear in XML files; and (3) improving the understanding of provenance data.""621511,""Potts, Clinton"
"635095"	"Precup, Doina"	"Learning good representations for and with reinforcement learning"	"Artificial intelligence (AI) has made great progress in isolating different aspects of intelligence and proposing flexible representations and powerful algorithms that lead to competence in specific tasks. For example, AI agents are better than humans at playing games like Go, a feat once considered impossible. However, the sort of flexible, robust, and autonomous competence routinely exhibited by humans, or even animals, remains elusive. The best AI systems are still tuned to specific problems. Our main research goal is to develop general AI methodology that relies, at its core, on reinforcement learning. Reinforcement learning is an approach to learning from interaction with an environment, inspired by animal learning theory. This proposal aims to design algorithms that can automatically create representations for reinforcement learning agents which allow them to model the world and to act at multiple time scales. We aim to provide new optimization criteria which describe formally what is a good set of abstract representations, provide gradient-based learning algorithms to learn such models, and demonstrate their effectiveness through empirical evaluations in simulated domains, game playing, as well as real time series prediction data sets. We will tackle the crucial problem of exploration, by explaining how an agent should move about its environment in order to optimize its learning speed. Finally, we will leverage these methods inside other algorithms that can benefit from multiple time scales, such as the training of deep, recurrent neural networks.""642723,""Precup, Doina"
"634792"	"PU, KEN"	"Managing Constraints as Data"	"Database has been a cornerstone in the data-driven digital age. Events and observations in the world generate large volumes of data which are safely stored in databases and efficiently analyzed by database queries. The results of the analysis further support decision making of a wide range of software systems, e.g. payroll and financial management, control systems, and operational research. These decisions must respect safety and correctness constraints. Examples of constraints are everywhere: payroll deadlines, maximal power capacity, server availability etc.. While the database technology excels at data storage and analytics, presently it fails to provide the same level of effectiveness for the management and verification of constraints. Currently, to fill this gap, application specific programs need to be manually developed by programmers. But such efforts are costly and highly error-prone. Constraint violations as the results of human errors and buggy software are frequently observed in different sectors of the industry and governments. This proposed research program is aimed to rectify the situation by extending the role of the database to include the management of constraints. The applicants objective is to extend the data model and the query language of databases to include logical constraints as first-class citizens just like data. This allows a well-defined declarative semantics to store and manage constraints and data in a unified framework. Building on top of state-of-art A.I. constraint satisfaction algorithms, the research program is to augment existing database technology to actively reason about decisions in the presence of constraints and data, thus alleviating the need for secondary safety-maintenance software. Such database engines can verify and actively synthesize data-driven decisions that are safe, correct and optimal. The proposed research program contributes to the theory and system design of future databases. Towards database theory, we will be studying (1) new constraint query operators and their properties, (2) new algorithms to manage mutually conflicting, incomplete and time varying constraints, and (3) distributed algorithms that are capable of processing large collections of constraints at Internet scale. Towards the system design of constraint database engines, we will perform research on (4) integration of constraint solvers with database engines, (5) implementation of highly concurrent constraint solvers, and (6) user interface design and implementation for interactive and crowdsourcing constraint satisfaction and verification. In summary, the success of the proposed research program will enable databases to manage constraints using a declarative query language. Database engines will help software systems to make safer decisions that are provably correct with respect to the constraints in the database.""632654,""Puddephatt, Richard"
"617731"	"Rabbat, Michael"	"Signal Processing Over Networks: Graph-Based Methods for Data Analysis"	"Using data to benefit humanity is a grand challenge for today's engineers and data scientists (e.g., via environmental monitoring, improving public health, and sustainability). This research program aims to develop novel signal processing theory and methods to address challenges associated with processing and learning from unstructured or irregularly-sampled data. Existing signal processing techniques are well-suited for data such as time series and images which have a regular, well-ordered domain. Many contemporary applications produce data that is massive, complex, and unstructured. Graphs provide a principled formalism to capture complex relationships among variables and entities. In some applications the graph may capture a physical structure underlying the signal (e.g., traffic intensities on links of a road network, demand signals at nodes of the smart grid, or a measure of people's opinions in a social network). Graphs may also serve as a useful means to encode which data sampling locations we expect to produce similar values when signals are sampled irregularly in space and/or time (e.g., EEG and/or MRI signals sampled on the surface of the brain, or a network of sensors spread irregularly over a region). In other applications, where the graph is not directly apparent, it may encode logical relationships among entities (e.g., correlative, causal, or otherwise influential relationships). In this case it is often of interest to infer a graph from the observations in order to better understand the structure, organization, and function of a complex system. This research program will make contributions to the burgeoning field of graph signal processing. The specific objectives of the proposed program are organized along three thrusts. (1) We will develop novel theory and methods for inferring graphs from signals under models where the signals are assumed to be smooth over the graph. In cases where signals cannot be observed at every vertex, we will infer structural or statistical properties of the graph that may still be useful for other applications like sampling. (2) We will develop novel theory and methods for approximating and compressing graph signals. While the theories of sampling and filtering graph signals are becoming more mature, little is known about what conditions on the signal-generating process and the graph structure are necessary for the resulting signal to be smooth or otherwise parsimoniously representable. The theoretical results will facilitate quantifying the tradeoff between the number of coefficients used to represent (i.e., approximate or compress) a graph signal and the resulting error incurred, given a family of graphs and graph signals. (3) We will develop methods for tracking in high-dimensional non-linear/non-Gaussian state-space models and sampling/filtering graph signals that exploit graph structure to improve computational efficiency.""637511,""Rabbat, Michael"
"637088"	"Ramahi, Omar"	"1"	"Electromagnetic Wireless Power Transfer and Harvesting in the Microwaves and Infrared Frequency Regimes"
"637187"	"Rashidzadeh, Rashid"	"Design for Testability and Hardware Security"	"The new generation of integrated circuits include high performance analog and digital blocks, processing units, memories, and sensors. While such new microchips provide opportunities to enhance the performance of portable devices significantly, they also pose new challenges. Developing manufacturing tests for advanced integrated circuits while ensuring their hardware security is a formidable task. A great deal of progress has been made in developing various test methodologies for microchips. Efficient Design-for-Testability (DFT) techniques such as scan and Built-in Self-Test (BIST) are widely used to carry out tests on digital circuits. However, the DFT methodologies have been developed without adequate attention to security implications. For instance scan-chain insertion, one of the most effective DFT techniques, can be utilized to access the critical information inside a chip. The requirements for testability and hardware security are in sharp contrast with one another. To test a chip, access to the internal circuits are needed to apply test vectors to desired sub-circuits and observe their responses. While such full access is considered ideal for manufacturing tests, it is clear that such unrestricted access to the internal circuits of a device can undermine its security. For decades, hardware was assumed to be the source of trust-and-security but this assumption is not true anymore due to outsourcing. The costs of a fabrication line are so high that only a few companies can afford to have an in-house fabrication line. The outsourcing of in-house fabrication to overseas foundries provides opportunities for malicious activities and paves the way for potential security threats known as hardware Trojans. The current solutions for testability have to be modified to detect undesired hardware modifications and prevent security breaches using the test infrastructure. The main objective of this work is to advance the design-for-testability and security techniques for the new generation of integrated circuits to ensure both testability and hardware security while reducing the overall manufacturing costs. The specific aims of this research proposal are to: (a) Develop a Design for Secure Testability Method for 3D ICs using an RFID based Authentication. (b) Implement a Test Technique to Detect Hardware Trojans. (c) Develop a Pre-bond Test Solution for 3D Stacked ICs. (d) Develop a Fault Model for FinFET Based Circuits.""620778,""Rashwan, Tarek"
"637574"	"Rathore, Akshay"	"Power Conversion, Pulse With Modulation, and Integration Techniques for All Electric Vehicles"	"The proposal focuses on designing and developing innovative power electronics topologies and novel modulation techniques for marine electrification and more electric aircraft (MEA). Marine Electrification (Sea transportation) and MEA (air transportation) are two heavy electric transportation systems and of different categories than ground transportation. Significant research on power electronics and drives for ground electric vehicles has been done and is at saturation now. However, power electronics research for marine and MEA is not mature yet. There are different challenges for power electronics for these two applications. Several industries are investigating on power electronics for these marine and MEA due to their different and specific requirements. Research program on marine and MEA is lacking in Canadian Universities. Therefore, the aim of this research proposal is to establish a research program on power electronics for marine and MEA. Power electronics is key enabling technology for electrifying the conventional transportation systems. Modulation schemes to control the switching states of the semiconductor devices of power electronics are important to meet the specific requirements and realize the desired attributes such as volume, weight, efficiency, etc. Marine and MEA involve MG power processing. To achieve high efficiency for higher power marine system, Medium Voltage (MV) solutions have been proposed and industries are adopting it. However, existing devices have limitation on their blocking voltage. Therefore, Modular Multilevel Converters (MMCs) are proposed for MV high power applications. To limit device switching losses at MV, it is better to modulate the devices at low switching frequency. However, it compromises on total harmonic distortion (THD) of inverter output currents. Therefore, this research program aims at developing Medium Voltage DC (MVDC) MMCs and optimal low frequency modulation for MVDC marine propulsion systems to reduce switching loses while not compromising on THD. Power electronic solutions such as matrix converter based direct ac-ac conversion without dc link and Active-Front-End rectifier with dc link will be investigated to eliminate bulky phase-shifting transformer. For MEA, power electronic solutions to develop ac/dc converter with variable frequency of 360 to 800 Hz will be investigated with minim THD. The proposed research program will strategize to implement high switching frequency modulation to limit the cost and volume of the magnetics, and filters. Wide bandgap devices, Silicon Carbide (SiC) and Gallium Nitride (GaN) and magnetic materials, Nano Crystalline will be investigated to develop more efficient power converters with low thermal requirements. The proposal will also investigate current-fed power electronics solutions owing to their current limiting, voltage gain, and short circuit protection features.""637999,""Ratti, Cristina"
"634747"	"Reformat, Marek"	"Knowledge Extraction via Learning Processes and Data Models with Imprecision"	"The web represents an immense repository of information. A number of sources of structured and even more often unstructured data is growing every day. There is no doubt that our dependency on data increases continuously. However, the increased amount of data - although recognized as positive and beneficial fact - creates an even bigger issue related to our ability to fully utilize that data. That situation increases the pressure to develop automatic and more efficient approaches suitable for advanced processing of data leading to creating logic structures that lifting raw data into a knowledge-like level. Fortunately, we are at the onset of significant and far-reaching changes in the way data are represented and stored on the web. The concept of knowledge graphs becomes a new way of expressing pieces of data and relations between them. Resource Description Framework - one of the most fundamental aspects of the Semantic Web - is recognized as the most suitable data format for representing knowledge graphs. The proposed research project puts a special emphasis on processes of constructing, updating and utilizing knowledge graphs built based on data and information obtained on the web and extracted from documents. A key innovation of this project is a fusion of web technologies, fuzzy-based techniques, and concepts of category theory and topos to fully explore data taking advantage of Resource Description Frameworks intrinsic interconnectivity, and setting up a basis for knowledge synthesis processes. These activities will lead to establishing coherent rudiments of knowledge creation processes. In a nutshell, the proposed methodology focuses on forming knowledge-rich structures following the steps: 1) extracting information from documents and representing it in a form of so-called information knowledge graphs that contain specific pieces of information; 2) clustering and generalization of those graphs leading to construction of conceptual knowledge graphs; 3) maintaining both types of graphs via incremental updates using aggregation and data assimilation techniques taking into account imprecision and confidence levels in different pieces of data; and 4) constructing logic structures in a form of internal logic of topos based on conceptual knowledge graphsand linking those structures with information graphs for validation and cognitive purposes. It is expected that the project will lead to significant contributions in methodologies aiming at building a new generation of systems that support the users in their activities related to collecting data from the web, and processing it towards creation of knowledge. This will lead to development of knowledge systems capable of validating correctness of information extracted from data, andsynthesizing new concepts based on it. Overall, the project encompasses state-of-the-art research and HQP training in an important for Canada IT area.""640653,""Reformat, Marek"
"634509"	"Rice, Jacqueline"	"Sociolinguistics Tools in the Analysis of Language Use in Software Development"	"Companies must ensure that software is high quality and understandable, the latter because it is virtually guaranteed that another person will need to read and understand software written by someone else, whether for purposes of upgrading, fixing, or replacing the code. In the field of natural language researchers have used machine learning to determine an authors gender with up to 90% accuracy; this process also allowed the identification of how the two groups used the language differently. This is valuable information for software development teams, whether we look at (for instance) differences between men and women, junior and senior developers, or developers with different native natural languages. While there is a great deal of research on peoples use of natural language, both written and spoken, there is very little on how people use artificial languages such as those used for writing software, and none at all on the sociolinguistics of artificial language use. I propose to address this gap with a view to using my discoveries to contribute to improving both software quality and software readability. One high level motivation of this work lies in the quest for diversity. If different groups use language in different ways, then can tools be tweaked to guide programmers toward more stylistically neutral code, thus reducing potential sociolinguistic differences between diverse groups? Or is this defeating the purpose of encouraging a diverse population of contributors and their unique perspectives? When teaching students to code, are there particular styles that might be more comfortable for different groups, and should we be encouraging this? We dont know the answers to these questions, but evidence from the natural languages suggests that we should be considering them. However as we investigate how a diverse population creates software, the underlying motivation is that of the need for quality code that can be clearly understood. If different groups write code differently, is that contributing to misunderstanding? Are there ways that we can, for instance, translate code to make it easier for another reader to understand? If there are significant differences, do these correspond to code quality? Finally, what has larger impact; language use at the micro levels that we are proposing to start with (i.e. line by line), or larger, structural and conceptual differences that form the overall architecture and design of large pieces of software? My research program aims to examine these questions, and I believe that this novel approach will have a significant impact on the field of software engineering.""621085,""Rice, Nicole"
"634511"	"Robillard, Martin"	"Checkable Representations of Design Decisions"	"Design is a significant activity of the software development life cycle whose output is a set of decisions on how to structure a system, along with rationale for these decisions. In practice, the information that constitutes a design can become progressively lost, or invalidated from repeated changes that introduce gradual inconsistencies between a design document and the code. When design information is lost or becomes invalid, future changes to a system become increasingly effort-intensive and error-prone, two factors that contribute to poor software quality. Software quality problems make headline news on a weekly basis and can severely impact public safety and quality of life in Canada and elsewhere. To mitigate the problem of design erosion in software systems, we propose to investigate new techniques for discovering, representing, and communicating low-level software design information. The goal of the research is to elaborate an approach that supports the efficient creation and use of checkable representations of design decisions in software development. The proposed research will involve three complementary facets: design decision recovery, design decision representation, and effective communication of design decisions. For the design recovery facet, we will consider a knowledge-based approach that integrates features derived from both static analysis and text mining. As part of the representation facet, we will seek to develop a representation system that adequately balances expressiveness, usability, and checkability for design decisions. The communication facet will target the problem of effectively communicating both existing decisions and violations of specified design decisions. An effective system for describing low-level design decisions in a checkable way has the potential to positively impact programming and other software development practices, lead to higher software quality, and improve software development efficiency. A programming system that explicitly supports asserting design decisions may also impact computing education by stimulating novice programmers to consciously consider design alternatives when implementing solutions.""639592,""Robillard, Martin"
"637215"	"Rochette, Martin"	"Optical sources for the 2-12 um spectral band of the mid-infrared"	"Mid-infrared sources are in demand for the identification and quantification of molecular species in gases, liquids and at the surface of solids, with applications for medical instrumentation, industrial processes, environment monitoring, and defence. Mid-infrared sources that are tunable, compact, robust, and with low power consumption would be key components within portable chemical sensors. In the past years, my research has successfully demonstrated the unique functionality of new designs of optical fiber sources emitting in the 1.5-2.0 um wavelength range such as optical parametric oscillators, wavelength converters, supercontinuum sources, and quantum communications sources. Chalcogenide glasses have been used for the fabrication of those sources because of their exceptionally high nonlinearity coefficient, enabling strong nonlinear effects to take place in a short waveguide and requiring low input power to operate. However, another important quality of chalcogenide glasses is their transparency in the wavelength range of 1.5-12.0 um, enabling the fabrication of optical sources emitting in the mid-infrared. In response to the high demand for mid-infrared optical sources and the strong potential of chalcogenide glasses in this wavelength range, within the next 5 years, my research program will target the development of mid-infrared optical sources that are broadly tunable, compact, robust, and with low power consumption. The proposed project addresses a growing concern across Canada: developing monitoring and sensing tools for health services, environment, industry and defence. Photonics technologies in the mid-infrared are lagging behind near-infrared photonics (0.8-1.7 um) which benefited from exceptional market competition and activity in the telecommunication market. As a result, few sources are available in the wavelength range of 1.7 um and beyond. This project harnesses current photonics technology and a scientifically mature optical material, namely chalcogenide glass, to close important wavelength gaps in the market offerings. Highly qualified personnel will emerge from this project. The graduate students will be specialists in mid-infrared technologies, a topic of great importance and holding many challenges for years to come. Sixteen students and PDFs will benefit from this opportunity. As an extension to the project, compact and portable chemical sensors would have a major impact for medical instrumentation (e.g. blood, plasma, saliva, or breath analysis), environment monitoring (e.g. air and water quality, pollutants monitoring), industrial process control (e.g. combustion processes, chemical reactions, exhaust monitoring), and defence (explosive hazard avoidance, personnel protection), just to name a few.""629156,""Rochette, Patrick"
"634517"	"Rubin, Julia"	"Software of Things: Composition and Integration"	"Our dependence on software continues to grow every day. Software systems themselves become increasingly larger and more complex. At the same time, software development cycles shrink and customers expect new products to be delivered to the market faster than ever. In such a reality, software construction becomes a compositional activity, where developers integrate code from in-house and third-party libraries, snippets of code borrowed from earlier projects, and code found online. Such borrowed code can comprise up to 90% of a typical software system. Integrating independently-developed code fragments in a predictable manner is a challenging task: developers often have a limited understanding of what each individual piece of software does, let alone how their composition behaves. The inability to reason about software functionalities and their interplay complicates software development processes, prolongs the time to market, and reduces the quality of the produced software. In fact, there are several well-known disasters caused by software reuse, composition, and integration problems, including the Ariane 5 rocket explosion, the KCG trading system failure, and the massive radiation overdoses inflicted by Therac-25 therapy machines. Motivated by these challenges, the research program proposed here aims at building foundations for efficient and reliable development of compositional software . The main distinction of this program from existing work is that it does not focus on approaches for specifying reusable software components or subsystems with well-defined interfaces. Instead, it leverages recent advances in the fields of formal methods and program analysis to develop paradigms for understanding and composing existing, already available software fragments . Specifically, this program explores the development of: 1) A formal model for representing software in terms of its conceptual features logical units of meaning to developers. The model is populated by extracting information about features, specifications of features (prerequisites and guarantees), and feature dependencies, from existing code artifacts. 2) Analysis techniques for reasoning about complex compositional software properties. These techniques aim at, for example, identifying unexpected side effects caused by reusing features outside of their original context and inconsistencies caused by independent evolution of related features. 3) Concepts for integrating the compositional model and analysis techniques into existing development tools and practices, including version control system workflows (e.g., that of Git). The outcomes of this work are expected not only to advance the state of science but also to reduce time to market and increase the quality of our software, making a significant contribution to the software industry, and to the Canadian and global economy.""621166,""Rubino, Cristina"
"634745"	"Ruths, Derek"	"Developing robust methods for the measurement of online social media populations"	"In June 2016, troubling evidence surfaced showing Googles search engine to be racist. At issue were the radically different images returned by the search terms three white teenagers and three black teenagers. The former search returned wholesome pictures of teenagers playing sports and hanging out. The latter returned mugshots. Google defended the search results, indicating that their search algorithms were informed by web content and the frequency with which it was accessed and linked. Google - the company that runs software used daily by millions of people to find information on the internet - asserted that any such bias in their search results were fundamentally a societal problem. And there, in essence, is the conundrum. We look to Google search along with myriad other online services from Yelp to OkCupid to Facebook to reflect the world. And in doing so, we give it the power to reinforce the social structures in it. When machine learning algorithms become vehicles for propagating undesirable stereotypes and marginalizing minority views which we call algorithmic prejudice - we have a problem. Addressing algorithmic prejudice presents two challenges. First, we must detect when a prejudice exists. Second, where a prejudice exists and is deemed necessary to correct, we require methods for adjusting the algorithms performance. These are the central tasks we consider in this proposal. We will tackle these problems in three phases. First, we will develop methods to aid in the detection of algorithmic prejudice. Second, in collaboration with domain experts, we will use our detection methods to do a detailed characterization of the prejudice present in algorithms for detecting hate speech and identifying political orientation. Using these findings, we will approach the final topic of this proposal, which is methods for correcting algorithmic prejudice either by modifying the algorithms themselves or by adjusting the data that the algorithms are trained on. As increasing numbers of services migrate online, we must ensure that these systems reflect the aspirations of society, not its worst and most predatory elements. Our hope is that this work will provide viable solutions for companies to ensure prejudice-free systems and that our findings stimulate further work on this important topics.""640948,""Ruths, Derek"
"637188"	"Sachdev, Manoj"	"Exploring Memory and Digital Circuit Boundaries for Energy Efficient Hardware"	"Impact of transistor scaling are evident everywhere. Often small as possible transistors are used to reduce the power, energy consumption, and to increase the packing density. Transistors with smaller dimensions exhibit a higher susceptibility to process variation. As a result, realization of robust, reliable circuit design becomes a challenge. In particular, SRAM circuits, and low-power, low-voltagedigital circuits show higher degree of variation owing to smallest possible transistor dimensions, and low-supply voltage requirements. In this research, we will investigate variability-aware design of digital and SRAM circuits in power and voltage constrained environments. In microprocessors up to 70-80% of transistors are in SRAMs. As a consequence, various aspects of Systems on Chip (SoC) power, energy, yield, quality, and reliability are influenced by SRAMs. This research proposal has two major components (i) SRAMs, and (ii) Logic circuits. Key research objectives for the SRAM are: (a) lower SRAM power consumption with architectural, circuit innovation to realize a reliable, ultra-low-voltage SRAMs. In particular, devise circuit techniques to alleviate the impact of process variations on important SRAM blocks such as sense amplifiers, SRAM cells, and (b) mitigate the impact of soft errors and weak failures through hardened by design and efficient implementation Error Correcting Codes (ECC) which may further improve low-voltage SRAM operation. The key research objective for logic circuits is ultra-low-voltage, energy efficient logic family and to realize ultra-low-voltage digital building blocks. The long term (5 years) objective of this research is to put all these ideas together in silicon to (a) fabricate fully functional ultra-low-voltage; low-power SRAMs in 28 nm CMOS technology; (b) design and fabricate soft error robust low-power SRAMs; (c) design and fabricate a low-power, low-voltage digital circuits such as 32/64b adder capable of working at 100 mV. In all instances, test chips will be manufactured, and measurements will be carried out. ""620544,""Sacher, Jessica"
"637349"	"Samarabandu, Jagath"	"Active Intrusion Prediction and Response for Computing Devices on the Internet"	"Computing infrastructure in Canada as well as across the world are increasingly under attack from intruders attempting to steal information that is safety, business and privacy critical. Such intruders are often backed by criminal enterprises or nation-state actors and are technically very sophisticated with plenty of resources. In 2015, British insurance company Lloyd's estimated the world-wide loss due to cyber crime USD $500 billion. By 2019, this is expected to quadruple to $2 trillion. A crucial weapon in the fight against this worsening crisis is an engineering solution to detect network intrusions and deploy countermeasures. Current intrusion detection systems work by monitoring network activity and looking for activity patterns that match known attack steps. Often, this results in detection after the attack has taken place and response is limited to recovery operations instead of thwarting an ongoing attack. This proposal aims to develop innovative solutions for intrusion detection and response that far surpass the state-of-the-art in following ways: a) generate many possible attack scenarios when an initial stage of an intrusion is detected, b) deploy additional monitoring tools that are specifically designed to gather evidence to confirm or deny these attack scenarios, c) plan defensive measures that can effectively counter the most likely scenarios and d) deploy these defensive measures in a way that is is least disruptive while continually evaluating the situation as above. Long term vision is to develop intelligent intrusion response systems that are aware of the security situation within the next decade. The short term objectives in this proposal that are designed to achieve the above goals include developing a novel way to represent intrusion events that can capture high-level intent as well as low-level network activity, recognizing combinations of intrusion events that are part of a larger attack and deploying these tools within a framework that allows building situational awareness. In the long term, this proposal aims to develop algorithms that can predict impending penetrations based on identification of ongoing attacks and develop countermeasures that can thwart these predicted attacks. A highlight of the proposed research is the development of algorithms that can learn from past attacks as well as setup decoy targets and dynamically change the network paths to divert attacks away from critical systems. The latter will also allow gathering crucial data about these attacks that will be used to design better detection tools in the future. Information security is becoming a critical need for the foreseeable future and innovative approaches as well as highly trained professionals are needed to meet the looming challenges. This program will train the engineers with necessary background as well as with hands-on experience, who will be able to fill this critical and growing need.""623407,""Samarasenan, Thilakasenan"
"636983"	"Sarunic, Marinko"	"Imaging eyes: ocular diagnostics by adaptive optics"	"Recent advancements in ophthalmic imaging have made significant impacts to understanding and treating diseases leading to vision loss. The eye is a uniquely transparent organ, permitting optical imaging of the retina, the light sensitive tissue at the back of the eye. Optical Coherence Tomography (OCT) is a commonly used diagnostic imaging modality that is increasingly being investigated for real-time volumetric imaging. However, even with the good optical performance of the human eye, the lateral resolution of OCT, and related optical imaging techniques, is limited. Adaptive Optics (AO) is needed to obtain the resolution required to visualize the retina at the cellular level. Originally developed for astronomical imaging, AO performs wavefront correction of the light entering the eye using deformable optics, correcting the imperfections in the cornea and intra-ocular lens and permitting diffraction limited focusing on the retina. The purpose of my research program is to investigate novel OCT, AO, and related high resolution imaging technology to overcome the challenges to visualization of the retinal structure and biological function. Short Term Objectives 1) Advance real-time OCT with quantitative analysis to guide retinal medical procedures; 2) Investigate Wavefront Sensorless Adaptive Optics for cellular resolution retinal imaging in vivo: 3) Demonstrate proof of concept experiments for light based retinal therapeutics. Significance Diseases affecting vision affect Canadas aging population and amount to billions of dollars in direct costs to the Canadian health care system. The development of novel high resolution diagnostic imaging technology for eye clinics with the potential to detect degenerative changes in the retina earlier and with higher sensitivity is essential to timely vision preserving interventions. Furthermore, the development of high resolution and functional retinal imaging capabilities for preclinical applications has potential to accelerate the development of novel therapies for a broad range of vision robbing and neurological diseases. The technological development has strong potential for generation of novel intellectual property, and for commercialization of novel imaging systems for vision research and vision health care. ""642301,""Sarunic, Marinko"
"637286"	"Sawan, Mohamad"	"Smart Brain-Microsystem Interfaces for Efficient Diagnostic and Treatment of Neurodegenerative diseases"	"Nowadays, neurodegenerative diseases are behind most neural central system dysfunctions, where an estimated 26% of population suffers from some of these brain disorders (Brain Behavior Research Foundation), and where little can be done to guaranty accurate diagnostics, and to secure efficient treatment. These circumstances motivated researchers to propose advanced implantable brain-microsystem interfaces in a good standing to facilitate access to various brain regions for both diagnostic and treatment purposes. Despite significant progresses in the development of biointerfaces, further synergism is needed for real-time reliable diagnostics of dysfunctions and efficient treatment procedures. Monitoring neural cell and neurotransmitter activities, and neuromodulate neural functions require design, assembly, and packaging processes of advanced microsystems. Our goal in this program is grouped into five main long-term objectives: 1) build wearable signal/image acquisition interfaces intended for prediction and detection of diseases, such as vision and epilepsy, that may be located at the retina level or in various extracortical regions; 2) implement implantable custom biosensors and actuators including massively parallel neurorecording interfaces for neural encoding and neurotransmitters characterization; 3) harvest energy from radio waveforms to operate proposed microsystems; 4) micromachine metallic/optic 3D Microelectrode arrays to improve neural interfaces; 5) implement low-power high data-rate wireless transceivers. Our previous long term experience on building smart medical microdevices, and achieved results when testing in cultured neural cells and/or in collaboration with researchers from relevant medical disciplines allowed us to locate the drawbacks of latest devices in order to propose needed advanced bioelectronic tools, which must efficiently interact with neural cells. The impact of projected diagnostic techniques and neuroprostheses implementation is expected to be major due to accurately locate the brain neurodegenerative diseases and to efficiently restore cortical functions and subsequently enhance/recover lost vital functions for patients, and understand other unknown diseases. Attention will be paid to train highly qualified personal in these multidisciplinary emerging neurotechnology specialities which are becoming mandatory to help aging population. Also, proposed microsystems involving remote monitoring and treatment, as well as energy harvesting, can be used in several other applications requiring wearable and implantable brain and body sensor networks.""625218,""Sawaoka, Hiromitsu"
"634541"	"Scott, Stacey"	"Improving the Effectiveness of Co-located Collaboration Technologies"	"In an increasingly connected global information society, teams commonly collaborate across time zones and continents. Yet, there are still tremendous advantages to engaging with people in a face-to-face (or co-located) environment. People can utilize a variety of communication channels, allowing rich, real-time visual and auditory communication, and providing important contextual information. These factors can increase the efficiency and effectiveness of communication and collaboration. However, when people wish to utilize digital information during co-located group work they are left with ill-suited computer interfaces designed for personal or remote collaboration use. This leads to awkward, inefficient interactions. The overarching goal of this research program is to develop computer interfaces better suited to the co-located collaboration context, where the available technology leverages the physically shared space, and the people and their behaviour within the space. This work leverages the increased connectedness of personal devices, including tablets, smartphones, wearables, and laptops, as well as two decades of interactive large display research. Together these technologies allow for an exciting mix of multi-device ecologies that can support a wide range of co-located group work. These devicesand their environmentsare also becoming much smarter via a multitude of on-board and add-on sensors that detect device movement and user behaviour. These additional sensory channels present an unprecedented opportunity to build computer interfaces that can better detect and respond to subtle behavioural cues to enable more effective and efficient computer interactions. Thus, the near-term goal of this research is to leverage the multi-sensory ability of smart devices to provide more sophisticated interfaces that leverage the physicality of the co-located environment and the people and the devices located within. The research will involve two main research themes, broken down into seven complementary projects, including: 1) investigation of social behaviours that facilitate co-located group work, and 2) development of multi-device user interfaces that foster beneficial social behaviours. Developing co-located computer interfaces that significantly improve the efficiency and effectiveness of group work across a variety of team situations from everyday office work to more high-stress, high-stakes situations has considerable opportunity for impacting our modern information society.""630172,""Scott, Stephen"
"637338"	"Sedaghat, Reza"	"An Adaptive Multi Objective Optimization Framework for Next Generation Resource Constrained Communication systems"	"Over the next five years, global IP networks will support up to 10 billion new devices and connections, increasing from 16.3 billion in 2016 to 36.3 billion by the end of 2020. The 5G communication network and service environment beyond 2020 need to cope with billions of small devices in the Internet of Things (IoT), and billions of heavy data consumers. The 5G network infrastructure will have to provide a multi-faceted security posture that mitigates new threats, malicious attacks, and evolving vulnerabilities across all devices, all network domains, and all applications wherever they are hosted. It will have to be a seamless infrastructure satisfying everyone's secure communication needs reliably as well as integrating new radio concepts, such as massive MIMO, ultra-dense networks, moving networks, and device-to-device, ultra-reliable, and massive machine communications. Next-generation constraint solvers for communications security has emerged as an important scientific discipline where many multifaceted complexities deserve the attention and synergy of interdisciplinary engineering communities. Both wireless and wireline network infrastructure are required not only to be highly scalable in terms of their capacity, but also to optimally handle different service needs of various security verticals. Security methodologies employed in transporting data between parties are crucial and involve a trade-off between cost and efficiency. A solution to this trade-off is to analyze cryptographic protocols using optimization methods for secure data communication. However, these optimization methodologies must not result in significant reduction of security level of the process. To achieve a multi-objective trade-off between cost, efficiency and reliability of security requirements, our research includes methods related to Bensons algorithm-based cryptographic techniques, Sub-permutation-based evolutionary key exchange, Binary particle swarm optimization for cipher method, Custom inflate-deflate cipher systems, etc. A crucial driving factor in many secure network fields (e.g. e-banking, e-trust, e-health etc.) is the threat level. Every secure communication system presents a collection of functional security levels, which guarantee the secrecy of the overall system. Our proposed research encapsulates new optimization approaches, which will be implemented to develop advanced multi-objective techniques for the efficient design of cryptographic communication protocols and to cope with the trade-offs between security, cost, and performance factors. In this research, we will develop techniques and algorithms, both theoretical and applied, for developing communication security management and policies. ""638697,""Sedaghati, Ramin"
"634560"	"Sedig, Kamran"	"Scaffold-based visual analytics tools for complex activities"	"According to the National Research Council of Canada, in support of Canadas digital economy, a high-priority area of research and development is software solutions that deal with the explosive growth of data and escalating needs for revolutionary ways to use computers to make decisions, synthesize information, and discover new knowledge. As professionals from a diverse set of fields (e.g., health, law, finance, insurance, energy, and education) increasingly use large and heterogeneous datasets to do research, innovate, solve problem, make decisions, and set policies, there is need for sophisticated software tools that support the daily complex activities in these fields. My research involves designing, implementing, and evaluating human-centred visual analytics tools (VATs) that support the execution of complex activities, such as studying causes of diseases, solving scientific problems, and analyzing health data. VATs facilitate the performance of these complex activities by combining automated machine learning models, visualizations, and user interactions. VATs take advantage of the visual, reasoning, and decision-making abilities of humans and the powerful data discovery and analysis strengths of computers. Complex activities cannot be performed entirely by VATs; human judgment and constant involvement are required to steer the activity process. This means that tasks must be carried out through a high degree of human-VAT collaboration. The human must (a) determine the input and interpret the output of all intermediate tasks, must (b) decide how to interactively steer machine learning models and visualizations, and must (c) choose which tasks to delegate to the tool and which to perform themselves. The design of VATs that couple human cognition and computation and support complex activities poses a series of challenges; such design must consider a host of interrelated issues: coordination and distribution of datasets and tasks, machine learning models, visualizations, user interactions, and human cognition. My research program aims at developing techniques, methods, principles, and frameworks for the design of human-centred visual analytics tools that support users in the execution of complex activities. I will build and study tools that enable a close collaboration between humans and their data, for analytic purposes. I will use scaffold-based techniques to support how humans work with complex data and machine learning models. As human activities become more and more complex, this type of research is indispensable. Well-designed VATs will enable all types of users to carry out knowledge work involving large data, helping to streamline their daily tasks by providing significant improvement in work efficiency. This is important for increasing Canadas global share of the growing information and communications technology market.""623859,""Sedor, Zeke"
"634455"	"Sekerinski, Emil"	"Theories and Tools for Sustainable Programming"	"According to a study by the International Energy Agency in 2013, the power consumption of electronic devices is climbing 6% per year, twice the increase in overall global consumption. Natural Resources Canada estimates that 10% of household electricity goes into electronic devices. Independent of that, the effectiveness of code reuse has lead to code size and complexity that makes programs need more hardware and maintenance resources than ideally and makes hard guarantees about reliability difficult. Given our increased reliance on computing devices in communication, transportation, health, etc., these contradictory trends are difficult to sustain. We intend to advance programming techniques that address power consumption, reliability, and code reuse through lean programming . At the ""low end"" of computing, sensor networks have gained traction, e.g. for indoor/outdoor guidance, urban planning, structural integrity monitoring, building automation, environmental monitoring, farming, and asset management. Ultra-low-power processors of such sensors can be powered by harvesting energy , e.g. solar, thermal, motion, sound, moisture, and RF fields, and are expected to perform without maintenance for years, e.g. embedded in concrete. We intend to propose a method for reliable intermittent computation with unreliable energy sources. Recently, we developed pState, a tool for the design of embedded systems that includes correctness analysis, quantitative analysis (e.g. power consumption, transmission reliability, message delay), code generation, and worst-case execution time analysis. We plan to implement intermittent computations by generating code that uses a combination of volatile and non-volatile memory and extend pState's analysis to the reliability of intermittent computations. At the ""high end"" of computing, the increase in the number of processor cores since 2005 allowed an increase in performance while keeping power consumption constant. Although multi-core processors are common in smartphones and data centers, popular programming models do not scale well with the number of cores. Guarded atomic actions are appealing as they are ""higher level"" and easier to use than explicit signaling and message passing as commonly found. Our recent work has shown how guarded atomic actions in concurrent objects can be implemented highly efficiently and scale well with the number of cores. We plan to develop a production-quality programming language based on guarded atomic actions and address three research questions: (1) how to handle properly exceptions in atomic actions, (2) how to support code reuse through dynamic mixins , as a flexible and safe composition mechanism, with atomic actions, and (3) how to introduce ownership in such a language to allow better static checking, simplify correctness conditions, and improve code efficiency.""631088,""Sekuler, Allison"
"634691"	"Selinger, Peter"	"Algebraic and number theoretic methods for quantum circuits"	"The purpose of this research project is to develop techniques for reducing the amount of resources required for performing practical quantum computing. Quantum computing is a form of computing that is based on the laws of quantum physics, rather than classical physics. It has the potential to be vastly more powerful than any of the computers existing today. There are certain computational problems for which there exist efficient quantum algorithms, although no efficient classical algorithm is known. The best-known example is Shor's 1994 quantum algorithm for factoring integers into primes. When the subject of quantum computing first emerged into the mainstream of computer science two decades ago, it was initially mostly of theoretical interest, as the development of practical quantum computers was literally decades away. However, this seems to be changing. There are now a number of government, public, and private organizations, including several in Canada, that are actively trying to build a scalable quantum computer and are making tangible progress. At the same time, it is becoming clear that, even if a scalable quantum computer can be built, the actual resources required to make it fault tolerant will be staggering - by some estimates, a computation that requires, say, a few hundred logical qubits could require hundreds of thousands of physical qubits and millions of years of computing time on the sort of hardware that is considered realistic in the near term. With this, practical considerations are suddenly thrust into the foreground of quantum computing. The resource reductions required to make quantum computing feasible can potentially come from a variety of places, for example, better error correction schemes, improved protocols for components such as state distillation, improved algorithms, and improved compilation techniques. My research is primarily focused in the latter area, and specifically on improved methods for unitary approximation and the optimization of logical quantum circuits.""640921,""Selinger, Peter"
"637272"	"Servati, Peyman"	"Nature inspired, environment friendly fibrous flexible electronics and photonics"	"This proposal focuses on design and engineering of novel fibrous materials and devices that are integrated in form of multifunctional stretchable textiles for empowering revolutionary applications in smart textiles, health monitoring and internet of things (IoT). Recent advances in nanomaterials and devices on thin flexible plastic films highlight significant opportunity for development of electronics in form of yarn and textile that provide breathability, ruggedness and biological compatibility. In fact, in natural systems, different fibrous structures ranging from muscle fibers, nerves, and veins merge to form a complex functional system. Such systems highlight significant opportunity for electronic fibers and textiles with superior flexibility, low cost roll-to-roll manufacturing, improved biocompatibility and possibility for multifunctional designs. The market for electronic textile is expected to reach $70B by 2022 [1] with several sectors including health and wellness, sports, fashion, military and IoT. In line with these opportunities, I have co-founded UBC Centre for Flexible Electronics and Textile (CFET), a member of Canadas Smart Textile and Wearable Alliance, to drive innovation in this important area. The existing expertise in novel nanofiber and textile materials and devices and state-of-the-art prototyping infrastructure form the foundation of this proposal. To mimic natural fibrous systems, we propose development of mechanically flexible yet rugged yarns that deliver functions such as light emission, sensing and energy storage. Different yarns can be knitted, woven or laminated for development of multifunctional textiles that are breathable and stretchable. This research program addresses scientific and technological challenges in material development, device engineering and fabrication of functional yarns for light emission, sensing and energy storage and design of novel integrated textile prototypes. We develop processes for formation of multi-layered organic light emitting diode layers on yarns for breathable and flexible display textiles that have potential for improved light extraction efficiency by virtue of the small diameter of the fibers. By functionalizing electrospun nanofibers and yarns with novel nanostructured films and quantum dots, we investigate development of highly sensitive multifunctional sensor arrays as a platform for development of malleable sensing textiles. Here, environment-friendly natural materials such as lignin will be explored as a source material for fibers. By controlling the nanoporous structure of proposed lignin yarns, we explore novel electrodes for energy storage devices such as supercapacitors and batteries with improved capacity, cycle life and flexibility. Integration of textile prototypes with communication and data processing circuitry for novel applications will be demonstrated. ""626351,""Service, Christina"
"617762"	"Shami, Abdallah"	"Towards Building Quality-Driven Next-Generation Mobile Networks with Carrier-Grade Virtualized Platforms"	"The demand for high-bandwidth network connectivity has been growing significantly over the past few years. As much as this rapid increase represents a business opportunity for mobile operators, it also pauses pressing network capacity challenges in need of a timely response. Investing in additional infrastructure is always there as a solution. However, this is far from a magic wand in this situation since the return-on-capital investments can be minimal. It is undeniable that upgrading network capacity depends heavily on the infrastructure upon which the network relies. However, realizing the true potential of a specific hardware setup is an objective that has more realistic room for improvement and visible impact. Operators are in insistent search for an innovative game-changing solution to elevate the network capacity beyond the expected network performance levels. Furthermore, network operators are juggling other multi-layered challenges as critical as increasing demand. Increasing energy, financial and social costs coupled with the lack of personnel with expertise to design, implement, and orchestrate a progressively complex hardware-based infrastructure are the primary concerns of operators. The scope of these issues is not limited merely to revenue loss; ripple effects manifest through lags in time-to-market, as well as supply chain challenges and general hindrances to innovation within the Information and Communications Technologies (ICT) industries. Virtualization technology gained more momentum offering a gateway reducing the opportunity cost for operators aiming at reducing dependency on proprietary hardware. Empowered with the ability to facilitate hardware and software decoupling and the means to achieve more effective capital investments with higher returns on capital, Virtualization technology has the potential to revolutionize the telecommunications industry. In this research program, the applicant aims to investigate and develop a virtualized telecommunications network framework that efficiently address the virtualization technology challenges while considering network resource utilization (e.g., wireless) and the rapidly changing computing resource market.""637350,""Shami, Abdallah"
"621727"	"Shariff, Roshan"	"Qubec"	"CANADA"
"634855"	"Smyth, William"	"Computing Patterns in Strings"	"A string is a sequence of symbols, usually called letters , drawn from some alphabet . The Bible can be thought of as a string, about two million positions long, on an alphabet of English letters, integers and punctuation symbols; the genome of every living thing can be thought of as a string, whose length is usually in the billions, on a four-letter DNA alphabet ( a,c,g,t ); a bit stream transmitted from space is a string, perhaps trillions of positions long, on alphabet ( 0,1 ). For literary, biological or military research, the patterns in these strings are fundamental: Where does a certain phrase recur in the Bible? How do repeated DNA segments in the genome indicate susceptibility to Parkinson's disease? What clues do certain recurring bit patterns provide about coded segments of the electronic transmission? In 1975 there were a few dozen researchers in string algorithms round the world; now there are surely many thousands, a result of the widespread use of computers for information storage and a huge upsurge in computational biology. For the last 15 years there have been two main themes of my research: indeterminate strings and the computation of regularities . In a DNA sequence it may be unclear whether a given entry is a or c , and so the indeterminate symbol {a,c} is used. We could then say that {a,c} matches another symbol {c,g} which in turn matches {g,t} -- but {a,c} certainly does not match {g,t} ! This seemingly innocuous difficulty, the nontransitivity of matching, makes the processing of indeterminate strings much more difficult. Thus a combinatoria l understanding of indeterminate strings becomes essential to the development of efficient methods for their processing. With indeterminate strings, as with ordinary ones, the main task is the recognition/computation of patterns called regularities . For example, the string acaacaa has period 3, since positions i and i+3 are always the same; at the same time, even though acaacaca is not periodic, it nevertheless has a cover aca, since an occurrence of aca covers every position. These regularities, and many others, are fundamental to the calculation of the patterns in strings that provide us with the understanding we seek about patterns in the real world. In a DNA sequence, a single letter that breaks the pattern can mark a crucial genomic vulnerability or advantage. For 15 years, much of my research has embraced these two themes. Using insights based on mathematical analysis, I seek to identify regularities, especially in indeterminate strings, that have real-world significance, and I try to design methods (""algorithms"") to compute them quickly, even for string lengths in the billions or trillions. For example: patterns in DNA that indicate susceptibility to specific diseases; coincidence of terminology in terabytes of Internet text that indicate meaning or topic; recurring appropriate bit patterns that suggest a coded message. Many patterns, many applications! ""625706,""Smythe, Tristan"
"635062"	"Stevenson, Suzanne"	"Probabilistic Models of Semantic and Pragmatic Acquisition and Processing"	"Despite tools such as Siri and Alexa which may suggest that the human-like systems in Her and Ex Machina are just around the corner realistic natural language interaction with a computer remains an elusive goal. Recent big data approaches can achieve impressive behaviours in text and speech processing, but human-like processing of language remains one of the grand challenges of artificial intelligence (AI). Capturing the complexity and extensibility of word meanings is a major obstacle to achieving that goal, yet humans learn word meanings and adapt them to new situations almost effortlessly. Only by modeling language abilities as a computational cognitive system can we hope to achieve human-like performance in future AI applications. The specific objective of this proposal is to devise state-of-the-art computational representations and algorithms that reflect leading-edge linguistic and psychological theories, with the goal of achieving human-like behaviour in representing, learning, and interpreting the meaning of words, leading to three related sets of research projects. The first set of projects will contribute novel algorithms drawing on alignment and graph methods from natural language processing (NLP) that can efficiently yield crosslinguistic semantic data structures. We will demonstrate that these representations more accurately capture properties of human semantic knowledge, and can thus support improved automatic language analysis tools. The second set of projects will demonstrate the benefit of adapting state-of-the-art machine learning techniques to integrate cognitive influences on word learning. Such techniques will enable us to extend the scientific understanding of word learning to acquisition of structured and abstract meanings. Given their basis in leading-edge computer science approaches, these findings can inform NLP methods for extracting rich semantic relations and exploiting top-down and bottom-up information in acquiring meaning. Our third set of projects will extend the state-of-the-art in cognitive models of reference by integrating incremental probabilistic learning methods that reflect cognitive factors. By developing models that capture experimentally-demonstrated influences and that adapt over the course of a conversation, we will contribute to increased understanding of the factors that must be reflected in NLP systems to match human expectations in conversation. Overall, the projects here demonstrate the dual benefits of the multidisciplinary research program: Bringing advances in computer science to bear on improving our understanding of human cognition as a computational system, and informing NLP by adapting recent linguistic and psycholinguistic insights within a computational framework.""630449,""Stewart, Alexandre"
"637327"	"Szczecinski, Leszek"	"Inference and Learning for Wireless Communications"	"The objective of this research program is to contribute to the understanding of the fundamental principles, limitations, and trade-offs in the design of the Bayesian-theory based algorithms for inference and learning with emphasis on their role in transmission of information in communication networks. The Bayesian formalism consists in modelling of all interacting entities as random variables. It was successfully applied in many areas of science and engineering, and the recent decades witnessed a progress in Bayesian reasoning. In particular, the graph-based models which describe the relationship between the variables using graphs obtained significant notoriety leading to a well structured approach to the development of the Bayesian algorithms using the principle of message passing. Yet another step bringing the Bayesian methods closer to practice are the variational Bayesian techniques which explicitly use factorization of the messages into predefined functions to simplify the marginalization required during the message passing. While the Bayesian framework is well known, the approximate approaches must be used in practice and these must be devised and analyzed knowing the application context; here, the transmission of information in wireless networks. We will also leverage the progress made in the general area of machine learning or pattern recognition. However, to remove the requirement for large amount of data, which is impractical for highly variable environment characteristic of wireless communications, we will use to the generative-model approach. The program will focus on two main problems: 1) the analysis and correction of approximate inference algorithms, and 2)the learning from the binary observations. As a particular scenarios to study the problems of inference, we will use the context of massive MIMO receiver. The questions of learning will be study in the context of a) identification of the source of errors in point-to-point transmission; and b) modelling of interference in multiuser wireless networks. We will simultaneously consider the issue of model selection strategies, related to problems 2) which rely heavily on the prior models. This research program will contribute the knowledge creation, as well as, will build technical expertise and contribute to the training of students in the area of Bayesian inference and learning with application to wireless communication. As witnesses by the growing presence of Bayesian techniques in many areas of applied science, as well as persistent growth of wireless networks, such an expertise is increasingly important and will benefit the students/interns involved in the project.""635423,""Szechtman, Fernando"
"637439"	"Szyszkowicz, Sebastian"	"Modelling and Planning Robust and Efficient Wireless Networks for Outdoor Dense Urban Coverage"	"The proposed research uses innovative mathematical and computational techniques to study and design systems for providing robust and efficient wirelesscoverage to dense urban areas. We use freely available real building and cellular tower locations in order to test our concepts with more realism, which will allow us to make more accurate assessments of the cost of different technologies and system designs, and then find good system designs that work well in the complex dense urban setting. - Small cells, and operation at millimeter-wave frequencies (28-300 GHz), are two essential technologies considered for future (post-2020) cellular networks to provide gigabit wireless access at low cost and power consumption to dense urban areas . The last five years have seen a lot of research concepts in how to deploy these technologies at the system level. By studying the wireless propagation in cities using urban maps, and designing algorithms for planning a large number of small cells on these maps, we will be able to assess the cost and capacity of these systems, and find the most optimal system designs . Small cells are also desirable because they reduce the power that a handheld device needs to transmit back to its cellular tower: this results in less wireless power transmitted into the body (greatly alleviating cellular health concerns, particularly during voice calls) and also prolonging the battery charge of the device. - The cell switch-off (CSO) concept has been a topic of research in green communications, and consists of switching off some cellular towers and part of the network during times of low usage (e.g., late at night, during working hours in suburban areas, in the evening in industrial parks, etc.). Finding good strategies to decide which cells to switch off when is an ongoing researchtopic. CSO is expected to reduce the energyconsumption and equipment wear for the operator, with savings passed to the subscribers, and a lower environmental impact. Our research aims at designing simple and practical CSO strategies that apply to both current and future cellular networks. The benefits of this project could already be seen in the next few years. - Augmenting cellular networks with flying drone cells. This idea is complementary to CSO: While current systems are over-engineered to provide wireless coverage in peak usagescenarios (and thus are very under-utilized most of the time), we wish to investigate whether it is possible to use a smaller ground network with lower wireless capacity, and augment the ground network with some flying drones used as cellular towers for short durations (a few hours) during peak times (daily peak hours; during parades, festivals, sports events; in emergency situations; etc.). This is a more far-reaching concept; our research will focus on studying where and how many drones to deploy in order to maximize their wireless coverage together jointly with the coverage of the ground network.""624006,""Ta, Kevin"
"634516"	"Tahvildari, Ladan"	"A Context-Aware Framework for Adaptive Software Security"	"Dynamically changing environments and threat landscapes require adaptive software that enables the change and modification of security mechanisms at runtime. Research has been conducted with two different emphases in this area: context-awareness and self-protection. Research focused on context-awareness is more concerned with how to model, process, and manage contextual information, but is limited in its understanding of how a system adapts itself in response to unanticipated security changes in the context information. On the other hand, research into self-protection focuses more on how to adapt the system's structure and/or behavior in response to requirements and/or unanticipated security changes, but pays less attention to how the context is modeled, processed, and managed. The long-term goal of the proposed research program is to gain full-awareness of cyber attacks at application-layers. In the next five years, the goal is to investigate, design, and develop a novel framework that supports consideration of context-awareness and adaptive security in an integrated manner. The specific objectives are: (1) to design an adaptive model for context recognition, (2) to design a set of requirements and runtime models for context comprehension, and (3) a set of algorithms to handle context projection and resolution. Existing approaches in literature lack the reasoning and learning capabilities required to gain context-awareness for cyber defense. However, the recent PIs preliminary work on modeling the interactions between the attacker and the adaptation manager as a Markov game suggests incorporating context in adaptive security to handle application-layer attacks. The PI will propose a set of integrated models and methods for monitoring, detecting attack-type uncertainty, and applying adaptation decisions in cyber space that are currently unachievable with existing techniques. This research program will create high-quality research capabilities. It will open a new research direction that significantly reduces the cost impact of cyber security threats for more than 1,300 Canadian software companies, and improves Canadas competitiveness in this vital sector. Industry will benefit from the findings of this program in two areas. The provision of a superior context-aware framework will allow for the development and implementation of sophisticated machine learning algorithms and methods for adaptive security at runtime, and the adaptive software security modeling techniques will help companies to employ effective methods to keep self-protecting systems up-to-date during their evolution and maintenance. This research program will train High Qualified Personnel (HQP) to work in the field of software engineering and adaptive security. The HQP will acquire a unique blend of interdisciplinary knowledge and apply that knowledge to improve adaptive software security.""629804,""Tai, LeeHwa"
"634713"	"Tang, Anthony"	"Mixed-Reality Interfaces for Remote Collaboration"	"Conventional video chat provides audio and video connection between distance-separated collaborators. But conventional video chat tools are inadequate for remote collaboration tasks that focus on objects and environments. Such remote collaboration tasks will increasingly become commonplace, including assistance tasks (e.g. machine repair), guidance tasks (e.g. remote tour), and inspection tasks (e.g. design critique). Conventional video chat does a poor job of giving collaborators: (i) awareness information about what a remote collaborator is doing, such as where they are looking, or the hand gestures they are making as part of speech (e.g. pointing while saying Look at this one), and (ii) an understanding of the remote space. These interfaces are critical, because remote objects and locations have become be the main focus of collaboration enabled by mobile devices. We will explore how to design Mixed Reality (MR) interfaces to support remote collaboration tasks. Such interfaces typically do two things: first, they visually anchor virtual content in the real world using augmented reality (e.g. projected visuals, head-mounted displays, or see-through displays), and second, they use sensors to capture aspects of environment (e.g. a depth camera scan/video capture of an environment) or collaborators interactions with the environment (e.g. their gestures or what they are looking at). We will produce theories and prototypes that describe and illustrate how to design MR interfaces that capture awareness information about collaborators (such as gaze, view and gesture information), and allow collaborators to independently explore a digital remote environment. This program is comprised of three sub-projects: (1) Software infrastructure. We will design and implement new infrastructure that allows us to rapidly prototype new kinds of MR experiences. (2) Capturing and visualizing awareness information. Our MR prototypes will focus on capturing collaborators interactions with the environment (e.g. where are they looking, what are they pointing at, etc.), representing and visualizing this to remote collaborators. (3) Enabling independent exploration. We will explore ways of digitally capturing and reconstructing remote environments to allow collaborators to independently view and explore these spaces. Our research will result in new knowledge about how to design and build MR interfaces that allow distance-separated collaborators to work together effectively. We will identify and address challenges that arise in this design space including user interface design, ease of use and fluidity, and technical infrastructure. Our work will inform the research community on designing effective MR interfaces for remote collaboration. It will inform Canadian companies as they continue to develop technologies for remote and telework scenarios.""621850,""Tang, Anthony"
"636982"	"Tang, Shuo"	"Point-of-Care Photonic Devices for Cancer Detection"	"Biophotonics is about generating and harnessing light for comprehending the functioning of cells and tissues. Due to the advantage of light such as noninvasive penetration into tissue, nonionizing, and real-time and high-resolution optical imaging, it is used in medicine to improve diagnosis, therapy and follow-up care, setting the trend towards point-of-care (POC) and personalized medicine. The global biophotonics market was valued at $34.29 billion in 2015, and estimated to reach $91.31 billion by 2024. This robust growth is attributed to the increasing aging population and demand for health care improvements. New device for cancer detection is one of the most important areas of biophotonics. The current standard of cancer diagnosis is to take tissue biopsies from suspicious tumors and perform histopathology analysis in a centralized lab. New POC devices that can shift the diagnostics to the doctors office will significantly improve the early detection of cancer. This research program will design and realize the next generation photonic devices for cancer detection, focusing on detecting circulating tumor cells (CTCs) in vivo inside blood vessels. CTCs are cancer cells of solid tumor origin shed into the bloodstream. CTC is a direct indication of metastasis which accounts for 90% of all cancer-related deaths. CTC also happens in patients with localized cancers and in earlier stage during tumor development. The numbers of CTCs is a critical measure of prognostic for survival or predictive of response to a specific therapy. Therefore, detecting CTCs has the fundamental importance for cancer detection and monitoring treatment outcome in personalized medicine. Most tumors grow unnoticeably without symptoms until they have reached a significant size or a late stage. As blood circulation will bring CTC to the peripheral vessels from over the entire body, we can potentially detect different types of cancers by finding CTCs in the bloodstream. There are ex vivo approaches to enumerate CTCs from whole blood sample. We envision that the next generation method should detect CTCs in vivo, where no blood needs to be drawn and the entire blood volume could be sampled to find rare CTCs, and the technique could be applied repeatedly. We will achieve our objective by designing novel in vivo flow cytometry (IVFC). IVFC can detect CTCs by shining light on blood vessel and detect signal that is specific to CTC. In the current IVFC used for animals, CTCs are labeled by toxic exogenous dyes or genetically modified fluorescent proteins. We will develop IVFC by utilizing the intrinsic contrast, particularly using optical coherence tomography to detect the morphological features of CTC by scattering, and using two-photon excited fluorescence to detect endogenous substances in CTC by autofluorescence. It will open a new area of detecting CTCs label-free and lead to next generation POC devices for improving cancer detection.""622519,""Tang, Skylar"
"634263"	"Thomaszewski, Bernhard"	"DF/CAD: Computer-Aided Design Tools for the Age of Digital Fabrication"	"The last three decades have seen tremendous progress in digital fabrication technology. Today, 3D printers are able to produce designs with ever growing resolution, constantly increasing speed, and a rapidly expanding range of materials.In order to leverage the potential of this new manufacturing complexity, we have to be able to create designs on the same level of complexity. But how do we design buildings, vehicles, planes, or robots when 3D printers can, and have to, use information at the submicron scale? In order to master the extreme complexity that future digital fabrication technology will enable, I envision a new generation of computer-aided design tools for digital fabrication technology ( DF/CAD ). Rather than overwhelming the designer with technical detail, DF/CAD tools capitalize on computation in order to promote human creativity, regardless of the complexity of the underlying problem. The research described in this proposal focuses on the three central components of DF/CAD tools: 1. High-performance simulation models that are able to predict functional aspects of designs with complex geometry and material distributions, 2. Optimization algorithms that can invert the simulation model in order to determine design parameters that lead to desired functional aspects, and 3. Design interfaces that, leveraging simulation and optimization algorithms, allow users to navigate complex design spaces in an intuitive and efficient way. The economic importance of 3D printing is already significant today, and it is growing at a rapid pace. The software that will be developed as part of this research program has the potential to benefit virtually all industries that rely on additive manufacturing. DF/CAD tools will also make 3D printing more accessible to Canadian individuals, thus promoting personalized design and fabrication of consumer goods.""639598,""Thomaszewski, Bernhard"
"637041"	"Thompson, Christopher"	"Improving the Timing and Spatial Resolution in Positron Emission Tomography (PET)"	"My research relates to improving position emission tomography (PET) for small animal and human brain imaging. PET is a functional imaging technique using molecules labelled with positron emitting atoms to investigate regional metabolism of that molecule by the formation of images of its utilization rate as opposed to the underlying anatomy. Recently this has been combined with magnetic resonance imaging (MRI) which provides images of the anatomy over which the PET image is superimposed. Working in collaboration with colleagues in Winnipeg and Vancouver, I have helped commercialize an insert for an MRI scanner which is a PET scanner in order to enable simultaneous PET and MRI imaging The commercial success of this project has provided an incentive for us to work towards a scaled-up version on our PET insert suitable for human brain imaging. It will be much faster and have better spatial resolution than current PET brain scanners, and be adaptable to any MRI scanner. My application covers the research I intend to undertake to support this project. 1) Improve the timing accuracy of the detectors in the PET scanner so that the advantages of time-of-flight PET as demonstrated in whole body PET images will apply to human brain imaging. This requires a timing resolution of about 200 psec FWHM. Using my very fast signal digitizing system, I will explore polynomial fitting to the samples before and during rising edge of the detector pulses. This should give a very robust estimate of the event time. I will then determine the optimal sample rate which provides the desired performance with the lowest sampling rate in order to reduce the complexity and cost of the instrument. 2) Most PET detectors are made from a large number of small scintillation crystals separated with reflective material. The cutting and polishing of these crystals is wasteful of material and labour intensive. An alternative is to use a single crystal, and a large number of independently read-out light sensors which is complicated and expensive compared with the multiplexing method we prefer. Recently I saw that a group in Japan used sub-surface laser engraving of a scintillation crystal to create walls which serve as light-guides. I will use the services of a laser ornament and jewellery manufacturer to etch walls in a single crystal in the same positions we segment our crystals into 409 elements. If successful, this would result in significant cost reductions in the manufacture of PET detectors. 3) Finally I will make a new version of my patented PET timing alignment technique to calibrate PET/MRI scanners. Now I use a fast photomultiplier (PMT) to detect the decay of positron emitting isotopes embedded in a plastic scintillator. The time between this and the detection of the annihilation photon by the PET scanners detectors is used to build a time offset correction table. I will test very fast silicon PMTs rather than a PMT to make an MRI compatible version. ""628873,""Thompson, Graham"
"634668"	"Trappenberg, Thomas"	"Deep learning with limited data"	"Deep neural networks is a machine learning technique based on a model of neuron-like elements. The ""deep"" refers here to networks with many layers that are capable of representing complex functions. Such models have been proven to be extremely valuable as they can learn hierarchical data representations from examples and predict previously unseen data. Such techniques enabled much of the recent progress in computer vision and pattern recognition, and the advancements of deep neural networks was made possible by the availability of large data sets (Big Data), some refinement of algorithms, and the availability of increased computational power with specialized processors such as graphical processing units (GPUs). Companies like Google, Microsoft, Amazon and Facebook, and also numerous smaller engineering firms and market analysis firms are embracing this new technology. However, many applications do not have the luxury of having a large set of examples available to train complex models. In the research program proposed here I am trying to develop methods for applications with limited data, which includes a variety of situations such as small number of measurements, biased examples, or examples from peripheral data. Applications with limited dataare common. For example, in the ocean science community it is common to have a fairly small coverage of measurements compared to the vastness of the oceans. My lab has started to investigate how artificial data can augment pre-training and facilitate the narrowing of the search space. For example, there exist a variety of physical models of ocean dynamics such as the exchange of CO2 with the atmosphere. While these models are generally considered to be insufficient for predictions on the required scale, it is possible that these simulations can provide data for the pre-training of deep networks. Another area that promises a whole new level of techniques to training deep networks is the guidance of the learning process by domain experts, also called transfer learning.Domain experts include hereby either deep neural networks that have been trained on complementary or similar domains, or even human experts. While transfer learning has been considered for some time, the use of deep networks to learn appropriate communication channels for learning opened exciting new possibilities for much progress in this area. Such expert training would provide network training that parallels human learning. Such techniques could further drastically reduce the need of large training sets. The long-term goal of this research program is to develop specific methods and tools to both evaluate a machine-learning problem in terms of its complexity and data need, and to provide a comprehensive toolbox to apply pre-learning and expert transfer learning strategies to problem domains that suffer from limited data.""627866,""Trask, Sarah"
"637561"	"Trovo, JooPedro"	"Improvement of Electric Vehicles Performance using Energy Storage Systems with High Specific Power"	"Electrified powertrain systems have gained a significant interest to develop energy-efficient Electric Vehicles ( EV s). Energy Storage System ( ESS ) is central to reach higher driving range, better climbing or acceleration capabilities, and increased energy efficiency. Currently, batteries are widely-used in EVs, but to obtain an acceptable driving range, batteries with High Specific Energy ( HSE ) are typically preferred but their specific power is normally low (~1kW/kg). Recent trends suggest that new battery systems with promising potential as Li-Ion capacitor or SuperCapacitors ( SC s) have been developed to give High Specific Power ( HSP ) (~12kW/kg). These systems could address part of the fast degradation of the main energy battery when coupled together, and increase the driving range by storing the braking energy more efficiently. Their control is a double challenge because it demands a high performance converters and an intelligent management. The long-term vision of my research is to develop performant, reliable and cost-effective energy conversion and ESSs for transport that will reduce overall fossil fuel consumption . Since 2008, my research has focused on the real time power sharing strategies for a multi-ESS EVs (battery and SCs) for increasing the global performance at the end of the driving cycle. In parallel, I have studied the Variable Inductor ( VI ) concept to solve the saturation issues of power inductors for high power transfers showing a considerable reduction of magnetic material, with better efficiency and power density. As an enhancement of the aforementioned research, the general objective of this Discovery Grant ( DG ) is to demonstrate that HSP-ESSs coupled through improved VI-based power converters, correctly coordinated by an intelligent multi-objective energy management algorithm, may increase the global efficiency and the driving performance of EVs. To achieve this goal, under an extended interdisciplinary challenge, I will pursue the following 3 specific objectives: 1) to analyze the energy efficiency of HSP-ESS, with the specific usage of VI-based converters, including theirs design and implementation; 2) to study the impact on the main ESS performance with the addition of the HSP-ESSs, using a new defined indicator of merits; 3) to develop algorithms encompassing multiple objective functions to which different degrees of importance are assigned as a function of some exogenous variables (traffic, user requirements, weather conditions, etc.), to address the random nature of the power demand of the driving cycle. The novelty of the proposed approach is to use the VI concept in order: 1) to develop more efficient, smaller and lighter DC/DC converters; and 2) to define multi-objective algorithms to better integrate the HSP-ESSs into the EV to address simultaneously two main challenges: the energy management of multi-ESS and the control complexity of the VI.""622398,""Trowell, Keena"
"621721"	"Tung, Frederick"		"NSERC"
"634951"	"Tzanetakis, George"	"Improvising with the dead"	"The end goal of the proposed research is rather simple to state. I would like to develop technologies to support the currently impossible experience of improvising music together with one or more musicians that are dead. This experience should be as close to actual music playing as possible. Although this goal is simple to state it is challenging to achieve and requires important advances in different research topics from several disciplines. Improvisation is present in all music cultures and can be identified and appreciated by most music listeners even in cultures they are not familiar with. The processes thatgovern improvisation are not well understood and also dependon the specific music culture studied. They are alsointrinsically tied to the physical actions of playing instruments. For example to understand and appreciate a solo by B.B King it is not sufficient to look at what notes are played but one also needs to consider the visceral connection between his body and the guitar.I believe that we are at a time when the experience of improvisingwith a dead musician is moving from science fiction to the realm ofpossibility as there are several recent key technological developments thatcan be leveraged for this purpose. Imagine being able to analyze the audio recordings of a legendary jazz musician, for example the incredible saxophone player Charlie Parker,in order to create a model of how he played and improvised. The model could be used to create a computer controlled virtual Charlie Parker that a jazz student could play with and learn from. In order to approximate the playing experience as accurately as possible augmented or virtual reality (AR/VR) devices would be used while playing an actual acoustic instrument. There is resurgence of AR/VR with many key industrial players involved. In the new interfaces for musicalexpression community there is extensive experience building effective cyber-physical systems for music. Music informationretrieval (MIR) techniques analyze millions of songs to extract content information to automatically recommend music. MIR techniques can also be used to extract information about what is being played in real time. My group has been active in these research areas and will continue to do so with this newfocus. Students working on these project will obtain experience in audio signal processing, machine learning, augmented and virtual reality, human computer interaction (HCI), programming languages and artificial intelligence (AI).As the virtual and physical words blend,new ways of interaction will be developed. The proposed work can also lead to advances in AI and the tools used to buildcomplicated, real-time, AI systems and shape how humans and such systems interact in the future. Finally,the advances envisioned in this proposal have the potential to radically transform how we create, distributeand perceive music (and multimedia in general) content in the future. ""634567,""Tzerpos, Vassilios"
"637475"	"Valaee, Shahrokh"	"Localization of Wireless Terminals via Deep Learning"	"Localization of wireless terminals has gained momentum over the last few years. Many applications are now being developed that provide location based services. Google map, Find My iPhone, Virtual Reality Headsets, and Pokmon Go are some examples of applications that use location information. Various new applications will be developed when 5G, Internet-of-Things, and Smart Cities technologies become available. Unfortunately, the GPS service is not available in indoors, or is very inaccurate in areas blocked by tall buildings such as downtown cores in major cities. New technologies should be developed to complement GPS and provide location estimation ubiquitously. Our research at the University of Toronto has been on location finding in recent past. We have developed new localization technologies based on received signal strength (RSS) using advance signal processing methods such as Compressive Sensing. We have studied WiFi received signal strength based localizations and tracking, device diversity through unsupervised learning, and crowdsourcing. We have two patented technologies and a pending patent application. Currently, we are working on location finding via image and video signal processing. The proposed research activity will be the continuation and refinement of our previous research. New studies have shown that using channel state information (CSI) of WiFi signals can give a significant gain in location accuracy, and that a single access point is sufficient to have decimeter level localization accuracy. There are, however, a few shortcomings in the reported literature that limits the application of these methods in practice. The method is only applicable to a pair of nodes, uses the whole ISM band in 2.4 GHz and 5.8 GHz, and its range is limited. Our work will answer some of the open problems in the application of CSI. We will use Deep Learning methods for location finding on RSS and CSI. Deep learning has gained momentum in tackling difficult problems such as voice and video classification, big data, and medical imaging. A deep neural network can extract a wide range of complex features that can be used in regression and classification. Our goal is to develop effective location finding methods that can locate users at decimeter level accuracy operating on off-the-shelf phones without any specialized hardware. ""642076,""Valaee, Shahrokh"
"635081"	"vanBeek, Peter"	"Combinatorial optimization in machine learning using constraint programming"	"Several important tasks in machine learning can be formulated as combinatorial optimization problems. For example, learning the structure of a Bayesian network from data can be formulated as a combinatorial optimization problem, where a score is defined that measures how well a candidate structure is supported by the observed data and the task is to find the structure with the lowest score. As a second example, learning a decision tree from labeled data can be formulated as a combinatorial optimization problem, where the aim is to find the decision tree that best predicts the data subject to regularization constraints. Both of these problems are NP-Hard in general to solve optimally but are also NP-Hard to solve approximately to within a reasonable factor. Thus, advanced search techniques are needed. This research proposal is an investigation into formulating and improving constraint programming and other advanced constraint-based search approaches for solving combinatorial optimization problems that arise in machine learning. In a constraint programming approach, one models a problem by specifying constraints on acceptable solutions and search is then used to find a solution that satisfies the constraints and optimizes a cost function. An important feature of constraint programming is that one can first focus on a declarative constraint model and then develop an efficient algorithm for that model, either a complete and optimal algorithm based on backtracking search or an incomplete and approximate algorithm based on local search. In general, the research will be application-driven and will address practical, important problems. The research will be guided by the two important applications alluded to above: learning the structure of a Bayesian network from data and learning a decision tree from data subject to regularization constraints.The scientific approach will include: developing improved constraint models, improving the upper and lower bounds used during the search, investigating alternative search spaces, investigating the incorporation of prior domain knowledge in the form of constraints, methods for model averaging by generating the k-best models, extending the methods to other directed acyclic probabilistic graphical models such as sigmoid belief networks, and extending the methods to other decision tree models such as multi-variate decision trees. The primary goals of the research projects are to develop faster algorithms for finding solutions, algorithms that find optimal or higher quality solutions, and algorithms that are more widely applicable in practice. Any improvements to the underlying solving algorithms have the potential to improve many applications of these machine learning approaches. A secondary goal of this work is to further develop constraint programming techniques that have general applicability to similar optimization problems.""636808,""Vanberkel, Peter"
"634336"	"Vardy, Andrew"	"Robots Organizing Environments: Collective Strategies based on Low-Cost Navigation"	"It is often stated that robots are best suited for dirty, dull, and dangerous jobs. This research program concerns the development of techniques that would allow a swarm of robots to organize their environment, a task that can easily be described as dirty, dull, and potentially dangerous. We focus on the problems of object clustering and sorting. In object clustering, there is only one type of of object in the environment and the goal is to gather all objects into one cluster. In object sorting, there are multiple types of objects and the goal is to form homogeneous clusters, ideally one for each type. This work falls under the domain of swarm robotics which isan approach to the design of multi-robot systems where the robots are constrained to sense and act locally. In a swarm robotic system, no single robot is in charge which also means that the failure of any single robot does not lead to the failure of the system. Previous approaches to object clustering and sorting only handle uniformly shaped objects which are easily identifiable. We are interested in addressing real-world challenges such as arbitrarily shaped objects which are not readily identifiable. Another real-world challenge is dealing with congestion where the robots become overcrowded and unproductive. Dealing with human users and using their feedback to modify the behaviour of a swarm is another significant real-world challenge that we will address. Our approach is innovative and practical in that we focus on the capabilities of inexpensive robots which can perform these tasks as a collective. Robots with the desired capabilities can be constructed with current off-the-shelf technology for less than $500. The key capabilities for what we term low-cost navigation include visual homing, the ability to return to previously visited places using vision, and odometry, the ability to estimate movement over short distances. We will exploit strategies for navigation to accelerate task performance by having robots move deliberately to previously visited places, as opposed to the currently popular strategy of randomized motion. The introduction of low-cost navigation capability into swarm robotics will help to push the field towards practical application. We envision robots cleaning homes, gathering garden waste, sorting recyclables, and bringing order to chaotic work environments. These tasks may be viewed as mundane, but that is exactly why we are interested in automating them. They are also ubiquitous. Therefore, workable solutions have the potential for a major societal impact and the development of such solutions has the capacity to generate significant economic activity and jobs. The HQP trained in this program will have the unique opportunity to exploit the technology from this research program to either create start-up companies or develop careers in the growing robotics industry.""634048,""Varela, Diana"
"634800"	"Vetta, Adrian"	"Computation in Auctions, Markets and Networks."	"This Discovery Grant proposal concerns algorithmic economics and mechanism design.In practical mechanism design, several aspects are of fundamental importance.In general, mechanisms should be resistant against strategic gamingand should produce economically efficient outcomes.Moreover, they should be simple and must be computational implementable.These attributes can, in turn, lead to other desirable properties such asspeed and fairness. The primary aim of this proposal will be to mathematically examine the performance trade-offs between the corresponding multiple objectives,and to design mechanisms with provably high performance.Three major areas of application concern the design of combinatorial auctions for selling spectrum,pollution reduction mechanisms, and the design of market places in industrial organization. For many of these projects, networks play a fundamental role in modelling the underlying problem structure. Consequently, algorithmic methods from graph theory and combinatorics will be at the heart of our quantitative analyses.""631503,""Vette, Albert"
"637396"	"Wang, David"	"Issues in Virtual Reality Simulators for Surgical Simulators"	"As technology advances our ability to create Virtual Reality (VR) environments, there is now the capability of increasing realism, particularly for training purposes. The focus of this proposal is on Haptic Audio Visual Environments (HAVEs). The particular application that will be the focus of this study is pedicle screw insertion for correcting deformities in idiopathic scoliosis. This is a highly risky surgery which currently has training accomplished either through the use of cadavers (where the change in material properties and the lack of blood pressure is a hindrance) or in the actual operating room on a live patient. The surgeon digs a channel through the pedicle using a free-hand technique, in order to insert a screw so that the spine can be mechanically straightened. A HAVE environment would ensure that training could be accomplished in a realistic manner with no risk to a patient. This proposal will develop the robotic actuators, control strategies, digital projection algorithms and audio capabilities to create a VR simulator which will be evaluated in conjunction with experts, fellows and residents at Sick Childrens Hospital. A longer term study will examine the feasibility of tele-collaboration between remote HAVE surgical simulators, which will allow the training of medical specialists in remote regions. The anticipated outcomes will immediately increase the number of experts for scoliosis surgery. The technological advances in HAVE design are, however, relevant to many other medical training applications, since a HAVE can be reprogrammed entirely in software to look like any other procedure. The tele-collaboration capabilities could be very beneficial in our sparsely populated nation. Finally, there will be the training of HQP with interdisciplinary skills of computer graphics, audio processing, mechatronics and human factors. ""624156,""Wang, DongFang"
"637103"	"Wang, Liwei"	"5"	"NSERC"
"637495"	"Wang, ZJane"	"Sparse Signal Processing and Modeling of High Dimensional Spatio-Temporal Data"	"Big high-dimensional spatio-temporal datasets, where the number of measured variables is larger than the sample size, are widely seen in real-world applications (e.g., functional magnetic resonance imaging (fMRI) signals and microarray gene expression time series data). Statistical estimation and inference in high-dimensional situations is fundamentally different from that in the classical setting (with large sample size and smaller number of variables) and many problems are largely open, requiring new statistical signal processing theory and methods involved in regression, clustering, prediction, classification and other statistical problems of high-dimensional data. Generally speaking, estimation and inference in high-dimensional data is not possible without assuming special low-rank (e.g., sparse) structures in the data. With this vision, the current research program will focus on sparse signal processing and modeling of high-dimensional data by both establishing theoretical foundations and developing application-specific novel algorithms. More specifically, the proposed research program will pursue the following main technical objectives: 1.) Investigating sparsity-aware estimation and inference problems for high-dimensional time series data. We aim to establish the theory and develop novel algorithms for 2nd-order inference of high-dimensional processes that can be non-iid (independent and identically distributed), non-stationary and non-Gaussian; 2.) Developing novel online sparse Principal Component Analysis (PCA) and online sparse Independent Component Analysis (ICA) algorithms for dimensionality reduction, since PCA and ICA are probably the most popular dimension reduction approaches; and 3.) Investigating motivating real-world health applications, including real-time fMRI based neurofeedback for Parkinsons Disease patients, video-based cardiac physiological monitoring, and Kinect/iPad based serious games for supporting elderly people's cognitive rehabilitation. The significance of this research lies in its focus on both the theory and practice of sparse modeling of high dimensional spatio-temporal data. The outcome of this research program will make significant contributions to both the theory and applications of big data analytics. Indeed, the notion data analytics is a more attractive capital-lite business than that related to hardware. As one key innovation accelerator in the so-called 3rd Platform era, big data analytics is reshaping many industries and revolutionizing many research areas. The proposed research will help take this vision one tiny step further. The research program will provide an opportunity for the graduate students to be trained in related cutting-edge technologies.""640434,""Wang, ZJane"
"634746"	"Watt, Stephen"	"Computer Algebra and Digital Mathematical Libraries"	"Computer algebra studies how to get computers to solve mathematical problems symbolically, for example to solve equations in terms of mathematical formulae, rather than by giving set of numerical values. The long-term goal of our research is to understand how to make the computer treatment of mathematics as easy and powerful as the computer treatment of text, images or other media. We will focus on four questions: 1. Symbolic Domains: What new symbolic domains will be most useful in computer algebra, and how can these be treated rigorously and efficiently? We have already developed algorithms for polynomials with symbolic exponents. What can be done with coefficient rings of symbolic characteristic, or with matrices of symbolic block structure, etc? The impact will go beyond inventing some nice mathematics it will significantly extend the utility of computer algebra systems. 2. Software for Mathematical Collaboration: All computer algebra systems today have the same type of interface that is a slight variation of the classical teletype conversation. The only difference is that the result can now be typeset or shown as a graph. We now have many more forms of computer-mediated interaction: blogs, comment trees, shared editing of documents, and so on. We shall investigate what should be the shared objects and interfaces of computer aided mathematical collaboration. 3. Digital Math Libraries and Knowledge Management: An astonishing amount of mathematical knowledge is captured already in digital form, be it near-comprehensive archives of PDF page images of the worlds research journals or the knowledge captured in computer algebra and proof systems. We shall build on our prior work on mathematical document analysis, computer algebra systems and mathematical knowledge management to explore how computer algebra and digital math libraries may most usefully be converged form a mathematical knowledge base. Beyond supporting the current forms of mathematical software, such mathematical knowledge tools should allow users of mathematics to check that multi-step processes give sound results, avoiding engineering disasters. 4. Compiling for Symbolic Computation: Programming languages for symbolic computation remain a challenge for optimizing compilers. In particular, house-keeping associated with modern object-oriented design prevents certain optimizations. We shall investigate how modern compiler techniques can be used to remove un-necessary storage allocations and arithmetic. While each of these topics has its own specific questions and goals, the three have many points of interaction. For example, the new document analysis techniques used to recognize mathematical expression in articles will have much in common with the recognition of handwritten expressions in collaborations. Taken together, these problems open new frontiers for computer algebra. ""631185,""Watter, Scott"
"617735"	"Williamson, Sheldon"	"DESIGN AND DEVELOPMENT OF A HIGH-EFFICIENCY WIRELESS FAST CHARGING SYSTEM FOR URBAN MODES OF AUTONOMOUS ELECTRIC MOBILITY AND TRANSPORTATION"	"Urban autonomous electric mobility (e-mobility)and electric mass transitare being realized quickly. Theyare the dire need for populous metropolitan cities.In addition, urban mass transit (buses, subways, and trams), among other modes of urban transportation, are a major challenge facing cities. In several overcrowded cities around the world, existing urban mobility systems are simply inadequate. Harmful emissions, accidents, knotty congestions, and unscheduled operations are just among the critical few issues that are creating transportation chaos worldwide. Autonomous and driverless e-mobility will certainly alleviate such disruptions. These newmodes of urban e-mobility will be smart and free of human error. The key ingredient to build a sustainable and efficient autonomous mobility infrastructure will most certainly include electrified transportation. Opportunistic static charging and dynamic, in-motion charging of urban autonomous e-mobility and masse-transport, powered either by high-energy batteries or high-power ultracapacitors, will be the need of the hour. Novel power electronic conversion systems and innovative coil designs will play a major role in interfacing high-energy/high-power electric storage systems to the charging infrastructure. The proposed NSERC DG program specifically address the above issues comprehensively, focusing on the design of universal, wide-ranging wireless fast charging infrastructures. Both static (opportunity charging) as well as in-motion (dynamic charging) systems will be designed, developed, tested, and verified. Wireless Power Transfer (WPT) can be essentially classified into two methodologies: Inductive Power Transfer (IPT) and Capacitive Power Transfer (CPT). IPT is the more established method, while CPT is an emerging approach, catering solely to low-power levels at the moment (up to 1.0 kW). The proposed NSERC DG program will aim to address several key challenges in advancing the state-of-the-art for both WPT methods. The specific focus of this proposal is high power, high-efficiency static and in-motion fast charging for future modes of urban autonomous e-mobility and mass electric transportation from e-bikes to e-buses. WPT charger ratings ranging from 1.0 kW to 50.0 kW will be investigated. The overall program is divided in to 6 major tasks: 1) IPT magnetics (coil) design; 2)Novel WPT power electronic converter designs; 3) Advanced WPT controller development; 4) Design of an in-motion (dynamic) opportunity fast charger (50 kW); 5)Capacitance design for CPT; hybrid IPT/CPT design; and 6) EMI shielding. The above exclusive researchtargets make the proposed programparticularly unique and specialized. This research will most definitely lead to future progress, more specifically, with regards to overall efficiency and performance improvement of autonomous e-mobility/e-transportation infrastructure. ""637552,""Williamson, Sheldon"
"637186"	"Wilton, Steven"	"Bringing Field-Programmable Gate Arrays to the Masses: Towards a Design Eco-System"	"For decades, improvements in integrated circuit (IC) technology have provided dramatic increases in computing power, enabling applications such as the low-cost high-speed internet and mobile computing. New applications are on the horizon, including embedded machine-learning algorithms, extreme big data computation problems, and applications associated with the emerging internet of things. However, advances in many of these important applications are being held back by the lack of sufficient power-efficient and cost-effective computing hardware capabilities. Due to the increased cost and technical challenges in manufacturing smaller and faster transistors, we can no longer rely on improvements in semiconductor technology to provide the required computing horsepower. This has led to a revolution in the computing industry. Many researchers now see hardware accelerators, and in particular accelerators based on Field-Programmable Gate Arrays (FPGAs), as a disruptive technology that can provide power-efficient computing capacity necessary for many of these emerging applications. For years, FPGAs have been extremely successful as low-cost, low-risk replacements for custom integrated circuits. However, as evidenced by Intels recent acquisition of Altera, and Microsofts efforts to bring FPGA technology to the cloud, FPGAs are now poised to emerge as mainstream software accelerators. Before FPGAs can evolve into this new role, critical challenges must be addressed. Hardware accelerators will only become ubiquitous if they can be designed, optimized, and debugged by software designers and domain experts. There has been significant effort developing high-level synthesis compilers which convert software code to a hardware design. However, a compiler is not enough. Software designers expect an entire eco-system which provides the ability to characterize, evaluate, optimize and iterate on their designs quickly. The proposed Discovery Grant project will be the backbone of a research program aimed at making FPGA acceleration accessible to software designers, by making such an ecosystem a reality. Specifically, the research will focus on three aspects: (1) an iterative design flow that brings the benefits of agile methodologies to hardware design, (2) improved core compilation technologies, and (3) improved device architectures. Together, these efforts will help make ubiquitous hardware acceleration a reality, enabling many emerging applications that are simply not feasible today.""625262,""Winch, Harrison"
"635038"	"Wong, Andrew"	"Pattern and Knowledge Discovery on Relational, Biosequence and Multiple Temporal Sequence Data"	"As information technology advances, a tremendous amount of data is generated in all industries. There is an increasing amount of attempts to leverage this large amount of data, with the assistance of high-performance computing to develop intelligent system for various applications. Despite their success, the models that turn data into these applications are often black-boxes. This has two drawbacks. First, the users have no trust since how is hidden. Second, it is difficult for human to interpret why. Here we introduce a new paradigm: From Pattern to Knowledge (P2K). It first discovers strong statistical associations/relations from data autonomously. It represents them as patterns, pattern clusters and their association/co-occurrence to reflect the what and where of critical information without explicit reliance on prior knowledge usually unavailable or difficult to get. It then comes up with the how of robust algorithms to conduct analysis and direct further search to disclose the why of the underlying mechanisms --- interpretable/verifiable. P2K will make existing machine intelligence approaches more robust and reliable while revealing useful and actionable knowledge. Hence, the objective of this proposal is to develop P2K, targeting on 3 types of data at its initial phase: relational, bio-sequence and multiple temporal sequence data. We choose bio-sequence from bioinformatics as a platform to validate the scientific values and effectiveness of P2K. In the last five years, from biosequences, we have developed algorithms to discover, prune, locate and analyze statistically significant patterns, pattern clusters and their association/co-occurrences so as to reveal local and distant functional domains and relationship without relying explicitly on prior knowledge or clues; b) use the patterns discovered as features for predictive analysis. The effectiveness of P2K is backed by strong publications. In the next five years, for biosequence data we will develop algorithms to predict binding sites/partners between proteins, protein and DNA/RNA, protein and aptamers to reduce users reliance on structures, saving them time/budget and help drug discovery and disease treatment to identify small molecules that can inhibit binding. For relational data, we will complete a scalable system to discover and analyze patterns for mixed-mode data, including using patterns extracted from business/finance reports via our text mining module Text-P2K in a semi-supervised fashion to assist decision making. For multiple time-series data, we will leverage the discovered temporal associations of pattern clusters to capture a wide range of local relations along and across individual series and use them as features/patterns for interpretation and forecasting. It can help finance firms to identify rare movements to control risk, and help factories to identify machinery faults in advance.""622486,""Wong, Andrew"
"634625"	"Wong, Kenny"	"Learning Analytics for Massive Open Online Courses"	"Massive Open Online Courses (MOOCs) have arisen as an increasingly popular way to deliver quality educational content to individual learners. On the delivery platform, much data can be gathered and analyzed about the learners, including demographics, discussion postings, assessment submissions, course progress events, and course marks. With growing amounts of data for many thousands of learners, the proposed research program aims to provide timely advice that identifies learning difficulties, and to offer suggestions that personalize the learning experience. Popular today with learners interested in professional development are MOOC specializations, each a cohesive sequence of self-paced courses followed by a scheduled capstone project course. Most of these learners have a degree, work in industry, have families, and must use their time effectively. From our experience in designing, constructing, producing, and delivering a MOOC specialization on software product management, we have a special opportunity to better understand and assist our learners, with aims to reduce any frustrations and increase engagement. While learning through a MOOC is often individualistic, we see a budding community of practice through the discussion forums. The proposed research explores incentives to encourage better contributions and sharing, to provide a richer learning experience that improves course quality. This research program seeks to develop effective concepts, methods, and tools to understand learners and their activities, to help improve learning, foster communities of practice, boost completion rates, and promote engagement. ""624581,""Wong, KevinChunKit"
"617751"	"Wong, Vincent"	"Toward Energy-Efficient and Ultra-Low Latency Wireless Networks: 5G and Beyond"	"Toward Energy-Efficient and Ultra-Low Latency Wireless Networks: 5G and Beyond The research program of the applicant is in the areas of wireless communications and networking. In recent years, wireless service providers have deployed Long Term Evolution (LTE) systems to facilitate mobile users in using their smartphones or tablets to connect to the Internet, access social networks, and launch different types of real-time and streaming multimedia applications. With the proliferation of Internet of Things (IoT) devices and applications, the next generation of wireless networks, namely the fifth generation (5G) networks, aim to not only provide a higher data rate to the existing mobile users, but also to support the connection of potential billions of IoT devices to the Internet. Machine-type communications (MTC) is an important enabler of IoT. The International Telecommunications Union classifies MTC into two categories: massive MTC (mMTC) and ultra-reliable and low latency communications (URLLC). mMTC is characterized by high connection density; i.e., a massive number of active low-cost and low-power MTC devices co-exist per cell. Sensor networks and wearables are examples of mMTC. URLLC requires reliable data transmissions with strict latency constraint of 10 milliseconds or less. URLLC is required for applications such as e-health and autonomous driving. To meet these challenging requirements, a true revolution of technologies in the radio access network and core network is needed. In this research program, the long-term objective is to support IoT applications with diverse delay and throughput requirements, and to improve the spectral and energy efficiencies of the emerging 5G wireless networks in a scalable manner. Within the five-year time frame, we will focus on the following inter-related short-term objectives : (a) to develop radio access network and core network architecture, incorporating software-defined networking (SDN), mobile edge computing, and dynamic network slicing, to improve the spectral efficiency and flexibility of 5G networks to support different types of devices (e.g., smartphones, MTC devices) and IoT applications; (b) to develop energy-efficient medium access control algorithms for mMTC devices, based on non-orthogonal multiple access (NOMA), to improve the scalability and connection density within a coverage area; and (c) to develop application-aware scheduling algorithms for URLCC devices, based on SDN, carrier aggregation, and NOMA, to support IoT applications with high reliability and ultra-low delay requirements. We anticipate that our research will find practical novel solutions and applications for the benefit of the local and the global telecommunications industry for 5G systems and beyond. They are expected to have a lasting impact in contributing towards the long-term competitiveness of Canadas technological innovation.""637361,""Wong, Vincent"
"637460"	"Woungang, Isaac"	"Design, Analysis, and Optimization of an End-to-End Cloud-Centric System for Internet of Things over HetNet"	"Cloud computing, with a focus on developing novel methods and policies to safeguard and protect information assets in the cloud, is a top priority of the Committee on Information Management in Business (CIMB) of the Treasury Board Secretariat of Canada. Also of interest is the Internet of Things (IoT), a new computing and design paradigm addressing the proliferation of devices directly connected to the Internet. These devices will generate a deluge of data requiring continuous monitoring and analysis. Efficient storage and processing technology will be necessary to enable remote access and analysis for decision-making purposes. In recent years, there has been growing interest from Canadian businesses and institutions in exploiting the potential of cloud-centric systems to serve the needs of the IoT. In addition, organizations that maintain and handle sensitive information are concerned about data security in the cloud environment. The lack of an end-to-end framework for cloud-centric systems is a key factor delaying the widespread adoption of cloud technologies to serve the IoT. IoT data must be accessible ubiquitously and transmitted efficiently among devices and remote cloud data centres over a mobile cloud computing platform. Once the data is offloaded in the cloud, its privacy and integrity must be preserved, even in the presence of a semi-honest Cloud Service Provider (CSP), i.e. a disgruntled, profiteering or curious CSP. In this research program, our primary objective is to develop an end-to-end framework for cloud-centric systems for the IoT that consists of (1) novel IoT-driven radio resource management (RRM) algorithms that exploit a common pool of wireless radio resources in a HetNet to enhance the IoT data transmissions to the cloud through machine-to-machine (M2M) communications, and (2) a novel secure IoT data deduplication framework for cloud storage, assuming a semi-honest CSP. The outcomes of this research program are expected to have a profound impact on how cloud systems can integrate data from various sources. We will provide models for the design, analysis, and optimization of cloud-centric IoT systems over HetNets, along with a data deduplication scheme for securing the data access and storage in the cloud. We believe that this research proposal is timely, considering the increasing demand for cloud-centric systems for IoT from the government and businesses in the information technology sector.""620520,""Wozny, Michael"
"637573"	"Wu, Bin"	"High-Power Converters: Topologies, Controls and Applications"	"High-power converters in the megawatt range are widely used in industry, such as steel, mining, power utility, renewable energy and petrochemical industries. With the technological advancement, a newtype of semiconductor switching devices, the silicon carbide (SiC) switches, have emerged. They are more efficient and fast than the other switching devices currently used in the high-power conversion systems, and their voltage and current handling capability starts to reach a level that can be practically used in high-power converters. The main objective of the proposed research program is to develop SiC device based high-power converters and their control schemes for next generation medium voltage (MV, 2.3kV 13.8kV) motor drives for higher energy efficiency and better system performance. The research program is expected to generate a number of leading-edge technologies and innovative designs for the high-power MV drives, which include novel SiC voltage source and current source converters, next generation 10kV-class transformerless drives, and advanced modulation and control algorithms for the MV drives. The developedtechnologies are expected to be utilizedin theCanadian driveindustry to enhance the performance of their products, reduce the manufacturing cost, and increase the productivity, making contributions to the Canadas economic growth. ""625481,""Wu, Bryce"
"617727"	"Wu, Ke"	"Structural and Functional Integration of Wireless Transceiver over Megahertz through Terahertz"	"In connection with the prospective fifth generation of mobile networks (5G) and the Internet of Things (IoT) deployments, this 5-year Discovery Grant research responds directly to the strategic and global vision to create and develop future, flexible smart wireless devices and systems. It strives to overcome core challenges in developing the next generation of information and communication technologies (ICT), such as intelligent mobility and autonomous operation, by focusing on merging the three fundamental wireless functions into one single transceiver, namely communication, sensing and powering. This holistic integration will create an unprecedented wireless system with aggregated radio-radar functions that can directly and uniquely interplay in order to resolve many long-standing issues in the field. In addition, the embedded wireless powering may allow the creation of a maintenance-free system. The applicant, his students and research fellows in collaboration with his colleagues and technical support staff are poised to theoretically and experimentally investigate, develop and demonstrate the worlds first wireless transceiver featuring multi-function and hetero-structure over select frequency ranges of megahertz through terahertz. The main research goals are to propose, study and explore highly original wireless transceiver architectures and technologies with the enabling integration platforms of wireless functions and hardware structures. Specifically, this research is concerned with the following concepts and demonstrations: (1) heterogeneous multilayer techniques with focus on hybrid coupling mechanisms and critical front-end circuits; (2) Integrated antenna arrays with Yagi topologies and mode diversities; (3) multiport interferometer architectures with joint radio-radar operations; (4) multiport interferometer techniques with simultaneous radio communication and power harvesting; and (5) multilayer multiport multi-function transceivers with the integrated scheme of data communication, parametric sensing and power harvesting. As opposed to conventional Schottky diodes, backward tunneling diodes are studied for demonstrating extremely low-power multi-port transceivers. As the first of its kind in the world, this research proposes a game-changing solution for the wireless community; in the long term, its technology can be far-reaching and disruptive in many different industries. There is an urgent need for Canada to sustain its ICT leadership and exploit this emerging field by exploring and developing new technologies, which will not only expand the reach of current systems but also trigger new applications and propel new markets. The successful development of this research project is strategically important for Canadian smart and better living, regulatory policy, technological education, economic growth, and sustainable future. ""637622,""Wu, Ke"
"637476"	"Wu, Xiaolin"	"Signal Processing Methods for Computational Displays"	"Recent years have seen intensified research on and burgeoning commercial interests in a new generation of optoelectronic displays, driven by a wide range of virtual, augmented and mixed reality applications in diversified fields from man-machine interactions, medicine, entertainment, to automobile, etc. Steady progress in the raw capabilities of display hardware and computers are the other important catalysts for the great enthusiasm in advanced display technologies on all sides: academia, industry and Wall Street. Many display functionalities that were considered prohibitively expensive can now be offered at acceptable costs. Just as the importance of inventing paper and printing machines to ancient civilizations, the potential of advanced high-tech display technologies cannot be overstated, which may shape the ways we communicate, learn, play, or even behave. This research is to advance my recent ground-breaking work of temporal psychovisual modulation (TPVM), which is a new paradigm of computational display that can potentially revolutionize the design of and user experiences with visual man-machine interfaces. Through an ingenious interplay of psychophysics, optoelectronics and mathematical modeling of human vision, TPVM can present multiple users different interference-free views simultaneously on the same display medium. It is ideally suited for multiuser collaborative virtual reality (VR) and augmented reality (AR). Unlike Google Glasses, Oculuss Rift and Microsofts Hololens, the end user viewing devices in the TPVM computational display system are simplified to light, simple, display-synchronized liquid crystal glasses. All heavy computations and complex optics required for generating eye-tracking 3D virtual environment are delegated to a central, powerful rendering engine and a high-speed display. Besides VR/AR, TPVM also has other exciting applications: anti-piracy displays, backward compatible 3D/2D displays, display-camera visible light communication, and privacy-protection displays. Scientifically and philosophically speaking, holography should be the ultimate display technology that may threaten to blur the boundary of mind and body. But due to daunting engineering difficulties in making holograms, people always seek for more practical alternatives. Recently, some futuristic 4D light field displays emerged, represented by Magic Leaps head-mounted virtual retinal display and MIT Media Labs tensor display; both fall into the category of computational displays like TPVM. In this research I will also launch an academic pursuit to gain theoretical understanding of the 2D/3D signal representation capacities of the aforementioned computational displays. ""624267,""Wu, Yi"
"637063"	"Xia, Guangrui"	"Germanium for next generation photonic and microelectronic devices"	"With sustained exponential growth, global internet traffic is expected to reach 2.3 zettabytes (2.3x2 70 ) by 2020 [2016 Cisco]. However, mainstream short-reach communications and on-chip interconnects have been dominated by metal wires, which are much slower, less energy efficient and hard to scale in size. Optical interconnections via silicon (Si) photonics have been widely recognized as a potential solution to overcome this bottleneck. Germanium (Ge) as the most Si-compatible semiconductor has been the underlying and enabling material for Si photonics. Ge has been widely used in photodetectors and modulators providing a data rate of 50 Gbps [2015 Chen, 2016 Srinivasan]. For Si-compatible lasers, Ge can be used as 1) transition layers between lasing materials such as InGaAs and AlGaAs and Si [2012 Lee, 2016B Lin, 2016 Liu, 2016 Nakao] due to its small lattice mismatch to them and the ease of integration with Si and 2) a lasing material thanks to bandgap engineering [2010 Liu, 2012 C-A]. On the microelectronics side, Ge has been widely used in SiGe heterojunction bipolar transistors (HBTs) for applications in wireless communications. We propose the following topics on Ge in Si photonics and microelectronics. 1. It is highly desired to have low defect density Ge films on Si to serve as III-V and Si transition layers. Aspect ratio trapping technology (ART) can produce high quality Ge. However, it needs additional fabrication steps and is inferior in thermal conduction. A low/high temperature (LT/HT) growth method is advantageous over ART in these two aspects. However, the Ge quality is not as good. Arsenic doping has been shown to greatly improve Ge quality [2016 Lee], while impacts from other dopants have not studied. We propose to study doping impacts on Ge quality using LT/HT method for high quality Ge film growth on Si. 2. We propose to study the potential and the optimizations of Ge-on-Si lasers by device modeling and simulations. 3. As higher concentration of Ge is used in HBT base layer, Si-Ge interdiffusion is becoming more problematic. We propose to study the interdiffusion behavior in PNP type HBTs, especially the impacts from phosphorus and carbon and the modeling of these impacts for faster and more energy efficient wireless communication systems. The proposed research will enable optoelectronic integrated circuit (OEIC) on Si platforms such as a single-chip optical transceiver, which provides the ability to download movies in seconds and are much cheaper and smaller than the current technology with external lasers. The research outcomes can lead to deeper penetration of optical fiber communications, faster wireless communications and significant advancements in the current information technology hardware industry. We truly believe that the research proposed is at the research frontier and will benefit Canada as a world leader in optical communications and information technology greatly. ""643175,""Xia, Jianguo"
"635080"	"Xiang, Yang"	"Tractable NAT-Modeled Bayesian Networks and Privacy Sensitive Construction of Agent Organizations"	"(1) Rigid, simplistic rules are often used for decision making in personal and mobile devices. For example, a phone number may be placed on a black list, due to report of a spam call from it, causing future calls from the number to be filtered. The rule ignores the possibility that the report may itself be a spam, leading to undesirable actions. Bayesian Networks (BNs), knowledge based systems capable of weighing complex context information, can aid users with more intelligent decisions. Since inference in general BNs is intractable, it is necessary to identify subclasses of BNs that enable efficient inference. They include BNs of low treewidth, and BNs of high treewidth but encoding Context-Specific Independence (CSI) in local structures and being compiled into Arithmetic Circuits or Sum-Product Networks. Independence of Causal Influence (ICI) encoded in Non-impeding noisy-AND Tree (NAT) models are orthoganal to CSI and NAT models are more compact than CSI based local structures, such as Algebraic Decision Diagrams. This research will investigate how to conduct tractable inference in NAT-modeled BNs with high treewidth, how to further improve inference efficiency by exploiting both CSI and NAT-expressible ICI, and how to acquire such BNs by machine learning. Its success will broaden subclasses of tractable BNs of high treewidth, making BN inference more widely deployable. (2) Cooperative intelligent systems (called agents) are well suited for applications such as monitoring complex equipment or collaborative design in supply chains. Often, agents cooperate through an organization. The Junction Tree (JT) is one such organization and is found superior than the often used Pseudotrees. An agent may embed rich knowledge, e.g., on an equipment subsystem, that is proprietary to the subsystem vendor and needs to remain private. However, common methods to construct JT organizations suffer from breach of such privacy. As a result, vendors risk losing intellectual properties. To improve privacy in these agent systems, this research studies how to construct JT organizations without privacy loss if possible and with the minimum loss if unavoidable. Flexible JT organization construction is also developed with privacy protection to handle changes in system composition, e.g., when an agent is added due to system expansion. Feasibility of fully autonomous, privacy protecting JT construction will be studied, e.g., without using an externally specified leader agent. Successful completion of this research will close the loop hole for privacy loss in agent systems built on JT organizations. The strong privacy guarantee, coupled with other superior computational properties of JT organizations, will make these agent systems more widely applicable. ""633075,""Xiao, Chijin"
"634750"	"Yao, Yiyu"	"Theory and Applications of Three-Way Decisions in Intelligent Information Systems"	"Thinking in threes is a frequently occurring theme in human knowing, understanding, and problem solving, for example, three parts of a whole, three components of a system or a theory, three perspectives/angles of an issue, three stages/eras of evolution, three levels of understanding, etc. In 2009, the applicant suggested, and has been working on, a theory of three-way decisions for thinking and processing in threes. The proposed research program continues the investigation of the theory and its applications. Three-way decisions can have two purposes: (a) to build cognitive tools that aid us in thinking in threes or trichotomies, and (b) to design and implement intelligent systems based on the same principles. For these purposes, a long-term, broad objective is to formulate a theory of three-way decisions by framing it as a multidisciplinary and interdisciplinary study. Another long-term, specific objective is to investigate the implications and applications of three-way decisions in designing and implementing intelligent systems. Following the philosophy of thinking in threes, short-term objectives and associated research activities are grouped into three integrative parts: Foundations, theory and models, and applications. My students and I will investigate a philosophy of thinking in threes and its cognitive basis and advantages by embracing ideas from different fields and disciplines. We will fully develop a trisecting-and-acting framework. Trisecting divides a whole into three distinctive and related parts and acting devises effective strategies to act upon the three parts. Within this framework, we will study concrete models, such as statistical models and sequential models. We will connect three-way decisions to other theories, in order to create novel methods of three-way analysis, three-way clustering, three-way classification, three-way recommendation, and many more. We will investigate three-way decisions for big data analytics. We will build prototype systems to validate the value of three-way decisions and our theoretical findings. Three-way decisions turn complexity into simplicity by using manageable and meaningful three parts, i.e., thinking in three granules, trisecting-and-acting, and trisect-and-conquer. Cognitive tools of three-way decisions can empower us to better deal with complexity. According to Kelly (IBM senior vice president), we are now at the most transformational phase of the cognitive era in the computing's evolution. By taking advantage of humans for insights and intuition and of machines for storage and speed, human-machine symbiosis embodying human thinking is a high-dividend research direction. Three-way decisions, as a special class of human ways to think, come at the right time and will rise in the future. The proposed research program is expected to have wide-range and long-lasting social, educational, and technological impacts.""638158,""yao, zhongwen"
"637146"	"Yazdani, Amirnaser"	"Next Generation Electric Power Distribution Networks: Architecture, Power Electronics, and Integration with the Legacy Electric Power System"	"The legacy electric power system is rapidly undergoing drastic changes towards modernization and enhanced sustainability. Thus, information and communications technologies (ICT) are being employed to enable various energy conservation and demand-response initiatives, fossil-fueled power plants are being replaced with large farms of renewable energy resources, such scattered energy resources as rooftop photovoltaic (PV) systems are being integrated in large numbers with electric power distribution networks, and new types of load such as plug-in electric vehicles are emerging. Concurrently, energy storage is being considered and researched as an effective tool for enhancing the consistency and predictability of the generation-demand portfolios of the future. The aforementioned changes have sparked research and development (RD) activities towards smooth adoption of the new technologies and practices, and to enable a smooth transition from the legacy power system into the power system of future. However, RD activities have thus far focused, almost exclusively, on the high-voltage (i.e., generation/transmission) level of the power system, while parallel developments are also required for the medium- and low-voltage (i.e., distribution) levels of the power system. This proposed research program aims to address the gap by contributing to the RD of the next generation electric power distribution networks, namely architectures, power-electronic energy conversion systems, and algorithms for operation and protection. This program will investigate and propose solutions for 1) enhanced employment of power electronics to improve reliability and quality of power delivery, fault current management, power-flow control, and more efficient integration of small- and medium-sized generators and energy storage systems, 2) next generation power-electronic converters for higher efficiency, power density, and reliability, and 3) employment of direct-current (DC) electricity for distribution and consumption of electric power. The proposed research program will train at least 13 HQP, consisting of 3 PhD, 2 MSc, and 8 undergraduate students, thus serving as a vital and timely contributor of highly-skilled engineers and future innovators for the Canadian power engineering industry and academia. Further, the advancement of knowledge through this research program and the quality publications that it will produce will enhance the profile of the Canadian academia at the global stage. ""635523,""Yazdani, Soroosh"
"634289"	"Ye, Qiang"	"High-Precision Localization in Mobile Wireless Networks"	"Localization is one of the key operations in mobile wireless networks. The Global Position System (GPS) has been widely used to locate mobile network nodes. However, GPS requires the line of sight to the satellites and stops working when the line of sight is not available (e.g. indoors or in a downtown canyon). In addition, GPS tends to consume much power, which is highly problematic for power-constrained devices such as smartphones and smartwatches. The problems associated with GPS have motivated many researchers to devise a variety of different localization methods for the scenarios in which GPS does not work well. Despite the fact that the state-of-the art non-GPS schemes can completely or partially avoid the use of GPS, they still suffer from several serious problems such as low localization precision and high computation complexity. In this project, we plan to explore varied non-GPS techniques to achieve high-precision mobile localization in both indoor and outdoor environments. Specifically, we plan to work on three types of localization mechanisms for the scenarios in which GPS does not work well: Refined Connectivity/Distance-based Localization, Sign-based Online Localization, and Localization Based on Crowdsourced Fingerprints. Refined Connectivity/Distance-based Localization is a generic method that can be applied to both indoor and outdoor environments. Sign-based Online Localization works well in a downtown canyon, where there are many signs and the line of sight to the satellites is often blocked by skyscrapers. Localization Based on Crowdsourced Fingerprints is tailored for pedestrians carrying power-constrained devices in a non-rural environment. One of our short-term objectives is to come up with the appropriate localization mechanisms for mobile wireless networks. The experimental results are expected to result in a series of high-impact publications in top-notch journals and conference proceedings. When possible, the proposed methods will be commercialized through the collaboration with industry partners. The other short-term objective is to train both undergraduate and graduate students through the proposed program, preparing them for their future academic or industry positions. Our long-term goal is to develop a first-class research lab that focuses on the research area of communication networks. We hope that, with the support from NSERC and other funding agencies, our lab will ultimately grow into a network research center that is competitive at the national/international level.""626305,""Ye, Tao"
"637325"	"Yongacoglu, Abbas"	"Enabling Techniques for Future Wireless Communications"	"We propose to continue our ongoing research activities in the physical layer area of future wireless communications systems. We will investigate, design, analyze and evaluate enabling techniques that can contribute to the evolution of future wireless systems. We view our research efforts in 3 directions. Our first research topic focuses on modeling and performance evaluation of heterogeneous networks (HetNets) made up of macro and small cells. We are working on developing accurate user and base station distribution models that take into account propagation as well as user behavior related aspects. These models will allow us to calculate the capacity of different network configurations under varying interference, channel conditions, traffic density, etc. and to design practical and efficient transmission/reception techniques and resource allocation strategies. The second topic is concerned with the role of drone base stations (BSs) in cellular networks. Drones are low altitude unmanned aerial vehicles (UAV). Drone-BSs can serve mobile cells of any size and thus help the ground network of BSs in providing high data rate coverage whenever there is an excessive need in space and time, especially when this excessive demand occurs in a rather difficult-to-predict manner. By proper placement and networking of the drone base stations we will improve capacity, throughput, reliability and power consumption of the system. The third direction deals with various issues related with full-duplex (FD) systems. Until recently receiving and transmitting the signal at the same time interval and frequency band was not possible. Recent developments on antenna technology and signal processing techniques now make FD feasible. We are investigating different structures, i.e. relaying, network coding, relay selection, where FD can be beneficial and we are developing analytical tools for their system performance evaluations.""622122,""Yoo, DaEun"
"637151"	"Youssef, Mohamed"	"Innovative Power Electronics Components, Control Systems, and Motor Design Techniques for Future Smart Power Drive Systems"	"In the era of society integration through the concept of Internet of Things (IoT), there is relentless need for revolutionary power electronic systems. Sound integration of subsystems and services, ranging from smart grid to health organizations to military sites and critical loads, is a must. A key to achieve this is a research program that will introduce an information system back boned by a power circuits designed in a novel way to reduce its cost but keep its reliability up to the standards. This research program represents a multidisciplinary technology that includes but not limited to; power semiconductors, control theory, microcomputers, electrical machine design, and converter circuits with very large scale integration (VLSI) capabilities. An important aspect of the power electronics applications in any society is about the utmost use of electricity. For example, the use of electric cars achieves a greener society; this would have been impossible without power electronic converters. Novel power electronic circuits will be developed, using the most recent advanced semiconductors like Gallium Nitride (GaN), Silicon Carbide (SiC), and hybrid ACCUFETs. The cost of these semiconductors and their associated control circuits is falling with time, while the cost of bulky passive components remains the same. This drives the power electronics engineer to provide a semiconductor based solution rather than a traditional passive solution. An example is the use of GaN converters in electric vehicles as they operate at very high frequencies, which reduces the size of inductors and capacitors; making them lighter and more compact. New semiconductors require driving circuits with reduced energy consumption, which is another merit. The objectives of the program are to: (1) design power electronic circuits for drive applications with new semiconductors; (2) Investigate the use of resonant converters to make the build light and compact; (3) The integration of these drive systems into the IoT will require a novel powering scheme for these backbone communication circuits, namely Power over the Ethernet, (4) Control of power electronic circuits using the state of the art digital controllers, and investigation of fuzzy and neural based controllers, and (5) Design of the so called Hybrid Motors, where mixed laminations from cost effective induction motors are blended carefully into the expensive permanent magnet motors to achieve the utmost cost effectiveness of future motors without degrading its performance. This research program will not only focus on better options for the power drive systems, but also micro controller based processing and fabrication of these new materials, in which the applicant possesses extensive industrial experience. Undoubtedly, this research program will prevail as a futuristic electrical engineering discipline.""618732,""Youssef, Mohamed"
"637311"	"Yu, Wei"	"Massive Connectivity and Massive Content Distribution for Future Wireless Access"	"The unprecedented growth of wireless communications in the past decade is about to undergo another technological revolution. Future wireless cellular network will not only enable ultra-high-speed wireless access anytime, anywhere, but also provide connectivity through millions of devices, in particular, sensors and actuators that will help realize the visions of smart homes, smart cars, and smart cities. The proposed research program will tackle the technological challenges of future wireless access driven by the multiple diverging requirements for enhanced broadband and massive connectivity. The proposal will investigate fundamental limits of massive content distribution, particularly in recognition of the opportunities to take advantage of storage and caching capabilities at the network edge in order to break the fundamental barrier of multicell interference in wireless cellular networks. The opportunities to cache user content at both the base-stations (BSs) and remote terminals lead to a content-centric view of the network, in which the interactions between parameters such as file popularity, user association strategy, and cooperation and interference mitigation techniques give rise to challenging system optimization problems that the proposed research program will aim to solve. Further, the opportunities for wireless multicast leads to the possibility of coded caching, which gives rise to new content delivery strategies that can significantly reduce the network backhaul requirement. The proposed research will carefully model these new transmission modes that are enabled by cloud computing and caching capabilities, and will bring caching, BS cooperation and interference cancellation opportunity into the wireless interference channel model in order to arrive at a new understanding of the content delivery capacity of cellular networks. Future wireless networks will not only significantly enhance broadband access, but also offer connectivity across massive number of devices for environmental monitoring, sensing, and control applications. These novel modes of communications present significant new challenges in system design, especially for device identification, channel estimation, and for short-packet transmission in delay sensitive applications. The proposed research will recognize the sporadic nature of device transmissions and examine the theoretical foundation of massive connectivity by casting active device identification as a sparse recovery problem. The proposed study will investigate the interplay between compressed sensing and information theory in order to illustrate the roles of physical layer technologies such as massive MIMO, cloud processing, and cooperative communication in massive device communication, and to understand how cellular networks should be engineered to meet the diverse user requirements for future wireless access.""642640,""Yu, Wei"
"634798"	"Yu, Xiaohui"	"Managing and Mining Urban Spatio-Temporal Data"	"The unifying theme of the proposed research is to address the challenges arising from managing and mining urban spatio-temporal data. The wide-spread use of smart phones, sensors and other IoT devices in cities world-wide has given rise to a huge volume of urban spatio-temporal data, which often present themselves as high-velocity continuous streams with considerable noise and uncertainties. These data record a vast amount of movement information of people, vehicles, etc., and serve as the backbone of a variety of applications, such as urban traffic management, road network planning, location-based services, and environmental monitoring. While governments, businesses and other organizations have realized the tremendous value of urban spatio-temporal data, how to effectively tap into this potential is still an elusive goal. Some of the questions we strive to answer in this research include: How to efficiently process continuous queries (such as k nearest-neighbor queries and partial route matching queries) and provide answers in real-time over spatio-temporal streams? How do develop an embedding model to learn human mobility patterns from personal, spatial and temporal aspects in an integrated manner?How to construct a probabilistic model to capture the underlying intention of movement so that we could have a deeper insight into the human mobility patterns? The results will advance the state of the art in the field of spatio-temporal data management and mining, providing new ways of indexing and querying data in a distributed fashion as well as offering novel probabilistic models for understanding human mobility patterns.This project has the potential to bring significant benefit to governments, businesses and ordinary people, through a variety of new applications and services enabled by the research results.""624190,""Yu, Xiaozhuo"
"635104"	"Yu, Yaoliang"	"Computational Foundations of Machine Learning in the Era of Big Data"	"Machine learning (ML), a field that develops software that can improve itself through learning and experience, has been largely driven by the availability of historical data, and by the need to develop efficient and scalable algorithms and supporting theories. Conversely, the success of ML in science, engineering, and commerce, along with technological innovations, has led to an unprecedented growth and enthusiasm in big data collection, thereby redefining computational efficiency and inviting system solutions. For example, the recent AlphaGo system of Deepmind that beats top human Go players needed 1900 CPUs and 280 GPUs to carry out the computation. How to balance computation with communication in this vast distributed cluster, without compromising system throughput or correctness? On the other hand, a small startup developing a mobile app may not afford the same computational power as Google, hence often has to turn into primitive solutions. How to build an algorithmic framework for ML that provides ''knobs'' to adjust the computational load, with explicit, controllable loss on the accuracy? Meeting such diverse computational needs in the big data era has thus been a grand challenge for the ML field. We attempt to address such computational challenge in ML and big data, through three complementary objectives: (1) Real problems are hard, but also structured. Over the years the importance of designing statistical methodologies and computational algorithms that can exploit certain structure in data and model has become evident. Encouraged by our previous work on sparsity and low-rankness, we propose to investigate two additional structures that are common in ML applications: monotonicity and multi-modality (in the tensor format), and developing efficient algorithms that benefit from the presence of such structures. (2) Data is always noisy and full of random fluctuations, hence diminishing the need of obtaining exact or even high-precision solutions in ML. Approximate computation, if done properly, can significantly reduce the computation time in ML. We initiate a systematic study of the tradeoffs of approximate computation in ML, from ''downgrading'' computationally expensive programs to simpler and cheaper ones, to ''optimally"" smooth nondifferentiable functions, and to attach measures of nonconvexity to nonconvex functions. (3) Distributed computation has become the norm in handling big datasets. We propose the Bounded Asynchronous Protocol (BAP) to better balance communication and computation in distributed ML systems, and we continue to investigate the speedups and convergence guarantees of typical ML iterative algorithms under BAP and possibly less stringent convex or smooth assumptions. Our work will further advance the computational theory and practice in ML, and the resulting algorithms and system will be fundamental for analyzing big datasets using ML methodologies.""623808,""Yu, Yingchao"
"637291"	"Zhang, Qijun"	"Advanced Statistical Modeling and Optimization Technologies for Yield-Driven Design of High-Frequency Electronic Circuits"	"The objective of this research is to develop next generation technologies for statistical modeling and optimization of high-frequency electronic components and packages in wireless and wireline communication systems. With increasing functionality, complexity, signal speed and bandwidth in wireless and wireline communication systems, the design specifications for the building block components and subsystems become more stringent. This in turn makes the effects from unavoidable manufacturing tolerances and process uncertainties in components and subsystems more pronounced in the overall system performance, affecting production yield and posing challenges in design. Statistical modeling and yield optimization directly taking into account the uncertainties in parameters as part of the design process become important. However, conventional statistical modeling and yield optimization techniques that are mature enough for equivalent circuit based design are not effective for todays electromagnetic (EM)/multiphysics-based design because of the prohibitive computational cost. A new statistical modeling and optimization paradigm is necessary to enable EM and multiphysics based yield-driven design. The proposed research will address these challenges. Built on top of our recent advances in statistical neuro-space mapping and cognition driven technologies for RF/microwave design, this research explores new frontiers in EM/multiphysics based statistical modeling and yield optimization. This research will develop new optimization algorithms exploiting fast parametric EM model with or without coarse engineering models, dramatically cutting the computational expenses of yield optimization of EM structures. The research will create unified parametric modeling algorithms for EM structures combining space mapping and knowledge-based neural network models. New dynamic statistical neuro-space mapping algorithm will be developed for statistical modeling of nonlinear devices covering the statistical behavior of both high- and low-frequency (such as trapping effects) responses. The research also aims to open a new frontier in modeling and design by extending EM-based statistical design to multiphysics-based statistical design. A new class of space mapping optimization algorithms with mapping between EM space and multiphysics space for microwave optimization will be introduced. The long term direction is a unified EM/multiphysics based methodology for fast and accurate statistical modeling and yield optimization for next generation high-frequency electronic design. The long-term impact will be faster design cycle, lower design cost, better design quality and increased manufacturing yield. It contributes to creating new knowledge and training of highly qualified technical personnel in areas of high-frequency electronic design. ""622795,""Zhang, RuoYu"
"621730"	"Zhang, Zichen"	"Qubec"	"CANADA"
"635110"	"Zilles, Sandra"	"Models and algorithms for interactive machine learning applied to formal languages and geometric concepts"	"A central problem in applied machine learning is that often data is required in larger quantities than are available or affordable, for instance, when costly lab experiments have to be conducted to generate data, as is often the case in biomedical research, or when patterns concerning a single user of a computer-based system (rather than patterns concerning a large pool of users) have to be learned. My proposed research in the field of computational learning theory addresses this problem by means of the theory of interactive machine learning. Interaction here means that the learning algorithm or the environment actively controls which information is exchanged about the target object to be learned. Interactive machine learning is of high relevance for a variety of applications, e.g., those in which a human interacts with and is observed by a learning system. My objective is to design and analyze formal models of interactive learning and to develop algorithmic techniques that can efficiently solve complex learning problems with less data than is currently possible. The models I propose stand in sharp contrast to models in which the learner receives data chosen at random according to some data distribution; in particular they aim at exploiting structural properties of the potential target objects in order to reduce the number of data points needed for learning in comparison to the case when data is sampled at random. Concerning the target objects for learning, I will focus on cases in which formal languages or geometric concepts are to be learned. The classes of formal languages I plan to study are variants of the so-called pattern languages. Pattern languages have been studied in computational learning theory for over 35 years, due to their appealingly simple definition, their interesting structural and language-theoretic properties, as well as their numerous applications in areas such as bioinformatics, automatic program synthesis, database theory, and pattern matching. They are well-suited to a study of interactive learning on text data. Geometric concepts, such as (unions of) axis-aligned boxes in n dimensions, linear halfspaces, etc., have enjoyed great popularity in computational learning theory since the early days of the field, partly because of the success of linear models in machine learning, but partly also because geometric concepts in the low-dimensional case provide us with an intuitive interpretation of successful learning algorithms as well as of data sets that are useful for learning. My suggestion is to leverage such intuitive interpretation for advancing our understanding of new (and not yet fully understood) models of interactive learning.""642729,""Zilles, Sandra"
"635057"	"Zouaq, Amal"	"Knowledge Graph Mining for the Linked Open Data Cloud"	"The development of structured data on the Web, through the linked data paradigm (aka the Web of data), has received considerable attention during the last few years, especially with the adoption of knowledge graphs by companies such as Google and Microsoft to enhance their search engines. The Linked Open Data Cloud (LOD), a set of interconnected data sets across domains, represents the latest developments of the Web of data. We have witnessed a huge development in the number of published RDF datasets. One of the main interests of the LOD is its potential to offer semantic search capabilities such as query answering with direct answers instead of sets of documents, and the ability to a) find answers from various knowledge sources and b) infer answers through reasoning mechanisms. However, the quantity of published datasets poses new challenges as there aren't any established mechanisms to ensure their quality. In particular, the deluge of RDF data without proper schemas and ontological models is of little interest for efficient query answering. Another challenge is the representation and coverage of current Web content, especially for domain knowledge. Despite the growth of the Web of data, the majority of Web content is still represented in unstructured formats and texts. Thus there is a necessity to expand the current LOD with data and schemas of good quality; to develop methods and tools that learn ontological schemas and knowledge bases from unstructured Web content; to measure and evaluate LOD quality and design correction and completion strategies for current Web structured data; and to develop use cases that concretely show the interest of the Web of data for query answering. In this discovery grant, we will focus on one particular resource on the Web which is the cross-domain encyclopedia Wikipedia. In the context of the Web of data, Wikipedia is linked to one of the main resources on the LOD, DBpedia, which is the RDF representation of Wikipedia based on Wikipedia infoboxes and categories. Being a hub on the LOD, DBpedia has become a central resource for several semantic analysis tasks. In particular, DBpedia is the backbone of several new industrial and academic semantic annotation services (e.g. IBM's Alchemy, DBpedia Spotlight) which are used to tag Web content and recognize entities and concepts in text. However, DBpedia suffers from the same quality problems previously described in terms of coverage of Wikipedia content, lack of proper ontological schema, and factual errors. The objective of this program is to develop tools and methods to correct, axiomatize and expand the DBpedia knowledge base. We will also demonstrate the interest of the learned knowledge base for better semantic annotation and query answering in the context of the Semantic Web.""631478,""Zovkic, Iva"
